From e647877e03bf040124ce43f11dc577467cc51f8b Mon Sep 17 00:00:00 2001
From: Michael Still <mikal@stillhq.com>
Date: Tue, 30 Sep 2014 10:11:29 +1000
Subject: [PATCH] Add proposed specs from the Juno cycle

Add proposed specs from the Juno cycle. I've done this because I
worry about the case where a future developer wants to pick up
something dropped by a previous developer, but has trouble
finding previous proposed specifications on the topic.

For spec filenames where there was more than one review which
proposed adding that file, I have added -proposal-[12345] to the
end of the filename.

Change-Id: Ie7dd7ed3550ab5a76d502ae9fdbd5566c0537813
---
 README.rst                                         |   3 +-
 doc/source/index.rst                               |   9 +
 .../Audit-compute-node-on-controller-recovery.png  | 280 ++++++++
 .../juno/proposed/action-type-aware-scheduling.rst | 196 ++++++
 .../proposed/add-an-index-column-to-nova-list.rst  | 164 +++++
 specs/juno/proposed/add-app-lock.rst               | 140 ++++
 .../proposed/add-delete-node-to-nova-manage.rst    | 162 +++++
 .../proposed/add-delete-on-termination-option.rst  | 260 ++++++++
 .../proposed/add-extra-specs-to-flavor-calls.rst   | 202 ++++++
 specs/juno/proposed/add-force-detach-to-nova.rst   | 216 ++++++
 .../juno/proposed/add-ironic-boot-mode-filters.rst | 127 ++++
 .../add-support-for-cinder-scheduler-hints.rst     | 151 +++++
 specs/juno/proposed/add-support-for-cpu-hotadd.rst | 297 +++++++++
 specs/juno/proposed/add-tags-for-os-resources.rst  | 258 +++++++
 .../proposed/add-transport-support-to-iscsi.rst    | 131 ++++
 specs/juno/proposed/add-usb-controller.rst         | 162 +++++
 specs/juno/proposed/add-useful-metrics.rst         | 200 ++++++
 .../proposed/add-utilization-based-weighers.rst    | 204 ++++++
 specs/juno/proposed/api-microversions-alt.rst      | 374 +++++++++++
 specs/juno/proposed/api-microversions.rst          | 740 +++++++++++++++++++++
 .../proposed/associate-lru-fixed-ip-address.rst    | 145 ++++
 .../audit-compute-node-on-controller-recovery.rst  | 144 ++++
 .../auto-disable-and-enable-hypervisor.rst         | 196 ++++++
 specs/juno/proposed/cache-qos-monitoring.rst       | 151 +++++
 .../change-instances-ownership-proposal-1.rst      | 168 +++++
 .../change-instances-ownership-proposal-2.rst      | 234 +++++++
 specs/juno/proposed/cinder-events.rst              | 217 ++++++
 .../cold-migrations-to-conductor-final.rst         | 153 +++++
 specs/juno/proposed/common-nova-metadata-cache.rst | 164 +++++
 specs/juno/proposed/compute-image-precache.rst     | 303 +++++++++
 specs/juno/proposed/compute-node-metrics-api.rst   | 149 +++++
 specs/juno/proposed/configure-tcp-keepalive.rst    | 135 ++++
 specs/juno/proposed/console-tls-mode.rst           | 147 ++++
 specs/juno/proposed/cpu-allocation-per-flavor.rst  | 229 +++++++
 .../juno/proposed/creating-hyperv-ha-instances.rst | 189 ++++++
 specs/juno/proposed/data-transfer-plugin.rst       | 148 +++++
 .../datastore-image-cache-update-improvements.rst  | 197 ++++++
 .../proposed/db-sync-models-with-migrations.rst    | 217 ++++++
 specs/juno/proposed/default-quota-flavor.rst       | 216 ++++++
 specs/juno/proposed/default-quotas.rst             | 229 +++++++
 specs/juno/proposed/default-schedule-zones.rst     | 150 +++++
 specs/juno/proposed/deprecate-baremetal-driver.rst | 229 +++++++
 specs/juno/proposed/dnsmasq-options-config.rst     | 146 ++++
 specs/juno/proposed/docker-hypervisor-plugin.rst   | 225 +++++++
 specs/juno/proposed/domain-quota-driver-api.rst    | 411 ++++++++++++
 specs/juno/proposed/domain-quota-driver-v3-api.rst | 341 ++++++++++
 .../juno/proposed/domain-quota-manage-commands.rst | 195 ++++++
 specs/juno/proposed/dynamic-adjust-disk-qos.rst    | 189 ++++++
 specs/juno/proposed/dynamic-logging.rst            | 148 +++++
 specs/juno/proposed/ec2-volume-filtering.rst       | 141 ++++
 specs/juno/proposed/ec2-volume-type.rst            | 124 ++++
 specs/juno/proposed/emc-sdc-libvirt-driver.rst     | 168 +++++
 ...enchancement-virtio-scsi-support-for-volume.rst | 143 ++++
 .../proposed/encrypted-live-migration-nova.rst     | 281 ++++++++
 specs/juno/proposed/exclude-cbs-in-snapshot.rst    | 189 ++++++
 specs/juno/proposed/extends-nova-hypervisor.rst    | 171 +++++
 .../extension-level-policy-as-default-v3-api.rst   | 150 +++++
 specs/juno/proposed/flavor-cpu-overcommit.rst      | 113 ++++
 specs/juno/proposed/flavor-quota-memory.rst        | 120 ++++
 specs/juno/proposed/freebsd-compute-node.rst       | 143 ++++
 specs/juno/proposed/generate-vmstates-graph.rst    | 170 +++++
 .../proposed/get-floatingip-by-all-tenants.rst     | 155 +++++
 .../juno/proposed/get-lock-status-of-instance.rst  | 171 +++++
 specs/juno/proposed/gpfs-instance-store.rst        | 130 ++++
 .../proposed/horizontally-scalable-scheduling.rst  | 248 +++++++
 specs/juno/proposed/host-metric-hook.rst           | 207 ++++++
 specs/juno/proposed/host-servers-live-migrate.rst  | 156 +++++
 specs/juno/proposed/hot-resize.rst                 | 430 ++++++++++++
 specs/juno/proposed/hyper-v-generation-2-vms.rst   | 146 ++++
 specs/juno/proposed/hyper-v-host-power-actions.rst | 116 ++++
 specs/juno/proposed/hyper-v-remotefx.rst           | 159 +++++
 specs/juno/proposed/hyper-v-rescue.rst             | 119 ++++
 .../juno/proposed/hyper-v-smbfs-volume-support.rst | 146 ++++
 specs/juno/proposed/idempotentcy-client-token.rst  | 276 ++++++++
 specs/juno/proposed/image-precacher.rst            | 161 +++++
 specs/juno/proposed/image-upload-module-plugin.rst | 183 +++++
 .../proposed/incremental-instance-snapshot.rst     | 221 ++++++
 specs/juno/proposed/instance-boot.rst              | 113 ++++
 specs/juno/proposed/instance-level-snapshots.rst   | 169 +++++
 specs/juno/proposed/instance-tasks-api.rst         | 278 ++++++++
 specs/juno/proposed/internal-dns-resolution.rst    | 297 +++++++++
 specs/juno/proposed/isnot-operator.rst             | 118 ++++
 specs/juno/proposed/isolate-scheduler-db.rst       | 282 ++++++++
 specs/juno/proposed/keypair-x509-certificates.rst  | 182 +++++
 specs/juno/proposed/libvirt-hugepage.rst           | 134 ++++
 .../libvirt-linux-net-refactor-for-freebsd.rst     | 197 ++++++
 .../proposed/libvirt-multiple-image-backends.rst   | 181 +++++
 specs/juno/proposed/libvirt-ovs-use-usvhost.rst    | 162 +++++
 .../libvirt-separate-virt-types-to-classes.rst     | 131 ++++
 .../juno/proposed/libvirt-smbfs-volume-support.rst | 142 ++++
 .../proposed/libvirt-storpool-volume-attach.rst    | 128 ++++
 .../proposed/libvirt-support-tpm-passthrough.rst   | 126 ++++
 specs/juno/proposed/lock-free-quota-management.rst | 218 ++++++
 specs/juno/proposed/log-guidelines.rst             | 415 ++++++++++++
 specs/juno/proposed/log-translation-hints.rst      | 192 ++++++
 .../proposed/lvm-driver-for-shared-storage.rst     | 197 ++++++
 .../proposed/message-in-update-notifications.rst   | 120 ++++
 specs/juno/proposed/metadata-service-callbacks.rst | 219 ++++++
 .../proposed/metadata-service-network-info.rst     | 291 ++++++++
 .../juno/proposed/migrate-non-active-instances.rst | 140 ++++
 .../monitoring-ip-availability-proposal-1.rst      | 168 +++++
 .../monitoring-ip-availability-proposal-2.rst      | 152 +++++
 .../monitoring-ip-availability-proposal-3.rst      | 153 +++++
 specs/juno/proposed/multi-attach-volume.rst        | 154 +++++
 .../nested-quota-driver-api-proposal-1.rst         | 116 ++++
 .../nested-quota-driver-api-proposal-2.rst         | 218 ++++++
 .../nested-quota-driver-api-proposal-3.rst         | 266 ++++++++
 specs/juno/proposed/neutron-migration.rst          | 326 +++++++++
 ...virt-volume-driver-for-Huawei-SDSHypervisor.rst | 185 ++++++
 specs/juno/proposed/nic-state-aware-scheduling.rst | 183 +++++
 specs/juno/proposed/no-db-scheduler.rst            | 238 +++++++
 specs/juno/proposed/no-downward-resize.rst         | 152 +++++
 specs/juno/proposed/no-migration-resize.rst        |  96 +++
 specs/juno/proposed/nodename-in-pci-device.rst     | 208 ++++++
 .../proposed/normalize-scheduler-weights-2.rst     | 191 ++++++
 ...a-api-extension-to-list-available-resources.rst | 194 ++++++
 specs/juno/proposed/nova-api-policy.rst            | 226 +++++++
 .../nova-compute-multi-backend-support.rst         | 200 ++++++
 specs/juno/proposed/nova-ephemeral-cinder.rst      | 137 ++++
 .../nova-vmware-vcdriver-nfs-image-copy.rst        | 144 ++++
 specs/juno/proposed/online-schema-changes.rst      | 282 ++++++++
 .../proposed/online-volume-extend-extension.rst    | 230 +++++++
 .../only-allow-admins-to-do-local-delete.rst       | 153 +++++
 .../pci-device-capability-aware-scheduling.rst     | 167 +++++
 specs/juno/proposed/pci-extra-info.rst             | 193 ++++++
 specs/juno/proposed/pci-hotplug-juno.rst           | 218 ++++++
 specs/juno/proposed/pcs-support.rst                | 117 ++++
 specs/juno/proposed/per-flavor-quotas.rst          | 350 ++++++++++
 specs/juno/proposed/periodic-heartbeat.rst         | 118 ++++
 specs/juno/proposed/persist-scheduler-hints.rst    | 188 ++++++
 .../proposed/policy-based-scheduing-engine.rst     | 269 ++++++++
 specs/juno/proposed/projects-to-aggregate.rst      | 117 ++++
 specs/juno/proposed/pxe-boot-instance.rst          | 147 ++++
 specs/juno/proposed/quota-state-management.rst     | 236 +++++++
 specs/juno/proposed/ram-as-percentage.rst          | 151 +++++
 specs/juno/proposed/refactor-virt-capabilities.rst | 145 ++++
 specs/juno/proposed/remove-fakelibvirt.rst         | 153 +++++
 specs/juno/proposed/restrict-image-types.rst       | 126 ++++
 .../juno/proposed/restrict-instance-migration.rst  | 176 +++++
 specs/juno/proposed/rootwrap-daemon-mode.rst       | 147 ++++
 .../proposed/schedule-available-node-return.rst    | 118 ++++
 specs/juno/proposed/scheduler-host-az-caching.rst  | 117 ++++
 .../proposed/separate-stats-from-periodic-task.rst | 140 ++++
 .../juno/proposed/separated-policy-rule-v3-api.rst | 159 +++++
 specs/juno/proposed/server-snapshot-support.rst    | 293 ++++++++
 specs/juno/proposed/server_http_proxy.rst          | 196 ++++++
 specs/juno/proposed/set-vm-swapfile-location.rst   | 127 ++++
 specs/juno/proposed/shelf-snapshot-selection.rst   | 179 +++++
 specs/juno/proposed/simultaneous-server-group.rst  | 492 ++++++++++++++
 specs/juno/proposed/slow-queries.rst               | 260 ++++++++
 .../proposed/soft-affinity-for-server-group.rst    | 163 +++++
 specs/juno/proposed/solver-scheduler.rst           | 233 +++++++
 .../specify-number-of-cores-per-socket.rst         | 121 ++++
 specs/juno/proposed/spot-instances.rst             | 164 +++++
 specs/juno/proposed/standardize-client-params.rst  | 332 +++++++++
 ...e-optimization-for-multi-datastore-clusters.rst | 211 ++++++
 specs/juno/proposed/support-keystone-v3-api.rst    | 218 ++++++
 specs/juno/proposed/synchronous-read-support.rst   | 162 +++++
 .../proposed/tenant-aggregate-exclusive-filter.rst | 181 +++++
 specs/juno/proposed/thunderboost-proposal-1.rst    | 189 ++++++
 specs/juno/proposed/thunderboost-proposal-2.rst    | 189 ++++++
 specs/juno/proposed/thunderboost-proposal-3.rst    | 189 ++++++
 .../juno/proposed/transfer-instance-ownership.rst  | 138 ++++
 specs/juno/proposed/usb-hot-plug.rst               | 173 +++++
 specs/juno/proposed/usb-passthrough-proposal-1.rst | 352 ++++++++++
 specs/juno/proposed/usb-passthrough-proposal-2.rst | 215 ++++++
 .../usb-passthrough-with-usb-controller.rst        | 149 +++++
 specs/juno/proposed/usb-redirection.rst            | 209 ++++++
 .../juno/proposed/use-configdrive-with-ironic.rst  | 178 +++++
 specs/juno/proposed/use-glance-v2-api.rst          | 167 +++++
 specs/juno/proposed/use-physical-cdrom.rst         | 125 ++++
 specs/juno/proposed/user-project-metadata.rst      | 171 +++++
 .../username-in-nova-list-for-admin-purpose.rst    | 166 +++++
 specs/juno/proposed/v2-api-detailed-quotas.rst     | 208 ++++++
 .../proposed/v3-api-neutron-network-support.rst    | 220 ++++++
 .../validate-targethost-live-migration.rst         | 157 +++++
 .../validate-tenant-user-with-keystone.rst         | 215 ++++++
 specs/juno/proposed/vcpus-in-api.rst               | 142 ++++
 specs/juno/proposed/virt-image-transfer-layer.rst  | 218 ++++++
 specs/juno/proposed/virt-properties-object.rst     | 125 ++++
 specs/juno/proposed/virtio-scsi-settings.rst       | 195 ++++++
 specs/juno/proposed/vm-cpu-pinning-support.rst     | 164 +++++
 specs/juno/proposed/vmware-clone-image-handler.rst | 231 +++++++
 ...vmware-encrypt-vcenter-passwords-proposal-1.rst | 179 +++++
 ...vmware-encrypt-vcenter-passwords-proposal-2.rst | 209 ++++++
 .../proposed/vmware-resource-pool-enablement.rst   | 165 +++++
 specs/juno/proposed/vmware-vm-ref-refactor.rst     | 147 ++++
 .../proposed/vnc-configurable-share-policy.rst     | 168 +++++
 tests/test_titles.py                               |   9 +-
 189 files changed, 36025 insertions(+), 3 deletions(-)
 create mode 100644 specs/juno/proposed/Audit-compute-node-on-controller-recovery.png
 create mode 100644 specs/juno/proposed/action-type-aware-scheduling.rst
 create mode 100644 specs/juno/proposed/add-an-index-column-to-nova-list.rst
 create mode 100644 specs/juno/proposed/add-app-lock.rst
 create mode 100644 specs/juno/proposed/add-delete-node-to-nova-manage.rst
 create mode 100644 specs/juno/proposed/add-delete-on-termination-option.rst
 create mode 100644 specs/juno/proposed/add-extra-specs-to-flavor-calls.rst
 create mode 100644 specs/juno/proposed/add-force-detach-to-nova.rst
 create mode 100644 specs/juno/proposed/add-ironic-boot-mode-filters.rst
 create mode 100644 specs/juno/proposed/add-support-for-cinder-scheduler-hints.rst
 create mode 100644 specs/juno/proposed/add-support-for-cpu-hotadd.rst
 create mode 100644 specs/juno/proposed/add-tags-for-os-resources.rst
 create mode 100644 specs/juno/proposed/add-transport-support-to-iscsi.rst
 create mode 100644 specs/juno/proposed/add-usb-controller.rst
 create mode 100644 specs/juno/proposed/add-useful-metrics.rst
 create mode 100644 specs/juno/proposed/add-utilization-based-weighers.rst
 create mode 100644 specs/juno/proposed/api-microversions-alt.rst
 create mode 100644 specs/juno/proposed/api-microversions.rst
 create mode 100644 specs/juno/proposed/associate-lru-fixed-ip-address.rst
 create mode 100644 specs/juno/proposed/audit-compute-node-on-controller-recovery.rst
 create mode 100644 specs/juno/proposed/auto-disable-and-enable-hypervisor.rst
 create mode 100644 specs/juno/proposed/cache-qos-monitoring.rst
 create mode 100644 specs/juno/proposed/change-instances-ownership-proposal-1.rst
 create mode 100644 specs/juno/proposed/change-instances-ownership-proposal-2.rst
 create mode 100644 specs/juno/proposed/cinder-events.rst
 create mode 100644 specs/juno/proposed/cold-migrations-to-conductor-final.rst
 create mode 100644 specs/juno/proposed/common-nova-metadata-cache.rst
 create mode 100644 specs/juno/proposed/compute-image-precache.rst
 create mode 100644 specs/juno/proposed/compute-node-metrics-api.rst
 create mode 100644 specs/juno/proposed/configure-tcp-keepalive.rst
 create mode 100644 specs/juno/proposed/console-tls-mode.rst
 create mode 100644 specs/juno/proposed/cpu-allocation-per-flavor.rst
 create mode 100644 specs/juno/proposed/creating-hyperv-ha-instances.rst
 create mode 100644 specs/juno/proposed/data-transfer-plugin.rst
 create mode 100644 specs/juno/proposed/datastore-image-cache-update-improvements.rst
 create mode 100644 specs/juno/proposed/db-sync-models-with-migrations.rst
 create mode 100644 specs/juno/proposed/default-quota-flavor.rst
 create mode 100644 specs/juno/proposed/default-quotas.rst
 create mode 100644 specs/juno/proposed/default-schedule-zones.rst
 create mode 100644 specs/juno/proposed/deprecate-baremetal-driver.rst
 create mode 100644 specs/juno/proposed/dnsmasq-options-config.rst
 create mode 100644 specs/juno/proposed/docker-hypervisor-plugin.rst
 create mode 100644 specs/juno/proposed/domain-quota-driver-api.rst
 create mode 100644 specs/juno/proposed/domain-quota-driver-v3-api.rst
 create mode 100644 specs/juno/proposed/domain-quota-manage-commands.rst
 create mode 100644 specs/juno/proposed/dynamic-adjust-disk-qos.rst
 create mode 100644 specs/juno/proposed/dynamic-logging.rst
 create mode 100644 specs/juno/proposed/ec2-volume-filtering.rst
 create mode 100644 specs/juno/proposed/ec2-volume-type.rst
 create mode 100644 specs/juno/proposed/emc-sdc-libvirt-driver.rst
 create mode 100644 specs/juno/proposed/enchancement-virtio-scsi-support-for-volume.rst
 create mode 100644 specs/juno/proposed/encrypted-live-migration-nova.rst
 create mode 100644 specs/juno/proposed/exclude-cbs-in-snapshot.rst
 create mode 100644 specs/juno/proposed/extends-nova-hypervisor.rst
 create mode 100644 specs/juno/proposed/extension-level-policy-as-default-v3-api.rst
 create mode 100644 specs/juno/proposed/flavor-cpu-overcommit.rst
 create mode 100644 specs/juno/proposed/flavor-quota-memory.rst
 create mode 100644 specs/juno/proposed/freebsd-compute-node.rst
 create mode 100644 specs/juno/proposed/generate-vmstates-graph.rst
 create mode 100644 specs/juno/proposed/get-floatingip-by-all-tenants.rst
 create mode 100644 specs/juno/proposed/get-lock-status-of-instance.rst
 create mode 100644 specs/juno/proposed/gpfs-instance-store.rst
 create mode 100644 specs/juno/proposed/horizontally-scalable-scheduling.rst
 create mode 100644 specs/juno/proposed/host-metric-hook.rst
 create mode 100644 specs/juno/proposed/host-servers-live-migrate.rst
 create mode 100644 specs/juno/proposed/hot-resize.rst
 create mode 100644 specs/juno/proposed/hyper-v-generation-2-vms.rst
 create mode 100644 specs/juno/proposed/hyper-v-host-power-actions.rst
 create mode 100644 specs/juno/proposed/hyper-v-remotefx.rst
 create mode 100644 specs/juno/proposed/hyper-v-rescue.rst
 create mode 100644 specs/juno/proposed/hyper-v-smbfs-volume-support.rst
 create mode 100644 specs/juno/proposed/idempotentcy-client-token.rst
 create mode 100644 specs/juno/proposed/image-precacher.rst
 create mode 100644 specs/juno/proposed/image-upload-module-plugin.rst
 create mode 100644 specs/juno/proposed/incremental-instance-snapshot.rst
 create mode 100644 specs/juno/proposed/instance-boot.rst
 create mode 100644 specs/juno/proposed/instance-level-snapshots.rst
 create mode 100644 specs/juno/proposed/instance-tasks-api.rst
 create mode 100644 specs/juno/proposed/internal-dns-resolution.rst
 create mode 100644 specs/juno/proposed/isnot-operator.rst
 create mode 100644 specs/juno/proposed/isolate-scheduler-db.rst
 create mode 100644 specs/juno/proposed/keypair-x509-certificates.rst
 create mode 100644 specs/juno/proposed/libvirt-hugepage.rst
 create mode 100644 specs/juno/proposed/libvirt-linux-net-refactor-for-freebsd.rst
 create mode 100644 specs/juno/proposed/libvirt-multiple-image-backends.rst
 create mode 100644 specs/juno/proposed/libvirt-ovs-use-usvhost.rst
 create mode 100644 specs/juno/proposed/libvirt-separate-virt-types-to-classes.rst
 create mode 100644 specs/juno/proposed/libvirt-smbfs-volume-support.rst
 create mode 100644 specs/juno/proposed/libvirt-storpool-volume-attach.rst
 create mode 100644 specs/juno/proposed/libvirt-support-tpm-passthrough.rst
 create mode 100644 specs/juno/proposed/lock-free-quota-management.rst
 create mode 100644 specs/juno/proposed/log-guidelines.rst
 create mode 100644 specs/juno/proposed/log-translation-hints.rst
 create mode 100644 specs/juno/proposed/lvm-driver-for-shared-storage.rst
 create mode 100644 specs/juno/proposed/message-in-update-notifications.rst
 create mode 100644 specs/juno/proposed/metadata-service-callbacks.rst
 create mode 100644 specs/juno/proposed/metadata-service-network-info.rst
 create mode 100644 specs/juno/proposed/migrate-non-active-instances.rst
 create mode 100644 specs/juno/proposed/monitoring-ip-availability-proposal-1.rst
 create mode 100644 specs/juno/proposed/monitoring-ip-availability-proposal-2.rst
 create mode 100644 specs/juno/proposed/monitoring-ip-availability-proposal-3.rst
 create mode 100644 specs/juno/proposed/multi-attach-volume.rst
 create mode 100644 specs/juno/proposed/nested-quota-driver-api-proposal-1.rst
 create mode 100644 specs/juno/proposed/nested-quota-driver-api-proposal-2.rst
 create mode 100644 specs/juno/proposed/nested-quota-driver-api-proposal-3.rst
 create mode 100644 specs/juno/proposed/neutron-migration.rst
 create mode 100644 specs/juno/proposed/new-libvirt-volume-driver-for-Huawei-SDSHypervisor.rst
 create mode 100644 specs/juno/proposed/nic-state-aware-scheduling.rst
 create mode 100644 specs/juno/proposed/no-db-scheduler.rst
 create mode 100644 specs/juno/proposed/no-downward-resize.rst
 create mode 100644 specs/juno/proposed/no-migration-resize.rst
 create mode 100644 specs/juno/proposed/nodename-in-pci-device.rst
 create mode 100644 specs/juno/proposed/normalize-scheduler-weights-2.rst
 create mode 100644 specs/juno/proposed/nova-api-extension-to-list-available-resources.rst
 create mode 100644 specs/juno/proposed/nova-api-policy.rst
 create mode 100644 specs/juno/proposed/nova-compute-multi-backend-support.rst
 create mode 100644 specs/juno/proposed/nova-ephemeral-cinder.rst
 create mode 100644 specs/juno/proposed/nova-vmware-vcdriver-nfs-image-copy.rst
 create mode 100644 specs/juno/proposed/online-schema-changes.rst
 create mode 100644 specs/juno/proposed/online-volume-extend-extension.rst
 create mode 100644 specs/juno/proposed/only-allow-admins-to-do-local-delete.rst
 create mode 100644 specs/juno/proposed/pci-device-capability-aware-scheduling.rst
 create mode 100644 specs/juno/proposed/pci-extra-info.rst
 create mode 100644 specs/juno/proposed/pci-hotplug-juno.rst
 create mode 100644 specs/juno/proposed/pcs-support.rst
 create mode 100644 specs/juno/proposed/per-flavor-quotas.rst
 create mode 100644 specs/juno/proposed/periodic-heartbeat.rst
 create mode 100644 specs/juno/proposed/persist-scheduler-hints.rst
 create mode 100644 specs/juno/proposed/policy-based-scheduing-engine.rst
 create mode 100644 specs/juno/proposed/projects-to-aggregate.rst
 create mode 100644 specs/juno/proposed/pxe-boot-instance.rst
 create mode 100644 specs/juno/proposed/quota-state-management.rst
 create mode 100644 specs/juno/proposed/ram-as-percentage.rst
 create mode 100644 specs/juno/proposed/refactor-virt-capabilities.rst
 create mode 100644 specs/juno/proposed/remove-fakelibvirt.rst
 create mode 100644 specs/juno/proposed/restrict-image-types.rst
 create mode 100644 specs/juno/proposed/restrict-instance-migration.rst
 create mode 100644 specs/juno/proposed/rootwrap-daemon-mode.rst
 create mode 100644 specs/juno/proposed/schedule-available-node-return.rst
 create mode 100644 specs/juno/proposed/scheduler-host-az-caching.rst
 create mode 100644 specs/juno/proposed/separate-stats-from-periodic-task.rst
 create mode 100644 specs/juno/proposed/separated-policy-rule-v3-api.rst
 create mode 100644 specs/juno/proposed/server-snapshot-support.rst
 create mode 100644 specs/juno/proposed/server_http_proxy.rst
 create mode 100644 specs/juno/proposed/set-vm-swapfile-location.rst
 create mode 100644 specs/juno/proposed/shelf-snapshot-selection.rst
 create mode 100644 specs/juno/proposed/simultaneous-server-group.rst
 create mode 100644 specs/juno/proposed/slow-queries.rst
 create mode 100644 specs/juno/proposed/soft-affinity-for-server-group.rst
 create mode 100644 specs/juno/proposed/solver-scheduler.rst
 create mode 100644 specs/juno/proposed/specify-number-of-cores-per-socket.rst
 create mode 100644 specs/juno/proposed/spot-instances.rst
 create mode 100644 specs/juno/proposed/standardize-client-params.rst
 create mode 100644 specs/juno/proposed/storage-optimization-for-multi-datastore-clusters.rst
 create mode 100644 specs/juno/proposed/support-keystone-v3-api.rst
 create mode 100644 specs/juno/proposed/synchronous-read-support.rst
 create mode 100644 specs/juno/proposed/tenant-aggregate-exclusive-filter.rst
 create mode 100644 specs/juno/proposed/thunderboost-proposal-1.rst
 create mode 100644 specs/juno/proposed/thunderboost-proposal-2.rst
 create mode 100644 specs/juno/proposed/thunderboost-proposal-3.rst
 create mode 100644 specs/juno/proposed/transfer-instance-ownership.rst
 create mode 100644 specs/juno/proposed/usb-hot-plug.rst
 create mode 100644 specs/juno/proposed/usb-passthrough-proposal-1.rst
 create mode 100644 specs/juno/proposed/usb-passthrough-proposal-2.rst
 create mode 100644 specs/juno/proposed/usb-passthrough-with-usb-controller.rst
 create mode 100644 specs/juno/proposed/usb-redirection.rst
 create mode 100644 specs/juno/proposed/use-configdrive-with-ironic.rst
 create mode 100644 specs/juno/proposed/use-glance-v2-api.rst
 create mode 100644 specs/juno/proposed/use-physical-cdrom.rst
 create mode 100644 specs/juno/proposed/user-project-metadata.rst
 create mode 100644 specs/juno/proposed/username-in-nova-list-for-admin-purpose.rst
 create mode 100644 specs/juno/proposed/v2-api-detailed-quotas.rst
 create mode 100644 specs/juno/proposed/v3-api-neutron-network-support.rst
 create mode 100644 specs/juno/proposed/validate-targethost-live-migration.rst
 create mode 100644 specs/juno/proposed/validate-tenant-user-with-keystone.rst
 create mode 100644 specs/juno/proposed/vcpus-in-api.rst
 create mode 100644 specs/juno/proposed/virt-image-transfer-layer.rst
 create mode 100644 specs/juno/proposed/virt-properties-object.rst
 create mode 100644 specs/juno/proposed/virtio-scsi-settings.rst
 create mode 100644 specs/juno/proposed/vm-cpu-pinning-support.rst
 create mode 100644 specs/juno/proposed/vmware-clone-image-handler.rst
 create mode 100644 specs/juno/proposed/vmware-encrypt-vcenter-passwords-proposal-1.rst
 create mode 100644 specs/juno/proposed/vmware-encrypt-vcenter-passwords-proposal-2.rst
 create mode 100644 specs/juno/proposed/vmware-resource-pool-enablement.rst
 create mode 100644 specs/juno/proposed/vmware-vm-ref-refactor.rst
 create mode 100644 specs/juno/proposed/vnc-configurable-share-policy.rst

diff --git a/README.rst b/README.rst
index df985fb..5a683fc 100644
--- a/README.rst
+++ b/README.rst
@@ -14,8 +14,9 @@ The layout of this repository is::
 
   specs/<release>/
 
-Where there are two sub-directories:
+Where there are three sub-directories:
 
+  specs/<release>/proposed: specifications proposed but not approved
   specs/<release>/approved: specifications approved but not yet implemented
   specs/<release>/implemented: implemented specifications
 
diff --git a/doc/source/index.rst b/doc/source/index.rst
index 69ca3d3..f3949db 100644
--- a/doc/source/index.rst
+++ b/doc/source/index.rst
@@ -40,6 +40,15 @@ Juno approved (but not implemented) specs:
 
    specs/juno/approved/*
 
+Juno proposed (but not approved) specs:
+
+.. toctree::
+   :glob:
+   :maxdepth: 1
+
+   specs/juno/proposed/*
+
+
 ==================
 Indices and tables
 ==================
diff --git a/specs/juno/proposed/Audit-compute-node-on-controller-recovery.png b/specs/juno/proposed/Audit-compute-node-on-controller-recovery.png
new file mode 100644
index 0000000..fb6e65c
--- /dev/null
+++ b/specs/juno/proposed/Audit-compute-node-on-controller-recovery.png
@@ -0,0 +1,280 @@
+<!DOCTYPE html>
+<html lang="en" dir="ltr" class="client-nojs">
+<head>
+<meta charset="UTF-8" />
+<title>File:Audit-compute-node-on-controller-recovery.png - OpenStack</title>
+<meta name="generator" content="MediaWiki 1.24wmf19" />
+<link rel="shortcut icon" href="/favicon.ico" />
+<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="OpenStack (en)" />
+<link rel="EditURI" type="application/rsd+xml" href="https://wiki.openstack.org/w/api.php?action=rsd" />
+<link rel="alternate" hreflang="x-default" href="/wiki/File:Audit-compute-node-on-controller-recovery.png" />
+<link rel="copyright" href="http://creativecommons.org/licenses/by/3.0/" />
+<link rel="alternate" type="application/atom+xml" title="OpenStack Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
+<link rel="stylesheet" href="https://wiki.openstack.org/w/load.php?debug=false&amp;lang=en&amp;modules=ext.uls.nojs%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.ui.button&amp;only=styles&amp;skin=strapping&amp;*" />
+<link rel="stylesheet" href="/w/skins/strapping/bootstrap/css/bootstrap.css?303" media="screen" />
+<link rel="stylesheet" href="/w/skins/strapping/bootstrap/awesome/css/font-awesome.css?303" media="screen" />
+<link rel="stylesheet" href="/w/skins/strapping/screen.css?303" media="screen" />
+<link rel="stylesheet" href="/w/skins/strapping/theme.css?303" media="screen" /><meta name="ResourceLoaderDynamicStyles" content="" />
+<link rel="stylesheet" href="https://wiki.openstack.org/w/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=styles&amp;skin=strapping&amp;*" />
+<style>a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}
+/* cache key: openstack_wiki:resourceloader:filter:minify-css:7:d3155f57bef5c67e78be5cab96908cad */</style>
+<script src="https://wiki.openstack.org/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=strapping&amp;*"></script>
+<script>if(window.mw){
+mw.config.set({"wgCanonicalNamespace":"File","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":6,"wgPageName":"File:Audit-compute-node-on-controller-recovery.png","wgTitle":"Audit-compute-node-on-controller-recovery.png","wgCurRevisionId":50228,"wgRevisionId":50228,"wgArticleId":4323,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"File:Audit-compute-node-on-controller-recovery.png","wgIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRestrictionUpload":[],"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"hidesig":true,"preview":false,"previewDialog":false,"publish":false},"wgULSAcceptLanguageList":[],"wgULSCurrentAutonym":"English","wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}"});
+}</script><script>mw.loader.implement("user.options",function($,jQuery){mw.user.options.set({"ccmeonemails":0,"cols":80,"date":"default","diffonly":0,"disablemail":0,"editfont":"default","editondblclick":0,"editsectiononrightclick":0,"enotifminoredits":0,"enotifrevealaddr":0,"enotifusertalkpages":1,"enotifwatchlistpages":1,"extendwatchlist":0,"fancysig":0,"forceeditsummary":0,"gender":"unknown","hideminor":0,"hidepatrolled":0,"imagesize":2,"math":1,"minordefault":0,"newpageshidepatrolled":0,"nickname":"","norollbackdiff":0,"numberheadings":0,"previewonfirst":0,"previewontop":1,"rcdays":7,"rclimit":50,"rows":25,"showhiddencats":0,"shownumberswatching":1,"showtoolbar":1,"skin":"strapping","stubthreshold":0,"thumbsize":5,"underline":2,"uselivepreview":0,"usenewrc":0,"watchcreations":1,"watchdefault":1,"watchdeletion":0,"watchlistdays":3,"watchlisthideanons":0,"watchlisthidebots":0,"watchlisthideliu":0,"watchlisthideminor":0,"watchlisthideown":0,"watchlisthidepatrolled":0,"watchmoves":0,"watchrollback":0,
+"wllimit":250,"useeditwarning":1,"prefershttps":1,"openid-hide":0,"openid-update-on-login-nickname":false,"openid-update-on-login-email":false,"openid-update-on-login-fullname":false,"openid-update-on-login-language":false,"openid-update-on-login-timezone":false,"usebetatoolbar":1,"usebetatoolbar-cgd":1,"translate":0,"translate-editlangs":"default","translate-recent-groups":"","translate-sandbox":"","echo-notify-show-link":true,"echo-show-alert":true,"echo-email-frequency":0,"echo-email-format":"plain-text","echo-subscriptions-email-system":true,"echo-subscriptions-web-system":true,"echo-subscriptions-email-user-rights":true,"echo-subscriptions-web-user-rights":true,"echo-subscriptions-email-other":false,"echo-subscriptions-web-other":true,"echo-subscriptions-email-edit-user-talk":false,"echo-subscriptions-web-edit-user-talk":true,"echo-subscriptions-email-reverted":false,"echo-subscriptions-web-reverted":true,"echo-subscriptions-email-article-linked":false,
+"echo-subscriptions-web-article-linked":false,"echo-subscriptions-email-mention":false,"echo-subscriptions-web-mention":true,"uls-preferences":"","language":"en","variant-gan":"gan","variant-iu":"iu","variant-kk":"kk","variant-ku":"ku","variant-shi":"shi","variant-sr":"sr","variant-tg":"tg","variant-uz":"uz","variant-zh":"zh","searchNs0":true,"searchNs1":false,"searchNs2":false,"searchNs3":false,"searchNs4":false,"searchNs5":false,"searchNs6":false,"searchNs7":false,"searchNs8":false,"searchNs9":false,"searchNs10":false,"searchNs11":false,"searchNs12":false,"searchNs13":false,"searchNs14":false,"searchNs15":false,"searchNs110":false,"searchNs111":false,"searchNs828":false,"searchNs829":false,"searchNs1198":false,"searchNs1199":false,"variant":"en"});},{},{});mw.loader.implement("user.tokens",function($,jQuery){mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\"});},{},{});
+/* cache key: openstack_wiki:resourceloader:filter:minify-js:7:f42f5cafad7c6884cbf7ada5c19eb62b */</script>
+<script>if(window.mw){
+mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","mediawiki.legacy.ajax","ext.uls.init","ext.uls.interface"]);
+}</script>
+		<style type='text/css'>
+		li#pt-openidlogin {
+		  background: url(/w/extensions/OpenID/skin/icons/openid-inputicon.png) top left no-repeat;
+		  padding-left: 20px;
+		  text-transform: none;
+		}
+		</style>
+
+<!--[if lt IE 7]><style type="text/css">body{behavior:url("/w/skins/strapping/csshover.min.htc")}</style><![endif]-->
+<meta name="viewport" content="width=device-width, initial-scale=1.0">
+</head>
+<body class="mediawiki ltr sitedir-ltr ns-6 ns-subject page-File_Audit-compute-node-on-controller-recovery_png skin-strapping action-view">
+
+<div id="userbar" class="navbar navbar-static">
+  <div class="navbar-inner">
+    <div style="width: auto;" class="container">
+
+      <div class="pull-left">
+                          <ul class="nav logo-container" role="navigation"><li id="p-logo"><a href="/wiki/Main_Page"  title="Visit the main page"><img src="https://wiki.openstack.org/w/images/thumb/c/c4/OpenStack_Logo_-_notext.png/30px-OpenStack_Logo_-_notext.png" alt="OpenStack"></a><li></ul>
+
+<!-- 0 -->
+          <ul class="nav" role="navigation">
+            <li class="dropdown" id="p-namespaces" class="vectorMenu">
+                            <a data-toggle="dropdown" class="dropdown-toggle brand" role="menu">File <b class="caret"></b></a>
+                                                                                                      <ul aria-labelledby="Namespaces" role="menu" class="dropdown-menu" >
+
+                <li id="ca-nstab-image" class="selected"><a href="/wiki/File:Audit-compute-node-on-controller-recovery.png"  title="View the file page [c]" accesskey="c" tabindex="-1">File</a></li><li id="ca-talk" class="new"><a href="/w/index.php?title=File_talk:Audit-compute-node-on-controller-recovery.png&amp;action=edit&amp;redlink=1"  title="Discussion about the content page [t]" accesskey="t" tabindex="-1">Discussion</a></li><li id="ca-viewsource"><a href="/w/index.php?title=File:Audit-compute-node-on-controller-recovery.png&amp;action=edit"  title="This page is protected.&#10;You can view its source [e]" accesskey="e" tabindex="-1">View source</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=File:Audit-compute-node-on-controller-recovery.png&amp;action=history"  title="Past revisions of this page [h]" accesskey="h" tabindex="-1">History</a></li></ul></li></ul>
+<!-- /0 -->
+
+<!-- 0 -->
+
+<!-- /0 -->
+
+<!-- 0 -->
+
+<!-- /0 -->
+
+<!-- 0 -->
+         </li>
+         </ul></ul>
+<!-- /LANGUAGES -->
+
+<!-- 0 -->
+
+          <ul class="nav" role="navigation">
+
+            <li class="dropdown" id="p-toolbox" class="vectorMenu">
+
+              <a data-toggle="dropdown" class="dropdown-toggle" role="button">Tools <b class="caret"></b></a>
+
+              <ul aria-labelledby="Tools" role="menu" class="dropdown-menu" >
+
+                <li id="t-info"><a href="/w/index.php?title=File:Audit-compute-node-on-controller-recovery.png&amp;action=info">Page information</a></li><li id="t-permalink"><a href="/w/index.php?title=File:Audit-compute-node-on-controller-recovery.png&amp;oldid=50228" title="Permanent link to this revision of the page">Permanent link</a></li><li class="divider"></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/File:Audit-compute-node-on-controller-recovery.png" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li class="divider"></li><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/File:Audit-compute-node-on-controller-recovery.png" title="A list of all wiki pages that link here [j]" accesskey="j">What links here</a></li>              </ul>
+
+            </li>
+
+          </ul>
+          
+<!-- /0 -->
+      </div>
+
+      <div class="pull-right">
+        
+<!-- 0 -->
+            <form class="navbar-search" action="/w/index.php" id="searchform">
+              <input id="searchInput" class="search-query" type="search" accesskey="f" title="Special:Search" placeholder="Search" name="search" value="">
+              <input type="submit" name="fulltext" value="Search" title="Search the pages for this text" id="mw-searchButton" class="searchButton btn hidden" />            </form>
+
+          
+<!-- /0 -->
+
+<!-- 0 -->
+          <ul class="nav pull-right" role="navigation">
+            <li class="dropdown" id="p-notifications" class="vectorMenu">
+                        </li>
+                        <li class="dropdown" id="p-createaccount" class="vectorMenu">
+                          </li>
+            <li class="dropdown" id="p-login" class="vectorMenu">
+                        </li>
+            <li class="dropdown" id="p-openidlogin" class="vectorMenu">
+            <li id="pt-openidlogin"><a href="/w/index.php?title=Special:OpenIDLogin&amp;returnto=File:Audit-compute-node-on-controller-recovery.png">Log in / create account with OpenID</a></li>            </li>
+                                    <li class="dropdown" id="p-personaltools" class="vectorMenu">
+              <a data-toggle="dropdown" class="dropdown-toggle" role="button">
+                <i class="icon-user"></i>
+                 <b class="caret"></b></a>
+              <ul aria-labelledby="Personal tools" role="menu" class="dropdown-menu" >
+              <li id="pt-uls" class="active"><a href="#" class="uls-trigger autonym">English</a></li>              </ul>
+            </li>
+                      </ul>
+          
+<!-- /0 -->
+      </div>
+
+    </div>
+  </div>
+</div>
+
+    <div id="mw-page-base" class="noprint"></div>
+    <div id="mw-head-base" class="noprint"></div>
+
+    <!-- Header -->
+    <div id="page-header" class="container signed-out">
+      <section class="span12">
+
+      
+      <ul class="navigation nav nav-pills pull-right searchform-disabled">
+
+      
+<!-- 0 -->
+
+                <li class=""><a href="http://www.openstack.org" id="n-Home" rel="nofollow">Home</a></li>
+                <li class=""><a href="http://www.openstack.org/software" id="n-Software" rel="nofollow">Software</a></li>
+                <li class=""><a href="http://www.openstack.org/user-stories" id="n-User-Stories" rel="nofollow">User Stories</a></li>
+                <li class=""><a href="http://www.openstack.org/community" id="n-Community" rel="nofollow">Community</a></li>
+                <li class=""><a href="http://www.openstack.org/profile" id="n-Profile" rel="nofollow">Profile</a></li>
+                <li class=""><a href="http://www.openstack.org/blog" id="n-Blog" rel="nofollow">Blog</a></li>
+                <li class=""><a href="http://wiki.openstack.org" id="n-Wiki" rel="nofollow">Wiki</a></li>
+                <li class=""><a href="http://docs.openstack.org" id="n-Documentation" rel="nofollow">Documentation</a></li>
+<!-- /LANGUAGES -->
+
+      </ul>
+
+    </section>
+    </div>
+
+    
+    
+    <!-- content -->
+    <section id="content" class="mw-body container 0">
+      <div id="top"></div>
+      <div id="mw-js-message" style="display:none;"></div>
+            <!-- bodyContent -->
+      <div id="bodyContent">
+                        <!-- jumpto -->
+        <div id="jump-to-nav" class="mw-jump">
+          Jump to: <a href="#mw-head">navigation</a>,
+          <a href="#p-search">search</a>
+        </div>
+        <!-- /jumpto -->
+        
+        <!-- innerbodycontent -->
+                  <div id="innerbodycontent" class="row nolayout"><div class="offset1 span10">
+            <h1 id="firstHeading" class="firstHeading page-header">
+              <span dir="auto">File:Audit-compute-node-on-controller-recovery.png</span>
+            </h1>
+            <!-- subtitle -->
+            <div id="contentSub" ></div>
+            <!-- /subtitle -->
+                        <div id="mw-content-text"><ul id="filetoc"><li><a href="#file">File</a></li>
+<li><a href="#filehistory">File history</a></li>
+<li><a href="#filelinks">File usage</a></li></ul><div class="fullImageLink" id="file"><a href="/w/images/4/46/Audit-compute-node-on-controller-recovery.png"><img alt="File:Audit-compute-node-on-controller-recovery.png" src="/w/images/thumb/4/46/Audit-compute-node-on-controller-recovery.png/727px-Audit-compute-node-on-controller-recovery.png" width="727" height="599" srcset="/w/images/4/46/Audit-compute-node-on-controller-recovery.png 1.5x, /w/images/4/46/Audit-compute-node-on-controller-recovery.png 2x" /></a><div class="mw-filepage-resolutioninfo">Size of this preview: <a href="/w/images/thumb/4/46/Audit-compute-node-on-controller-recovery.png/727px-Audit-compute-node-on-controller-recovery.png" class="mw-thumbnail-link">727 × 599 pixels</a>. <span class="mw-filepage-other-resolutions">Other resolution: <a href="/w/images/thumb/4/46/Audit-compute-node-on-controller-recovery.png/291px-Audit-compute-node-on-controller-recovery.png" class="mw-thumbnail-link">291 × 240 pixels</a>.</span></div></div>
+<div class="fullMedia"><a href="/w/images/4/46/Audit-compute-node-on-controller-recovery.png" class="internal" title="Audit-compute-node-on-controller-recovery.png">Original file</a> &#8206;<span class="fileInfo">(764 × 630 pixels, file size: 38 KB, MIME type: image/png)</span>
+</div>
+<div id="mw-imagepage-content" lang="en" dir="ltr" class="mw-content-ltr"><p>Explains audit implementation in nova.
+</p>
+<!-- 
+NewPP limit report
+CPU time usage: 0.004 seconds
+Real time usage: 0.002 seconds
+Preprocessor visited node count: 1/1000000
+Preprocessor generated node count: 4/1000000
+Post‐expand include size: 0/2097152 bytes
+Template argument size: 0/2097152 bytes
+Highest expansion depth: 1/40
+Expensive parser function count: 0/100
+-->
+
+<!-- Saved in parser cache with key openstack_wiki:pcache:idhash:4323-0!*!*!*!*!*!* and timestamp 20140917111403 and revision id 50228
+ -->
+</div><h2 id="filehistory">File history</h2>
+<div id="mw-imagepage-section-filehistory">
+<p>Click on a date/time to view the file as it appeared at that time.
+</p>
+<table class="wikitable filehistory">
+<tr><td></td><th>Date/Time</th><th>Thumbnail</th><th>Dimensions</th><th>User</th><th>Comment</th></tr>
+<tr><td>current</td><td class='filehistory-selected' style='white-space: nowrap;'><a href="/w/images/4/46/Audit-compute-node-on-controller-recovery.png">12:42, 28 April 2014</a></td><td><a href="/w/images/4/46/Audit-compute-node-on-controller-recovery.png"><img alt="Thumbnail for version as of 12:42, 28 April 2014" src="/w/images/thumb/4/46/Audit-compute-node-on-controller-recovery.png/120px-Audit-compute-node-on-controller-recovery.png" width="120" height="99" /></a></td><td>764 × 630 <span style="white-space: nowrap;">(38 KB)</span></td><td><a href="/w/index.php?title=User:Liyi_Meng&amp;action=edit&amp;redlink=1" class="new mw-userlink" title="User:Liyi Meng (page does not exist)">Liyi Meng</a> <span style="white-space: nowrap;"> <span class="mw-usertoollinks">(<a href="/w/index.php?title=User_talk:Liyi_Meng&amp;action=edit&amp;redlink=1" class="new" title="User talk:Liyi Meng (page does not exist)">Talk</a> | <a href="/wiki/Special:Contributions/Liyi_Meng" title="Special:Contributions/Liyi Meng">contribs</a>)</span></span></td><td dir="ltr">Explains audit implementation in nova.</td></tr>
+</table>
+
+</div>
+<ul>
+<li id="mw-imagepage-upload-disallowed">You cannot overwrite this file.</li>
+</ul>
+<h2 id="filelinks">File usage</h2>
+<div id="mw-imagepage-nolinkstoimage">
+<p>There are no pages that link to this file.
+</p>
+</div>
+</div>          </div></div>
+                <!-- /innerbodycontent -->
+
+                <!-- printfooter -->
+        <div class="printfooter">
+        Retrieved from "<a dir="ltr" href="https://wiki.openstack.org/w/index.php?title=File:Audit-compute-node-on-controller-recovery.png&amp;oldid=50228">https://wiki.openstack.org/w/index.php?title=File:Audit-compute-node-on-controller-recovery.png&amp;oldid=50228</a>"        </div>
+        <!-- /printfooter -->
+                        <!-- catlinks -->
+        <div id='catlinks' class='catlinks catlinks-allhidden'></div>        <!-- /catlinks -->
+                        <div class="visualClear"></div>
+        <!-- debughtml -->
+                <!-- /debughtml -->
+      </div>
+      <!-- /bodyContent -->
+    </section>
+    <!-- /content -->
+
+      <!-- footer -->
+
+      
+      <div id="footer" class="footer container">
+        <div class="row">
+    
+            <ul id="footer-places">
+                              <li id="footer-places-privacy"><a href="/wiki/OpenStack:Privacy_policy" title="OpenStack:Privacy policy">Privacy policy</a></li>
+                              <li id="footer-places-about"><a href="/wiki/OpenStack:About" title="OpenStack:About">About OpenStack</a></li>
+                              <li id="footer-places-disclaimer"><a href="/wiki/OpenStack:General_disclaimer" title="OpenStack:General disclaimer">Disclaimers</a></li>
+                              <li id="footer-places-mobileview"><a href="https://wiki.openstack.org/w/index.php?title=File:Audit-compute-node-on-controller-recovery.png&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
+                                        </ul>
+                                <ul id="footer-icons" class="noprint">
+                  <li id="footer-copyrightico">
+                    <a href="http://creativecommons.org/licenses/by/3.0/"><img src="/w/skins/common/images/cc-by.png" alt="Attribution 3.0 Unported (CC BY 3.0)" width="88" height="31" /></a>
+                  </li>
+                  <li id="footer-poweredbyico">
+                    <a href="//www.mediawiki.org/"><img src="/w/skins/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" width="88" height="31" /></a>
+                  </li>
+                </ul>
+                  </div>
+      </div>
+      <!-- /footer -->
+
+
+    <script>/*<![CDATA[*/window.jQuery && jQuery.ready();/*]]>*/</script><script>if(window.mw){
+mw.loader.state({"skins.strapping":"loading","site":"loading","user":"ready","user.groups":"ready"});
+}</script>
+<script src="https://wiki.openstack.org/w/load.php?debug=false&amp;lang=en&amp;modules=skins.strapping&amp;only=scripts&amp;skin=strapping&amp;*"></script>
+<script>if(window.mw){
+mw.loader.load(["mediawiki.action.view.postEdit","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","ext.uls.pt"],null,true);
+}</script>
+<script src="//bits.wikimedia.org/geoiplookup"></script>
+<script src="https://wiki.openstack.org/w/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=scripts&amp;skin=strapping&amp;*"></script>
+<script type="text/javascript">
+var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
+document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
+</script>
+<script type="text/javascript">
+var pageTracker = _gat._getTracker("UA-17511903-1");
+pageTracker._trackPageview();
+</script><script>if(window.mw){
+mw.config.set({"wgBackendResponseTime":344});
+}</script>
+  </body>
+</html>
diff --git a/specs/juno/proposed/action-type-aware-scheduling.rst b/specs/juno/proposed/action-type-aware-scheduling.rst
new file mode 100644
index 0000000..01e3f6b
--- /dev/null
+++ b/specs/juno/proposed/action-type-aware-scheduling.rst
@@ -0,0 +1,196 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================================================
+Action type aware scheduling
+=============================================================================
+
+https://blueprints.launchpad.net/nova/+spec/action-type-aware-scheduling
+
+Currently in ``FilterScheduler`` there is no variable indicating which action
+triggered the request that is currently processed. The behavior of filters and
+weighers may be different depending on the fact if currently processed request
+is scheduling / rescheduling / migration / resize / live-migration /
+evacuation / unshelving.
+
+What this blueprint proposes is to add a new variable which would keep an
+action type. It should be accessible from custom scheduler filters and
+weighers. This could be accomplished using ``filter_properties`` dictionary - a
+new key like ``'action_type'`` can be added there.
+
+
+Problem description
+===================
+
+Right now it's difficult to check which action triggered scheduling in
+FilterScheduler, you need to use ``task_state`` from ``filter_properites`` and
+some other properties to distinguish between them:
+
+* if ``task_state`` is ``"scheduling"`` you need to use ``retry`` parameter
+   to distinguish between scheduling and rescheduling.
+* if ``task_state`` is ``"resize_prep"`` you need to check if old and new VM
+   flavor is the same to distinguish between migration and resize.
+* if ``task_state`` is ``"migrating"`` then it's live migration (this one is
+   really confusing).
+* if ``task_state`` is ``"unshelving"`` then this request was triggered by
+   unshelving action.
+* another action to distinguish is evacuation, as once blueprint
+   *find-host-and-evacuate-instance* will be implemented, evacuation will also
+   be able to trigger scheduling.
+
+Therefore if your custom filter needs this information to properly schedule a
+VM, you need to use complicated if-else statement. It would be much easier if
+there was a flag in ``filter_properties`` that indicates action type.
+
+With this feature you will be able to schedule your VMs according to action
+that triggered scheduling. For example (if this blueprint will be implemented:
+https://blueprints.launchpad.net/nova/+spec/find-host-and-evacuate-instance)
+you would be able to keep multiple compute nodes for emergency evacuation
+purpose and place VMs on them only when scheduling was triggered by evacuation.
+In order to achieve this, user would need to create his own simple filter and
+add some hosts to the host-aggregate. Then, inside filter, user can check if
+the currently executed action is evacuation. If so, the only thing left is to
+check if the currently filtered host is in the appropriate (created before)
+host-aggregate. This way users can achieve the functionality of leaving some
+hosts for evacuation purposes.
+
+User can also keep a host for rescheduling only to be sure that if other hosts
+fail booting a VM, then the scheduling decision will always be to place on
+this one particular host. This can be done using custom filter like in the
+above example.
+
+To be clear, there is no advanced logic in the implementation of this
+blueprint. It proposes to update ``filter_properties`` so that users have more
+flexibility in writing their own filters and weighers. They will be aware
+what action triggered scheduling of the VMs. It may also be useful in the
+future for reporting purposes.
+
+
+Proposed change
+===============
+
+We intend to change:
+
+* ``compute/api.py`` file where ``filter_properties`` for migrate and resize
+   are created;
+* ``conductor/manager.py`` file for create and unshelve actions;
+* ``conductor/tasks/live_migrate`` for live migration action;
+
+In these files we will insert ``action_type`` element into
+``filter_properties``. It will be set to the corresponding action. Probably
+best type for this is ``nova.compute.instance_actions`` enum and we want to add
+missing live migration action to it.
+
+
+Alternatives
+------------
+
+Another way of doing this is to modify ``FilterScheduler`` and add this flag in
+``schedule_run_instance`` using aforementioned if-else statement. This is an
+improper way of developing this feature because in the future there can be an
+action type that is indistinguishable from others using only the current set of
+``filter_properties``.
+
+
+Data model impact
+-----------------
+
+None
+
+
+REST API impact
+---------------
+
+None
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+None
+
+
+Performance Impact
+------------------
+
+Solution we propose is adding only one assignment to methods where scheduling
+requests are created. This should have no impact on performance.
+
+
+Other deployer impact
+---------------------
+
+If deployer is using his custom filters he will have more information to decide
+on which node a particular VM should be launched. Additional flag in
+``filter_properties`` won't affect any already developed filters or
+``FilterScheduler`` extensions.
+
+
+Developer impact
+----------------
+
+Developers will have more flexibility in defining rules in stock filters.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  mateusz-blaszkowski-8
+
+Other contributors:
+  michal-dulko-f
+
+
+Work Items
+----------
+
+* Insert ``action_type`` to ``filter_properties`` in proper places.
+* Add unit tests for all scheduling actions checking if ``action_type``
+   element is present in ``filter_properties``.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+As this feature at the start will be used only if user will modify filters or
+``FilterScheduler`` on his own it doesn't need any new integration tests. Unit
+tests will be sufficient.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+None
+
+
diff --git a/specs/juno/proposed/add-an-index-column-to-nova-list.rst b/specs/juno/proposed/add-an-index-column-to-nova-list.rst
new file mode 100644
index 0000000..4a5d54f
--- /dev/null
+++ b/specs/juno/proposed/add-an-index-column-to-nova-list.rst
@@ -0,0 +1,164 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================================================
+add an index column to 'nova list'
+=================================================================
+
+https://blueprints.launchpad.net/python-novaclient/+spec/
+add-an-index-column-to-nova-list
+
+Now 'nova list' can not give the amount of instances easily,
+so I want to add an index column to 'nova list' in python-novaclient.
+
+Problem description
+===================
+
+* Now 'nova list' only shows 'ID'/'Name'/'Status'/.. columns.
+  That's OK for normal using
+
+* But if you have more than 200 VMs or even larger in one environment,
+  'nova list' will print out lots of lines even if
+  I add some filters (such as '--name'/'--status'/..)
+
+* My environment often has more than 500 instances, so it's very hard to
+  count the amount of them. Here's a result of 'nova list' on one environment
+  only has 150 instances: (http://paste.openstack.org/show/60553/)
+
+* Now I just have to use "nova list | wc" to count them(minus 4 lines).
+
+So, if an index column add to 'nova list' in novaclient,
+it can easily give the amount of the VMs under any filters.
+
+It'll benefit the operation and improve the usability.
+
+P.S. Cinder also has the similar problem if one environment has much volumes.
+I have submitted a similar BP in launchpad:
+(https://blueprints.launchpad.net/python-cinderclient/+spec/
+add-an-index-column-to-cinder-list-functions)
+
+Proposed change
+===============
+
+Only add one 'index' column to 'nova list' in novaclient.
+The response of the command will print like this:
++-------+-------+------+---------+------------+-------------+---------------+
+| Index | ID    | Name | Status  | Task State | Power State | Networks      |
++-------+-------+------+---------+------------+-------------+---------------+
+| 1     | uuid1 | vm01 | SHUTOFF | -          | Shutdown    | test=10.0.0.3 |
+| 2     | uuid2 | vm02 | ACTIVE  | -          | Running     | test=10.0.0.4 |
+| 3     | uuid3 | vm03 | ACTIVE  | -          | Running     | test=10.0.0.5 |
+| 4     | uuid4 | vm04 | ACTIVE  | -          | Running     | test=10.0.0.6 |
+| 5     | uuid5 | vm05 | ACTIVE  | -          | Running     | test=10.0.0.7 |
+| 6     | uuid6 | vm06 | SHUTOFF | -          | Shutdown    | test=10.0.0.8 |
++-------+-------+------+---------+------------+-------------+---------------+
+
+* The 'index' will always be sorted consecutively in ascending order
+* It won't add any records/columns into database, and also won't influence
+  the behaviour of those commands
+
+Alternatives
+------------
+
+Add one filter like '--number' to instead of the design above.
+
+* If we add one filter like 'nova list --number', the response may only print
+  the amount number, like this:
+
+  # nova list --number
+  > 6
+
+* This filter can be also executed with other filters, like this:
+
+  # nova list --status active --number
+  > 4
+
+This modification will not change the response of 'nova list',
+it only add one filter to count.
+So I think it's also a usable method to achieve the goal
+even if it's not more clear for operators than the former design.
+
+Therefore, I paste it here to hear your advice. Thanks.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Assignee: wingwj <wingwj@gmail.com>
+
+
+Work Items
+----------
+
+Code this function & related tests in python-novaclient
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Unit tests will add to check this function in python-novaclient
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+None
\ No newline at end of file
diff --git a/specs/juno/proposed/add-app-lock.rst b/specs/juno/proposed/add-app-lock.rst
new file mode 100644
index 0000000..45e7ac0
--- /dev/null
+++ b/specs/juno/proposed/add-app-lock.rst
@@ -0,0 +1,140 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Add application lock VM support for nova
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/add-app-lock
+
+Problem description
+===================
+
+Services like Trove use run in Nova Compute Instances.  These Services try
+to provide an integrated and stable platform for which the service can run
+in a predictable manner.  Such elements include configuration of the
+service, networking, installed packages, etc.  In today¹s world, when Trove
+spins up an Instance to deploy a database on, it creates that Instance with
+the Users Credentials.  Thus, to Nova, the User has full access to that
+Instance through Nova API.  This access can be used in ways which
+unintentionally compromise the service.
+
+In Nova side, we need to provide a method to put such Instances in a read-only or
+invisible mode from the perspective of Nova, that is, the Instance can only
+be managed from the Service from which it was created.
+
+http://lists.openstack.org/pipermail/openstack-dev/2014-April/031952.html
+has detailed requirement and discussion result.
+
+Proposed change
+===============
+
+Add an applock role like we did for admin role. the applock role will uesd:
+
+User without AppLock role  - can apply/remove user lock to instance.
+                             Cannot perform operations is any lock is
+                             set on the instance
+User with AppLock role - can apply/remove application lock to instance.
+                         Cannot perform operations on the instance if
+                         the admin lock is set
+User with Admin role - can apply/remove admin lock.
+                       Can perform any operations on the instance.
+
+Alternatives
+------------
+
+The service need to manage the instance by itself and
+
+Data model impact
+-----------------
+
+No data model changed
+
+REST API impact
+---------------
+
+There is no update in v2/v3 API from caller perspective. Instead, in
+implementation in the v2/v3 API, the role of the caller will be checked
+and will be used (in the context param)
+
+in policy.json file, something like following can be added by user:
+
+"context_is_applock" : "role:applock"
+"admin_or_applock_or_owner":  "is_admin:True or is_applock:True or project_id:%(project_id)s",
+
+"compute_extension:admin_actions:lock": "rule:admin_or_applock_or_owner",
+
+Security impact
+---------------
+
+The administrator is responsible for grant the applock role to users.
+and the user who have applock role is only able to lock/unlock the instance,
+and the actions it takes can be reverted by amdin, so the security impact is low.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+End user will find they can't operator their VM if their instances are locked by
+the user with applock or admin role and they can't unlock it.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jichenjc
+
+Work Items
+----------
+
+Support applock logic in compute layer
+v2 API update
+v3 API update
+policy update
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+unit test will be used to guarantee the code added
+and tempest cases will also be added.
+
+
+Documentation Impact
+====================
+
+Doc need to be updated to let operator know they can define a applock role
+to user and the user with that role can lock/unlock user created vm.
+
+References
+==========
+http://lists.openstack.org/pipermail/openstack-dev/2014-April/032149.html
diff --git a/specs/juno/proposed/add-delete-node-to-nova-manage.rst b/specs/juno/proposed/add-delete-node-to-nova-manage.rst
new file mode 100644
index 0000000..ae488cd
--- /dev/null
+++ b/specs/juno/proposed/add-delete-node-to-nova-manage.rst
@@ -0,0 +1,162 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+
+==========================================
+Add 'delete-node' to nova-manage
+==========================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/add-delete-node-to-nova-manage
+
+Currently there is no way to delete a compute node from an existing
+environment - the compute node can be marked as deleted, but this doesn't
+delete anything. For the sake of sanity with regards to database content
+and also for the ease of use aspect, it would be nice to have the ability
+to not only delete a compute node from an environment, but actually have it
+deleted, not just marked as such.
+
+
+Problem description
+===================
+
+There is currently no way to manually delete a compute node aside from making
+manual database changes.
+
+For an end-user, this makes it impossible to delete old decommissioned nodes.
+
+For an operator, it makes it a hassle to deal with, since it is easier to just
+type a command and have the CLI make the API call, which in turn will make
+the necessary database changes for you.
+
+
+Proposed change
+===============
+
+Add an API call that will allow for actual compute node deletion from a
+database - in all applicable tables.
+
+Add CLI funtionality to make this API call and verify success.
+(This will likely just be needed for python-novaclient, but adding here for
+completeness.)
+
+
+Alternatives
+------------
+
+I have seen no other alternatives to deleting an old node aside from making
+database changes. The goal here is to make that manual labor obsolete and
+to also allow for the permanent deletion where desired.
+
+
+Data model impact
+-----------------
+
+There should be no changes needed, since the proposition is simply allowing
+for making database queries, not schema changes.
+
+
+REST API impact
+---------------
+
+* delete-node
+
+  * Permanently delete retired compute node.
+
+  * POST or DELETE - not sure which is more ideal here.
+
+  * /delete-node/
+
+  * Pass UUID of hypervisor
+
+
+Security impact
+---------------
+
+This should not require anything other than 'nova' database user access,
+since my proposed change only affects hypervisor information.
+
+This proposed change should also be only accessible via 'nova-manage', versus
+simply 'nova' so that it can only be run an admin.
+
+
+Notifications impact
+--------------------
+
+None.
+
+
+Other end user impact
+---------------------
+
+There should be a python-novaclient call created for my proposed API addition.
+
+
+Performance Impact
+------------------
+
+None.
+
+
+Other deployer impact
+---------------------
+
+None.
+
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  None
+
+Other contributors:
+  None
+
+
+Work Items
+----------
+
+* Create API call
+
+* Create call function in python-novaclient
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+A simple test to create a new hypervisor in the database, and then delete
+that hypervisor should be sufficient.
+
+
+Documentation Impact
+====================
+
+There will need to be a change to the related nova pages in the documentation.
+
+I can [and would]  personally take care of the doc changes.
+
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/add-delete-on-termination-option.rst b/specs/juno/proposed/add-delete-on-termination-option.rst
new file mode 100644
index 0000000..151c214
--- /dev/null
+++ b/specs/juno/proposed/add-delete-on-termination-option.rst
@@ -0,0 +1,260 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================================================
+add delete_on_termination option for attaching volume to an existing server
+===========================================================================
+
+https://blueprints.launchpad.net/nova/+spec/add-delete-on-termination-option
+
+Add delete_on_termination option when attaching volume for an exist server.
+
+Problem description
+===================
+
+There is parameter known as delete_on_termination option for attaching volume
+when creating a new server, but no option when attaching a volume to an exist
+server. so the result will be different when deleting the server.
+
+Attach a volume when creating a new server, the API contains
+'block_device_mapping', such as:
+"block_device_mapping": [
+{"volume_id": "<VOLUME_ID>", "device_name": "/dev/vdc",
+"delete_on_termination": "true"}]
+
+There is no option 'delete_on_termination' when attaching a
+volume to an exsit server, the POST data likes:
+{"volumeAttachment":{"volumeId":"<VOLUME_ID>", "device":"/dev/sdb"}}
+
+Proposed change
+===============
+
+Add the same option 'delete_on_termination' when attaching a
+volume to an exsit server, the POST data likes:
+{"volumeAttachment":{"volumeId":"<VOLUME_ID>", "device":"/dev/sdb"
+"delete_on_termination": "true"}}
+
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+API for attaching a volume to an exist instance:
+
+Scenarios:
+Case 1: If delete_on_termination is true, the volume will be delete when
+delete the instance.
+Case 2: If delete_on_termination is false, the volume will not be delete
+when delete the instance.
+(Default behavior)
+
+V2 API specification:
+POST: v3/{tenant_id}/servers/{server_id}/os-volume_attachments
+
+V3 API specification:
+POST: v3/servers/{server_id}/os-volume_attachments
+
+Request parameters:
+* server_id: The UUID for the server of interest to you.
+* volumeId: ID of the volume to attach.
+* device: Name of the device such as, /dev/vdb.
+* delete_on_termination(Optional): Whether to remove the volume when the
+server is terminated(default: False).
+* volumeAttachment: A dictionary representation of a volume attachment.
+
+  JSON request:
+             {
+              "volumeAttachment":
+                        {
+                         "volumeId": "a26887c6-c47b-4654-abb5-dfadf7d3f803",
+                         "device": "/dev/vdd",
+                         "delete_on_termination": True
+
+                        }
+
+             }
+
+ JSON response:
+            {
+             "volumeAttachment":
+                        {
+                         "device": "/dev/vdd",
+                         "id": "a26887c6-c47b-4654-abb5-dfadf7d3f803",
+                         "serverId": "0c92f3f6-c253-4c9b-bd43-e880a8d2eb0a",
+                         "volumeId": "a26887c6-c47b-4654-abb5-dfadf7d3f803",
+                         "delete_on_termination": True
+
+                        }
+
+            }
+
+ Sample v2 request:
+   POST: v2/{tenant_id}/servers/{server_id}/os-volume_attachments -d '{
+     "volumeAttachment": {
+                          "volumeId": "a26887c6-c47b-4654-abb5-dfadf7d3f803",
+                          "device": "/dev/vdd",
+                          "delete_on_termination": True
+
+                         }
+
+    }'
+
+ Sample v3 request:
+  POST: v3/servers/{server_id}/os-volume_attachments -d '{
+    "volumeAttachment": {
+                         "volumeId": "a26887c6-c47b-4654-abb5-dfadf7d3f803",
+                         "device": "/dev/vdd",
+                         "delete_on_termination": True
+
+                        }
+
+   }'
+
+ JSON schema definition::
+  attach = {
+            'type': 'object',
+             'properties':
+              {'attach':
+                {
+                 'type': 'object',
+                  'properties':
+                   {
+                    'volume_id': {'type': 'string', 'format': 'uuid'},
+
+                    'device': {
+                     'type': 'string',
+                     # NOTE: The validation pattern from match_device() in
+                     #       nova/block_device.py.
+
+                     'pattern': '(^/dev/x{0,1}[a-z]{0,1}d{0,1})([a-z]+)[0-9]*$'
+
+                              },
+
+                    'disk_bus':
+                     {
+                      'type': 'string'
+
+                     },
+
+                    'device_type':
+                     {
+                      'type': 'string',
+
+                     },
+
+                    'delete_on_termination':
+                     {
+                      'type': 'boolean',
+
+                     },
+
+                   },
+
+                 'required': ['volume_id'],
+                 'additionalProperties': False,
+
+                },
+
+              },
+
+            'required': ['attach'],
+            'additionalProperties': False,
+
+           }
+
+HTTP response codes:
+v2:
+Normal HTTP Response Code: 202 on success
+v3:
+Normal HTTP Response Code: 200 on success
+
+Validation:
+'delete_on_termination' must be boolean.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+If developers attach a volume to an exist server.
+they must define 'delete_on_termination' option to determine
+whether or not delete the volume when deleting server.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  idopra
+
+Work Items
+----------
+
+* add option 'delete_on_termination' when attaching a volume to an exsit
+
+* server.
+
+* users whether or not delete the volume based on 'delete_on_termination'
+
+* option.
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+Tempest tests to be added to ensure the various hypervisor drivers implement
+this feature.
+
+Documentation Impact
+====================
+
+Changes to be made to the attach volume API documentation to include the
+additional
+parameter 'delete_on_termination' that can be passed in.
+
+References
+==========
+
+None.
+
diff --git a/specs/juno/proposed/add-extra-specs-to-flavor-calls.rst b/specs/juno/proposed/add-extra-specs-to-flavor-calls.rst
new file mode 100644
index 0000000..0678758
--- /dev/null
+++ b/specs/juno/proposed/add-extra-specs-to-flavor-calls.rst
@@ -0,0 +1,202 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+Add extra specs info to flavor list and show calls
+==================================================
+
+https://blueprints.launchpad.net/nova/+spec/add-extra-specs-to-flavor-list
+
+In this blueprint we aim to add the extra specs information to the flavor list
+and show call responses.
+
+
+Problem description
+===================
+
+When we make a flavor show or flavor list call, currently the response does
+not include the extra specs information. We need to make an additional call to
+get the extra specs corresponding to that flavor.
+
+
+Proposed change
+===============
+
+In order to get both the flavor information as well as the extra specs
+associated with the flavor in one API call, I propose that we include the extra
+specs information in the response of the flavor list and show calls itself.
+This will avoid an extra call to get all the information about a flavor.
+
+The current db call to get flavor information already makes the call to get the
+extra specs associated with the flavor. So the only change that is required
+here is to add the extra specs to the response template. Hence, there will be
+no change in the db calls and performance or response time.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+API requests for the flavor list and show call will not change.
+There will be a change in the API responses:
+
+Flavor show call - V3 API specification:
+
+Request:
+GET: v3/flavors/{flavor_id}
+Response::
+
+    {
+        "flavor": {
+            "id": "1",
+            "links": [
+                {
+                    "href": "http://openstack.example.com/v3/flavors/1",
+                    "rel": "self"
+                },
+                {
+                    "href": "http://openstack.example.com/flavors/1",
+                    "rel": "bookmark"
+                }
+            ],
+            "extra_specs": {"key1": "value1", "key2": "value2"},
+            "name": "small",
+            "ram": 512,
+            "disk": 10,
+            "swap": 0,
+            "vcpus": 1
+        }
+    }
+
+Flavor list call - V3 API specification:
+
+Request:
+GET: v3/flavors
+Response::
+
+    {
+        "flavors": [
+            {
+                "id": "1",
+                "name": "256 MB Server",
+                "ram": 256,
+                "disk": 10,
+                "vcpus": 1,
+                "links": [
+                    {
+                        "href": "http://openstack.example.com/v3/1",
+                        "rel": "self"
+                    },
+                    {
+                        "href": "http://openstack.example.com/flavors/1",
+                        "rel": "bookmark"
+                    }
+                ],
+                "extra_specs": {"key1": "value1", "key2": "value2"},
+            },
+            {
+                "id": "2",
+                "name": "512 MB Server",
+                "ram": 512,
+                "disk": 20,
+                "vcpus": 2,
+                "links": [
+                    {
+                        "href": "http://openstack.example.com/v3/flavors/2",
+                        "rel": "self"
+                    },
+                    {
+                        "href": "http://openstack.example.com/flavors/2",
+                        "rel": "bookmark"
+                    }
+                ],
+                "extra_specs": {"key1": "value1", "key2": "value2"},
+            }
+        ]
+    }
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The response of the API needs to change in the documentation.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    aditirav
+
+Work Items
+----------
+
+* Changes to be made to the API responses of the flavor list and show calls.
+* Changes to the python nova client to not make an additional call to get
+  extra specs associated with the flavor.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest tests to be added to check if the flavor list and show calls have the
+extra specs information included in the response.
+
+
+Documentation Impact
+====================
+
+Changes to be made to the flavor API documentation to include the extra specs
+information in the response of show and list calls.
+
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/add-force-detach-to-nova.rst b/specs/juno/proposed/add-force-detach-to-nova.rst
new file mode 100644
index 0000000..a3106ad
--- /dev/null
+++ b/specs/juno/proposed/add-force-detach-to-nova.rst
@@ -0,0 +1,216 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================================================
+Add force detach volume to nova
+=================================================================
+
+https://blueprints.launchpad.net/nova/+spec/add-force-detach-to-nova
+
+Add a mechanism to forcibly detach a volume from an instance in Nova.
+This is more graceful than performing the same operation only in cinder
+because Nova can clean up its own records about the attachment.
+
+Problem description
+===================
+
+We have two separate copies of the Nova service(call them nova A and nova B)
+and one cinder service. Attach a volume to an instance in nova A and then
+services of nova A become abnormal. Because the volume also want to be used
+in nova B, and can't detach this volume by using nova A. So using cinder api
+"force detach volume" to free this volume. But when nova A has recovered,
+nova can't detach this volume from instance by using nova api "detach volume",
+as nova checks the volume state must be "attached".
+
+So after using "force detach volume" in cinder, we can't detach this volume
+from nova, can't clean up attachment records in nova and libvirt xml.
+
+Proposed change
+===============
+
+1. Add optional "force_detach" parameter which is boolean variable
+   (default value is false) in nova detach api
+
+2. When force_detach is true, nova will detach the volume for cleaning
+   the attach information whether volume status is attached or detached
+
+3. Then call force detach api in cinder to make consistent state between
+   nova and cinder
+
+4. It will not affect the volume if it has been attached to other VM and
+   just clean up the attachment information of the VM which the volume was
+   attached to from nova side.
+   Meanwhile "force_detach" will just affect the "general volume",
+   not detach the root disk.
+
+
+Alternatives
+------------
+
+Add a new extension api "reset volume attach state" which defaults to admin
+only. It will clean up the records about the attachment and reset the
+attachment state of the volume in nova A. Then call force detach api in cinder
+to make consistent state between nova and cinder.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+The rest api look like this in v2::
+
+ {nova-api}/v2/{project_id}/servers/{server_id}/os-volume_attachments/
+ {volume_id}
+
+    {
+        "volumeDetachment": {
+            "force_detach":"true"
+        }
+    }
+
+the json validition scheme in v2::
+
+    volumeDetachment = {
+    'type': 'object',
+    'properties': {
+        'volumeDetachment': {
+            'type': 'object',
+            'properties': {
+                'force_detach': {
+                    'type': 'boolean'
+                },
+            },
+            'required': [],
+            'additionalProperties': False,
+        },
+    },
+    'required': [],
+    'additionalProperties': False,
+    }
+
+and in v3 it is::
+
+ {nova-api}/v3/servers/{server_id}/action
+
+    {
+        "detach": {
+            "volume_id": "54445bc5-7332-4215-a289-6036c1be79e7"
+            "force_detach": "true"
+        }
+    }
+
+the json validition scheme in v3::
+
+    detach = {
+    'type': 'object',
+    'properties': {
+        'detach': {
+            'type': 'object',
+            'properties': {
+                'volume_id': {
+                    'type': 'string', 'format': 'uuid'
+                },
+                'force_detach': {
+                    'type': 'boolean'
+                },
+            },
+            'required': ['volume_id'],
+            'additionalProperties': False,
+        },
+    },
+    'required': ['detach'],
+    'additionalProperties': False,
+    }
+
+If the request fails, volume status and attachment information will roll back.
+The json validation will be checking the value of "force_detach" should be
+"true" or "false"(also supporting 1 or 0) and case insensitive.
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+1. add force_detach.begin to notification
+
+2. add force_detach.end to notification
+
+Other end user impact
+---------------------
+
+This function will be added into python-novaclient when this work finish.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+The admin only policy for this api will be added into policy.json.
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Assignee: wanghao749 <wanghao749@huawei.com>
+
+
+Work Items
+----------
+
+1. Code this function in v2
+
+2. Code this function in v3
+
+3. Finish tempest tests
+
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Unit tests and tempest tests will check "force detach" function.
+In tempest, it will test force detach volume from the cinder side
+and make sure that nova can still force detach, as well as making
+sure that a force detach in nova calls the cinder force detach API.
+Further, test force detach in nova will not affect the volume which
+has been attached to other instance.
+
+Documentation Impact
+====================
+
+A description of this function will be added into Compute API V2
+and V3 Reference.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/add-ironic-boot-mode-filters.rst b/specs/juno/proposed/add-ironic-boot-mode-filters.rst
new file mode 100644
index 0000000..b79bb02
--- /dev/null
+++ b/specs/juno/proposed/add-ironic-boot-mode-filters.rst
@@ -0,0 +1,127 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================
+Add ironic boot mode filters
+=====================================
+
+
+https://blueprints.launchpad.net/nova/+spec/add-ironic-boot-mode-filters
+
+This spec proposes to add new filter to be used with ironic virt driver
+to select the ironic node based on the given boot mode.
+
+Problem description
+===================
+
+Operator wants an ability to specify the boot mode for deploying an image
+on an ironic node which supports the given boot mode.
+
+
+Proposed change
+===============
+
+Add a filter to select the ironic node based on a given
+boot mode.
+
+1. A filter which uses image property to select the node.
+   The required boot mode is given as an image property - boot_mode=uefi|bios.
+   The filter will extract this image property and compares it with the ironic
+   node property "supported_boot_modes".
+
+2. A filter which uses a key-value pair present in flavor extra_specs field.
+   The required boot mode is given as boot_mode=uefi|bios in flavor extra_specs
+   field. This filter will extract the "boot_mode" from flavor extra_specs
+   filed and compares it with the ironic node property "supported_boot_modes".
+
+3. Make necessary changes in nova.virt.ironic driver to update the ironic
+   node's instance_info field with the requested boot_mode.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+The operator needs to update the scheduler's nova.conf to activate filters,
+also he has to set approiate image property or nova flavor extra_specs field
+with boot_mode=ueif|bios.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Faizan Barmawer
+
+Other contributors:
+  <None>
+
+Work Items
+----------
+
+* New filter IronicBootModeFilter
+* Pass the boot_mode to ironic nodes instance_info field.
+
+
+Dependencies
+============
+
+https://blueprints.launchpad.net/nova/+spec/add-ironic-driver
+
+Testing
+=======
+
+Unit testing.
+
+Documentation Impact
+====================
+This filter has to be appropriately documented in ironic virt driver
+documents.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/add-support-for-cinder-scheduler-hints.rst b/specs/juno/proposed/add-support-for-cinder-scheduler-hints.rst
new file mode 100644
index 0000000..1bbefd6
--- /dev/null
+++ b/specs/juno/proposed/add-support-for-cinder-scheduler-hints.rst
@@ -0,0 +1,151 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================================
+Add support for providing cinder scheduler hints when creating volumes
+======================================================================
+
+https://blueprints.launchpad.net/nova/+spec/add-support-for-cinder-scheduler-hints
+
+When booting an instance with nova boot --block-device i.e. asking Nova to
+auto-create storage for a new instance, there is currently no way for Nova to
+hint to the Cinder scheduler how it should go about creating/placing this
+volumes. This specification proposes adding support for storing cinder
+extra_specs in Nova flavors and passing them to Cinder when volumes are
+created.
+
+
+Problem description
+===================
+
+Cinder supports volume placement based on type, capacity etc using the Cinder
+scheduler but Nova does not support passing any such information to Cinder when
+auto-creating volumes to be attached to a new instance.
+
+The cinderclient supports passing scheduler hints when using the v2 API. By
+adding hints to Nova flavors we can control where Cinder creates storage,
+whether that be physical location, type or indeed any other filter Cinder
+supports.
+
+
+Proposed change
+===============
+
+The proposed change is envisaged to be reasonably simple. It does, however,
+depend on support in Nova for the Cinder V2 API which is being added as part of
+https://review.openstack.org/43986. Once this lands, the solution proposed here
+would allow Nova to do the following:
+
+* Store Cinder scheduler hints in Nova flavors
+
+* On creation of new instance that requires volumes to be created, pass those
+  hints to Cinder (if v2 API is available/enabled)
+
+Alternatives
+------------
+
+There are no current alternatives supported in Nova at this time since no such
+volume info is passed to cinder when creating volumes.
+
+Data model impact
+-----------------
+
+No change to the current data model will be required since we will be using the
+existing extra_specs container.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+* No change to Nova deployment will be required. For this to work, Cinder v2
+  API must be available and Cinder must support the scheduler hints provided to
+  it.
+
+* If the v2 API is not available/enabled and cinder scheduler hints have been
+  set in a flavor used to boot an instance, an error will be raised.
+
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  hopem
+
+Other contributors:
+  None.
+
+Work Items
+----------
+
+* Should be possible to do all this in one patchset.
+
+
+Dependencies
+============
+
+* This change will depend on the Cinder V2 api in order to have any affect as
+  support for passing hints is a cinderclient v2 API operation.
+
+* Requires https://blueprints.launchpad.net/nova/+spec/support-cinderclient-v2
+
+
+Testing
+=======
+
+The primary test case here would be as follows:
+
+1. Configure cinder with > 1 storage backend and volume type.
+2. Create nova flavor with extra-spec container cinder scheduler hint.
+3. Nova boot with --block-device-mapping and --flavor from (2).
+4. Check that instance booted correctly and that auto-created vols are as
+   expected.
+
+
+Documentation Impact
+====================
+
+Nova flavor documentation should be updated to reflect this new support for
+cinder scheduler hints in extra_specs.
+
+
+References
+==========
+
+[1] https://blueprints.launchpad.net/cinder/+spec/scheduler-hints
+[2] https://blueprints.launchpad.net/nova/+spec/support-cinderclient-v2
diff --git a/specs/juno/proposed/add-support-for-cpu-hotadd.rst b/specs/juno/proposed/add-support-for-cpu-hotadd.rst
new file mode 100644
index 0000000..eb00c14
--- /dev/null
+++ b/specs/juno/proposed/add-support-for-cpu-hotadd.rst
@@ -0,0 +1,297 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================================================
+Add support for vcpu hotadd
+=================================================================
+
+https://blueprints.launchpad.net/nova/+spec/add-support-for-cpu-hotadd
+
+Cpu hotadd feature has been supported by qemu(since version 1.6.2)
+and libvirt(since version 1.2). This feature is useful for some users
+to scale the VM without reboot or migrate the instances.
+The blueprint aims to implement vCPUs hot-add feature in nova.
+
+Now only libvirt driver can support this function.
+
+Problem description
+===================
+
+Currently we can change the flavor and rebuild the instance to add vCPUs.
+But it is not support adding vCPUs to a running instance without rebuild
+to effect.
+
+We should have the ability to add vCPUs immediately without rebuild instances.
+
+Now only libvirt driver can support this function.
+The VMware Driver can support the feature in later blueprints.
+By the way, not all Linux kernels and Windows operating systems support it.
+It's dependent upon whether the guest OS supports it.
+
+Specifically, newer Linux kernels (please check the document for details),
+Windows Server 2012 (Standard and Enterprise Edition),
+Windows Server 2008 Datacenter Edition have support for CPU hot add.
+
+Proposed change
+===============
+
+QEMU version must be greater than or equal to 1.5.0,
+libvirt version must be greater than or equal to 1.1.0.
+
+Only the image contains 'qemu_guest_agent' in its metadata
+could support this function.
+
+1. Add a new extension API "hot_modify_vcpu_units"
+   which the owner and admin can use it.
+
+2. Add the "hotplug_vcpus" API in base virt driver.
+
+3. Add an extra key-value pair like "{'maxVCPUs': (int)maxNums}" into
+   extra_specs of flavor to define the instance's max vCPUs number.
+   The 'VCPUs' used in the original flavors actually indicate
+   the initial vCPUs number for creating instances in libvirt.
+   The 'maxNums' can not be smaller than 'VCPUs' in flavor
+
+   All instances using the flavors without this tag shall be processed as usual,
+   but they can not be applied the new feature
+
+   If the flavor's maxVCPUs is smaller than its vCPUs
+   (that means administrator sets the wrong configuration..),
+   The tag will be ignored, and the createVM/startVM request will be processed
+   like normal flavor's instance as usual
+
+4. Need to specify whether this image has installed 'qemu-guest-agent'
+   using image metadata, like 'hw_qemu_guest_agent=yes'.
+   Images without this tag will be raised 'NotSupportedException'
+   if the API is called.
+
+5. The 'vcpus' field in DB Instances table will be used to record
+   real vCPUs of instances. It aims to ensure vCPUs number correct
+   if instance is rebooted/started/rebuilt/evacuated and so on.
+   (That means all the functions above need to modify together in this BP.)
+   If one flavor does not have "maxVCPUs" tag in its extra_spec, the value of
+   "vcpus" equals to "VCPUs" in its flavor.
+
+6. The vCPUs calculation will be changed from "guest.vcpus = flavor.vcpus"
+   to "guest.vcpus = instance['vcpus']" in 'to_xml()'
+   for spawn()/hard_reboot()/rescue()/..
+
+7. Change one element in the config-xml of instance in libvirt driver
+   from "<vcpu>X</vcpu>" to "<vcpu current='X'>%maxNums%</vcpu>",
+   if 'maxNums' is specified in flavor.
+
+8. The 'get_vcpu_used()' in compute node for resource updating won't be changed.
+   Here we don't ensure all instances in the host has enough vCPUs to hotplug.
+   We still use the original 'dom.vcpus()' to count the current using vCPUs.
+   These two values are the same for normal old-style instances.
+
+9. The API 'DescribeInstance' needs to add one key-pair property
+   into the response body to show the current 'vCPUs' of this instance
+   which can be obtained from DB instances table.
+
+Alternatives
+------------
+
+1. Support image without 'qemu-guest-agent'
+
+In the above design, only the image installed 'qemu_guest_agent'
+can support this feature.
+
+Actually, if image doesn't contain qemu-guest-agent,
+after "hotplug_vcpus" action in libvirt, the user can execute
+some commands manually in the instance to activate the new plugged vCPUs:
+
+'echo 1 > /sys/devices/system/cpu/cpuX/online'
+
+* The 'X' means which vCPU you want to active.
+* The range of 'X' is from 0 to (amount of vCPUs - 1).
+* For example, the 'cpu0' means your instance only has one vCPU.
+* The count of the executions relies on the vCPUs number your instance has.
+
+The reason why this proposal isn't chosen is the manual operations is not
+friendly enough to end-users.
+
+* If normal images need to be supported, the API response ought to
+  give two different answers to end-users based on your image attribute,
+  like 'Your operation took effort.' or 'Your operation is applied, but you
+  need to execute some commands in your instance...'.
+
+* Moreover, if one instance plugged new 4 vCPUs,
+  end-user needs to execute the command four times.
+
+So this is not a good idea IMO.
+
+
+2. Add this feature for extending "resize" to "hot resize"
+
+The idea came from John Garbutt in reviewing on the PatchSet11.
+
+* Add one optional param like 'online=true'
+* Opt out the params which haven't been supported for hot pluging (mem, disk)
+  in resize when using the 'online' flag
+* Do an off-line resize to the same size that would double check the params.
+  Disk processing will be still the same it used to be.
+
+But now libvirt can only support cpu hot-plug,
+cpu hot-unplug & mem hot-plug/unplug has not been supported.
+
+Therefore, if I implement the feature into "resize",
+that means this new "hot resize" API can only support one "hotplug" feature.
+It is almost the same than the original "resize" function.
+
+The other reason I don't want to add the feature into resize(),
+is this feature needs some requirements(metadata, image..).
+So It's not a good idea to add more branches of judgement/execution
+in resize()'s logic.
+
+So I don't think this is a completed solution for "hot" resize.
+I paste the idea here to hear other reviewers' opinion.
+
+
+Data model impact
+-----------------
+
+The original 'vcpus' field in table Instances will be used
+to record current using vCPUs number of instances.
+
+
+REST API impact
+---------------
+
+1. Add one API "hot_modify_cpu_units"
+
+The rest API look like this in v2::
+ /v2/{project_id}/servers/{server_id}/action
+
+    {
+        'hot_modify_cpu_units':{
+        'vcpus':2
+        }
+
+    }
+
+and in v3 it is like::
+ /v3/servers/{server_id}/action
+
+    {
+        'hot_modify_cpu_units':{
+        'vcpus':3
+        }
+
+    }
+
+The 'vcpus' means the new vCPUs number you want to change to. No default value.
+
+The status code will be 200 when the request has succeeded.
+If the request fails, corresponding exception will be raised.
+The related information(like 'vcpus' in Instances table) will be roll back.
+
+* The 'vcpus' in body must be bigger than the current vCPUs number of instance.
+  If not, one exception like 'NotImplementedError' will be raised.
+
+* The 'vcpus' in body must be smaller than the 'maxVCPUs' param in flavor.
+  If not, one new exception like 'VCPUsLimitExceeded' will be raised.
+
+* All instances which flavor without 'maxVCPUs' will be raised
+  'NotSupportedException' if the API is called.
+
+* The instances which image without 'qemu_guest_agent' tag will be raised
+  'NotSupportedException' if the API is called.
+
+2. Modify API "DescribeInstance"
+
+Needs to add one optional key-pair property into the response body
+to show the current 'realVCPUs' of this instance.
+The value will not be None or '':
+
+{
+    'server':{
+    'currentVCPUs':4
+    }
+
+}
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+Needs to add two new types of notification:
+1. hot_modify_cpu_units.start
+2. hot_modify_cpu_units.end
+
+Other end user impact
+---------------------
+
+This function will be added into python-novaclient when this work finish.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+The policy for this API will be added into policy.json.
+The default value is 'rule:admin_or_owner'.
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Assignee: wingwj <wingwj@gmail.com>
+
+
+Work Items
+----------
+
+1. Code this function in v2
+2. Code this function in v3
+3. Finish tempest tests
+4. Code this function in python-novaclient
+
+
+Dependencies
+============
+
+Now only libvirt driver can support this function.
+
+1. QEMU >= 1.5.0
+2. libvirt >= 1.1.0
+
+
+Testing
+=======
+
+Unit tests and tempest tests will check "hot_modify_cpu_units" function.
+UTs for API/DB/libvirt driver will be examined separately.
+In integrated tests and tempest, it will test them together.
+
+Documentation Impact
+====================
+
+A description of this function will be added into Compute API V2
+and V3 Reference.
+
+
+References
+==========
+
+None
\ No newline at end of file
diff --git a/specs/juno/proposed/add-tags-for-os-resources.rst b/specs/juno/proposed/add-tags-for-os-resources.rst
new file mode 100644
index 0000000..0f6914d
--- /dev/null
+++ b/specs/juno/proposed/add-tags-for-os-resources.rst
@@ -0,0 +1,258 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================
+Add tags for OpenStack resources
+================================
+
+https://blueprints.launchpad.net/nova/+spec/add-tags-for-os-resources
+
+In this blueprint we aim to implement functionality adding tags for OpenStack
+resources.
+
+
+Problem description
+===================
+Currently, just the EC2 API of OpenStack support to add tags for objects.
+And now just support to tag for instance using "metadata" which instead of
+lists of strings (tags). It's inappropriate in a way, because "metadata" is
+the attributes of objects, "tags" is user-facing annotation of objects.
+
+Tags enable you to categorize your resources in different ways, for example,
+by purpose, owner, or environment. Once you add a tag(such as for purpose)
+to some resources, you can get the resources by the tag, and it's intuitive
+to manage the resources.
+
+So We should support for tags added to any searchable object in OpenStack
+Compute REST APIs.
+
+
+Proposed change
+===============
+
+Add tags REST APIs operations about CreateTag/DescribeTag/DeleteTag/UpdateTag
+
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+* Add "Tags" data model::
+
+   class Tags(BASE, NovaBase):
+    """Represents tags in the datastore."""
+    __tablename__ = 'tags'
+    __table_args__ = ()
+
+    id = Column(Integer, primary_key=True, nullable=False)
+    resource_id = Column(String(36), nullable=False)
+    resource_type = Column(String(36), nullable=False)
+    tag_value = Column(String(255), nullable=False)
+    user_id = Column(String(255))
+    project_id = Column(String(255)
+
+
+REST API impact
+---------------
+
+* CreateTag
+     * POST
+     * Normal Response Code: 200
+     * Expected error http response code(s)
+           - 400: Invalid parameters
+           - 403: Not authorized
+           - 404: Resource not found
+           - 500: Not implement to tag for this resource type
+     * v2|v3/{project_id}/tags
+     * Sample request for v2|V3::
+
+        {"tags": {
+            "tag_value": ["tag1", "tag2"],
+            "resources": [{
+                              "type": "instance",
+                              "resource_ids": ["ins1", "ins2"]
+                          },
+                          {
+                              "type": "flavor",
+                              "resource_ids": ["fla1",]
+                          }
+            ]
+        }}
+     * Sample response for v2|V3::
+
+        {"tags": [{
+                      "id": "id1"
+                      "tag_value": "tag1",
+                      "type": "instance",
+                      "resource_id": "ins1"
+                  },
+                  {
+                      "id": "id2"
+                      "tag_value": "tag1",
+                      "type": "instance",
+                      "resource_id": "ins2"
+                  },
+                  {
+                      "id": "id3"
+                      "tag_value": "tag1",
+                      "type": "flavor",
+                      "resource_id": "fla1"
+                  },
+                  {
+                      "id": "id4"
+                      "tag_value": "tag2",
+                      "type": "instance",
+                      "resource_id": "ins1"
+                  },
+                  {
+                      "id": "id5"
+                      "tag_value": "tag2",
+                      "type": "instance",
+                      "resource_id": "ins2"
+                  },
+                  {
+                      "id": "id6"
+                      "tag_value": "tag2",
+                      "type": "flavor",
+                      "resource_id": "fla1"
+                  }]
+        }
+* DescribeTag
+     * GET
+     * Normal Response Code: 200
+     * Expected error http response code(s)
+           - 404: Tag not found
+     * Filter by tag_value/resource_type/resource_ids
+     * v2|v3/{project_id}/tags?tag_value='tag1'&resource_type='instance'
+     * Sample response for v2|V3::
+
+        {"tags": [{
+                      "id": "id1"
+                      "tag_value": "tag1",
+                      "type": "instance",
+                      "resource_id": "ins1"
+                  },
+                  {
+                      "id": "id2"
+                      "tag_value": "tag1",
+                      "type": "instance",
+                      "resource_id": "ins2"
+                  },
+                  {
+                      "id": "id3"
+                      "tag_value": "tag1",
+                      "type": "instance",
+                      "resource_id": "ins3"
+                  }]
+        }
+* UpdateTag
+     * PUT
+     * Normal Response Code: 200
+     * Expected error http response code(s)
+           - 404: Tag not found
+     * v2|v3/{project_id}/tags/{tagId}
+     * Sample request for v2|V3::
+
+        {"tag_value": "new_tag"}
+
+     * Sample response for v2|V3::
+
+        {"tags": {
+                      "id": "id1"
+                      "tag_value": "new_tag",
+                      "type": "instance",
+                      "resource_id": "ins1"
+                  }}
+
+* DeleteTag
+     * DELETE
+     * Normal Response Code: 202
+     * Expected error http response code(s)
+           - 403: Not authorized
+           - 404: Tag not found
+     * v2|v3/{project_id}/tags/{tagId}
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+* Add tags.create to notification
+* Add tags.delete to notification
+
+
+Other end user impact
+---------------------
+
+Python-novaclient should support the functionality:
+  tag-create/tag-delete/tag-describe/tag-update
+
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  huangtianhua <huangtianhua@huawei.com>
+
+
+Work Items
+----------
+
+* Add REST APIs for v3
+* Add tempest tests for the APIs
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Unit tests and tempest tests will check these functions.
+
+
+Documentation Impact
+====================
+
+A description of this function will be added into Compute API V3 Reference.
+
+
+References
+==========
+
+* http://osdir.com/ml/openstack-dev/2014-04/msg01900.html
diff --git a/specs/juno/proposed/add-transport-support-to-iscsi.rst b/specs/juno/proposed/add-transport-support-to-iscsi.rst
new file mode 100644
index 0000000..83e051c
--- /dev/null
+++ b/specs/juno/proposed/add-transport-support-to-iscsi.rst
@@ -0,0 +1,131 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Add transport support to iscsi
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/add-transport-support-to-iscsi
+
+Refactor ISCSIDriver to support other iSCSI transports besides TCP
+which will allow minimal changes to support iSER.
+
+Problem description
+===================
+
+Currently, Vendors who have their iSCSI driver, and want to add RDMA transport
+cannot leverage their existing plug-in driver which inherit from iSCSI
+And must modify their driver or create an additional plug-in driver which
+inherit from iSER, and copy the exact same code.
+
+Proposed change
+===============
+
+On the initiator side the only difference between TCP and RDMA is in the
+interface flag (--interface=[iface])
+
+e.g. "iscsiadm -m discoverydb -t st -p ip:port -I iser --discover"
+
+The required changes are:
+
+* Add a parameter called "enable_rdma_by_default=False" to enable rdma.
+  The operation would mean try RDMA and if it fails fall back to TCP.
+* Integrate current iser methods (_get_host_device, etc') to return both
+  iscsi and iser device paths lists.
+* The existing ISER code will be removed.
+
+Alternatives
+------------
+
+Currently, there's an ISER subclass in iscsi for Cinder (starting Havana)
+I think that the right/better approach is as suggested by this blueprint.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+iSER (iSCSI over RDMA) allow 5x faster bandwidth compared to using iSCSI TCP.
+
+Other deployer impact
+---------------------
+
+If we just removed LibvirtISERVolumeDriver it would break existing deployment.
+Instead, this will be deprecated in Juno, and we will keep Juno compatible
+with Icehouse configurations by having One cycle where Nova prints a
+deprecation warning upon use of this class, before it can be deleted.
+
+At the beginning of the class add a log print, e.g.:
+LOG.warning(_('The LibvirtISERVolumeDriver is now deprecated and will be
+removed in the Juno release.'))
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Assignee: shlomis <shlomis@mellanox.com>
+
+Work Items
+----------
+
+* Remove iser from 'libvirt_volume_drivers' here:
+  ./nova/virt/libvirt/driver.py
+* Remove iser code and opts.
+  Add 'enable_rdma_by_default' and implementation here:
+  ./nova/virt/libvirt/volume.py
+
+Dependencies
+============
+
+Cinder blueprint:
+https://blueprints.launchpad.net/cinder/+spec/add-transport-support-to-iscsi
+
+Testing
+=======
+
+The same as for iSCSI TCP
+
+Documentation Impact
+====================
+
+The new configuration parameter (enable_rdma_by_default) and the deprecation
+of the iser volume driver should be documented
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/add-usb-controller.rst b/specs/juno/proposed/add-usb-controller.rst
new file mode 100644
index 0000000..5793342
--- /dev/null
+++ b/specs/juno/proposed/add-usb-controller.rst
@@ -0,0 +1,162 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================
+Add support for USB controller
+==============================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/add-usb-controller
+
+Users have requirements of using USB device, the detailed information can
+refer to BP in
+https://blueprints.launchpad.net/nova/+spec/usb-passthrough.
+
+If not specify appropriate USB controller for USB device, USB device will
+use the default piix3-usb-uhci, which results in some problems.
+
+1. The low speed of USB device.
+
+2. If use spice client to redirect USB device to VM, the mismatched speed may
+prevent the connection.
+
+USB 2.0 has other good point that it will result in dramatically lower CPU
+usage when the USB tablet is present for VNC/SPICE.
+
+As described above, I think that support USB 2.0 controller is valuable in
+Openstack.
+
+
+Problem description
+===================
+
+Use cases:
+
+1. User creates a VM, the system creates a default ehci USB controller,
+attach USB tablet to the ehci controller.
+
+Proposed change
+===============
+
+1. Add function of create ehci controller in libvirt driver when create
+VM.
+
+Details:
+
+Use object 'LibvirtConfigGuestController' for USB controller in libvirt
+driver, create ehci xml in 'get_guest_config' function.
+
+The xml may like:
+
+<controller type='usb' index='1' model='ehci'/>
+
+All values of above properties are constant.
+
+2. Add function of specify USB controller for USB tablet in libvirt driver
+when create VM.
+
+Details:
+
+Add 'address' element to USB tablet object 'LibvirtConfigGuestInput',
+specify ehci controller in 'get_guest_config' function.
+
+The xml may like:
+
+<input type='tablet' bus='usb'>
+
+<address type='usb' bus='1' port='1'/>
+
+</input>
+
+The values of address are constant.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+USB 2 / USB 3 controllers will improve guest performance as compared to USB 1
+controller. eg if we plug USB tablet into a USB2 controller,  guest CPU usage
+will be much reduced.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <Jing Yuan>
+
+
+Work Items
+----------
+
+Step 1: Create ehci xml in 'get_guest_config' function.
+
+Step 2: Modify USB tablet object, specify ehci controller for USB tablet
+in 'get_guest_config' function.
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+https://blueprints.launchpad.net/nova/+spec/usb-passthrough
diff --git a/specs/juno/proposed/add-useful-metrics.rst b/specs/juno/proposed/add-useful-metrics.rst
new file mode 100644
index 0000000..cfa8431
--- /dev/null
+++ b/specs/juno/proposed/add-useful-metrics.rst
@@ -0,0 +1,200 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================
+Add useful metrics into Utilization Based Scheduling
+====================================================
+
+https://blueprints.launchpad.net/nova/+spec/add-useful-metrics
+
+Add some useful metrics into Utilization Based Scheduling (UBS), such as
+network monitor and power consumption monitor.With them, UBS has more options
+to do scheduling based on those metrics by setting the ratio of each metric
+in weigher.
+
+Problem description
+===================
+
+The framework for Utilization Based Scheduling has already been
+implemented in Icehouse:
+https://blueprints.launchpad.net/nova/+spec/utilization-aware-scheduling
+
+And some of the monitors like CPU monitor were implemented under the
+above blueprint. That is users can specify the VMs to be launched on
+those hosts whose CPU utilization is relatively idle.
+
+For other useful monitors -
+Network monitor: network environment e.g. network traffic of a host is
+also another aspect the scheduler should be aware of by network bandwidth.
+If the VMs on a host are all running network sensitive applications now
+such as Web servers and FTP servers, it makes sense that a new user raises a
+request to avoid running a new network-sensitive VM on that busy host.
+
+Power monitor: suppose the scheduler needs to choose a host to
+launch a new VM and the power of the hosts are collected as 100Watts,
+120Watts and 150Watts. A filter can be made to filter out all the hosts with
+higher power consumption to balance the power consumption of all hosts, or
+filter out all the hosts with lower power consumption to save power -
+consolidate to run VMs on a host and avoid additional power-on of the others.
+
+Another example is power capping. Users can use filters to set a power
+threshold for all hosts in the data center, 300Watts for instance for server
+protection. In that case, all hosts whose power consumption is higher than
+300Watts are filtered out when the scheduler chooses the available hosts.
+
+Later on, weigher can be made to choose the hosts with lower power consumption
+to balance the power consumption of all hosts, or choose the hosts with higher
+power consumption to save power.
+
+The plan is to add network/power monitor or other metrics into
+the UBS framework to enable the capability for useful use cases.
+
+Proposed change
+===============
+
+Implement those monitors to monitor those useful metrics under
+nova/compute/monitors.
+
+So far, we hope to add network monitor, and power monitor.
+
+Network monitor: the network monitor collects data about network, for instance,
+network bandwidth, the rates of packets sent or received, etc., in case that a
+VM cares more on network environment of the host.
+
+Power monitor:
+Some background is:
+Intel node manager is a server management technology that allows management
+software to accurately monitor and control the platform's power and thermal
+behaviors through industry defined standards: Intelligent Platform Management
+Interface (IPMI) and Data Center Manageability Interface (DCMI).
+
+It allows the IT admin to monitor the power and thermal behaviors of servers
+and make reasonable operations or policies according to the real-time
+power/thermal status of data center. Using Intel Node Manager, the power and
+thermal information of those servers can be used to improve overall data
+center efficiency and maximize overall data center usage. Data center
+managers can maximize the rack density with the confidence that rack power
+budget won't be exceeded.
+
+The monitor could read the power consumption from Intel node manager via IPMI.
+It facilitates power management, energy saving and others when scheduling VMs.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+A user will be able to set ratios of those useful metrics to specify how the
+metrics are going to be weighed, including the existing CPU utilization,
+ongoing power consumption of the host, to pick up a host where an instance
+could be created.
+
+Performance Impact
+------------------
+
+The monitors which were added in Icehouse and are going to be added are not
+enabled by default. When users are interested in that utilization data, the
+config file needs to be changed to enable the monitor. So by default, there
+is no performance impact.
+
+In Icehouse, there is an additional column which was added in table
+ComputeNode as a JSON blob to save utilization data after a hot discussion
+in the community. In this blueprint in Juno, we have no plan to change any
+database schema, and we are taking advantage of existing update mechanism
+in resource tracking to load the monitors and poll in Icehouse.
+
+Other deployer impact
+---------------------
+
+There already exists a configuration option "compute_monitors" under the
+[DEFAULT] section. By default, it is commented out. And its default value
+is []. If the monitor is expected to be used, we should add it into that.
+That is, "compute_monitors=monitor1,monitor2".
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  edwin-zhai
+
+Other contributors:
+  lianhao-lu
+  shane-wang
+
+Work Items
+----------
+
+1) Implement the base class of Node Manager
+2) Power consumption monitor
+3) Network monitor
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Network monitor is workable on each host. No specific test needed for
+network monitor, except unit tests.
+
+For power consumption monitor, they require to work with IMPI, and run
+on Intel platforms so far with Intel node manager.
+We plan to run them and test whether schedulers can use that data to do
+scheduling with weighers, before submitting the patches.
+
+Later on, we hope to set up a third party CI testing to make sure each further
+commit doesn't break the monitors, which means the monitors still can collect
+data any time.
+
+Documentation Impact
+====================
+
+The description of the new monitor options need to be added into the
+description of the existing "compute_monitors" configuration option
+as new possible values.
+
+References
+==========
+
+http://www.intel.com/content/www/us/en/servers/ipmi/ipmi-home.html
+http://www.intel.com/content/www/us/en/data-center/dcmi/data-center-manageability-interface.html
+http://wiki.openstack.org/wiki/User_talk:Fengqian#Why_and_how_to_use_Intel_Node_Manager_in_OpenStack
+http://review.openstack.org/#/c/65631/
+http://review.openstack.org/#/c/64403/
+http://review.openstack.org/#/c/64404/
diff --git a/specs/juno/proposed/add-utilization-based-weighers.rst b/specs/juno/proposed/add-utilization-based-weighers.rst
new file mode 100644
index 0000000..2089ac8
--- /dev/null
+++ b/specs/juno/proposed/add-utilization-based-weighers.rst
@@ -0,0 +1,204 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Add utilization based weighers
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/add-utilization-based-weighers
+
+Utilization Based Scheduling (UBS) has implemented a MetricsWeigher to
+set the ratios for different metrics to specify how much they are considered
+when scheduling. Even though it is flexible, it is not easy for admins to
+use it because probably admins don't know how many metrics are available,
+what each metric exactly means, and how to treat them usually.
+The plan is to create a predefined weighers as samples from advanced
+scheduling algorithms based on the experimental results.
+For instance, MyCPUUtilWeigher inheriting from MetricsWeigher can consider CPU
+load, CPU utilization percentage, and CPU frequency if needed, and we set
+proper ratios on them.
+
+Problem description
+===================
+
+Utilization Based Scheduling (UBS) has implemented a MetricsWeigher to
+set the ratios for different metrics to specify how much they are considered
+when scheduling. For instance, at least admins need to set:
+
+- weight_setting, which means how the metrics are going to be weighed. This
+  should be in the form of "<name1>=<ratio1>, <name2>=<ratio2>, ...", where
+  <nameX> is one of the metrics to be weighed, and <ratioX> is the
+  corresponding ratio.
+
+Additionally, admins need to add <name1>, <name2>, ... into the nova.conf file
+in order that the resource tracker will load the corresponding monitors to
+support collecting different metrics.
+
+    [metrics]
+    weight_setting = name1=1.0, name2=-1.0
+
+However, for CPU, admins probably don't know which metrics are for CPU and how
+they are going to tream them by proper ratios.
+
+Proposed change
+===============
+
+In our plan, we hope to create a predefined weigher whose weight_setting
+and others are set based on some experimental results. Then users just need
+to set weight_multiplier, like they do for RAMWeigher in the current Nova.
+
+In the implementation, the weighers are going to be implemented as python
+classes inheriting from class MetricsWeigher.
+
+And we hope to generate a sample nova.conf being filled in proper monitors
+to get those metrics if possible, or explain that in a README file.
+
+For example, firstly, if an admin cares CPU utilization for a period of time
+and doesn't want to schedule its VM on an "always-busy" host. We could figure
+out that CPU load would be an important aspect to measure by experiments.
+Specifically, 5-minute CPU load is more important and expected to be used by
+users. The value of 5-min CPU load is between 0 and 1, which is collected by
+the CPU monitor. 0.00 means relatively idle, 1.00 means relatively busy in the
+last 5 minutes.
+
+Secondly, let's consider CPU frequency which is also got from the CPU monitor.
+Suppose that the admin divides all hosts into 3 types by CPU frequency simply:
+fast(>2GHz), medium(1-2GHz), and slow(<1GHz), and sets scores 1.0, 0.5, and 0
+respectively. That's,
+
+    Normalized CPU frequency = 1.0 if its frequency > 2GHz
+                               0.5 if its frequency is between 1-2GHz
+                               0 if its frequency < 1GHz.
+
+Or, we also can do normalization on CPU frequency to make its
+values to be between 0 and 1 if needed by
+
+    Normalized CPU frequency =
+    (max frequency of all hosts- its real frequency)
+    / (max frequency of all hosts - min frequency of all hosts)
+
+Thirdly, let's consider CPU utilization.
+For CPU utilization, suppose the user doesn't care a lot on it because it
+stands for CPU usage at a time instead of the history and the future.
+
+Those above are all the aspects which MyCPUUtilWeigher should consider.
+So we can set the following weight_setting for admins to use.
+
+<5-min CPU load>=<-0.6>
+
+Explanation: We care the 5-min CPU load the most and set to 0.6, but
+"-" means less is better because less means idle.
+
+<Normalized CPU frequency>=<0.3>
+
+Explanation: We also care the CPU frequency of the host hope to launch VMs on
+fast servers, there is no "-" means more is better because more is faster.
+
+<CPU utilization>=<-0.1>
+
+Explanation: We don't care a lot on the current CPU utilization because it
+doesn't stand for the history and the future but now. So set it to be 0.1.
+And "-" means less is better because higher CPU utilization means busier.
+
+With that, the cloud admin can set the weigher in the nova.conf file by
+ram_weight_multiplier = 0.0
+mycpuutil_weight_multiplier = 1.0
+
+All of the above for MyCPUUtilWeight is only an assumption as an example,
+but the final weigher should be based on the experiments by tuning the ratios
+and comparing with the expections on scheduling results. Certainly, with that,
+users are still able to use MetricsWeigher to customize their own weighers.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Users should have more meaningful weighers to use, which is more convenient.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  shane-wang
+
+Other contributors:
+  lianhao-lu
+
+Work Items
+----------
+
+We plan to implement CPUUtilWeigher first since CPU monitor has been merged.
+
+Dependencies
+============
+
+If we hope to implement more meaningful weighers based on MetricsWeigher.
+We have to depend on the approval and the implementation of
+blueprint add-useful-metrics whose nova spec proposal is at
+https://review.openstack.org/#/c/89766/.
+
+If not, no dependency.
+
+Testing
+=======
+
+Some specific weighers may need specific monitors which are supported on the
+3rd party hardware platforms, such as power consumption monitor.
+
+The 3rd party testing is to make sure each commit doesn't break the capability
+of each monitor to collect metric data.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/api-microversions-alt.rst b/specs/juno/proposed/api-microversions-alt.rst
new file mode 100644
index 0000000..93227d3
--- /dev/null
+++ b/specs/juno/proposed/api-microversions-alt.rst
@@ -0,0 +1,374 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+API Microversions (Alternate Proposal)
+==========================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/api-microversions
+
+(This is in no way a complete spec covering this space, but it's
+trying to narrow in on a few things.)
+
+Problem description
+===================
+
+As a community we are really good at evolving interfaces and code over
+time via incremental development. We've been less good at giant big
+bang drops of code. The Nova API has become sufficiently large, and
+constantly growing through new extensions, that it's not likely to be
+able to ever do a new major version of the API.
+
+At the same time the escape hatch that we've provided for allowing
+innovation in the API, through adding extensions, has grown to the
+point where we now have extensions to extensions, under the assumption
+that the extension list is a poor man's versioning mechanism. While
+true, this has let to pain and debt, and prevents us from making
+certain changes, like deprecating pieces of the API that are currently
+non sensible (or flat out broken).
+
+We must come up with a better way that serves the following needs:
+
+- Makes it possible to evolve the API in an incremental manner, which
+  is our strength as a community
+- Provides backwards compatability for consumers
+- Provides cleanliness in the code to make it less likely that we'll
+  do the wrong thing and break the world.
+
+A great interface is one that goes out of it's way to make it hard to
+use incorrectly. A good interface tries to be a great interface, but
+bends to the realties of the moment.
+
+
+Proposed change
+===============
+
+We have 3 main concerns:
+
+- How will the end users use this, and how to we make it hard to use
+  wrong
+- How will the code be internally structured, and how do we make it
+  easy to see in code that you are about to break the world (not just
+  from testing, but from code structure).
+- How will we test this in integration. And what limits does that
+  impose.
+
+
+Type of versioning
+------------------
+
+The base proposal provides options, and a hybrid approach. I believe
+this is too complicated and confusing to get right. In complexity lie
+dragons, also giant sloths, equally dangerous, but slower to emerge.
+
+(For the purposes of the following discussion "the API" is all core
+and optional extensions in the upstream Nova tree.)
+
+Versioning of the API should be 1 monotonic counter. It should be in
+the form X.Y.Z:
+
+- X: any backwards incompatible change (that includes removal of parameters)
+  This X will bump by each incompatible change.
+  However if the biggest X is experimental at the time we don't need to bump X.
+- Y: backs compatible changes
+  This Y will bump by each compatible change.
+- Z: critical bug fixes on stable branches
+  This Z will bump by each bug fix. Z might be always 0 and just reserved.
+
+So the versioning does not depend on the release boundary(Juno, etc.).
+Each X version grows on its own versioning and each X has its own Y.Z.
+For example, there are three X versions like:
+
+- 2.100.0: backwards compatible changes have happened 100 times.
+- 3.14.0: backwards compatible changes have happened 14 times.
+- 4.0.0: backwards compatible changes have not happened yet.
+
+A version response would look as follows
+
+::
+
+    GET /
+    {
+        "versions": [
+            {
+                "id": "v2.0",
+                "links": [
+                      {
+                        "href": "http://localhost:8774/",
+                        "rel": "self"
+                    }
+                ],
+                "status": "CURRENT",
+                "updated": "2011-01-21T11:33:21Z"
+                "version": "2.100"
+                "min_version": "2.88"
+            },
+            {
+                "id": "v3.0",
+                "links": [
+                      {
+                        "href": "http://localhost:8774/",
+                        "rel": "self"
+                    }
+                ],
+                "status": "EXPERIMENTAL",
+                "updated": "2011-01-21T11:33:21Z"
+                "version": "3.14"
+                "min_version": "3.0"
+            }
+        ]
+    }
+
+This specifies the min and max version that the server can
+understand. min_version will start at 2.0 representing the current 2.0
+API. It may eventually be uplifted if there are support burdens we
+don't feel are adequate to support. For instance if version 2.1 was
+the drop of the XML API, at some point in the future 2.1 would be
+the minimum version provided.
+
+Client Interaction
+------------------
+
+A client specifies the version of the API they want via the following
+approach, a new header::
+
+  X-OS-Compute-Version: 2.114, experimental, vnd:rax
+
+This conceptually acts like the accept header, with some nuance.  We
+introduce 3 concepts here, global API version, experimental flag, and
+vendor flags.
+
+Semantically:
+
+- if X-OS-Compute-Version is not provided, act as if min_version was
+  sent.
+- if X-OS-Compute-Version is sent, respond with the API at that
+  version. If that's outside of the range of versions supported,
+  return 406 (or some other future determined appropriate error).
+- if X-OS-Compute-Version: latest (special keyword) return Max
+  Version response.
+- if experimental is sent, return results with *all* experimental
+  extensions enabled.
+- if vnd:VENDORNAMESPACE is sent, return results that include that out
+  of tree vendor extensions.
+
+This means out of the box, with an old client, an OpenStack
+installation will return vanilla OpenStack responses at v2. The user
+or SDK will have to ask for something different.
+
+Experimental and vendor, by being virtue of out of tree, do not
+benefit from versioning. It's an all or nothing affair. There are less
+unique snowflakes in the world than people think, and we actually want
+these things back in tree.
+
+Nova Tree
+---------
+
+In the nova tree, there are two ways to implement different microversion
+functions.
+
+- The way 1: A decorator would be introduced to absorb the differences
+  between microversions. (This way is already described on v2-on-v3-api)
+
+::
+
+   servers.py:
+
+   @translate_body(version="2", diff_v2)
+   def show(...):
+       """this method is for v3 and there are differences of response body.
+          diff_v2 represents the differences"""
+       ....
+
+- The way 2: A decorator would be introduced to label routing functions
+  version specification like the following sample code.
+  @api.version also supports experimental=True/False (default
+  False). And vnd="". It's a fatal error to combine either of those
+  flags with the version flag.
+  This will provide an in tree way of signaling when new methods come
+  into play, as well as a selection for routing requests to different
+  paths based on versions allowed.
+  (Note: yes this is a *ton* of new work, but for long term tree sanity
+  I think we need it).
+
+::
+
+   servers.py:
+
+   @api.version(introduced="2.0")
+   def show(...):
+       ....
+
+   @api.version(introduced="3.0")
+   def show_v3(...)
+       """A bigger badder index listing"""
+       ....
+
+Basically, we need to implement microversion functions with the way 1
+as possible to avoid copy&paste code and reduce meintenance cost.
+However sometimes we need the way 2 for avoiding spaghetti code if there
+are big differences between microversions. So we can choose either way
+by considering the balance for each API.
+
+Alternatives
+------------
+
+See non-alt proposal
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+Additional version information added to GET / even in the base case,
+it should be minimally disruptive.
+
+Otherwise the whole proposal is basically REST impact
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+SDK authors will need to start using the X-OS-Compute-Version header
+to get access to new features. The fact that new features will only be
+added in new versions will encourage them to do so.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Discuss things that will affect how you deploy and configure OpenStack
+that have not already been mentioned, such as:
+
+* What config options are being added? Should they be more generic than
+  proposed (for example a flag that other hypervisor drivers might want to
+  implement as well)? Are the default values ones which will work well in
+  real deployments?
+
+* Is this a change that takes immediate effect after its merged, or is it
+  something that has to be explicitly enabled?
+
+* If this change is a new binary, how would it be deployed?
+
+* Please state anything that those doing continuous deployment, or those
+  upgrading from the previous release, need to be aware of. Also describe
+  any plans to deprecate configuration values or features.  For example, if we
+  change the directory name that instances are stored in, how do we handle
+  instance directories created before the change landed?  Do we move them?  Do
+  we have a special case in the code? Do we assume that the operator will
+  recreate all the instances in their cloud?
+
+Developer impact
+----------------
+
+Discuss things that will affect other developers working on OpenStack,
+such as:
+
+* If the blueprint proposes a change to the driver API, discussion of how
+  other hypervisors would implement the feature is required.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Who is leading the writing of the code? Or is this a blueprint where you're
+throwing it out there to see who picks it up?
+
+If more than one person is working on the implementation, please designate the
+primary author and contact.
+
+Primary assignee:
+  cyeoh-0
+
+Other contributors:
+  <launchpad-id or None>
+
+Work Items
+----------
+
+Work items or tasks -- break the feature up into the things that need to be
+done to implement it. Those parts might end up being done by different people,
+but we're mostly trying to understand the timeline for implementation.
+
+
+Dependencies
+============
+
+* Include specific references to specs and/or blueprints in nova, or in other
+  projects, that this one either depends on or is related to.
+
+* If this requires functionality of another project that is not currently used
+  by Nova (such as the glance v2 API when we previously only required v1),
+  document that fact.
+
+* Does this feature require any new library dependencies or code otherwise not
+  included in OpenStack? Or does it depend on a specific version of library?
+
+
+Testing
+=======
+
+Historically API stability in Nova was really only maintained because
+Tempest covered such a large portion of the API. That will not be
+realistic for all versions possible here.
+
+Nova unit tests will need to be enhanced to do most of the heavy
+lifting.
+
+When new tests are added to Tempest, they will specify the minimum
+version they function on (we are already starting down that path for
+novaclient testing).
+
+Tempest will run Nova tests twice during a run. At min_version
+(escentially specifying no OS-Compute-Version), and at at version
+detected by the setup tool (currently devstack) for the max version
+Nova supports in the branch in question.
+
+Scenario tests will occur at max version.
+
+Documentation Impact
+====================
+
+What is the impact on the docs team of this change? Some changes might require
+donating resources to the docs team to have the documentation updated. Don't
+repeat details discussed above, but please reference them here.
+
+
+References
+==========
+
+Please add any useful references here. You are not required to have any
+reference. Moreover, this specification should still make sense when your
+references are unavailable. Examples of what you could include are:
+
+* Links to mailing list or IRC discussions
+
+* Design summit session https://etherpad.openstack.org/p/juno-nova-v3-api
+
+* Recent discussions:
+  https://wiki.openstack.org/wiki/Nova/ProposalForAPIMicroVersions
diff --git a/specs/juno/proposed/api-microversions.rst b/specs/juno/proposed/api-microversions.rst
new file mode 100644
index 0000000..028574f
--- /dev/null
+++ b/specs/juno/proposed/api-microversions.rst
@@ -0,0 +1,740 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+API Microversions
+==========================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/api-microversions
+
+Over the long term we need a way to be able to introduce changes to
+the REST API to both fix bugs and add new features, in a more
+controlled and efficient way.
+
+Some of these may be backwards incompatible. But this needs to be
+done in a way which both deployers and users of the REST API are able
+to reasonably manage the changes which will occur.
+
+At the same time, we really would like a way to introduce new APIs
+as "experimental", have no backwards compatibility guarantees until
+we are happy the API is stable and well tested. Note, a client will
+have to explicitly request the "experimental" version.
+
+Problem description
+===================
+
+Over time we need to be able to make changes to the Nova REST API some
+of which are backwards compatible and others which are backwards
+incompatible. We need a way to simultaneously support cloud operators
+and users of the REST API who want the old behaviour as well as those
+who want the newer behaviour, sometimes in the same deployment.
+
+A simple backwards compatible change may be the addition of an extra
+parameter in a response to a query. In a more complicated change a
+change may be wanted such that a request parameter is no longer valid
+or the meaning of what the parameter being specified changes.
+
+In both of these examples we would like to have a mechanism where the
+user of the API is able to specify how they would like the REST API to
+behave. There also needs to be a default behaviour if no explicit
+request is made by the API caller.
+
+
+What does the version mean?
+---------------------------
+
+So each API version is one of:
+
+* stable: will be available forever
+
+* experimental: may be removed, at any time, and might be changed
+  in a backwards compatible way, as any time
+
+In addition, each API version is also one of:
+
+* backwards compatible with existing clients of previous stable versions.
+  Changes that are backwards compatible include the addition or return values,
+  or extra parameters could be added, and error return codes may have changed.
+
+* not backwards compatible with existing clients, so return values
+  may have been removed, or renamed, success return codes may have
+  changed, and the semantics of the return values may have changed.
+
+What the client needs
+---------------------
+
+From the API user perspective, it needs to:
+
+* know if the feature it needs is available, i.e. needs to know the
+  specific version of an extension
+
+* ensure it gets the data it is looking for, in the format it expects.
+  We can assume json clients don't mind if they get extra data
+
+So the client will request the version major version it wants, so it
+get the data in a compatible form. But it can query the specific
+extension to find if that is value. Basically it request the "latest"
+version, but the "latest" in a stream of backwards compatible versions.
+
+Older clients, will continue to request the API in the same way they
+have always done, so they will always get the "original" format of
+the API.
+
+Note: we could make clients request a specific version of the API
+extension. Howerver, if the changes are backwards compatible, the
+client shouldn't break if given the latest version in the same
+compatible services of versions. This also simplifies the server
+side filters of API versions.
+
+What the server needs
+---------------------
+
+The server needs to know what format to output, so:
+
+* if no version is requested, we default to the old format
+* if a version is requested, that format is supported
+
+The client will request the version using Accepts headers,
+but this is covered in more detail in the proposed changes section.
+
+What developers need
+--------------------
+
+Currently, updating an existing extension to return extra values is extremely
+heavy weight for developers, and doesn't really help clients very much.
+
+When we add a new value into the stable API, that value must stay there
+forever. Similarly, adding a new stable API extension, we probably have
+to maintain that forever.
+
+Given people deploy from trunk, we effectively have to maintain a compatible
+API from each commit to the next. This really slows down the review of API
+changes, because it is so expensive when we get it wrong.
+
+This spec proposes that we add APIs in a very explicitly "experimental" form.
+The main aim is to allow us to evolve new APIs within the nova tree.
+
+The idea with the experimental version is that it could be removed,
+rather than becoming stable in a future release. The idea being that we can
+try out things, and get them tested, before we make it stable.
+
+Out of tree API extensions
+--------------------------
+
+Officially, we provide zero support for out of tree extensions.
+
+However, we want interoperability between all OpenStack clouds, so we
+should have a way for vendors to iterate quickly, and be able to upstream
+their changes.
+
+To do this, lets have vendor out of tree extensions be presented as
+experimental APIs, that may be altered when they are upstreamed,
+and are not guaranteed to be stable until they are upstreamed.
+
+
+Proposed change
+===============
+
+There are two main proposals for micro visioning:
+
+* version the whole API as one
+
+* version every API extension individually
+
+This proposal is a third way, where we have both a global version,
+and a per extension version. This works in a similar to how we version
+our RPC interfaces.
+
+The global version helps us:
+
+* encourage a level of consistency between all the API extensions
+
+* allow clients can request a simple single version that represents the
+  latest version of all plugins, at the point of that client being
+  written
+
+But also having a version for each extension means:
+
+* client writers can tell which extensions have actually changed
+
+
+What does the version number mean?
+----------------------------------
+
+The global version will be of the form: "X.Y.Z" or "X.Y" (where Z=0)
+and X Y Z are all integers, except Z can also be "beta"
+
+* Z changes for bug fixes, on stable branches
+
+* Y changes for any backwards compatible changes to the API
+
+* X changes for any backwards incompatible changes to the API
+
+* if Z = "beta" then it is an experimental version
+
+Any time any extensions changes, the global number increments, and the
+changed extension uses that new version as its version number.
+
+Now each extension has the following properties:
+
+* supported_versions (full list of all versions, in order,
+  including experimental version, if any)
+* introduced
+* last_changed
+* deprecated (?)
+
+Globally we have the following properties:
+
+* latest stable versions (i.e. v2 latest and v3 latest, once its stable)
+
+How do you request a specific version
+-------------------------------------
+
+If just want the latest stable version, then you can request:::
+
+  Accept: application/vnd.openstack-org.compute-v2+json
+
+In the server response you get the latest version where X=2 for the
+specific extension you are talking to, for example:::
+
+  Content-Type: application/vnd.openstack-org.compute-v2.24+json
+
+To make backports work, if the client requests a specific stable version:::
+
+  Accept: application/vnd.openstack-org.compute-v2.123+json
+
+You might get that version with a bug fix, say v2.123.2, but it
+should still just report:::
+
+  Content-Type: application/vnd.openstack-org.compute-v2.123+json
+
+If you request a stable version, and our extensions does have that version
+you just get the stable version that was before the requested version. So
+it is matching on 2.A where A > 123. So you request this:::
+
+  Accept: application/vnd.openstack-org.compute-v2.123+json
+
+And you might get this version returned:::
+
+  Content-Type: application/vnd.openstack-org.compute-v2.22+json
+
+But you must always request experimental versions directly:::
+
+  Accept: application/vnd.openstack-org.compute-v2.123.beta+json
+
+If an extension doesn't implement that exact version, it responses with
+the error 406 (Not Acceptable).
+
+If the client doesn't send an Accept header, then we return the default
+version, which is basically the version that would have been returned
+before we introduced this versioning.
+
+Versioning Examples
+-------------------
+
+To make this clearer, I will talk through some use examples...
+
+Old v2 clients
+**************
+
+Older clients will not request a version, so they will always get back v2.0
+which is currently the v2 API.
+
+To cover the general case, we report the "default" version.
+
+Introducing what used to be v2.1
+********************************
+
+In Juno we want v2.1 to be experimental. So by default clients get routed
+to the old v2 implementation, which its old visioning rules.
+
+In the mean time, all versions of the API will be experimental, and as such
+that means the client will have to always explicitly request the version
+it is testing, such as:::
+
+  Accept: application/vnd.openstack-org.compute-v2.123.beta+json
+
+Note that the beta version must be backwards compatible with the last stable
+version of that major version, so in this case 2.0.0. But any following stable
+version only need by compatible with the last stable version, it can ignore
+any previous stable versions.
+
+The v2.1 changes will introduce some better sanity checking, so its possible
+it breaks "badly behaved" clients, but should not break any "well behaved"
+client. The plan is to release this v2.1 with Juno, so people can test this
+and ensure client will continue to work after all the extra versions have
+been added.
+
+To help this effort, there will be an special header to request the latest
+version of what we previously called "v2.1":::
+
+  Accept: application/vnd.openstack-org.compute-v2.beta+json
+
+This should itself be considered experimental, as it will be removed when v2
+becomes a stable API. It would then return the actual version in the
+usual way:::
+
+  Content-Type: application/vnd.openstack-org.compute-v2.123.beta+json
+
+When we want v2.1 to be come the default, the default version will be updated
+to be the last beta of v2.
+
+From that point, clients with no version specified will get that version.
+And the server will update its "default_version" flag from 2.0.0 to the
+chosen version.
+
+Changes to the default version
+******************************
+
+For bugs and additions to v2, we handle them as normal, until v2.1 becomes
+the new default. Except to say, and change to v2 should also be made in
+v2.1 and ideally tested in both APIs.
+
+Once v2.1 is the default version, should there turn out to be a bug in that
+version, we will use the Z number to hotfix the default version,
+as if we were backporting a fix in a stable branch, and update the default
+version the server reports.
+
+Note this means, if we backport a fix to a stable branch, we must backport
+all the fixes to that extension.
+
+Introducing what used to be v3
+********************************
+
+v3 is a backwards incompatible change, that could be made available as an
+experimental API.
+
+This should be treated like v2.1, except the client requests:::
+
+  Accept: application/vnd.openstack-org.compute-v3.beta+json
+
+Again, this alias to the latest beta will be removed once v3 is stable.
+
+The version returned to clients from the server would be something like:::
+
+  Content-Type: application/vnd.openstack-org.compute-v3.123.beta+json
+
+Once we have a stable version, it should be requested by:::
+
+  Accept: application/vnd.openstack-org.compute-v3+json
+
+What happens to v2 after we have v3?
+************************************
+
+While this is not a property that is required by the above versioning
+scheme. One it makes sense for a client to request the v3 API and do
+everything they need to do, we should stop adding features in the v2
+API.
+
+Without this, we start to get parallel streams of development, and
+clients would have to choose between what was available in v2 vs v3.
+
+Note, we are planning to have parallel streams of development on the
+beta version of v3, as it has proved impossible to develop the next
+version of the API in one "big bang".
+
+It is very possible that we choose not ever release the version that is
+currently called v3, and keep only v2.x, but that is a valid step forward
+with this proposed version scheme. We could create a future versions of v3
+that proposes a radically different API, and that is fine within this
+proposed scheme.
+
+Introducing a new extension
+***************************
+
+As with any API change, you bump the global number, and use that number
+as the first version of that API.
+
+Should the first version be 2.5, if a client requested 2.4, the server
+should reposed with 406 Not Acceptable. But if you request the latest
+stable version of v2, it would respond with the latest version of that
+plugin.
+
+Checking the servers latest version would tell you if the new extension
+is happening.
+
+Modifying an exiting extension
+******************************
+
+If it is just a bug fix, and has no client impact (no extra values,
+and no extra features), simply change 2.4 to 2.4.1 on the specific
+extension.
+
+Any backwards compatible modification, you bump 2.4 to 2.5 and use that
+version to start activating that modification.
+
+If the client requests an older version, you should work as you did
+before.
+
+Modifying an exiting experimental version
+*****************************************
+
+If a client requests a specific experimental version, it should never
+change, just like any specific stable version that is requested.
+
+However, we could delete that version at any time during a release
+cycle, without warning.
+
+In addition, any experimental version need only be backwards compatible
+with the previous stable version, you don't need to be backwards
+compatible with any previous experimental versions.
+
+So, consider the extension with a version 1.2, we then need
+1.3.beta, global version is 1.3. That might add a new attribute
+"colour".
+
+Then version 1.4.beta might add "color" instead, and we remove
+version 1.3.beta from the code. As it is in beta, there is no
+need for a deprecation cycle. The client has to expect it may
+get deleted at any time, and is only there for testing.
+
+Then we need to add "transparency" field. Now we create a version
+1.4.beta that adds in "transparency", but its best to make that
+change on top of 1.4.beta, so it adds both transparency and "color"
+to the previous stable.
+
+Once we have tested the new value in tempest, etc, we can decide
+when to turn that into a table release. We do that by adding a new
+global version, say 1.14, and we delete the 1.3.beta version.
+
+Note we cannot just make 1.4.beta into 1.4, because that would
+change the result of request the API version 1.5 from that extension
+which would have returned the 1.2 version before the API came out
+of beta.
+
+However we now have 1.4.beta, but we might decide we didn't need that
+after all, so we remove it, or we could promote it later.
+
+TODO - this breaks down should we have dropped color and kept transparency
+If we kept them independent, so transparency only add transparency and does
+not add color, then they race each other to graduate as a stable API.
+
+Possible fix - we only ever allow a single beta version per extension,
+so if you make changes, you always delete the old one, and create a new
+merged proposal. This will hurt the testing a little, but could work.
+
+Deprecating an exiting extension
+********************************
+
+Should we want to remove an extension we can deprecate it.
+
+When deprecated, we increment the global version, such that:
+
+* clients requesting the latest stable get 406 Not Accepted Errors
+* but a client could still request the specific version if they want it
+
+Say the global version was 2.14, we bump to 2.15. And if the client
+whats the now deprecated API, it has to request it like this:::
+
+  Accept: application/vnd.openstack-org.compute-v2.15.deprecated+json
+
+TODO - does this even make sense? its basically a backwards incompatible
+change.
+
+Backporting an API fix
+**********************
+
+When we backport the fix, we bump the revision number of the extension.
+So 2.4.0 becomes 2.4.1. As normal, the fix is first applied to trunk,
+then back ported into the stable branch.
+
+Should the stable branch only contain 2.3.0, and we want to backport 2.4.3
+to a branch, we will have to backport all the versions up to and including
+2.4.3. Such that if you request that version the API is always the same.
+
+However, in trunk, we can add 2.4.3, even if the current version is 2.8.0
+
+
+Vendor specific extension
+*************************
+
+This follows the same rules as "beta" extensions, except beta is replace
+by "vnd-XXX" where XXX the company name, so examples could be:
+"vhd-hp-public-cloud" or "vnd-rax"
+
+The key thing to note is this version is considered experimental, just like
+beta version. The only reason for the different version key is to ensure
+trunk would not release a version that matches what the vendor has produced.
+
+Just as with experimental versions, it should be backwards compatible with
+any previous major versions of the extension.
+
+
+How the API server advertises the versions
+------------------------------------------
+
+Request the root URL of the and you get something like this:::
+
+    TODO - json home doc goes here to describe request/response/query
+    parms for each available version
+
+    http://tools.ietf.org/html/draft-nottingham-json-home-03
+
+Request "/versions" and you get a description of the global versions:::
+
+    {
+        "default_version": "application/vnd.openstack-org.compute-v2+json",
+        "versions": {
+          "application/vnd.openstack-org.compute-v2+json": "2.0",
+          "application/vnd.openstack-org.compute-v2.beta+json": "2.29.beta"
+          "application/vnd.openstack-org.compute-v3.beta+json": "3.66.beta"
+        }
+    }
+
+Notes:
+
+* removing the version from the URL
+* leaving old URLs to only respond to the "v2.1" version
+
+
+Alternatives
+------------
+
+There are various things we could change about the current proposal:
+
+* See the TODOs
+
+* we could have started at v1 for v2, but that is probably more confusing
+  than it is worse
+
+Properties Common to all proposals
+**********************************
+
+* V2.1 will replace V2
+  https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api
+* Older clients never get broken
+* Allow new clients to discover what new parameters they can send
+* Allow new clients to request more information from newer APIs
+* Out of scope of this specification for both proposals are issues
+  around of discovery as they would work the same way with both.
+
+Proposal 1 - Every plugin is versioned
+**************************************
+
+With the V3 API framework everything is a plugin and has its own
+version number. The version number of any plugin could be changed
+independently from the version number of any other plugin.
+
+Pros/Cons:
+
+* (Pros) It is easy to increase each microversion because the order is
+  separated in each plugin.
+* (Cons) It is difficult to specify microversion when clients want to
+  use new features because they need to check each plugin microversion
+  before sending a request.
+
+Proposal 2 - API wide microversion
+**********************************
+
+There is a single monotonically incrementing microversion which
+applies across the API:
+
+* eg 201, 202, 203, 204, 205, ... , 310, 311, ...
+* With each version increment the new API contains an API change that
+  may or may not be backwards compatible.
+* Note that version 204 of the API has all the changes contained in
+  versions before it, ie changes in 201, 202 and 203.
+* It is not possible for a client to request only the API changes
+  contained in version 201 and 204. If a client wants the API changes
+  contained in version 204, it will also have all the API changes
+  which occurred before then (which may be quite unrelated to the
+  specific API they are making the request to).
+
+A client is able to send an Accepts header specifying the range of the
+micoversioned API that they are willing to deal with the server:
+
+* Server would respond with a version range that they can comply with
+  which may or may not be within the range requested.
+* Client would make a request with an accepts header acceptable to the
+  server with the request in a format for that version
+* If no server version is acceptable, client would need to give up/upgrade
+  (long term goal would be eventually be to drop very old microversions).
+* plugin specific versioning may not be relevant, though it could be
+  left so a version 0 still indicates "experimental"
+
+Pros/Cons:
+
+* (Pros) It is easy to specify microversion when clients want to use
+  new features because they don't need to check each plugin microversion
+  and can use a single microversion for all plugin features.
+* (Cons) If deployers apply some upstream patches, which add new features,
+  to their own OpenStack environments, the behaviors would be different
+  from the upstream OpenStack behaviors. For example, I'd like to pick the
+  following case that some deployer applies the patches of microversions 205
+  and 206 without 203/204. In this case if clients specify 206 as the
+  microversion for plugin A, the environments based on upstream provide
+  feature 1 and feature 2 to the clients. However the deployer's one does
+  not provide these features because of not applying 203 and 204 patches.
+  These different behaviors are not good for the interoperability.
+
+===================================   ===========================
+upstream (microversion)               some deployer's environment
+===================================   ===========================
+add new feature 1 to plugin A (203)   not apply
+add new feature 2 to plugin A (204)   not apply
+add new feature 3 to plugin B (205)   apply 205
+add new feature 4 to plugin B (206)   apply 206
+===================================   ===========================
+
+Proposal 3 - The combination of both microversions
+**************************************************
+
+Described above.
+
+Pros/Cons:
+
+* (Pros) Nova project can provide stable combination of new features when
+  each cycle release by choosing plugin microversions for the API wide
+  microversion.
+* (Pros) By defining the combination, Nova project(upstream) will be able
+  to reduce different behaviors on different environments. That is good
+  for the interoperability.
+* (Pros) We can remove some plugin microversion behaviors if these behavoirs
+  are not included in the API wide microversions.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+Need a definition of what the client accept header will actually look like here
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+python-novaclient will need to support sending a client accept header.
+
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Discuss things that will affect how you deploy and configure OpenStack
+that have not already been mentioned, such as:
+
+* What config options are being added? Should they be more generic than
+  proposed (for example a flag that other hypervisor drivers might want to
+  implement as well)? Are the default values ones which will work well in
+  real deployments?
+
+* Is this a change that takes immediate effect after its merged, or is it
+  something that has to be explicitly enabled?
+
+* If this change is a new binary, how would it be deployed?
+
+* Please state anything that those doing continuous deployment, or those
+  upgrading from the previous release, need to be aware of. Also describe
+  any plans to deprecate configuration values or features.  For example, if we
+  change the directory name that instances are stored in, how do we handle
+  instance directories created before the change landed?  Do we move them?  Do
+  we have a special case in the code? Do we assume that the operator will
+  recreate all the instances in their cloud?
+
+Developer impact
+----------------
+
+Discuss things that will affect other developers working on OpenStack,
+such as:
+
+* If the blueprint proposes a change to the driver API, discussion of how
+  other hypervisors would implement the feature is required.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Who is leading the writing of the code? Or is this a blueprint where you're
+throwing it out there to see who picks it up?
+
+If more than one person is working on the implementation, please designate the
+primary author and contact.
+
+Primary assignee:
+  cyeoh-0
+
+Other contributors:
+  <launchpad-id or None>
+
+Work Items
+----------
+
+Work items or tasks -- break the feature up into the things that need to be
+done to implement it. Those parts might end up being done by different people,
+but we're mostly trying to understand the timeline for implementation.
+
+
+Dependencies
+============
+
+* Include specific references to specs and/or blueprints in nova, or in other
+  projects, that this one either depends on or is related to.
+
+* If this requires functionality of another project that is not currently used
+  by Nova (such as the glance v2 API when we previously only required v1),
+  document that fact.
+
+* Does this feature require any new library dependencies or code otherwise not
+  included in OpenStack? Or does it depend on a specific version of library?
+
+
+Testing
+=======
+
+Please discuss how the change will be tested. We especially want to know what
+tempest tests will be added. It is assumed that unit test coverage will be
+added so that doesn't need to be mentioned explicitly, but discussion of why
+you think unit tests are sufficient and we don't need to add more tempest
+tests would need to be included.
+
+Is this untestable in gate given current limitations (specific hardware /
+software configurations available)? If so, are there mitigation plans (3rd
+party testing, gate enhancements, etc).
+
+
+Documentation Impact
+====================
+
+What is the impact on the docs team of this change? Some changes might require
+donating resources to the docs team to have the documentation updated. Don't
+repeat details discussed above, but please reference them here.
+
+
+References
+==========
+
+Please add any useful references here. You are not required to have any
+reference. Moreover, this specification should still make sense when your
+references are unavailable. Examples of what you could include are:
+
+* Links to mailing list or IRC discussions
+
+* Design summit session https://etherpad.openstack.org/p/juno-nova-v3-api
+
+* Recent discussions:
+  https://wiki.openstack.org/wiki/Nova/ProposalForAPIMicroVersions
diff --git a/specs/juno/proposed/associate-lru-fixed-ip-address.rst b/specs/juno/proposed/associate-lru-fixed-ip-address.rst
new file mode 100644
index 0000000..938dc91
--- /dev/null
+++ b/specs/juno/proposed/associate-lru-fixed-ip-address.rst
@@ -0,0 +1,145 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================
+Associate least recently used fixed IP address
+==============================================
+
+https://blueprints.launchpad.net/nova/+spec/associate-lru-fixed-ip-address
+
+In this blueprint, we propose to enhance the fixed IP associate DB API
+functions by having them return the least recently used IP address instead
+of the lowest address, which is likely the most recently used address.
+
+Problem description
+===================
+
+End users creating servers have an optimal experience when they can get
+their applications serving in the cloud as immediately as possible. This
+includes networking infrastructure having routes already populated and
+services such as DNS resolving as expected, right away.
+
+Problems can arise if an end user receives a fixed IP address that was
+very recently released by a different user, which is likely in a busy
+and dynamic cloud environment that assigns IP addresses on a first-available
+basis. The IP address associated with the new user's server may have had
+reverse DNS set to point elsewhere or the old user may have had a custom
+hostname configured, requiring DNS to be updated before the new server
+can serve traffic. The problem can be made less likely if IP addresses
+were to be associated on a least recently used basis, instead.
+
+Proposed change
+===============
+
+We propose a change to the fixed_ip_associate, fixed_ip_associate_pool, and
+floating_ip_fixed_ip_associate functions to add a call e.g.::
+
+  fixed_ip_ref = model_query(context, models.FixedIp, session=session,
+                             read_deleted="no").\
+                             filter(network_or_none).\
+                             filter_by(reserved=False).\
+                             filter_by(instance_uuid=None).\
+                             filter_by(host=None).\
+ +                           order_by('updated_at').\
+                             with_lockmode('update').\
+                             first()
+
+order_by('updated_at') to pick the least recently used fixed IP address and
+return it, instead of the first available, in nova/db/sqlalchemy/api.py
+
+Alternatives
+------------
+
+Another approach would be a configurable policy for associating fixed IP
+addresses with policies like 'least recently used' and 'first available.'
+This would be a more significant change and involves adding yet another
+configuration flag. It is hoped that the least recently used policy is
+an overall improvement for users creating servers.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Calls of the fixed_ip_associate, fixed_ip_associate_pool, and
+floating_ip_fixed_ip_associate functions will be more costly as the filtered
+rows will be ordered by 'updated_at' before returning the first item.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  melwitt
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+ * Add an order_by('updated_at') call to the query of the FixedIp model, right
+   before the with_lockmode('update') call, in the fixed_ip_associate,
+   fixed_ip_associate_pool, and floating_ip_fixed_ip_associate functions in
+   nova/db/sqlalchemy/api.py
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+This change can be tested by adding unit tests to nova/tests/db/test_db_api.py
+which verify that the fixed_ip_associate, fixed_ip_associate_pool, and
+floating_ip_fixed_ip_associate functions return the least recently used/updated
+IP address in the fake DB.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/audit-compute-node-on-controller-recovery.rst b/specs/juno/proposed/audit-compute-node-on-controller-recovery.rst
new file mode 100644
index 0000000..f453c7c
--- /dev/null
+++ b/specs/juno/proposed/audit-compute-node-on-controller-recovery.rst
@@ -0,0 +1,144 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================
+Audit compute node for controller node recovery
+===============================================
+
+Launchpad blueprint:
+
+Not created
+
+This blueprint proposes an mechanism to audit compute node, which could be used
+during a controller node is recovered from crash and make the information in
+database sync with the status in compute node ASAP,therefore bring the controller
+node back in service with minimum services disturbance.
+
+Problem description
+===================
+
+A controller node is the central part of OpenStack. It normally serves a lot of
+services that keep OpenStack running, e.g. database, message queue, keystone,
+nova-scheduler, nova-conductor, neutron server, dashboard, etc. In OpenStack, HA
+services are supported on controller node in order to improve system availability,
+but there is no clear solution on how to recover from crash in case of a disaster
+happens. This could not fill the requirement on a critical system that well.
+Because: 
+As End User, it is important to have OpenStack and guest services with minimum
+interruption when OpenStack is recovered from crash or backup. 
+
+As Deployer, it should be possible to backup and recover OpenStack system.
+
+To make these possible, audit compute node against controller node database is a
+basic requirement.
+
+The similar idea could go with network and storage as well. However, this
+blueprint will only focus on compute service.
+
+Proposed change
+===============
+
+The proposal is described in the diagram below from the high level
+
+.. image:: Audit-compute-node-on-controller-recovery.png 
+         :width: 768 px
+         :height: 640 px
+         :align: center
+         :scale: 100%
+
+As described in the diagram: 
+
+* 1 When nova-conductor is created, it associates to message queue. 
+* 2-5 Right after that, it issue an audit request to all active compute nodes.
+* 6-7 compute node reports the instances status back to into database through
+  nova-conductor. It will flag set timestamp of audit activity in database as well.
+* 8-10 compute node reports the current compute resource to database through nova
+  conductor. After update compute resource, it will always check if an audit
+  procedure is needed by checking audit timestamp.
+* 11-14 This is the same as 3-7
+
+Instances and compute resource are the items to be audited.
+
+Alternatives
+------------
+None
+
+Data model impact
+-----------------
+
+
+In the ComputeNode table, there will a new column named last_audit to store a
+timestamp. This should be updated in nova/db/sqlalchemy/models.py:class ComputeNode
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+
+A new notifications that notify Deployer the outcome of the audit. 
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+Might slow system startup
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+
+Primary assignee:
+  <liyi-meng>
+
+Other contributors:
+  <launchpad-id or None>
+
+Work Items
+----------
+* Update ComputeNode scheme
+* Update ComputeManager to collect node information during audition
+* Update nova-conductor
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+TODO
+
+Documentation Impact
+====================
+
+Deployment document to describe the feature.
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/auto-disable-and-enable-hypervisor.rst b/specs/juno/proposed/auto-disable-and-enable-hypervisor.rst
new file mode 100644
index 0000000..037a372
--- /dev/null
+++ b/specs/juno/proposed/auto-disable-and-enable-hypervisor.rst
@@ -0,0 +1,196 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+handle-auto-diabled-for-hypervisor
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/handle-auto-diabled-for-hypervisor
+
+migrate disabled reason field work around from 'AUTO:' to a separated field
+'auto_disabled'.
+
+
+Problem description
+===================
+
+There is a workaround in Icehouse when a compute service is automatically
+disbled when hypervisor is temporary not available. A 'AUTO:' is added to
+disabled_reason column to indicate that the service is automatically disabled.
+Operator who deliberatly add 'AUTO:' as prefix might lead to problem.
+
+Proposed change
+===============
+
+* Instance object and db update
+
+Add a column into service table named 'auto_disabled' with boolean.
+Add a column into service object, whenever we load a Service object,
+look for the AUTO:, set auto_disabled=True, and strip out the AUTO:
+Then, over time, the data will be migrated,
+but we won't have to do it during the migration.
+
+* Logic description
+
+Following are existing allowed transitions logic, the patch will NOT change
+the existing logic, only the disabled reason will be update to
+'Auto disabled by hypervisor' if it's auto disabled from Enabled.
+
+Enabled --> Manually disabled :
+disabled = True, auto_disabled = False, disable reason = user specified
+
+Enabled --> Auto disabled :
+disabled = True, auto_disabled = True,
+disabled reason = 'Auto disabled by hypervisor'
+
+Manually disabled --> Manually Enabled :
+disabled = False, auto_disabled = False, disabled reason = None
+
+Auto Disabled --> Auto Enabled :
+disabled = False, auto_disabled = False, disabled reason = None
+
+Auto Disabled --> Manullay Enabled :
+disabled = False, auto_disabled = False, disabled reason = None
+
+Manually disabled --> Auto Enabled :
+Not allowed under current logic
+
+Alternatives
+------------
+
+we might keep the service table as before, as problem section described,
+it might affect operator.
+
+Data model impact
+-----------------
+
+In service object, one column will be added:
+fields = {
+'id': fields.IntegerField(),
+.....
+'disabled_reason': fields.StringField(nullable=True),
+'availability_zone': fields.StringField(nullable=True),
+'compute_node': fields.ObjectField('ComputeNode'),
++   'auto_disabled': fields.BooleanField(),
+}
+
+In service table schema, one column will be added:
+class Service(BASE, NovaBase):
+....
+report_count = Column(Integer, nullable=False, default=0)
+disabled = Column(Boolean, default=False)
+disabled_reason = Column(String(255))
++   auto_disabled = Column(Boolean(create_constraint=False), default=False)
+
+The field with a prefix '+' is to indicate the proposed added return field
+
+As proposed change section indicates, no need to do off-line db migration
+since we are able to translate previous db record when we load them.
+
+REST API impact
+---------------
+
+Both v2 and V3 API will be updated to include auto_disabled field:
+
+In v2 API, extension os-services-auto-disabled will be added to
+advertise the extra information.
+alias: os-services-auto-disabled
+name: ServicesAutoDisabled
+namespace: http://docs.openstack.org/compute/ext/services-auto-disabled/api/v2
+
+GET /v2/a8dbeffef284447785f8fbc045acb889/os-services
+response:
+
+"services"
+{
+"binary": "nova-compute",
+"host": "host1",
++"auto_disabled": "True",
+"disabled_reason": "test2",
+"state": "up",
+"status": "disabled",
+"updated_at": "%(timestamp)s",
+"zone": "nova"
+}
+
+The field with a prefix '+' is to indicate the proposed added return field
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Admin will see 'auto_disabled' column to indicate the service is
+automatically disabled.
+
+Because the data model migrations happen when each service objects
+is loaded there will be a small period when both the old and
+new mechanisms co-exist.
+
+Deployers will need to change any tools they have now to detect auto
+disabled services from the reason string.
+
+There is no off-line db migration needed so whenever a service is loaded,
+if it see a 'AUTO:' in the db field, the object load function will handle that
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jichenjc
+
+Work Items
+----------
+
+1) Change mechanism of service object load/store
+2) Change libvirt autodisable judgement (from AUTO: -> auto_disabled)
+3) Change v2/v3 api layer code by adding the auto_disabled field
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest cases will be updated to include new field test.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+https://review.openstack.org/#/c/86869/
+https://review.openstack.org/#/c/79617/
diff --git a/specs/juno/proposed/cache-qos-monitoring.rst b/specs/juno/proposed/cache-qos-monitoring.rst
new file mode 100644
index 0000000..a0a2e63
--- /dev/null
+++ b/specs/juno/proposed/cache-qos-monitoring.rst
@@ -0,0 +1,151 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================
+ Add CPU CQM (Cache Qos Monitoring) management
+==============================================
+
+
+https://blueprints.launchpad.net/nova/+spec/cache-qos-monitoring
+
+CQM:Enables monitoring of cache occupancy on a per-thread (via RMID) basis.
+
+RMID : The ID of the application or thread on a core as specified by the
+OS/VMM. This ID is provided by the OS/VMM, and used by the platform to tag
+and monitor events such as cache occupancy.
+
+Intel's new CPUs introduce Cache QoS Monitoring (CQM) allows an Operating
+System, Hypervisor or similar system management agent to determine the usage
+of cache by applications running on the platform. The initial implementation
+is directed at last level cache (LLC) monitoring.
+
+We want to add this new feature into OpenStack in this blueprint. And this
+need kernel and xen to support the CQM feature. So We will do some work in
+kernel, xen, libvirt, xenapi and OpenStack.
+
+When get one instance's' cache data, should via a RMID, but the total RMID
+numbers of different platforms are differents. So we want to add a RMID
+managerment in Nova.
+
+
+Problem description
+===================
+
+The main use case is to monitor the cache of each intstance. The user can
+enable the CQM function in the Nova and get the instance's 'cache data from
+Ceilometer agent[3].
+#TBD
+
+
+Proposed change
+===============
+
+This blueprint dependencies on Paul Murray's blueprint[1].
+Add a RMID resource plugin to manager the RMID resource. The RMID plugin record
+how many RMIDs are used and the total RMID numbers of host.
+
+The deployer should add a key ("resource:cpu_cqm":True ) to extra_specs to mark
+if the instance need to use the CQM monitor, the RMID plugin will put the key to
+instance_system_metadata, when libvirt/xenapi creates the instance, it will start
+the instance's cache monitoring. if the instance is created successfully, the
+plugin will record one RMID was used.
+
+Also need a RMID filter to check if the host has RMID to the new instance, if
+not, will return 0.
+
+#TBD (more details about the libvirt module)
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+The blueprint used the extra_resources field in the compute node table to
+communicate the resource tracking information. This field was added to the
+database in Icehouse-2 but has not yet been used.
+The extra_specs field will be added to the instances table in
+Paul Murray's blueprint[1].
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+The plugins will be configured in the blueprint extensible-resource-tracking
+ways.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  shuangtai-tian
+  qiaowei-ren
+
+
+Work Items
+----------
+
+* Add the RMID resource plugin to resource tracker
+* Add the RMID resource consumer plubin to the host manager
+* Add extra_specs ("resource:cpu_cqm":True) to instance_system_metadata
+
+
+Dependencies
+============
+
+* This blueprint dependency on extensible-resource-tracking https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
+* Also dependecy on the new version of kernel, xen, libvirt and xenapi.
+
+
+Testing
+=======
+
+Unit tests are sufficient to cover feature changes.
+
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+[1]https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
+[2]https://blueprints.launchpad.net/nova/+spec/cache-qos-monitoring
+[3]https://blueprints.launchpad.net/ceilometer/+spec/cache-qos-monitoring
diff --git a/specs/juno/proposed/change-instances-ownership-proposal-1.rst b/specs/juno/proposed/change-instances-ownership-proposal-1.rst
new file mode 100644
index 0000000..b8c41ca
--- /dev/null
+++ b/specs/juno/proposed/change-instances-ownership-proposal-1.rst
@@ -0,0 +1,168 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+..
+  This template should be in ReSTructured text. The filename in the git
+  repository should match the launchpad URL, for example a URL of
+  https://blueprints.launchpad.net/nova/+spec/awesome-thing should be named
+  awesome-thing.rst .  Please do not delete any of the sections in this
+  template.  If you have nothing to say for a whole section, just write: None
+  For help with syntax, see http://sphinx-doc.org/rest.html
+  To test out your formatting, see http://rst.ninjs.org/
+  Please wrap text at 80 columns.
+
+=============================================
+Change Instances Owernship - Project and User
+=============================================
+
+https://blueprints.launchpad.net/nova/+spec/change-instance-ownership
+
+Instances in Nova should have its ownership (User Owner and
+Project Owner field) transferable.
+
+Problem description
+===================
+
+In Nova the instances have the information of which user and which project
+owns it. Although we can change information like the instance name, we can't
+change it's user and/or project owner.
+
+Currently, when an user is removed from a project its instance(s) persists in
+the project with the old ownership. The instance may be inaccessible by other
+users.
+
+Use Case 1: In a project an user is replaced by a new one, the new user should
+be able to access the instance owned by the old user.
+
+Use Case 2: An user is transferable to a new project and wants to continue
+using the old instance. The instance should be transfered to the new project.
+
+Proposed change
+===============
+
+In order to implement this we are going to implement a new extension in Nova
+API called change_instance_ownership, where it will be identified the current
+project and instance, and the new instance user and/or project.
+
+The extension will update the instance fields project_id and/or user_id.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+V2 API specification:
+PUT: v2/{tenant_id}/os-change-instance-ownership/{server_id}/action
+
+Request parameters:
+* tenant_id: The ID for the tenant or account in a multi-tenancy cloud.
+* server_id: The UUID for the server of interest to you.
+* user_id: Specify the rescue action in the request body.
+* adminPass(Optional): Use this password for the rescued instance.
+Generate a new password if none is provided.
+* rescue_image_ref(Optional): Use this image_ref for rescue.
+
+JSON request:
+{"user_id": "1ff5aca21382468085df62c12d74b280",
+"project_id": "3560124f13754b1da30b52ed3593ba69"}
+
+JSON response:
+
+
+Sample v2 request:
+PUT: /v2/9d67101f/os-change-instance-ownership/4e12-90b4-4aa8-8f08-c1e3/action -d '{"user_id":"1ff5aca21382468085df62c12d74b280"}'
+
+HTTP response codes:
+v2:
+Normal HTTP Response Code: 200 on success
+
+Security Impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+Other End user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Deployer impact
+---------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Who is leading the writing of the code? Or is this a blueprint where you're
+throwing it out there to see who picks it up?
+
+If more than one person is working on the implementation, please designate the
+primary author and contact.
+
+Primary assignee:
+  <tellesmvn>
+
+Other contributors:
+  <afaranha>
+
+Work Items
+----------
+
+* Implement change ownership
+* Create tests to change ownership
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Test checking several use cases.
+
+
+Documentation Impact
+====================
+
+Changes in Nova Extension API to include the new functionality.
+
+
+References
+==========
+
+https://review.openstack.org/#/c/81079/
+New One: https://review.openstack.org/#/c/85480/
diff --git a/specs/juno/proposed/change-instances-ownership-proposal-2.rst b/specs/juno/proposed/change-instances-ownership-proposal-2.rst
new file mode 100644
index 0000000..5b964b2
--- /dev/null
+++ b/specs/juno/proposed/change-instances-ownership-proposal-2.rst
@@ -0,0 +1,234 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Change Server Owernship
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/change-instance-ownership
+
+We want to add a new feature in Nova to allow it to perform ownership change in
+nova servers. It should be able to change the owner in both an user level and
+also project level. This will allow to move one server between projects and
+change the user who is responsible for it when needed.
+
+
+Problem description
+===================
+
+As a cloud admin I want to be able to give a server responsability to a new
+owner, this is mainly for a new user owner but there are some cases where a new
+project need to be set in the server, for example, when the actual user changes
+project, or when the new user are in a different project.
+
+Use Case 1
+----------
+
+* The user was transfered to another project;
+* The user needs to keep his server;
+* It's need to transfer the server to the new project;
+
+Use Case 2
+----------
+
+* The actual project was splited in two projects (For example, Google was
+  working on Google Docs development and them the project was splited in Google
+  Docs and Google Spreadsheet);
+* Some users was moved to the new project;
+* These users server need to be transfered with them;
+
+
+Proposed change
+===============
+
+We are goig to add a server action to allow users to change a server ownership.
+The users allowed to perform this action will be specified in the policy file.
+
+Our proposal are going to follow a similar guideline used in volume transfer
+blueprint (https://wiki.openstack.org/wiki/VolumeTransfer), a blueprint that
+creates a similar feature, but for volumes in Cinder, that was already approved
+and merged.
+
+We are going to change the server to the new owner and also all the attached
+resources that it depends on project or user: Volume, network address, Flavor,
+Security Group and Quota. And apply some checks before changing the ownership,
+like checking the new owner available quota.
+
+For volume, the VolumeTransfer feature can be used to transfer any attached
+volume to the new project owner. For flavor, it'll continue with the previous
+one, since it represents in which flavor it was created, also, when booting an
+server with a flavor A, and then deleting the flavor, the server still with
+the deleted flavor name. For Security Group, the API will have an optional
+field that will allow the user to set the new security group, by default, it
+will be set to the default security group. For network address, it'll works
+similar to security group, the user will pass an IP, and by deafult the server
+will get an available IP in the new project. For the Quota, it'll change to the
+new project default quota.
+
+
+Alternatives
+------------
+
+Actually a directly server transference cannot be made. In order to simulate it
+this can be made by booting a new server from a snapshot with the same
+attributes (Falvors, quota, network, etc).
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+V2 API specification:
+POST: /v2/​{project_id}​/servers/​{server_id}​/action
+
+V3 API specification:
+POST: /v3/servers/​{server_id}​/action
+
+Request parameters:
+
+* project_id: The ID for the project where the server to be transfered is
+  hosted.
+* server_id: The UUID for the server to be transfered.
+* user_id (Optional): Specify the new server user owner in the request body.
+* project_id (Optional): Specify the new server project owner in the request
+  body.
+* security_group_id (Optional): Specify the security group ID.
+* addr (Optional): Specify the server network address.
+
+JSON request::
+
+    {
+        "user_id": "1ff5aca21382468085df62c12d74b280",
+        "project": {
+            "project_id": "3560124f13754b1da30b52ed3593ba69",
+            "security_group_id": "1",
+        },
+        "addr": "1.2.3.4"
+    }
+
+
+JSON response:
+
+This operation does not return a response body.
+
+
+Sample v2 request:
+
+PUT: /v2/9d67101f/servers/4e12-90b4-4aa8/action -d '{"user_id":"new_user_id"}'
+
+
+Sample v3 request:
+
+PUT: /v2/servers/4e12-90b4-4aa8/action -d '{"user_id":"new_user_id"}'
+
+
+HTTP response codes:
+
+Normal HTTP Response Code: 200 on success
+
+Validation:
+
+* 'user_id' must be of a id-str format and an existing user id.
+* 'project_id' must be of a id-str format and an existing project id.
+* 'security_group_id' must be of a id-str format and an existing security group
+  ID.
+* 'addr' must be of a valid network address.
+
+Security impact
+---------------
+
+As it may need to change resources from one project to another, it can be
+dangerous to lose control of a resource, since the permission to do it
+is in the policy file, and a resource can be placed in an undesirable place
+that is unreachable to the user. Also, an error may occur and some resources
+may not be tranfered, raising an inconsistence in the server.
+
+When changing the ownership is also dangerous to change it to an user or a
+project that is not desired to have access to that server. As example,
+changing the server from an Apple project to a Microsoft project.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+This new feature will only perform when the user calls it, and then will take
+a time to change the resources. After the resource is changed and checked the
+feature stops.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <afaranha>
+
+Other contributors:
+  <tellesmvn>
+
+Work Items
+----------
+
+* Implement change server user ownership (continues in the same project)
+* Implement change server project ownership (also changing the server
+  resources that depends on the project)
+* Implement restrictions (the new user needs to participates in the new
+  project, check if there is avaiable quota for the new user and the new
+  project, etc)
+* Create tempest tests
+
+
+Dependencies
+============
+
+It needs to use the Keystone Client in order to check that the new user
+participates in the new project.
+
+
+Testing
+=======
+
+We need tests that ensure the new feature is working correctly, if the checks
+are raising exception when expected, the resources attached to the nova
+server are being transfered also, etc.
+
+
+Documentation Impact
+====================
+
+A description of this functionality will be added into Compute API V2 and V3
+Reference.
+
+
+References
+==========
+
+Volume Transfer
+* https://wiki.openstack.org/wiki/VolumeTransfer
diff --git a/specs/juno/proposed/cinder-events.rst b/specs/juno/proposed/cinder-events.rst
new file mode 100644
index 0000000..2c46474
--- /dev/null
+++ b/specs/juno/proposed/cinder-events.rst
@@ -0,0 +1,217 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================================
+Move some Nova-Cinder interactions to the events framework
+==========================================================
+
+https://blueprints.launchpad.net/nova/+spec/cinder-events
+
+Nova allows other services to notify it about events it may be waiting using
+it's REST API (introduced in [1]_), and Neutron already uses it for certain
+interactions. This is a model that will work well with Cinder too.
+
+
+Problem description
+===================
+
+There are several requests Nova can make to Cinder, which will result in a
+long-running operation being started, however the only way for Nova to check
+on the status of the operation is through dumb polling. Some of the examples
+include creating a volume from a snapshot and downloading an image to a
+volume.
+
+There have been several bugs related to this (see [2]_ and [3]_), caused by
+the fact that the dumb-polling solution with a fixed timeout and interval does
+not suffice when faced with varying deployments, Cinder back-ends and data
+volumes.
+
+
+Proposed change
+===============
+
+* Cinder adds support for sending back events to Nova. This is covered by
+  [4]_. As part of this change, Cinder will also need to accept
+  'instance_uuid' and 'event_tag' field for any of the API calls that will
+  result in an event being sent back (see Work Items section for more info),
+  as it is needed for Nova to route the event to an appropriate
+  compute node.
+
+* Use the existing framework for waiting for instance events introduced in
+  [5]_ when 1) Snapshotting volumes, 2) Creating volumes from snapshots on
+  boot 3) Creating images from volumes on boot.
+
+* Ideally we would like to have Cinder send "heartbeat" events in
+  preconfigured intervals in addition to a "success" or "failure" event.
+  Heartbeats are meant to replace a fixed global timeout, and are basically
+  Cinder reporting back to Nova that the operation is still in progress.
+  Heartbeat events will have a pre-configured timeout on both Nova and Cinder
+  sides so that failures can be noticed, and reacted upon, resulting in a
+  smoother user experience.
+
+* In case Cinder sends only an event once the operation is done and does not
+  implement the "heartbeat" event (see [4]_) , we are still left with issues
+  described by [2]_ and [3]_, however we are still left with a more flexible
+  framework.
+
+
+Alternatives
+------------
+
+One of the approaches mentioned at one point was to make the Cinder statuses,
+more informative, so for example instead of returning "Downloading", Cinder
+could return "Downloading 37%", so that Nova could base it's polling intervals
+and possibly decisions to back out of the action on it. However, we already
+have the framework for proper callbacks in place [1]_, and the callback approach
+provides us with a more flexible and general architecture.
+
+
+Data model impact
+-----------------
+
+None
+
+
+REST API impact
+---------------
+
+None
+
+
+Security impact
+---------------
+
+* os-server-external-events API extension is admin only by default, so Cinder
+  will have to store admin credentials. It is possible to change this through
+  the policy file and make it based on roles.
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+None
+
+
+Performance Impact
+------------------
+
+There should be no immediate performance impact, however it is worth noting
+that every snapshot/boot operation that triggers Cinder interactions proposed
+by this BP will result in an increased number of API calls to Nova API.
+
+
+Other deployer impact
+---------------------
+
+* Deployers will need to update Cinder to the version that will send events
+  first. As Nova uses a simple GET on Cinder resources to poll for status
+  currently, Cinder versions that support events will still accept polling.
+  Newer Cinder will be able to work even with Nova deployments that don't (yet)
+  understand events or expose the API, since those Nova deployments will not
+  send the instance_uuid with parameter in the API call, which will let Cinder
+  know it needs not respond with events and will be polled instead.
+
+* Depending on whether Cinder introduces the "heartbeat" callback, we will be
+  introducing a cinder_heartbeat_event_timeout option.
+
+
+Developer impact
+----------------
+
+* None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <ndipanov@redhat.com>
+
+
+Work Items
+----------
+
+* Cinder is sending events when snapshot, create volume from snapshot and
+  download image to a volume operations.
+
+* Replace using _await_block_device_map_created compute manager method (that
+  implements Cinder polling for status) with usage of ComputeVirtAPI
+  wait_for_instance_event context manager. This usage can likely be completely
+  confined to the block device classes in virt.block_device module.
+
+* Change Cinder client to pass in the 'instance_uuid' and 'event_tag' field
+  to the create method. There will be places in the code
+  that will still be calling this method and not expect a callback (mainly
+  proxy API) so it will not be passed in all the places where this API is
+  used. The 'event_tag' field will be auto-generated UUID for each request to
+  Cinder, and is used to distinguish between different requests made for a
+  single instance.
+
+* In case Cinder implements heartbeat events (as discussed above), make the
+  code aware of these and introduce a configurable
+  cinder_heartbeat_event_timeout configuration option.
+
+Cinder will be supplying nova with a new type of event: 'volume-create'. Here
+is an example JSON representing the body of the API POST (see [1]_)::
+
+    {
+        "events": [
+            {
+                "name": "volume-create",
+                "server_uuid": "71b20737-7f90-4513-abde-4a2dfd8ae97e"
+                "status": "in-progress"
+                "tag": "0239e1f3-2603-4426-b1e4-786e0eaee2c3"
+            }
+        ]
+    }
+
+In the above example, the only thing that warrants more explanation over the
+standard events data model described in [1]_ is the tag field.
+Tag will be set to the 'event_tag' field that nova sent in the original
+request. This is needed so that nova can distinguish between different
+request sent on behalf of the same instance.
+
+
+Dependencies
+============
+
+* This BP depends on Cinder changes that are covered by [4]_
+
+
+Testing
+=======
+
+* No changes to Tempest are required since code paths affected are already
+  covered by several tests in Tempest.
+
+
+Documentation Impact
+====================
+
+* "Heartbeat" events will introduce a new configuration option that may be
+  tweaked depending on general load and we will want to document it.
+
+* "Other deployer impact" section outlines the update procedure that should be
+  documented.
+
+
+References
+==========
+
+.. [1] https://blueprints.launchpad.net/nova/+spec/admin-event-callback-api
+.. [2] https://bugs.launchpad.net/nova/+bug/1253612
+.. [3] https://review.openstack.org/#/c/42876/6/nova/compute/manager.py
+.. [4] https://blueprints.launchpad.net/cinder/+spec/send-volume-changed-notifications-to-nova
+.. [5] https://review.openstack.org/#/c/74576/
diff --git a/specs/juno/proposed/cold-migrations-to-conductor-final.rst b/specs/juno/proposed/cold-migrations-to-conductor-final.rst
new file mode 100644
index 0000000..37979a8
--- /dev/null
+++ b/specs/juno/proposed/cold-migrations-to-conductor-final.rst
@@ -0,0 +1,153 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================================
+Conductor: finish moving cold migrations to conductor
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/cold-migrations-to-conductor-final
+
+Continue the pending work and move all remaining resize/migrate functions
+to conductor.
+
+Problem description
+===================
+
+Today, there is a minimal coordination for cold migrations outside of the
+compute nodes. The migration/resize operation starts in the source
+compute host and it ends up in the target compute host.
+
+The consensus is that the coordination of the migrations should be moved to
+conductor, where we can eventually elimitate the migration state living
+on the compute nodes and the implicit trust between compute nodes required
+to pull off a migration.
+
+In order to do that, we still need to move all remaining resize/migrate
+functions out of compute manager to conductor.
+
+The change should allow all migrations related functions to co-exist and
+share code as much as possible.
+
+The current Resize/Migrate workflow is -
+
+- compute.api.resize
+- conductor.ComputeTaskAPI.resize_instance
+- conductor.rpcapi.ComputeTaskAPI.migrate_server
+- conductor.manager.migrate_server
+- conductor.manager._cold_migrate
+  (calls scheduler for destinations, deals with quota reservations)
+- compute.rpcapi.prep_resize
+- compute.manager.prep_resize (creates migration object)
+- compute.rpcapi.resize_instance (call for the same host)
+- compute.manager.resize_instance
+- compute.rpcapi.finish_resize (call for destination compute host)
+
+Proposed change
+===============
+
+This is a continuation of the previous blueprint implemented in icehouse-3.
+
+The follow workflow is expected after the change -
+
+- compute.api.resize
+- conductor.ComputeTaskAPI.resize_instance
+- conductor.rpcapi.ComputeTaskAPI.migrate_server
+- conductor.manager.migrate_server
+- conductor.manager._cold_migrate
+  (calls scheduler for destinations, deals with quota reservations)
+
+The conductor.manager._cold_migrate method would deal with preparation for
+resize/migrate as compute.manager.prep_resize does nowadays.
+
+The calls for resize_instance, finish_resize and confirm_resize
+would be made by the conductor method. Removing the implicit trust
+between compute nodes and putting all the control under the conductor
+service.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  timello
+
+Work Items
+----------
+
+None.
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+None.
+
+Documentation Impact
+====================
+
+None.
+
+References
+==========
+
+ * Initial blueprint for the change -
+   https://blueprints.launchpad.net/nova/+spec/cold-migrations-to-conductor
+ * Mailing list discussions -
+   http://lists.openstack.org/pipermail/openstack-dev/2013-April/007997.html
+   http://lists.openstack.org/pipermail/openstack-dev/2013-April/008213.html
+ * Russell's blog post -
+   http://blog.russellbryant.net/2013/05/13/openstack-compute-nova-roadmap-for-havana/
diff --git a/specs/juno/proposed/common-nova-metadata-cache.rst b/specs/juno/proposed/common-nova-metadata-cache.rst
new file mode 100644
index 0000000..e1fb58d
--- /dev/null
+++ b/specs/juno/proposed/common-nova-metadata-cache.rst
@@ -0,0 +1,164 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================
+Support a common cache for metadata
+===================================
+
+https://blueprints.launchpad.net/nova/+spec/common-nova-metadata-cache
+
+Currently, the Nova metadata API stores its cache in memory, which
+prevents it from being shared amongst multiple hypervisors and thus
+reduces the hit rate. This blueprint proposes to optionally replace
+the in-memory cache with dogpile.cache, allowing cache sharing between
+hypervisors.
+
+Problem description
+===================
+
+Under heavy use, the Nova metadata API can be a significant
+performance bottleneck. This use pattern can be common with
+configuration management systems that draw their own metadata from
+Nova, for instance.
+
+In our experience, this bottleneck can be greatly improved by
+implementing a longer-lived, shared cache. I have already proposed a
+change, https://review.openstack.org/#/c/119421/, that will make the
+cache time configurable, but that is of limited benefit when the cache
+cannot be shared amongst multiple hypervisors.
+
+
+Proposed change
+===============
+
+By using ``dogpile.cache`` to back the metadata cache, we can provide
+the option to share the cache between hypervisors by using memcached
+(for instance) and greatly increase the cache hit rate.
+
+The use of a common cache will be optional; the simple in-memory cache
+will still remain supported in order to keep simple deployments simple.
+
+Alternatives
+------------
+
+The only other way to increase cache hit rates is to increase the TTL
+of cache items, which we have already proposed.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+This actually makes it more difficult to launch a resource exhaustion
+attack, since the cache will no longer be stored in memory local to
+each hypervisor, but instead in a central system that can be protected
+(and set to expire the cache) independently.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+If the central caching feature is enabled, the operator can expect:
+
+* Memory usage on each individual hypervisor to decrease;
+* Cache hits on the metadata API (and thus performance thereof) to
+  increase;
+* A new service (or, if using the same ``dogpile.cache`` backend as
+  Keystone, an existing memcached service) that uses more memory.
+
+Other deployer impact
+---------------------
+
+A new group, ``[metadata_cache]``, should be added to
+``nova.conf``. It will contain the following options, all of which are
+exact analogs to the same options in the ``[cache]`` section of
+``keystone.conf`` (see
+http://docs.openstack.org/developer/keystone/configuration.html#caching-layer):
+
+* ``enabled``
+* ``debug_cache_backend``
+* ``backend``
+* ``expiration_time``
+* ``backend_argument``
+
+The defaults will be::
+
+    enabled = yes
+    debug_cache_backend = no
+    backend = dogpile.cache.memory
+    expiration_time = 15
+    backend_argument =
+
+This will mimic the current state of a completely in-memory cache.
+
+Developer impact
+----------------
+
+When writing unit tests that may use the metadata API, developers will
+need to select the memory cache backend.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  stpierre
+
+Work Items
+----------
+
+#. Write the caching layer module. This can be cribbed substantially
+   from the existing Keystone key-value store, although it will be
+   simpler in many ways because the data it will be storing is more
+   predictable.
+#. Update the metadata request handler to use the new caching layer
+   instead of the in-memory cache.
+
+Dependencies
+============
+
+None. ``dogpile.cache`` is already a global requirement.
+
+Testing
+=======
+
+The change linked above already extends unit tests to actually test
+the metadata request handler, so they should be sufficient to test
+that portion of it. New tests will need to be added to test the
+caching layer, but since that's not exposed functionally no new
+tempest tests should be necessary.
+
+Documentation Impact
+====================
+
+The options listed above will need to be added to the
+documentation. Since the configuration is substantially similar to the
+Keystone caching layer, it is expected that much of the Keystone
+documentation can be borrowed whole cloth.
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/compute-image-precache.rst b/specs/juno/proposed/compute-image-precache.rst
new file mode 100644
index 0000000..142572f
--- /dev/null
+++ b/specs/juno/proposed/compute-image-precache.rst
@@ -0,0 +1,303 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================
+Compute Image Precaching Capability
+===================================
+
+https://blueprints.launchpad.net/nova/+spec/compute-image-precache
+
+This blueprint looks to improve build times.  Currently builds times
+are directly related to image downloading.  This is due to the
+on-demand nature of VM launch today.
+
+This proposal looks to extend/leverage compute's existing on-demand
+downloading capability to prepopulate images.  The current mechanism
+is used in launching VMs which is performed on-demand.  We would like
+to download images without VM launch.
+
+Problem description
+===================
+Currently image downloading is a component of VM launch.  As such
+administrators have no way to precache images that are commonly used.
+Admins use out of band mechanisms to precache images based on specific
+drivers being used.  Admins also need a mechanism which can control
+number of downloads.  Scalability issues arise in large deployments
+where downloading becomes a bottleneck if too many images are
+downloading at the same time.  The use case being explored is image
+caching of common images for public clouds.
+
+This spec will add a mechinism to preclude images from being garbage
+collected by the current periodic task _run_image_cache_maanger_pass in
+the compute manager.
+
+Proposed change
+===============
+The admin API will change by extending the POST call under os-hosts to
+precache an image on a given host.  The specifics are provided below
+under REST API impact.
+
+The underlying compute will be extended with RPC request to cache an
+image on the host::
+
+    def host_cache_image(ctxt, images)
+       """ Precache image on host
+       :param context: context permissions for actions of precaching
+       :param images: the glance uuids of the images that will be cached
+       """
+
+This allows outside services to systematically precache images on
+hosts and control the number of hosts that are downloading.
+Notifications will be used to allow admin to know when images are
+being downloaded.
+
+Also, to preclude images from being garbage collected by
+_run_image_cache_manager_pass, we will add a cached_images hook filter.
+The periodic task _run_image_cache_manager_pass defaults to run ever
+24 hours.  The hook filter will be a configuration option in the nova
+compute.  The hook filter will be passed to the driver method
+manage_image_cache and have the following signature::
+
+    def cached_images(context, images)
+       """ cached_images return a dict of images that should remain on host
+       :param context: context permissions for actions
+       :param images: List of UUID of images to check
+       """
+
+The default cached_images hook will make a glance call to check the
+images' meta properties.  Using x-image-meta-property-cache_in_nova as
+a switch, the cached_images returns a dict of which images should and
+should not be cached.  True means the driver should maintain the image
+regardless of current usage or age.  False means the driver is free to
+remove the image.
+
+The name x-image-meta-property-cache_in_nova was chosen based on the
+pre-existing property used by the Xen driver.
+
+Alternatives
+------------
+Four alternatives were considered prior to the proposed solution above.
+These solutions are currently in use in various forms in public clouds
+today.  The first solution is to build VMs to cache the image and then
+delete the build using a targeted host mechanism.  This is less than
+desirable for a couple of reasons.
+
+* The process requires a fair amount of book keeping and doesn't
+  scale.
+* Hosts could be temporarily fully utilized.  As such launching a VM
+  may not be an option even if caching is desired.
+
+The second option is to do this outside nova via script.  This allows
+more flexibility in download options and implementation.  However, no
+common functions can be shared down the road between drivers.  Also,
+the script would be prone to breaking due to changes in driver
+implementations along the way.  For instance, in the XenAPI
+implementation, cached images are recognized by the name "Glance
+<uuid>" if this ever changed, this would need to change in both the
+external script and nova.  This breakdown is not preferred.
+
+The third option is use aggregates and a periodic function to
+disseminate precaching.  Aggregates were not considered as it is
+currently a nova optional/extension feature.  Storing the information
+and using a periodic function implies state which is not a goal of
+this proposal.  The storing of cached images implies some form of
+stickiness and management which is not a goal of this proposal.
+
+The last option is to change nothing - Let image be downloaded and
+cached on-demand.  The initial build on a host may be slow depending
+on image size.  Network throughput may also suffer if many instances
+are built together.
+
+Data model impact
+-----------------
+None.
+
+REST API impact
+---------------
+We would like to provide this for both V2 and V3 APIs.
+
+Perform precaching
+
+V2 API specification:
+POST: v2/os-hosts/{host_name}/action
+
+V3 API specification:
+POST: v3/os-hosts/{host_name}/action
+
+Request parameters:
+* host_name: The name of the host of interest to you.
+* precache: Specify precaching action in the request body
+* images: the UUIDs of images we wish to precache
+
+JSON request:
+{ "precache": {"images": ["695ca76e-fc0d-4e36-82e0-8ed66480a999"]}}
+
+JSON response:
+{"images": ["695ca76e-fc0d-4e36-82e0-8ed66480a999"]}
+
+JSON schema definition::
+
+    precache = {
+        'type': 'object',
+        'properties': {
+            'precache': {
+                'type': ['object', 'null'],
+                'properties': {
+                    'images': {
+                        "type": "array",
+                        items: {
+                            "type": parameter_types.image.ref
+                        }
+                    }
+                 },
+                'required': ['image']
+                'additionalProperties': False,
+            },
+        },
+        'required': ['precache'],
+        'additionalProperties': False,
+    }
+
+Sample v2 request:
+POST: v2/os-hosts/deathstar/action -d { "precache":
+{"image": ["695ca76e-fc0d-4e36-82e0-8ed66480a999"]}}
+
+Sample v3 request:
+POST: v3/os-hosts/deathstar/action -d { "precache":
+{"image": ["695ca76e-fc0d-4e36-82e0-8ed66480a999"]}}
+
+HTTP response codes:
+v2:
+* Normal HTTP Response Code: 200 on hint to start cache being delivered.
+* HTTP Response Code: 400 on failure due to extension not being available
+* HTTP Response Code: 409 on image property cache_in_nova not existing or False
+v3:
+Same as v2.
+
+Validation:
+'image' must be of a uuid-str format.
+Failure Response Code: HTTPBadRequest with "Invalid image ref format" message.
+
+Security impact
+---------------
+The new Admin API will add the associated policy settings.
+Policy.jsons will be added for POST precache images actions.
+It will default to admin_context.  It is recommended that
+ONLY deployer/operators be allowed to access this command.
+
+Notifications impact
+--------------------
+Notifications will be added to compute.metrics and
+compute.host.image.cache.{start,error,end}.
+
+The compute.metrics will include a new resource field called
+'compute.image.cache' and contain a list of currently cached images.
+Since this information is sent periodically it will reflect the
+current state of compute.
+
+compute.host.image.cache.{start,error,end} will follow the same model
+as compute.instance.create.{start,error,end}.  Sending notifications
+at the start of caching, error, end of caching.  Relevant fields
+which will be passed as part of the payload are:
+* image_ref_url: Image URL (from Glance) that is being cached
+* created_at: Timestamp for when caching was started
+* downloaded_at: Timestamp for when cached was completed
+* message: High-level message describing notification. if the event
+type is an error, will contain error details.
+
+Other end user impact
+---------------------
+None.
+
+Performance Impact
+------------------
+The performance could significantly speed up the problem of
+distributing images to many hosts and subsequent build times.
+
+By default configuration, once a day the host will query for each
+image to determine whether it's a cached image or not.  The hosts
+periodic test may be spread, but clustered in a multiple minute
+interval.  This means the glance service needs to be scaled to handle
+the API load.
+
+Other deployer impact
+---------------------
+Deployers must be aware that just because an image is precached
+does not mean it will persist forever.  Deployers will need to be
+aware of both driver operation and configuration as a
+driver can evict images.  We have added a cached_images filter
+but this must be configured and the image property honored.
+
+The steps to caching an image will consist of the following:
+1) Perform a glance PUT on the image which sets the property
+cache_in_nova to True.
+2) For hosts where precaching is useful call the precache API
+
+The steps to remove cached images not currently in use is:
+1) Perform a glance PUT on the image which sets the property
+cache_in_nova to False.
+2) Wait for the next periodic call of _run_image_cache_manager_pass
+which removes unused images.
+
+Customer image prepopulation can best be leveraged by directing
+customers to a specific cell.  The cell being prepopulated would give
+the best build times while limiting the prepopulation to a subset of
+the fleet.  Longer term, a follow-on blueprint which feeds cached
+image information to the scheduler would also be beneficial to this
+feature.  The follow-on work will be part of a separate blueprint.
+
+The impact from a host perspective is the same as the time and network
+bandwidth it takes to launch a VM of the image type without caching on
+a host.  We do not anticipate this changing what would be the impact
+of a single download.
+
+The feature overall does allow for downloads to run in parallel (even
+encourages it).  This means there is a larger system implication.
+Deployers will need to recognize how many images and boxes they want
+to cache at a single time.  This affects load on glance servers or
+other services providing images.
+
+Developer impact
+----------------
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Primary assignee:
+  leifz
+  belliott
+
+Other contributors:
+  alaski
+
+Work Items
+----------
+- Create api and hooks for drivers
+- Create precache xenapi driver reference implementation
+- Create a gating verification test with tempest/devstack
+- Update _run_image_cache_manager_pass to handle cached_images hook
+- Adjust libvirt driver imagecache to recognize cached_images hook
+- Create precache libvirt driver implementation (gate)
+
+Dependencies
+============
+None.
+
+Testing
+=======
+- devstack integration
+- Tests to poke the new admin API in Gating.
+
+Documentation Impact
+====================
+Need to document new Admin API.
+
+References
+==========
+None
diff --git a/specs/juno/proposed/compute-node-metrics-api.rst b/specs/juno/proposed/compute-node-metrics-api.rst
new file mode 100644
index 0000000..79348d1
--- /dev/null
+++ b/specs/juno/proposed/compute-node-metrics-api.rst
@@ -0,0 +1,149 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+Adding API to list the metrics of the hypervisors
+==================================================
+
+https://blueprints.launchpad.net/nova/+spec/compute-node-metrics-api
+
+We need to add an API to list the metrics of each hypervisor stored
+in the DB.
+
+Problem description
+===================
+
+With the Icehouse blueprint implementation
+https://blueprints.launchpad.net/nova/+spec/utilization-aware-scheduling,
+now various monitor plug-ins can be configured to report hypervisor
+metrics periodically. These metrics could be used in scheduling process
+through the MetricsWeigher configured by the administrator.
+
+Adding an API extension to list the available metrics of each compute
+node could help the administrator configure the MetricWeigher.
+
+Proposed change
+===============
+
+New API extensions will be added:
+
+GET /v2/os-hypervisors/{hypervisor_id}/metrics
+GET /v3/os-hypervisors/{hypervisor_id}/metrics
+
+
+Alternatives
+------------
+
+We could also change the currently API:
+
+    GET /v2/os-hypervisors/{hypervisor_id}
+    GET /v3/os-hypervisors/{hypervisor_id}
+
+to return metrics data along with the current hypervisor informations.
+
+However, consider the potential amount of data returned here(there
+might be dozens or hundreds metrics monitored), it would be better to
+have a separate API to list the metrics data.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+GET /v2/os-hypervisors/{hypervisor_id}/metrics
+GET /v3/os-hypervisors/{hypervisor_id}/metrics
+
+A array 'metrics' will be returned in the response of this API, listing
+all the metrics data available. The new response will looks like::
+
+    {
+        "metrics": [
+            {
+                "name": "metric.name1",
+                "source": "source",
+                "timestamp": "2014-04-01T00:00:00Z",
+                "value": 1.0,
+            },
+            {
+                "name": "metric.name1",
+                "source": "source",
+                "timestamp": "2014-04-01T00:00:00Z",
+                "value": 5.0,
+            },
+        ]
+    }
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  lianhao-lu
+
+Work Items
+----------
+
+None
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+None
+
+
+Documentation Impact
+====================
+
+This will impact the API doc.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/configure-tcp-keepalive.rst b/specs/juno/proposed/configure-tcp-keepalive.rst
new file mode 100644
index 0000000..cd4640f
--- /dev/null
+++ b/specs/juno/proposed/configure-tcp-keepalive.rst
@@ -0,0 +1,135 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Configure TCP Keepalive
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/configure-tcp-keepalive
+
+In this blueprint we aim to add configuration options to tune TCP keepalive.
+These will be used to enable/disable, and tune, TCP keepalive.
+
+
+Problem description
+===================
+
+To avoid socket leaks because of connections being left idle for any reason,the
+nova code has tcp keepalive automatically turned on in places like wsgi.py.
+However, there is no way to configure keepalive parameters.
+
+
+Proposed change
+===============
+
+nova.conf already has a tcp_keepidle setting. This is meant to be the the
+interval between the last data packet sent (excluding just ACKs) and the first
+keepalive probe; after the connection is marked to need keepalive, this counter
+is not used any further.
+In order to implement this blueprint I propose we specify the following
+configuration options:
+
+ * tcp_keepalive_interval, which is the interval between subsequential keepalive
+   probes
+ * tcp_keepalive_count, which is the number of unacknowledged probes to send before the
+   connection is considered dead
+ * tcp_keepalive_configure, which is the switch to turn on configuration of
+   tcp_keepalive. If this is False, default TCP values would be used.
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+This change will allow cluster owners to set tcp_keepidle, etc to values
+suitable for their setup, if they choose to do so. At present, this value
+defaults to 10 minutes (600 seconds), which is not necessarily good
+everywhere.
+
+Other deployer impact
+---------------------
+
+There will be three new configuration options in nova.conf. Deployers can
+choose to tune tcp keepalive by turning on the tcp_keepalive_configure
+flag. There is already an option tcp_keepidle for the initial idle window,
+so we only need to add options to set the probe interval and count. Some of
+these options are not supported on OS X, and the code must take this into account.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    praneshpg
+
+
+Work Items
+----------
+
+* Changes to be made to nova.conf to add more configuration options for
+  tcp_keepalive
+* Changes to code in nova to read in these config values, and handle systems
+  where some socket settings might not be available
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+There must be documentation to describe what the keepalive parameters mean, when
+and why they should be used, etc.
+
+References
+==========
+
+* TCP Keepalive HOWTO http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/index.html
+
diff --git a/specs/juno/proposed/console-tls-mode.rst b/specs/juno/proposed/console-tls-mode.rst
new file mode 100644
index 0000000..9006646
--- /dev/null
+++ b/specs/juno/proposed/console-tls-mode.rst
@@ -0,0 +1,147 @@
+..
+     This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================
+Enable tls mode for console access in openstack
+===============================================
+
+https://blueprints.launchpad.net/nova/+spec/console-tls-mode
+
+In this blueprint, we aim to connect to spice/vnc console
+of the vm using SSL wrapped socket, if enabled in the configuration.
+
+Problem description
+===================
+
+Both spice and vnc have tls mode configuration available in qemu.conf.
+If it is turned on, additional tls port is opened up for spice/vnc which
+accepts SSL connection.
+
+Currently openstack provides a way to connect to the spice/vnc console of vm
+using nova-spicehtml5proxy or nova-novncproxy.
+
+These proxies connect to the console using non-SSL socket.
+
+There should be a configuration option provided in nova.conf
+called console_tls_mode. If it is turned on,
+then the proxy will attempt to connect to the spice/vnc tls port using SSL.
+
+This will help provide authentication
+and encrypt all connections from proxy to the console.
+
+Proposed change
+===============
+
+ * Introduce console_tls_mode parameter in nova.conf.
+ * Today, when users request spice/vnc console url,
+   nova-consoleauth stores mapping of token -> console info
+   which includes port at which spice/vnc server is listening.
+   Change: normal port or tlsPort will be stored in the mapping
+   based on what console_tls_mode is configured in nova.conf.
+ * Currently, when users connect to the console url via novncproxy
+   or nova-spicehtml5proxy, nova-consoleauth will validate console port of vm,
+   which user is trying to connect by calling validate_console_port rpcapi.
+   It calls validate_console_port in nova compute manager.
+   Change: nova compute manager will validate normal port or tlsPort
+   based on what console_tls_mode is set in the configuration.
+ * Currently, nova websocketproxy uses non-SSL socket
+   to connect to spice/vnc server.
+   Change: nova websocketproxy will use SSL wrapped socket
+   to connect to spice/vnc server based on what console_tls_mode is configured.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+* Mapping corresponding to the token stored in consoleauth
+  will contain tlsPort or normal port based on what
+  console_tls_mode is configured.
+* If console_tls_mode is configured, nova websocket proxy will connect
+  to spice/vnc server using ssl wrapped socket.
+  This will provide authentication and encrypted communication
+  between proxy and compute node.
+
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+* If console_tls_mode is configured, deployer will have to ensure
+  ssl certificates are mentioned in compute driver config file.
+* Also, ensure that tls mode is turned on
+  so that spice/vnc server listens on secure tls port.
+
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  meghal
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+Defined above in Proposed Change section.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Existing tempest tests will have to be enhanced.
+
+Documentation Impact
+====================
+
+New console_tls_mode will have to be documented.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/cpu-allocation-per-flavor.rst b/specs/juno/proposed/cpu-allocation-per-flavor.rst
new file mode 100644
index 0000000..1423daf
--- /dev/null
+++ b/specs/juno/proposed/cpu-allocation-per-flavor.rst
@@ -0,0 +1,229 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================
+Filter, cpu allocation ratio per-flavor
+=======================================
+
+https://blueprints.launchpad.net/nova/+spec/cpu-allocation-per-flavor
+
+The aim of this blueprint is to provide an option to schedule for a specific
+flavor a vCPU overcommitment.
+
+The general use case is to offer to Operator the possibility to create flavors
+which consume less than 1 core of the host, also he wants to keep the
+possibility to mix different flavors.
+A resource management needs to be used by Operator to cap the timeslice used by
+all of the flavors running on Host. Nova provides this feature by
+configuring `Instance Resource Quota` [1]_, basically using a ratio between
+period and quota which matches the cpu allocation ratio will enforce the
+timeslice consumed by the flavors.
+
+Notes:
+- On system based on Linux, cgroups [2]_ can be used to isolate resource usage.
+- On Windows WSRM [3]_ provides resource management.
+
+
+Problem description
+===================
+
+Currently `CoreFilter` and `CoreFilterAggregate` provide the ability to
+schedule on a host more vCPUs that the host can handles. This feature is
+provided by configuring the option "cpu_allocation_ratio" in nova.conf or in
+aggregates metadata for a group of hosts.
+
+Operator wants the possibility to configure the vCPU over-commitment for
+specific flavors.
+
+Over-commitment based on host
+-----------------------------
+
+:HostA: 2 cores, cpu_allocation_ratio=2
+:FlavorA: 1 vCPU
+:FlavorB: 2 vCPU
+
+For a requested flavor the scheduler will interprets:
+ - FlavorA: 1 vCPU / 2 = 0.5 core
+ - FlavorB: 2 vCPU / 2 = 1 core
+
+Over-commitment based on flavor
+-------------------------------
+
+:HostA: 2 cores
+:FlavorA: 1 vCPU, cpu_allocation_ratio=2
+:FlavorB: 2 vCPU
+
+For a requested flavor the scheduler will interprets:
+ - FlavorA: 1 vCPU / 2 = 0.5 core
+ - FlavorB: 2 vCPU / 1 = 2 cores
+
+Over-commitment based on flavor + host
+--------------------------------------
+
+:HostA: 2 cores, cpu_allocation_ratio=2
+:FlavorA: 1 vCPU, cpu_allocation_ratio=4
+:FlavorB: 2 vCPU
+
+For a requested flavor the scheduler will interprets:
+ - FlavorA: 1 vCPU / 4 = 0.25 core
+ - FlavorB: 2 vCPU / 2 = 1 core
+
+Proposed change
+===============
+
+The purpose of this blueprint is to extend CoreFilterAggregate to create
+a new filter called CoreFilterFlavor which could be able to read in the flavor
+extra-specs the option `cpu_allocation_ratio` - If no value is found, the
+filter fallback to the per-aggregate logic then to the global configuration in
+nova.conf to schedule the requested flavor.
+
+1. Consider Host of 2 cores hosting 2 VMs, those 2 VMs created with flavors
+   ratio as 1.5 and 3.0
+
+It means:
+::
+
+    1 vCPU / 1.5 = 0.6
+  + 1 vCPU / 3.0 = 0.3
+  = 0.9
+
+Host is using 0.9 of his 2 cores. OK 0.9 <= 2
+
+2. Now consider launching a new guest with the ratio 3.0
+
+It means:
+::
+
+    1 vCPU / 1.5 = 0.6
+  + 1 vCPU / 3.0 = 0.3
+  + 1 vCPU / 1.5 = 0.6
+  = 1.5
+
+Host is using 1.5 of his 2 cores. OK 1.5 <= 2
+
+3. Now consider launching a new guest with the ratio 1.5
+
+It means:
+::
+
+    1 vCPU / 1.5 = 0.6
+  + 1 vCPU / 3.0 = 0.3
+  + 1 vCPU / 1.5 = 0.6
+  + 1 vCPU / 1.5 = 0.6
+  = 2.1
+
+Host is using 1.5 of his 2 cores. KO 2.1 <= 2
+
+Alternatives
+------------
+
+A work based on a new metric has been started on the wiki [4]_
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Performance Impact
+------------------
+
+The scheduler needs to retrieve flavors running on Host to compute the ratio
+used by each guests.
+
+Other deployer impact
+---------------------
+
+* Operator needs to update the scheduler's `nova.conf` to activate the
+  filter. *Important: CoreFilterFlavor cannot be used with CoreFilter or
+  CoreFilterAggregate*. For more information about filters please refer to
+  the doc [5]_.
+
+::
+  scheduler_default_filters=CoreFilterFlavor
+
+* To configure the ratio for a flavor Operator needs to set the option
+  `cpu_allocation_ratio` into the extra-specs of the flavor.
+
+::
+  nova flavor-key m1.small set cpu_allocation_ratio=2
+  or
+  nova-manage flavor set_key --name m1.small
+  --key cpu_allocation_ratio --value 2
+
+* Complete example to configure a flavor that uses 1/4 of a core Host
+
+::
+  nova flavor-key m1.small set cpu_allocation_ratio=4
+  nova flavor-key m1.small set quota:cpu_period=1000000
+  nova flavor-key m1.small set quota:cpu_quota=250000
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  sahid-ferdjaoui
+
+
+Work Items
+----------
+
+None
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Unit tests can cover all of the feature.
+
+
+Documentation Impact
+====================
+
+* 'doc/source/devref/filter_scheduler.rst' needs to be updated.
+
+
+References
+==========
+
+.. [#] https://wiki.openstack.org/wiki/InstanceResourceQuota
+.. [#] https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt
+.. [#] http://en.wikipedia.org/wiki/Windows_System_Resource_Manager
+.. [#] https://wiki.openstack.org/wiki/Nova/SchedulingBasedOnInstanceResourceQuota
+.. [#] http://docs.openstack.org/developer/nova/devref/filter_scheduler.html
diff --git a/specs/juno/proposed/creating-hyperv-ha-instances.rst b/specs/juno/proposed/creating-hyperv-ha-instances.rst
new file mode 100644
index 0000000..2089e06
--- /dev/null
+++ b/specs/juno/proposed/creating-hyperv-ha-instances.rst
@@ -0,0 +1,189 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================================================
+HyperV nova driver enhancement to create highly available instances
+===================================================================
+
+https://blueprints.launchpad.net/nova/+spec/creating-hyperv-ha-instances
+
+This blueprint creating-hyperv-ha-instances allows creating of highly
+available instances in a HyperV hosts that are in a failover cluster.
+Highly available virtual machines are defined by the the document
+http://technet.microsoft.com/en-us/library/cc967323.aspx as
+'Highly available virtual machines, also known as HAVMs, can easily be
+migrated to a different virtual machine host in a failover cluster to provide
+continuing service when their current host needs maintenance. If their current
+host fails, the HAVMs automatically migrate to a different host in the cluster
+through a process known as failover.'
+The following whitepaper provides more details.
+Failover Clustering in Windows Server 2008 R2 - Whitepaper
+
+Problem description
+===================
+
+Existing HyperV nova driver does not provide the benefits available
+on HyperV hosts configured in a failover cluster and therefore when a
+host in the cluster fails the instance is not available to the user.
+
+Proposed change
+===============
+
+Add support to configure high availability for an instance in the
+HyperV nova driver. This is done by modifying the instance creation
+steps in the following way
+
+* 1. Create instances on shared storage
+* 2. Configure the instance as highly available
+
+Also, the image cache should also be created on the shared storage
+
+A new boolean config option failover_clustering will be added. When the option
+is set to true, the proposed changes take effect.
+
+The user can override the config on a per instance basis by specifying the
+property hyperv_ha as false
+
+Alternatives for implementation of the proposed change
+
+* 1. Using the config option approach the existing driver behaviour can be
+  modified without adding a new driver deriving from the existing driver.
+  However with this approach, the implementation will have conditional switch
+  since WMI calls to get cluster data are done using a different namespace
+  and classes.
+* 2. Create a new driver that derives from the existing driver. Only override
+  the methods that require changes.
+
+It is proposed to using alternative 1 described above since the changes are
+limited to spawn method.
+
+Configuration requirements that have to be met by the HyperV host for
+HA to work correctly
+(Ref: http://technet.microsoft.com/en-us/library/cc742396.aspx)
+
+Networking - All nodes in the same cluster must use the same name for the
+virtual network that provides external networking for the virtual machines.
+
+Processor - If the nodes in the cluster use different processor versions,
+make sure that the virtual machine is configured for processor compatibility.
+
+Security - All nodes in the cluster must use the same authorization policy.
+
+Storage - Must use shared storage that is supported by both the Failover
+Clustering feature and Hyper-V.
+
+Alternatives
+------------
+
+* 1. A single driver manages all the hosts in the cluster. The problem
+  with this is the server hosting the nova-compute should always be available.
+  To make this work the nova-compute service must be made a clustered service.
+* 2. The nova-compute runs in a VM that is hosted in a host in the cluster.
+  This VM is configured as a highly available VM. The drawback of this
+  approach is that the remote WMI are slow and therefore will impact the
+  performance of the driver.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Instance deployments will have additional WMI calls done to detect the shared
+storage. However since the image cache is on the shared storage, the number of
+downloads from glance is reduced. Earlier each host has its own cache and
+therefore needed to download its own copy.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+* This change only impacts the HyperV nova driver. Other drivers will not be
+  impacted due to this change.
+
+Implementation
+==============
+
+The connections are done to the root/MSCluster namespace
+First, the HA resource group of the VMs disk is obtained using the query
+ASSOCIATORS OF {MSCluster_Resource.Name=insert_disk_name_here}
+where ResultRole = GroupComponent
+ResultClass = MSCluster_ResourceGroup
+If its not part of a group a new group can be created using CreateGroup
+
+The resource is created using the CreateResource method
+The resource is brough online using BringOnline method
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  kiran-kumar-vaddi
+
+Other contributors:
+
+Work Items
+----------
+
+* Modify the code the selects the location to spawn an instance to select only
+  shared storage (Cluster Shared Volumes)
+* Enable HA on the instance before starting the instance
+* The image cache must be on shared storage
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+The unit tests will be modified to test the branches introduced by the above
+work items
+
+Documentation Impact
+====================
+
+A new boolean config option failover_clustering will be added. When the option
+is set to true, the instances are configured as highly available instances.
+
+The user can override the config on a per instance basis by specifying the
+property hyperv_ha as false.
+
+References
+==========
+
+Failover Clusters in Windows Server 2008 R2
+http://technet.microsoft.com/en-us/library/ff182338(v=ws.10).aspx
+
+Failover Clustering Overview (Win 2012 and Win 2012 R2)
+http://technet.microsoft.com/en-us/library/hh831579.aspx
\ No newline at end of file
diff --git a/specs/juno/proposed/data-transfer-plugin.rst b/specs/juno/proposed/data-transfer-plugin.rst
new file mode 100644
index 0000000..2c1def5
--- /dev/null
+++ b/specs/juno/proposed/data-transfer-plugin.rst
@@ -0,0 +1,148 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================================
+Add a service plug-in for data transfer among nova nodes
+========================================================
+
+https://blueprints.launchpad.net/nova/+spec/data-transfer-service-plugin
+
+
+Nova needs an abstract plug-in module to provide the data transfer service
+for the data communication between machines. There are two general use
+cases for this data transfer:
+
+1. The machines are located in one network, e.g. one domain, one cluster,
+etc. The characteristic is the machines can access each other directly via
+the IP addresses(VPN is beyond consideration).
+2. The machines are located in different networks, e.g. two data centers,
+two firewalls, etc. The characteristic is the machines can not access each
+other directly via the IP addresses(VPN is beyond consideration).
+
+
+
+Problem description
+===================
+
+Nova has implemented the native functions for some hypervisors to do live
+migration, e.g. libvirt, VMware VCenter, etc. Shared storage(NFS) and
+block(iSCSI) migration have been implemented as well. To meet the need of
+migrating data(like VMs) between the machines within one network as the
+first use cases mentioned above, this is fairly enough. To transfer large data
+(e.g. 50G) from one machine to another in a different network or outside the
+firewall, in which case we are unable to establish the NFS or iSCSI
+connection, other transfer protocols, like FTP, need to be taken into account.
+
+This proposal will abstract a data transfer plug-in for the transfer protocols
+to implement. Implementations for any protocols, like NFS, iSCSI, ftp,
+bitTorrent, etc, can be put in this module for nova to use.
+
+Furthermore, FTP will be taken to transfer data between two machines in
+different networks fill up the second use case. BitTorrent, which has been
+implemented for image download in Xen, can be implemented as well for
+transferring the same data among many machines simultaneously.
+
+
+Proposed change
+===============
+
+* A new data transfer module is created to modulize all the transfer protocols,
+  providing some basic interfaces to implement, including creating a connection,
+  closing a connection, transferring data out, and transferring data in, etc.
+
+* Nova has already implemented an image download module and another image upload
+  module has been proposed(https://review.openstack.org/#/c/84671/). The
+  implementation for this proposal can be built based on two of these modules.
+
+Alternatives
+------------
+
+It is possible to implement different transfer protocols in different ways, but
+putting all of them in one module seems to be a neat way to manage them.
+
+Abstracting this module as a separate library is also a good idea, but at the
+beginning, this proposal needs another one upload plug-in to be finished first.
+This one can reuse the code of the download and upload plug-in.
+
+Data model impact
+-----------------
+
+A new module for the data transfer will be introduced.
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Different protocols may be applied for different use cases. General speaking, using
+a proper protocol for a proper use case will improve the data transfer performance.
+
+Other deployer impact
+---------------------
+
+The deployer will configure differently for different transfer protocols.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  houshengbo(Vincent Hou)
+
+
+Work Items
+----------
+
+* TBD
+
+
+Dependencies
+============
+
+* The image upload module plug-in: https://review.openstack.org/#/c/84671/
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+* Protocol performance may vary in different conditions:
+  http://amigotechnotes.wordpress.com/2013/12/23/file-transmission-with-diffe
+  rent-sharing-solution-on-nas/
diff --git a/specs/juno/proposed/datastore-image-cache-update-improvements.rst b/specs/juno/proposed/datastore-image-cache-update-improvements.rst
new file mode 100644
index 0000000..875a9a6
--- /dev/null
+++ b/specs/juno/proposed/datastore-image-cache-update-improvements.rst
@@ -0,0 +1,197 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================================
+VMware nova driver - datastore image cache update improvements
+==============================================================
+
+https://blueprints.launchpad.net/nova/+spec/datastore-image-cache-update-improvements
+
+This blueprint allows images to launch faster by copying images from other
+image caches, rather than always from glance. With VMware ESX, you copy
+images from glance, to nova-compute, then from nova-compute to the
+datastore. Its much more efficient to use vCenter APIs to copy the image
+from a datastore where the image is already cached.
+
+Problem description
+===================
+
+When using the VMware ESX hypervisor driver each ESX cluster is presented
+to Nova as a single compute node, with the nova-compute service acting as
+a proxy into the VMware controller. In the general Nova model each compute
+node is considered a separate service with its own image cache, but in the
+VMware case this it is possible to provide a more efficient image cache
+mechanism which extends across compute nodes.
+
+A datastore is a logical container that stores the virtual machine files.
+This includes both the disk (vmdk), configuration (vmx) files and
+snapshot files.
+
+A datastore can be created from different types of physical storage
+like local direct attached storage, iSCSI, FC SAN and NFS.
+The physical storage is presented to all host in the cluster and therefore
+the datastore is accessible from all the hosts of the cluster.
+Each cluster will typically have its own set of datastores and this is
+the recommended configuration. This configuration enable the live migration
+of VMs across all hosts of the cluster.
+
+In the existing design, when an instance creation is done, the following occur
+
+* Step 1.The existence of the image is checked in the cache
+  (directory named _base_dir) on the clusters datastore
+* Step 2.If the image is not available in the cache, then
+  (i) the image is downloaded from glance into nova-compute
+  (the VM where the compute service runs)
+  (ii) Then the image is transferred from the nova-compute to the datastore
+  by vCenter
+* Step 3.Instance is spawned using the cached image
+
+Step 2 is a CPU intensive and time and bandwidth consuming task.
+The existing optimization being done throttle the number of images
+transferred and removed unnecessary queues.
+This is applicable for the first time the image is transferred to
+a datastore cache.
+
+Since the cache is maintained at a per cluster level, In case the same image
+is deployed into another cluster that exists on the same vCenter
+the process is repeated. This would mean that the same image
+is getting transferred multiple times between glance and nova for each
+cluster present in vCenter.
+
+Proposed change
+===============
+
+If an image is available in the cache of one cluster, then copy of
+image from glance to nova-compute can be completely avoided by using
+vCenter APIs to copy the cached image from one datastore to another datastore.
+By using vCenter the progress of the task can be tracked as well.
+This approach works for clusters within the same vCenter.
+With this approach the modified steps would be
+
+* Step1. The existence of the image is checked in the cache (_base directory)
+  on ALL the clusters datastores using vCenter APIs
+* Step2. If the image is not available in any clusters cache, then the
+  image is downloaded from glance into nova-compute, and then from the
+  compute service to the datastore in vCenter (i.e existing functionality)
+* Step3. If the image is available in the cache of another cluster, then the
+  image is copied to the other clusters cache using the vCenter API. This
+  is very much efficient than the two hop transfer.
+* Step4. Instance is spawned using the cached image as normal.
+
+The list of images available in the cache will be maintained in a map on a
+per cluster basis. In the VMware nova driver, there already exists a
+periodic task that deletes unused images from the cache. The same periodic
+task will be enhanced to update the map that has the details of the
+images available in cache
+
+Alternatives
+------------
+
+Another alternative is to schedule instances to the clusters that have the
+image already cached. This will work fine until the cluster has capacity.
+To use this alternative, the following changes will be required
+(1) the VMware nova driver will have to publish the
+list of images in its cache.
+(2) A new scheduler filter will then select the nova-compute (cluster) that
+already has the image in its cache. If there are no clusters that have the
+image then it would return all clusters (existing behaviour)
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Given that in a private cloud there will be different set of standard
+images, and the number of clusters in vCenter can be between 12 to 64,
+enhancing the model used to update the image cache in the datastore will
+significantly improve user experience, reduce instance creation times,
+reduce CPU load on the server(typically VM) running nova-compute and
+reduce traffic on the network
+In the intial tests we notice that a tranfer of a 2GB image takes
+3 minutes. The CPU utilization of nova-compute during image copy goes
+upto 80% for the time of the transfer.
+After this change, it will take 1 minute and nova-compute CPU will not be
+used for the transfer, since the transfer is delegated to vCenter and
+vCenter does utilizes less amount of CPU to do the transfer.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+* The similar approach can be used by other nova drivers that support clusters
+  and cache images on a per cluster basis.
+* This change only impacts the VMware nova driver. Other drivers will not be
+  impacted due to this change.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  kiran-kumar-vaddi
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* Build the map of image cache ids on start of the compute service
+* Update the map of the image cache ids when new image is uploaded
+* During spawn, If image is already present in the cache then use the new
+  code to copy the image from one datastore to another
+* During spawn, If image is _not_ already present in the cache then use
+  existing code.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit tests are sufficient since the actual copy is done by the vCenter API
+The unit tests will be modified to test the branches introduced
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/db-sync-models-with-migrations.rst b/specs/juno/proposed/db-sync-models-with-migrations.rst
new file mode 100644
index 0000000..804f196
--- /dev/null
+++ b/specs/juno/proposed/db-sync-models-with-migrations.rst
@@ -0,0 +1,217 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================================
+Sync db models with migrations
+======================================================================
+
+https://blueprints.launchpad.net/nova/+spec/db-sync-models-with-migrations
+
+We use declarative_base in nova.db.models just for reflection,
+not for db creation. For creation we use migrations. The actual state of
+the models is not the same as the state of a database.
+
+
+Problem description
+===================
+
+Db models don't reflect actual state of database correctly. For example, some
+indexes are in db, but were skipped in models, and value of nullable parameter
+for some columns is not as in models.
+
+Also we need to add tests to compare the schema to models. We need to perform
+this check for mysql and postgresql as there are cases when different engines
+handle things differently.
+
+Proposed change
+===============
+
+1. Add missed indexes and unique constraints to __table_args__.
+2. Correct nullable values for columns.
+3. Fix indexes which are not the same as described in model.
+4. Add opportunistic tests on mysql and postgresql to compare models to final
+   schema versions after all migrations have been applied. For comparing schema
+   to models we are going to use alembic tools, so we won't need to handle
+   cases when different engines apply migrations differently - (for example,
+   when creating fk).
+
+This will probably allow us to find some mistakes or missing indexes and make
+work with db more transparent.
+
+
+Alternatives
+------------
+
+* do nothing. This is an undesirable alternative, because without comparing
+  schema and models we can't be sure that our migrations are correct, i.e.
+  result of migration complies with db models and migration does what we want.
+  Actually the lack of consistense is the reason why there are some mistakes
+  in migrations now.
+
+* generate schemas from models, move to different migration engines.
+
+  For generating schemas from models we need to be sure that our models haven't
+  got mistakes (for example, now there are some indexes which are being
+  skipped). We need to add testing of schema and migrations at this point to
+  find mistakes.
+
+  Moving nova to alembic was discussed some time ago and some work was done in
+  this direction (https://review.openstack.org/#/c/15196/2), but it have not
+  been merged by now. The suggested approach required converting Nova database
+  migration scripts from sqlalchemy-migrate to alembic by the autoconversion
+  script. So, in this case, checking schema and migrations would be useful
+  to be sure that our migrations are correct. This also applies to the
+  case if we choose manual convertion of migration scripts to alembic.
+
+
+Data model impact
+-----------------
+
+It is possible that some changes which affect the data model will be required
+if we find some discrepancies.
+
+As we want db state to comply with models, we have to:
+
+* fix mistakes with nullable parameters for columns (we need a migration for
+  this). For some tables and columns the value of a nullable parameter defined
+  in migration is not as described in models.
+
+    * 'pci_devices'
+        * 'deleted'
+        * 'dev_type'
+        * 'product_id'
+        * 'vendor_id'
+    * 'quota_usages'
+        * 'resource'
+
+  *Compatibility with Icehouse:*
+
+  1. We want to make column 'deleted' of table 'pci_devices' nullable. This
+     change will be compatible with Icehouse.
+  2. Making other columns listed above not nullable. Problems can arise if
+     users already have NULL values there. When upgrading we can check if
+     there are NULL values in these columns and convert them to some string
+     constant (as all this columns have string type) before altering these
+     columns. This migration doesn't require changes to database api or
+     nova.objects.
+
+* add missing indexes to __table_args__ :
+
+    * 'quota_usages'
+        * 'ix_quota_usages_user_id'
+    * 'reservations'
+        * 'ix_reservations_user_id'
+
+  *Compatibility with Icehouce:* adding missing indexes won't affect
+  compatibility.
+
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Some impact is possible if we change the value of a nullable parameter for
+columns in some tables.
+
+We need to do this for the following tables and columns:
+
+    * 'pci_devices'
+        * 'deleted'
+        * 'dev_type'
+        * 'product_id'
+        * 'vendor_id'
+    * 'quota_usages'
+        * 'resource'
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+This blueprint requires some database schema migrations. We will need
+migrations to:
+
+* fix nullable parameters for some columns in 'quota_usages' and 'reservations'
+  tables.
+
+* fix indexes in following tables:
+    * 'migrations'
+        * 'migrations_instance_uuid_and_status_idx'
+    * 'quota_usages'
+        * 'ix_quota_usages_user_id_deleted'
+    * 'reservations'
+        * 'ix_reservations_user_id_deleted'
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+
+Primary assignee:
+  jvarlamova
+
+
+Work Items
+----------
+
+1. Add missed indexes and unique constraints to __table_args__.
+2. Correct nullable values for columns.
+3. Correct indexes.
+4. Add tests to compare models to final schema versions after migrations are
+   applied.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+We will add opportunistic testing for mysql and postgresql to ensure that now
+models and migrations are in sync. We will perform migrations on mysql and
+postgresql and compare schema to models (using OpportunisticTestCase from
+oslo). For comparing schema to models we are going to use alembic tools.
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+Similar blueprints in other OpenStack projects:
+
+https://blueprints.launchpad.net/openstack/?searchtext=db-sync-models-with-migrations
diff --git a/specs/juno/proposed/default-quota-flavor.rst b/specs/juno/proposed/default-quota-flavor.rst
new file mode 100644
index 0000000..b3227c2
--- /dev/null
+++ b/specs/juno/proposed/default-quota-flavor.rst
@@ -0,0 +1,216 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================
+Default project quota flavor
+============================
+
+https://blueprints.launchpad.net/nova/+spec/default-quota-flavor
+
+This blueprint is to define a default quota flavor for a specific project.
+
+The main use for this is to set up a quota flavor used to configure by default
+the project user quotas when the user is created.
+
+
+Problem description
+===================
+
+Currently when an operator creates a new user, the quotas have to be configured
+manually one by one based on the user requirements.
+
+This blueprint comes from the operators feedback:
+https://etherpad.openstack.org/p/operators-feedback-mar14 -> line 448
+
+
+Proposed change
+===============
+
+To add a relation between a quota flavor and a project. This will be used
+during the user creation process to define the user quotas.
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+Add this table to the db:
+
+Model DefaultProjectQuotaFlavor
+    """Represents the relation between a project and a quota flavor"""
+
++------------------+------------+----------+-------+
+| Name             | Type       | Nullable |   PK  |
++==================+============+==========+=======+
+| id               | Integer    | False    | True  |
++------------------+------------+----------+-------+
+| tenant_id        | String(255)| False    | False |
++------------------+------------+----------+-------+
+| quota_flavor_id  | String(255)| False    | False |
++------------------+------------+----------+-------+
+
+
+REST API impact
+---------------
+
+Get Default Quota Flavor
+************************
+
+* Method: GET
+* Path: /os-quota-flavors/default/{tenant_id}
+* Desc: Shows the default quota flavors for a tenant
+* Resp: Normal Response Codes 200
+
++------------+-------+--------+-----------------------------------------------+
+| Parameter  | Style | Type   | Desc                                          |
++============+=======+========+===============================================+
+| tenant_id  | URI   | string | The ID for the tenant for which you want to   |
+|            |       |        | list the default quota flavor                 |
++------------+-------+--------+-----------------------------------------------+
+
+Set Quota Flavor
+*******************
+
+* Method: PUT
+* Path: os-quota-flavors/default/{tenant_id}
+* Desc: Set a quota flavor as default for a specific tenant
+* Resp: Normal Response Codes 200
+
++------------+-------+--------+-----------------------------------------------+
+| Parameter  | Style | Type   | Desc                                          |
++============+=======+========+===============================================+
+| tenant_id  | URI   | string | The ID for the tenant for which you want to   |
+|            |       |        | associate the quota flavor                    |
++------------+-------+--------+-----------------------------------------------+
+| flavor_name| plain | string | The name for the flavor to be set as default  |
++------------+-------+--------+-----------------------------------------------+
+
+Update Quota Flavor
+*******************
+
+* Method: POST
+* Path: os-quota-flavors/default/{tenant_id}
+* Desc: Updates a quota flavor as default for a specific tenant
+* Resp: Normal Response Codes 200
+
++------------+-------+--------+-----------------------------------------------+
+| Parameter  | Style | Type   | Desc                                          |
++============+=======+========+===============================================+
+| tenant_id  | URI   | string | The ID for the tenant for which you want to   |
+|            |       |        | associate the quota flavor                    |
++------------+-------+--------+-----------------------------------------------+
+| flavor_name| plain | string | The name for the flavor to be set as default  |
++------------+-------+--------+-----------------------------------------------+
+
+Delete Quota Flavor
+*******************
+
+* Method: DELETE
+* Path: os-quota-flavors/default/{tenant_id}
+* Desc: Deletes a default quota flavor for a tenant
+* Resp: Normal Response Codes 200
+
++------------+-------+--------+-----------------------------------------------+
+| Parameter  | Style | Type   | Desc                                          |
++============+=======+========+===============================================+
+| tenant_id  | URI   | string | The ID for the tenant for which you want to   |
+|            |       |        | delete the default quota flavor               |
++------------+-------+--------+-----------------------------------------------+
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+This change affects the client user interface. The following commands are
+added:
+
+quota-flavor-default
+    --tenant-id <tenant_id>
+
+quota-flavor-default-update
+	--tenant-id <tenant_id>
+	--flavor-name <flavor_name>
+
+quota-flavor-default-delete
+	--tenant-id <tenant_id>
+
+Performance Impact
+------------------
+
+None
+
+
+Other deployer impact
+---------------------
+
+None
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee: sergio-j-cazzolato
+Other contributors: gseverina
+
+
+Work Items
+----------
+
+None
+
+
+Dependencies
+============
+
+Depends on blueprint quota-flavors:
+https://blueprints.launchpad.net/nova/+spec/quota-flavors
+
+
+Testing
+=======
+
+None
+
+
+Documentation Impact
+====================
+
+Documentation needed for:
+
+* Rest API
+* Client Interface
+
+
+References
+==========
+
+Link to notes from a summit session:
+https://etherpad.openstack.org/p/operators-feedback-mar14 -> line 448
diff --git a/specs/juno/proposed/default-quotas.rst b/specs/juno/proposed/default-quotas.rst
new file mode 100644
index 0000000..560a117
--- /dev/null
+++ b/specs/juno/proposed/default-quotas.rst
@@ -0,0 +1,229 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============
+default-quotas
+==============
+
+https://blueprints.launchpad.net/nova/+spec/default-quotas
+
+This blueprint allows the operators to update and show the default quotas
+through the nova API.
+
+
+Problem description
+===================
+
+Currently an operator is not able to show or update the default quotas trough
+the nova API, it has to be done through configuration.
+
+Proposed change
+===============
+
+To address this I propose to use the current quotas API to define the default
+quotas by sending 'defaults' instead of the tenant_id.
+
+Those default values will be stored in the database as any other value.
+This mechanism will allow to set up default values for resources created in
+runtime such as quotas by flavors named flavor_<flavor_id>.
+
+In case default values are already stored in the nova.conf, those won't be
+ported to the db to keep backward compatibility.
+
+When a default value is changed through the API, it won't overwrite the
+nova.conf, Considering the scenario when there are many nova API running in
+parallel. When a default value is defined in both db and config, the used will
+be the db.
+
+By default, in case a default is not defined in the db neither in the config
+the value used for the missing resource will be -1.
+
+The default values in the config will be set as deprecated and a warning will
+be raise in the logs when nova API is started in case a default value is set.
+
+Alternatives
+------------
+
+An alternative is to add metadata to the resources with the default quota
+information. For instance, to add an extra-spec to the flavor indicating its
+default quota value and use the configuration for the current resources.
+
+
+Data model impact
+-----------------
+
+Tables are not impacted.
+
+The default quota values will be stored in the same table being empty the
+project_id column.
+
+A Migration script has to be created in order to update the default values
+based on the configuration values.
+
+
+REST API impact
+---------------
+
+Change in the request to get the default quotas.
+* Method: GET
+* Path: /os-quota-sets/defaults
+* Resp: Normal Response Codes 200
+
+JSON response
+
+{
+ "quota_set": {
+  "cores": 20,
+  "fixed_ips": -1,
+  "floating_ips": 10,
+  "injected_file_content_bytes": 10240,
+  "injected_file_path_bytes": 255,
+  "injected_files": 5,
+  "instances": 10,
+  "key_pairs": 100,
+  "metadata_items": 128,
+  "ram": 51200,
+  "security_group_rules": 20,
+  "security_groups": 10
+
+ }
+
+}
+
+Change in the request to update the default quotas.
+* Method: POST
+* Path: /os-quota-sets/defaults
+* Resp: Normal Response Codes 200
+
+JSON response:
+
+{
+ "quota_set": {
+  "force": "True",
+  "instances": 9,
+  "ram": 102400
+
+ }
+
+}
+
+There are not changes in the delete operation.
+
+There are not changes in the JSON Schema.
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+This change affects the CLI by adding a new optional values when default quotas
+are shown/updated:
+
+quota-show
+    --tenant-id <tenant_id>
+    --user <user-id>
+    --defaults
+
+quota-update
+    --tenant-id <tenant_id>
+    --user <user-id>
+    --defaults
+    --instances <instances>
+    --cores <cores>
+    --ram <ram>
+    --floating-ips
+    --fixed-ips <fixed-ips>
+    --metadata-items <metadata-items>
+    --injected-files <injected-files>
+    --injected-file-content-bytes <injected-file-content-bytes>
+    --injected-file-path-bytes <injected-file-path-bytes>
+    --key-pairs <key-pairs>
+    --security-groups <security-groups>
+    --security-group-rules <security-group-rules>
+    --force
+
+In both cases when the --defaults parameter is passed, if --tenant and/or
+--user are passed too, an error message will be displayed.
+
+Performance Impact
+------------------
+
+The main impact on performance is when some quota operations will require to go
+to the db to get default quotas instead of get them from the configuration.
+
+Other deployer impact
+---------------------
+
+A migration script has to be executed to update the db with the default values
+based on the config.
+
+For the operators who currently use the config as a way to manage the quota
+default values, after this patch is applied, the quotas have to be managed
+through the nova API.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee: sergio-j-cazzolato
+
+
+Work Items
+----------
+
+Tasks to do as part of this bp:
+
+* Changes the quotas API and logic layer to support the defaults value.
+* Create the DB script.
+* Update the client to support the defaults.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest tests are needed to validate:
+
+* The API changes
+* The db migration
+
+Documentation Impact
+====================
+
+Documentation needed for:
+
+* Rest API
+* Client Interface
+* Operators Guide
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/default-schedule-zones.rst b/specs/juno/proposed/default-schedule-zones.rst
new file mode 100644
index 0000000..f719e7e
--- /dev/null
+++ b/specs/juno/proposed/default-schedule-zones.rst
@@ -0,0 +1,150 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================
+Support multiple default schedule availability zones
+====================================================
+
+https://blueprints.launchpad.net/nova/+spec/schedule-set-availability-zones
+
+Define multiple default schedule availability zones.
+
+
+Problem description
+===================
+
+For deployments that use availability zones (example: avz_a, avz_b, avz_c,
+avz_critical) when an user creates an instance if the availability zone is not
+explicitly defined (ex: --availability-zone=avz_b) the instance is schedule in
+the availability zone defined by the configuration option
+"default_schedule_zone".
+
+This is a problem because the resources for the "default_schedule_zone"
+will be overloaded compared with the others avzs if users don't define an
+available zone.
+
+If configuration option "default_schedule_zone" is not defined all availability
+zones are considered at schedule time. This is not interesting if some of them
+shouldn't be default (ex: avz_critical).
+
+
+Proposed change
+===============
+
+The goal of this blueprint is to have the possibility to have multiple schedule
+ availability zones as default, instead only one.
+
+This means:
+deprecate "default_schedule_zone" configuration option and have instead 
+"default_schedule_zones=[]"
+
+Using the same example of the Problem description:
+"default_schedule_zones=[avz_a, avz_b, avz_c]"
+
+When an instance is created, if the user doesn't define an availability zone,
+it will be schedule into one compute node in the list of default schedule
+availability zones. The advantage of this approach is that when a deployment has
+multiple availability zones the instances will be spread between them instead
+overload only one, if users don't specify an availability zone.
+
+To implement this change is necessary to remove to current behavior of
+saving in "instances" table the default "availability_zone" when a user doesn't
+specify one. Anyway when query for the avz of an instance nova doesn't use this
+value.
+
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+The configuration option "default_schedule_zone" will be deprecated and replaced
+by "default_schedule_zones=[]"
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <moreira-belmiro-email-lists>
+
+Work Items
+----------
+
+* deprecate "default_schedule_zone" and replace it by "default_schedule_zones"
+  configuration option.
+
+* if an user doesn't specify an availability zone it shouldn't be added in the
+  DB (remove the current behavior).
+
+* change AvailabilityZoneFilter to support multiple "default_schedule_zones".
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+All changes will have unit tests.
+
+
+Documentation Impact
+====================
+
+The documentation considering "default_schedule_zone" configuration option
+needs to be updated for "default_schedule_zones".
+
+
+References
+==========
+
+None.
+
diff --git a/specs/juno/proposed/deprecate-baremetal-driver.rst b/specs/juno/proposed/deprecate-baremetal-driver.rst
new file mode 100644
index 0000000..180fae8
--- /dev/null
+++ b/specs/juno/proposed/deprecate-baremetal-driver.rst
@@ -0,0 +1,229 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================
+Upgrade from a nova "baremetal" deployment to Ironic
+====================================================
+
+https://blueprints.launchpad.net/nova/+spec/deprecate-baremetal-driver
+
+This specification describes the requirements to providing an upgrade path from
+a deployment of the nova.virt.baremetal driver to the nova.virt.ironic driver.
+It outlines the data migration path and service upgrade process for such an
+upgrade.
+
+Problem description
+===================
+
+The community has split out the functionality of provisioning bare metal
+servers into a separate program, which includes the ironic and
+python-ironicclient projects. While the original intent of the
+nova.virt.baremetal driver was to be an experimental proof-of-concept for
+TripleO, it may have been deployed in some production environments to
+facilitate high-performance compute workloads.
+
+It is unreasonable to expect operators who have chosen to use
+nova.virt.baremetal to lose all state and delete all instances during an
+upgrade. Migration tools will be available within Ironic.
+
+NOTE: The service upgrade is only supported within the same release version,
+and will only be supported in the first integrated release containing Ironic.
+
+For example, if this work is completed during the Juno cycle, an upgrade from
+"juno baremetal" to "juno ironic" will be supported, but a direct upgrade from
+"icehouse baremetal" to "juno ironic" will NOT be supported. That should be
+accomplished by first upgrading from "icehouse baremetal" to "juno baremetal"
+and then to "juno ironic".
+
+Proposed change
+===============
+
+At the start of a release cycle following the cycle in which this work is
+completed, the artifacts of baremetal will be delete from the Nova tree.
+This includes: the baremetal virt driver, baremetal host manager, 'nova_bm'
+database schema and its migration tests.
+
+The API extension will be replaced by a read-only proxy API. This will forward
+the following API commands to Ironic:
+- baremetal-interface-list
+- baremetal-node-list
+- baremetal-node-show
+
+
+Alternatives
+------------
+
+Three alternatives have been discussed.
+
+* Do not provide any upgrade path; this met with significant opposition.
+
+* Provide a data-only migration (eg, require that instances be deleted
+  prior to, or as part of, the migration). This was also met with opposition.
+
+* Rather than a data extract-and-load script, one could enroll instances
+  via Ironic's REST API. This would require ironic's REST API to accept nodes
+  that have non-null provision_state and power_state, which it expressly
+  does not allow today. This change would require significant changes
+  to the provisioning API and state management within the conductor service.
+
+Data model impact
+-----------------
+
+The nova_bm schema and all supporting DB migration tests may be deleted.
+
+REST API impact
+---------------
+
+The baremetal extension to Nova's REST API will be replaced with a read-only
+proxy API to Ironic. This will check policy, and forward the user's token
+to Ironic for secondary validation (Ironic requires "admin" privileges).
+
+The following commands will NOT be proxied:
+- baremetal-interface-add
+- baremetal-interface-remove
+- baremetal-node-create
+- baremetal-node-delete
+
+The endpoints for these methods will return a 404 NOT FOUND.
+
+Notifications impact
+--------------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  devananda
+
+Other contributors:
+  adam_g
+  romcheg
+
+Work Items
+----------
+
+* Create new API proxy/extension
+
+* Database extraction-and-loading script
+  (in Ironic's tree)
+
+* Flavor update script
+  (in Ironic's tree)
+
+* Operator documentation
+  (in Ironic's tree)
+
+* Grenade tests
+  (in Grenade)
+
+Dependencies
+============
+
+This proposal depends primarily upon the acceptance of the nova.virt.ironic
+driver into the Nova codebase, and secondarily on grenade testing of the
+migration script and upon several open changes in tempest which will allow
+Ironic to pass tempest/api/compute.
+
+
+Testing
+=======
+
+A Grenade test will need to be developed that can:
+
+* deploy nova with the fake virt driver
+
+* populate nova_bm database with baremetal nodes and interfaces that map to
+  local VMs
+
+* create dummy images in glance for nova-bm's deploy kernel and ramdisk and
+  create a flavor referencing them
+
+* install ironic, build and publish new deploy kernel and ramdisk
+
+* perform data migration
+
+* reconfigure nova to use ironic, start ironic, and restart nova-compute
+
+* run tempest
+
+
+Documentation Impact
+====================
+
+Upgrade documentation must be written and maintained for one release cycle.
+
+The proposed upgrade path is:
+
+* build ironic deploy ramdisk and load it in glance
+
+* create empty ironic database
+
+* start maintenance period
+
+* stop nova-compute services which are configured to use the
+  nova.virt.baremetal driver
+
+* update flavor metadata in Nova to reference new deploy kernel & ramdisk
+
+* extract data from nova_bm and import to ironic, using the provided tool.
+  This tool must accept separate database credentials for each database.
+
+* start ironic services
+
+* observe ironic log files to ensure take over completed w/o errors
+
+* reconfigure nova-scheduler to use the ironic host manager, and, if desired,
+  the exact match scheduler filters, then restart it
+
+* reconfigure nova-compute service to use the nova.virt.ironic driver
+  and the ClusteredComputeManager, then restart it
+
+* observe nova-compute log files to ensure it has connected to ironic and is
+  reporting available resources accurately
+
+* end maintenance period
+
+References
+==========
+
+https://etherpad.openstack.org/p/juno-nova-deprecating-baremetal
+
+https://etherpad.openstack.org/p/juno-nova-mid-cycle-meetup
+
+https://review.openstack.org/#/q/topic:ironic_grenade,n,z
+
+https://review.openstack.org/#/q/topic:ironic_tempest,n,z
diff --git a/specs/juno/proposed/dnsmasq-options-config.rst b/specs/juno/proposed/dnsmasq-options-config.rst
new file mode 100644
index 0000000..1ed9aa5
--- /dev/null
+++ b/specs/juno/proposed/dnsmasq-options-config.rst
@@ -0,0 +1,146 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================
+ Support arbitrary DNSMasq options
+===================================
+
+https://blueprints.launchpad.net/nova/+spec/dnsmasq-options-config
+
+This feature will allow admins to specify arbitrary dnsmasq options in
+``nova.conf``.
+
+Problem description
+===================
+
+In order to specify options to the dnsmasq command run by
+``linux_net.py``, it is necessary to hack ``linux_net.py``; there is
+no way to specify arbitrary options currently.  Nova supports a small
+subset of options, but dnsmasq has many, many more.
+
+This is similar to the ``extra-dhcp-opts`` extension to Neutron that
+allows setting arbitrary, potentially vendor-specific DHCP options on
+specific ports.  Since Nova networking only supports a single "vendor"
+-- dnsmasq -- that simplifies matters somewhat.  Additionally, since
+Nova networking does not have hardware ports, the dnsmasq options set
+will apply to all clients.
+
+Proposed change
+===============
+
+Add a new ``dnsmasq_opts`` option to ``nova.conf`` that accepts a list
+of command-line options that will be passed directly to ``dnsmasq``.
+This will be a simple cherry-pick (with unit tests added) of
+https://review.openstack.org/#/c/20197/
+
+Alternatives
+------------
+
+Individual options could be added to ``nova.conf`` for each dnsmasq
+option needed.  This is not future-proof.
+
+Instead of always setting dnsmasq options globally, we could allow
+setting them on a per-network basis.  We already run one dnsmasq
+server per DHCP network, but we would need to find an elegant way to
+configure options for each network.  If this approach were taken, we
+would want to make at least some of the other dnsmasq options that
+have been abstracted individually into ``nova.conf`` similarly
+flexible -- ``dnsmasq_config_file``, ``dns_server``, ``multi_host``,
+etc.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+This will permit an administrator with access to modify ``nova.conf``
+to pass potentially dangerous or insecure options to dnsmasq.  This is
+not a security flaw *per se*; caveat configurator, obviously.
+
+Since write access to ``nova.conf`` is required to modify the dnsmasq
+options, there is no possibility of a privilege escalation attack
+unless permissions on ``nova.conf`` were already incorrect.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+This adds a new option to ``nova.conf``, but it has a sane default
+(empty) that matches the current state.  No changes will be necessary
+to existing deployment workflows.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  stpierre
+
+Other contributors:
+  nicolas.simonds
+
+Work Items
+----------
+
+* Cherry-pick https://review.openstack.org/#/c/20197/ onto the latest
+  master.
+* Add logic to ensure that ``dnsmasq_opts`` does not override or
+  duplicate any DNSMasq options set through other means (e.g., as
+  defaults or with other configuration options).  Update the sample
+  configuration to avoid containing a duplicate option
+  (``--strict-order``).
+* Add unit tests.
+
+All of this is done in https://review.openstack.org/#/c/97969/
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+Unit tests will be added for the new functionality.
+
+Documentation Impact
+====================
+
+Update the sample ``nova.conf`` to demonstrate the new options.
+
+References
+==========
+
+Original code review: https://review.openstack.org/#/c/20197/
diff --git a/specs/juno/proposed/docker-hypervisor-plugin.rst b/specs/juno/proposed/docker-hypervisor-plugin.rst
new file mode 100644
index 0000000..e6b6c8a
--- /dev/null
+++ b/specs/juno/proposed/docker-hypervisor-plugin.rst
@@ -0,0 +1,225 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Add the docker hypervisor plugin to Nova
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/docker-hypervisor-plugin
+
+As of the Icehouse release, the nova-docker hypervisor plugin was removed since
+so many of the Tempest tests were failing. Many of the tests were failing due
+to missing APIs in docker and subsequently missing functionality in the plugin.
+There has been significant progress implementing those missing features and
+fixing various bugs associated with the already existing features. By adding
+this plugin back to the Nova source tree, we will enable operators to more
+easily manage docker containers via Nova APIs.
+
+The main features, which were missing and are now implemented, causing many of
+the Tempest test failures were as follows:
+
+* Pause and Unpause - The API for pausing and unpausing a container was missing
+  in Docker as well as the hypervisor plugin. The API has now been added to
+  Docker and is now implemented in the plugin.
+
+* Snapshots - This was primarily broken due to image data not staying
+  synchronized between Glance and the Docker registry. This has been addressed
+  in a bug fix to Docker; however, we continue to refactor the Docker
+  hypervisor plugin and we'll start pushing Docker images directly into Glance
+  rather than pushing only image meta-data into Glance.
+
+* Volume Attach and Detach - The API for adding and removing a device to Docker
+  is missing. There is currently a patch set posted[2] for review, which adds
+  the capability to attach and detach a host device to a running docker
+  container.  Additionally, the implementation in the hypervisor plugin[3] has
+  been added to call the Docker API to add/remove a device to/from a container.
+  This will be merged once the Docker API has been merged.
+
+Problem description
+===================
+
+A detailed description of the problem:
+
+Nova is currently missing the capability of managing Docker containers. The
+Docker hypervisor plugin is useful to OpenStack operators in that it plugs into
+the compute design quite nicely. Implementing container management as a
+hypervisor plugin allows the use of container placement via the scheduler. It
+also allows the implementation of many Nova APIs without having a separate set
+of command line tools to invoke the Nova APIs.
+
+* In order for OpenStack Operators to manage docker containers with the
+  nova-docker hypervisor plugin, it requires that they pull in the source from
+  stackforge and manually configure Nova to use it. By adding it to the
+  directory of hypervisor plugins in Nova, Operators can configure the plugin
+  with ease.
+
+* Adding the driver to the Nova source tree has several benefits. 1) It will
+  expose the driver to continual integration testing. 2) Configuring the plugin
+  to work with Nova becomes trivial. 3) Developers and maintainers of the
+  plugin are continuously aware of Nova changes rather than having to chase
+  after the changes made. 4) The Docker hypervisor plugin becomes part of the
+  integration Nova release.
+
+Proposed change
+===============
+
+The proposal is that we move the stackforge/nova-docker directory, which
+contains all the code for the docker hypervisor plugin into the
+openstack/nova/nova/virt source directory along side the rest of the hypervisor
+plugins shipped with Nova.
+
+Alternatives
+------------
+
+Several alternatives exist to work around not having native Docker container
+management in OpenStack. They are as follows:
+
+* Get the nova-docker hypervisor plugin from stackforge and configure Nova to
+  use it.
+
+* Introduce a new container service in OpenStack that is completely independent
+  of Nova APIs and is soley focused on container capabilities.
+
+* Disregard the capabilities granted to a hypervisor plugin and just evolve
+  Docker container management as a Heat project plugin. This would add
+  the capability to create and destroy containers; however, the approach would
+  now allow the the use of granular Nova APIs, nor would it take advantage of
+  using the compute related services such as the scheduler.
+
+* Not include Docker container management in OpenStack at all.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+* The user (Operator) will be able to use the standard Nova APIs and command
+  line tools to manage docker containers.
+
+Performance Impact
+------------------
+
+* Containers allow lightweight instances to be started very fast.  They also
+  consume fewer resources on the host machine.
+
+Other deployer impact
+---------------------
+
+* The Docker hypervisor plugin will become easier to configure. The
+  configuration settings already exist in nova.conf to use the Docker
+  hypervisor plugin, the only change here is that the plugin code would be
+  installed with the rest of the hypervisor plugins.
+
+Developer impact
+----------------
+
+* There would be added consideration in future development to support
+  containers.
+
+Implementation
+==============
+
+* The Tempest integration failures need to be resolved, which we are making
+  good progress on
+
+* The stackforge code needs to be placed in the Nova source tree
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <ewindisch>
+
+Other contributors:
+  <calfonso>
+  <imain>
+
+Work Items
+----------
+
+* Copy the stackforge/nova-docker source code into nova/nova/virt.
+
+* Ensure all tests that logically apply to containers pass.
+
+Dependencies
+============
+
+* The features of the driver are still being worked on and require that several
+  new docker APIs that have been posted as pull requests[2] end up being
+  merged.  This will continue to be that case leading up to the Juno release.
+
+* Although Snapshots are working via synchronizing image meta-data between
+  Glance and the Docker registry, we are actively integrating a change that
+  pushes Docker images directly into Glance. This should help stabilize the
+  Snapshot feature as well as other image operations (creation/removal).
+
+Testing
+=======
+
+* The existing Tempest integration tests cover the APIs that need to work for
+  the Docker driver to be fully compliant.  Some tests may not apply to
+  containers and as such may need to be disabled.  Most of these are simply
+  logical and can be identified easily.
+
+* The Tempest configuration has a number of tests that don't apply to
+  containers that need to be turned off. We are still experimenting with the
+  configuration to make sure we have the right coverage. The settings we are
+  currently turning off are: api_v3, suspend, resize.
+
+Documentation Impact
+====================
+
+The docs team would need to add the Docker hypervisor plugin details back to
+the docs, listing the availability of the driver and the module name to set as
+the compute_driver value to have Nova load the driver.
+
+References
+==========
+
+[1]Docker hypervisor plugin source:
+https://github.com/stackforge/nova-docker
+
+[2]Docker pull requests:
+https://github.com/dotcloud/docker/pull/6369
+
+[3]Nova-docker volume patchset:
+https://review.openstack.org/#/c/101243
+
+Status of Tempest CI before making our improvements:
+http://lists.openstack.org/pipermail/openstack-dev/2014-February/028471.html
+
+Information on bringing Docker hypervisor plugin support into the Nova tree:
+http://openstack.etherpad.corp.redhat.com/NovaDockerUpstream
+
+Containers Service Thread:
+http://lists.openstack.org/pipermail/openstack-dev/2013-November/thread.html#19637
+
+Design session in Hong Kong on to support this work:
+https://etherpad.openstack.org/p/docker-nova-hkg
+
+Discussion regarding block storage support in containers:
+https://etherpad.openstack.org/p/container-block-storage
+
+Openstack container meetings:
+https://wiki.openstack.org/wiki/Meetings/Containers
diff --git a/specs/juno/proposed/domain-quota-driver-api.rst b/specs/juno/proposed/domain-quota-driver-api.rst
new file mode 100644
index 0000000..ff328ee
--- /dev/null
+++ b/specs/juno/proposed/domain-quota-driver-api.rst
@@ -0,0 +1,411 @@
+::
+
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================
+domain-quota-driver-api
+=======================
+
+https://blueprints.launchpad.net/nova/+spec/domain-quota-driver-api
+
+Nova V2 APIs for Domain Quota Management will enable OpenStack projects to
+enforce domain quotas in Nova.
+
+
+Problem description
+===================
+
+Currently, OpenStack services make usage of quotas to limit the project
+resources. For example, the "Instances" quota represents the number of
+instances that can be created in a project. So, the existing "DbQuotaDriver"
+in Nova allows to set the Quotas at Project/Tenant and User Level. But there
+is a need to enforce the quotas at Domain Level as well. A new driver called
+"Domain Quota Driver" is made available to enforce domain quotas in Nova.
+Also, to use this driver, a new set of APIs are required for CRUD operations
+on quotas at domain, project and user level. This driver depends on
+Keystone V3 context i.e the auth token should be generated using Keytone V3.
+If Keystone V2 is used, then the APIs for using domain quotas will fail.
+
+
+Proposed change
+===============
+
+A new driver called "Domain Quota Driver" will be made available in addition
+to the existing "DbQuotaDriver". The Nova can use two drivers in parallel,
+they will be called by the Quota Engine. New tables will be added to the Nova
+Database to store the quotas and its usages for a domain. The Domain Quota
+Driver strictly follows the hierarchy in implementing the quotas which means
+that administrator has to first set the quotas for domain before setting the
+quotas for a project in that domain. The same also holds for projects and
+users. The administrator has to first set the quotas for project before
+setting the quotas for users in that project. If no quotas are set, then
+the default values will be used at all levels. Ideally both the drivers can
+co-exists. But if the administrator used os-quota-sets (i.e) DbQuotaDriver to
+set quotas at project level and then moved to os-domain-quota-sets i.e
+DomainQuotaDriver, then the administrator has to set the quotas for the domain
+before doing any changes to the quota limits of the projects. Also, the
+minimum values that can be set for a quota parameter in the domain has to be
+greater than or equal to the sum of the quota parameter values of all the
+projects in that domain. For vice-versa case i.e moving from
+os-domain-quota-sets to domain-quota-sets, there is no such problem.
+
+The DomainQuotaDriver always maintains that sum of quota values of siblings
+at a level should always be less than or equal to the quota levels at their
+parent. The reverse condition is also applicable i.e the quota values at a
+parent should always be greater than the sum of quota values of its children.
+So, when an update of quota is requested, these rules are always followed.
+
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+- Three new Tables will be added namely 'domain_quotas', 'domain_quota_usages'
+  and 'domain_reservation'. The schema of these three tables is:
+
+- A database migration named '234_create_domain_quotas_tables.py' which includes
+  the table schema as
+
+.. code-block:: none
+
+   domain_quota = Table('domain_quotas', meta,
+            Column('id', Integer, primary_key=True, nullable=False),
+            Column('created_at', DateTime),
+            Column('updated_at', DateTime),
+            Column('deleted_at', DateTime),
+            Column('deleted', Integer),
+            Column('domain_id', String(255)),
+            Column('resource', String(255), nullable=False),
+            Column('hard_limit', Integer()),
+            mysql_engine='InnoDB',
+            mysql_charset='utf8')
+
+    domain_quota_usage = Table('domain_quota_usages', meta,
+            Column('id', Integer, primary_key=True, nullable=False),
+            Column('created_at', DateTime),
+            Column('updated_at', DateTime),
+            Column('deleted_at', DateTime),
+            Column('deleted', Integer),
+            Column('domain_id', String(255)),
+            Column('resource', String(255), nullable=False),
+            Column('in_use', Integer, nullable=False),
+            Column('reserved', Integer, nullable=False),
+            Column('until_refresh', Integer),
+            mysql_engine='InnoDB',
+            mysql_charset='utf8')
+
+    domain_reservation = Table('domain_reservations', meta,
+        Column('id', Integer, primary_key=True, nullable=False),
+        Column('created_at', DateTime),
+        Column('updated_at', DateTime),
+        Column('deleted_at', DateTime),
+        Column('deleted', Integer),
+        Column('uuid', String(length=36), nullable=False),
+        Column('domain_id', String(255)),
+        Column('usage_id', Integer, nullable=False),
+        Column('resource', String(length=255)),
+        Column('delta', Integer, nullable=False),
+        Column('expire', DateTime),
+        mysql_engine='InnoDB',
+        mysql_charset='utf8')
+
+
+REST API impact
+---------------
+
+A new openstack extension will be added and called "os-domain-quota-sets".
+The following APIs will be provided to enforce domain quotas by using
+"Domain Quota Driver"
+
+    * Show Quotas
+        * Show quotas for a domain/tenant/user
+        * GET Method
+        * 202 - OK
+        * 403 - FORBIDDEN
+        * v2/{tenant_id}/os-domain-quota-sets/{domain_id}
+        * project_id, user_id
+        * JSON request - None
+        * JSON response -
+
+.. code-block:: none
+
+                'quota_set': {
+                        'type': 'object','properties': {
+                            'instances': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'cores': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'ram': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'floating_ips': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'fixed_ips': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'metadata_items': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_files': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_file_content_bytes': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },injected_file_path_bytes': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'key_pairs': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'security_groups': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'security_group_rules': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'id':{
+                                'type': 'string', 'minLength': 0,
+                                'maxLength': 255, 'pattern': '^[a-fA-F0-9]*$'
+
+                            }
+                        },'required': ['instances', 'cores', 'ram',
+                                       'floating_ips', 'fixed_ips',
+                                       'metadata_items', 'injected_files',
+                                       'injected_file_content_bytes',
+                                       'injected_file_path_bytes',
+                                       'key_pairs', 'security_groups',
+                                       'security_group_rules'],
+                        'additionalProperties': False
+                    }
+
+    * Show Default Quotas
+        * Show default quotas for a domain
+        * GET
+        * 200 - OK
+        * 403 - FORBIDDEN
+        * v2/{tenant_id}/os-domain-quota-sets/{domain_id}/defaults
+        * None
+        * JSON request - None
+.. code-block:: none
+
+                        'quota_set': {
+                            'type': 'object','properties': {
+                                'instances': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'cores': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'ram': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'floating_ips': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'fixed_ips': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'metadata_items': {
+                                    'type': 'int', 'pattern': '^[-9]+$'
+                                },'injected_files': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_file_content_bytes': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_file_path_bytes': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'key_pairs': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'security_groups': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'security_group_rules': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'id':{
+                                    'type': 'string', 'minLength': 0,
+                                    'maxLength': 255, 'pattern': '^[a-fA-F0-9]*$'
+                                }
+                            },'required': ['instances', 'cores', 'ram',
+                                           'floating_ips', 'fixed_ips',
+                                           'metadata_items', 'injected_files',
+                                           'injected_file_content_bytes',
+                                           'injected_file_path_bytes',
+                                           'key_pairs', 'security_groups',
+					   'security_group_rules'],
+			    'additionalProperties': False
+                    }
+
+    * Update Quotas
+        * Update quotas for a domain/tenant/user
+        * PUT
+        * 200 - ACCEPTED
+        * 403 - FORBIDDEN
+        * v2/{tenant_id}/os-domain-quota-sets/{domain_id}
+        * project_id, user_id
+        * JSON request -
+.. code-block:: none
+
+                        'quota_set': {
+                            'type': 'object','properties': {
+                                'instances': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'cores': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'ram': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'floating_ips': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'fixed_ips': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'metadata_items': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_files': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_file_content_bytes': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_file_path_bytes': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'key_pairs': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'security_groups': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'security_group_rules': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                }
+
+                           },'required': ['instances', 'cores', 'ram',
+			                  'floating_ips', 'fixed_ips',
+                                          'metadata_items', 'injected_files',
+                                          'injected_file_content_bytes',
+                                          'injected_file_path_bytes',
+                                          'key_pairs', 'security_groups',
+                                          'security_group_rules'],
+                            'additionalProperties': False
+
+                    }
+
+         * JSON response -
+.. code-block:: none
+
+                    'quota_set': {
+                        'type': 'object','properties': {
+                            'instances': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'cores': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'ram': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'floating_ips': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'fixed_ips': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'metadata_items': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_files': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_file_content_bytes': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_file_path_bytes': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'key_pairs': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'security_groups': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'security_group_rules': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },
+                        },'required': ['instances', 'cores', 'ram',
+                                       'floating_ips', 'fixed_ips',
+                                       'metadata_items', 'injected_files',
+                                       'injected_file_content_bytes',
+                                       'injected_file_path_bytes', 'key_pairs',
+                                       'security_groups',
+                                       'security_group_rules'],
+                        'additionalProperties': False
+                    }
+
+    * Delete Quotas
+        * Delete quotas for a domain/tenant/user
+        * DELETE
+        * 200 - ACCEPTED
+        * 403 - FORBIDDEN
+        * v2/{tenant_id}/os-domain-quota-sets/{domain_id}
+        * project_id, user_id
+        * JSON request - None
+        * JSON response - None
+
+
+Security impact
+---------------
+
+This change uses the Keystone V3 token to get the domain_id from the scope.
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+New commands will be added to python-novaclient so as to call the above
+mentioned APIs for Domain Quota Management
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+    - One config option added to nova.conf i.e "domain_quota_driver"
+    - This feature comes into immediate effect after it is merged
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+     Primary assignee:
+         vinod-kumar-boppanna
+
+Work Items
+----------
+
+    Nova REST V2 API: DONE
+
+    Nova Quota Driver DB Methods: DONE
+
+    Nova REST API unit tests: DONE
+
+    The following is up for review:
+          Addressed by: https://review.openstack.org/#/c/75967/
+
+Dependencies
+============
+
+  - This depends on Keystone V3 Context. Also, the Keystone V3 client
+    should use V3 API and should not fall back to V2 API. Code has been
+    already merged to solve this and addressed by
+    https://review.openstack.org/#/c/75731/
+
+Testing
+=======
+
+    The Integration and Unit tests are added and they can be tested by running
+    'test_domain_quotas'. Before running this, add a environment variable
+    "NOVA_TEST_CONF" and point this to nova.conf file
+
+Documentation Impact
+====================
+
+    The APIs documentation needs to be updated to include the new extension
+    API. For more information, look at
+    https://wiki.openstack.org/wiki/APIs_for_Domain_Quota_Driver
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/domain-quota-driver-v3-api.rst b/specs/juno/proposed/domain-quota-driver-v3-api.rst
new file mode 100644
index 0000000..b964a35
--- /dev/null
+++ b/specs/juno/proposed/domain-quota-driver-v3-api.rst
@@ -0,0 +1,341 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================
+domain-quota-driver-v3-api
+==========================
+
+https://blueprints.launchpad.net/nova/+spec/domain-quota-driver-v3-api
+
+Nova V3 APIs for Domain Quota Management will enable OpenStack projects to
+enforce domain quotas in Nova
+
+
+Problem description
+===================
+
+This feature is an extension to feature specified in the blueprint
+https://blueprints.launchpad.net/nova/+spec/domain-quota-driver-api
+
+The difference is that, the above blueprint provides a specification to V2
+APIs and this blueprint provides specification for V3 APIs.
+
+
+Proposed change
+===============
+
+A new driver called "Domain Quota Driver" will be made available as specified
+in the blueprint
+(https://blueprints.launchpad.net/nova/+spec/domain-quota-driver-api)
+
+The V3 APIs uses the same driver to implement the CRUD operations for
+Quota Management for Domain/Tenant/User and available under extension
+"os-domain-quota-sets".
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+A new openstack extension will be added and called "os-domain-quota-sets".
+The following APIs will be provided to enforce domain quotas by using
+"Domain Quota Driver"
+
+    * Show Quotas
+        * Show quotas for a domain/tenant/user
+        * GET Method
+        * 202 - OK
+        * 403 - FORBIDDEN
+        * v3/os-domain-quota-sets/{domain_id}
+        * project_id, user_id
+        * JSON request - None
+        * JSON response -
+                'quota_set': {
+                        'type': 'object','properties': {
+                            'instances': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'cores': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'ram': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'floating_ips': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'fixed_ips': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'metadata_items': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_files': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_file_content_bytes': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },injected_file_path_bytes': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'key_pairs': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'security_groups': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'security_group_rules': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'id':{
+                                'type': 'string', 'minLength': 0,
+                                'maxLength': 255, 'pattern': '^[a-fA-F0-9]*$'
+
+                            }
+
+                        },'required': ['instances', 'cores', 'ram',
+                                       'floating_ips', 'fixed_ips',
+                                       'metadata_items', 'injected_files',
+                                       'injected_file_content_bytes',
+                                       'injected_file_path_bytes', 'key_pairs',
+                                       'security_groups',
+                                       'security_group_rules'],
+
+                        'additionalProperties': False
+
+                    }
+
+    * Show Default Quotas
+        * Show default quotas for a domain
+        * GET
+        * 200 - OK
+        * 403 - FORBIDDEN
+        * v3/os-domain-quota-sets/{domain_id}/defaults
+        * None
+        * JSON request - None
+                        'quota_set': {
+                            'type': 'object','properties': {
+                                'instances': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'cores': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'ram': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'floating_ips': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'fixed_ips': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'metadata_items': {
+                                    'type': 'int', 'pattern': '^[-9]+$'
+                                },'injected_files': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_file_content_bytes': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_file_path_bytes': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'key_pairs': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'security_groups': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'security_group_rules': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'id':{
+                                    'type': 'string', 'minLength': 0,
+                                    'maxLength': 255, 'pattern': '^[a-fA-F0-9]*$'
+
+                                }
+
+                            },'required': ['instances', 'cores', 'ram',
+                                       'floating_ips', 'fixed_ips',
+                                       'metadata_items', 'injected_files',
+                                       'injected_file_content_bytes',
+                                       'injected_file_path_bytes', 'key_pairs',
+                                       'security_groups',
+                                       'security_group_rules'],
+
+                              'additionalProperties': False
+
+                    }
+
+    * Update Quotas
+        * Update quotas for a domain/tenant/user
+        * PUT
+        * 200 - ACCEPTED
+        * 403 - FORBIDDEN
+        * v3/os-domain-quota-sets/{domain_id}
+        * project_id, user_id
+        * JSON request -
+                        'quota_set': {
+                            'type': 'object','properties': {
+                                'instances': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'cores': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'ram': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'floating_ips': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'fixed_ips': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'metadata_items': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_files': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_file_content_bytes': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'injected_file_path_bytes': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'key_pairs': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'security_groups': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+                                },'security_group_rules': {
+                                    'type': 'int', 'pattern': '^[0-9]+$'
+
+                                }
+
+                           },'required': ['instances', 'cores', 'ram',
+                                       'floating_ips', 'fixed_ips',
+                                       'metadata_items', 'injected_files',
+                                       'injected_file_content_bytes',
+                                       'injected_file_path_bytes', 'key_pairs',
+                                       'security_groups',
+                                       'security_group_rules'],
+
+                            'additionalProperties': False
+
+                    }
+
+         * JSON response -
+                    'quota_set': {
+                        'type': 'object','properties': {
+                            'instances': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'cores': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'ram': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'floating_ips': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'fixed_ips': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'metadata_items': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_files': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_file_content_bytes': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'injected_file_path_bytes': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'key_pairs': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'security_groups': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+                            },'security_group_rules': {
+                                'type': 'int', 'pattern': '^[0-9]+$'
+
+                            },
+
+                        },'required': ['instances', 'cores', 'ram',
+                                       'floating_ips', 'fixed_ips',
+                                       'metadata_items', 'injected_files',
+                                       'injected_file_content_bytes',
+                                       'injected_file_path_bytes', 'key_pairs',
+                                       'security_groups',
+                                       'security_group_rules'],
+
+                        'additionalProperties': False
+
+                    }
+
+    * Delete Quotas
+        * Delete quotas for a domain/tenant/user
+        * DELETE
+        * 200 - ACCEPTED
+        * 403 - FORBIDDEN
+        * v3/os-domain-quota-sets/{domain_id}
+        * project_id, user_id
+        * JSON request - None
+        * JSON response - None
+
+Security impact
+---------------
+
+This change uses the Keystone V3 token to get the domain_id from the scope.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+    - This feature comes into immediate effect after it is merged
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+     Primary assignee:
+         vinod-kumar-boppanna
+
+Work Items
+----------
+
+    Nova REST V3 API: DONE
+
+    Nova Quota Driver DB Methods: DONE
+
+    Nova REST API unit tests: DONE
+
+    The following is up for review:
+          Addressed by: https://review.openstack.org/#/c/78630/
+
+
+Dependencies
+============
+
+  - This depends on Keystone V3 Context. Also, the Keystone V3 client should
+    use V3 API and should not fall back to V2 API. Code has been already merged
+    to solve this and addressed by  https://review.openstack.org/#/c/75731/
+
+
+Testing
+=======
+
+    The Integration and Unit tests are added and they can be tested by running
+    'test_domain_quotas'. Before running this, add a environment variable
+    "NOVA_TEST_CONF" and point this to nova.conf file
+
+
+Documentation Impact
+====================
+
+    The APIs documentation needs to be updated to include the new extension
+    API. For more information, look at
+    https://wiki.openstack.org/wiki/APIs_for_Domain_Quota_Driver
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/domain-quota-manage-commands.rst b/specs/juno/proposed/domain-quota-manage-commands.rst
new file mode 100644
index 0000000..8bced6b
--- /dev/null
+++ b/specs/juno/proposed/domain-quota-manage-commands.rst
@@ -0,0 +1,195 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================
+domain-quota-manage-commands
+============================
+
+https://blueprints.launchpad.net/nova/+spec/domain-quota-manage-commands
+
+Nova Commands for Domain Quota Management will enable OpenStack projects
+to enforce quotas in Nova at Domain/Tenant/User levels.
+
+
+Problem description
+===================
+
+This feature is an extension to feature specified in the blueprint
+https://blueprints.launchpad.net/nova/+spec/domain-quota-driver-api
+
+The difference is that, the above blueprint provides a specification to
+V2 APIs and this blueprint provides Nova command line tools and it
+internally uses the V2 APIs.
+
+
+Proposed change
+===============
+
+The Nova V2 Commands uses the V2 APIs of Quota Management for
+Domain/Tenant/User and available under nova commands
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+This change uses the Keystone V3 token to get the domain_id from the scope.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Following commands have been added to nova for Domain Quota Management
+
+$> nova domain-quota-show
+
+List the quotas for a domain/tenant/user.
+
+Positional arguments:
+  <domain-id> ID of domain to list the quotas for.
+
+Optional arguments:
+  --tenant <tenant-id> ID of tenant to list the quotas for.
+  --user <user-id> ID of user to list the quotas for.
+
+$> nova domain-quota-defaults
+
+List the default quotas for a domain.
+
+Positional arguments:
+  <domain-id> ID of domain to list the default quotas for.
+
+$> nova domain-quota-delete
+
+Delete quota for a domain/tenant/user so their quota will Revert back to
+default.
+
+Positional arguments:
+  <domain-id> ID of domain to delete quota for.
+
+Optional arguments:
+  --tenant <tenant-id> ID of tenant to delete quota for.
+  --user <user-id> ID of user to delete quota for.
+
+$> nova domain-quota-update
+
+Update the quotas for a domain/tenant/user.
+
+Positional arguments:
+  <domain-id> ID of domain to set the quotas for.
+
+Optional arguments:
+  --tenant <tenant-id> ID of tenant to set the quotas for.
+
+  --user <user-id> ID of user to set the quotas for.
+
+  --instances <instances> New value for the "instances" quota.
+
+  --cores <cores> New value for the "cores" quota.
+
+  --ram <ram> New value for the "ram" quota.
+
+  --floating-ips <floating-ips> New value for the "floating-ips" quota.
+
+  --fixed-ips <fixed-ips> New value for the "fixed-ips" quota.
+
+  --metadata-items <metadata-items> New value for the "metadata-items" quota.
+
+  --injected-files <injected-files> New value for the "injected-files" quota.
+
+  --injected-file-content-bytes <injected-file-content-bytes>
+
+    New value for the "injected-file-content-bytes" quota.
+
+  --injected-file-path-bytes <injected-file-path-bytes>
+
+    New value for the "injected-file-path-bytes" quota.
+
+  --key-pairs <key-pairs> New value for the "key-pairs" quota.
+
+  --security-groups <security-groups> New value for the "security-groups" quota
+
+  --security-group-rules <security-group-rules>
+
+    New value for the "security-group-rules" quota.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+    - This feature comes into immediate effect after it is merged
+
+Developer impact
+----------------
+
+    None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+     Primary assignee:
+         vinod-kumar-boppanna
+
+Work Items
+----------
+
+    Nova V2 Commands: DONE
+
+    The following is up for review:
+          Addressed by: https://review.openstack.org/#/c/76347/
+
+
+Dependencies
+============
+
+  - This depends on Keystone V3 Context. Also, the Keystone V3 client should
+    use V3 API and should not fall back to V2 API. Code has been already
+    merged to solve this and addressed by
+    https://review.openstack.org/#/c/75731/
+
+
+Testing
+=======
+
+None
+
+
+Documentation Impact
+====================
+
+    The APIs documentation needs to be updated to include the new commands as
+    explained in section "Other End user impact".
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/dynamic-adjust-disk-qos.rst b/specs/juno/proposed/dynamic-adjust-disk-qos.rst
new file mode 100644
index 0000000..1819905
--- /dev/null
+++ b/specs/juno/proposed/dynamic-adjust-disk-qos.rst
@@ -0,0 +1,189 @@
+
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================
+Add dynamic adjust disk QOS support for libvirt
+===============================================
+
+https://blueprints.launchpad.net/nova/+spec/dynamic-adjust-disk-qos
+
+Currently the disk's QOS contain the following ability:
+total_bytes_sec, read_bytes_sec, write_bytes_sec, total_iops_sec,
+read_iops_sec, write_iops_sec.
+
+There are two ways to set the disk's QOS.
+1. Set extra in flavor and boot from the flavor.
+nova flavor-key m1.small set quota:disk_read_bytes_sec=10240000
+2. Create volume with QoS spec and attach volume to instance.
+cinder qos-create high_read_low_write consumer=”front-end” \
+read_iops_sec=1000 write_iops_sec=10
+
+cinder type-create type1
+cinder qos-associate [qos-spec-id] [type-id]
+cinder create –display-name high-read-low-write-volume –volume-type type1 \
+100 nova volume-attach vm-1 high-read-low-write-volume /dev/vdb
+
+The above is the initialization. There is no way to adjust QOS without
+taking some action which may makes the instance's status change or volume's
+status change.
+
+The user boots the VM which has setted disk's QOS and has important business
+such as web service. After a period of days, the initial setting can't meet
+the requirement of the business, the user wants to get more I/O flow and
+keeps the VM running. The user can ask for more QOS by the admin dynamic
+adjust of the disk's QOS.
+
+This proposal is just to set the disk's QOS for libvirt at the running time,
+and it is no any business effect to the user who use the instance.
+The QOS ability is dependent on the hypervisor driver and backend storage of
+the cinder.
+
+
+Problem description
+===================
+
+We have no method to dynamic adjust the disk's QOS during the instance is
+running or without detaching the volume.
+
+The admin who want to adjust the disk's QOS must shut down the instance or
+detach the disk which wants to set QOS form the instance first, it make
+business interrupt.
+
+
+Proposed change
+===============
+
+The basic points to note about this change are:
+
+* Add a new extension API "set_disk_qos" which the admin can use it.
+* Add the set method in base virt driver.
+* The disk's QOS which admin set will be saved in database.
+
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+The disk's QOS will be saved in database.
+
+REST API impact
+---------------
+
+The rest API look like this in v2:
+ /v2/{project_id}/servers/{server_id}/action
+
+    {
+        "set_disk_qos":{
+        "read_bytes_sec":10240000,
+        "write_bytes_sec":10240000,
+        "total_bytes_sec":40960000,
+        "read_iops_sec":2000,
+        "write_iops_sec":1000,
+        "total_iops_sec":5000
+        }
+
+    }
+and look like this in v3:
+ /v3/servers/{server_id}/action
+
+    {
+        "set_disk_qos":{
+        "read_bytes_sec":10240000,
+        "write_bytes_sec":10240000,
+        "total_bytes_sec":40960000,
+        "read_iops_sec":2000,
+        "write_iops_sec":1000,
+        "total_iops_sec":5000
+        }
+
+    }
+
+The key from above which is appear will be set.
+
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+Implementation
+==============
+I plan to add "set_disk_qos" API which will be added in the path
+"/api/openstack/compute/contrib/", and add set method in virt libvirt.
+Once it success, the disk QOS information save in disk_qos table which will
+be added in nova database.
+The disk_qos table has the following fields:
+instance-uuid, volume-id, read_bytes_sec, write_bytes_sec, total_bytes_sec,
+read_iops_sec, write_iops_sec, total_iops_sec.
+The disk QOS information will be cleared when the disk is detached.
+
+
+Assignee(s)
+-----------
+
+Primary assignee:
+   <hs.chen@huawei.com>
+
+
+
+Work Items
+----------
+
+* Add "set_disk_qos" API.
+* Add set method in libvirt.
+* Save disk's QOS.
+* Add testcase of the API.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Unit tests and tempest tests will check function.
+
+Documentation Impact
+====================
+
+A description of this function will be added into Compute API V2 and V3
+Reference.
+
+
+References
+==========
+
+https://wiki.openstack.org/wiki/InstanceResourceQuota
+https://blueprints.launchpad.net/cinder/+spec/pass-ratelimit-info-to-nova
diff --git a/specs/juno/proposed/dynamic-logging.rst b/specs/juno/proposed/dynamic-logging.rst
new file mode 100644
index 0000000..d8b1c3e
--- /dev/null
+++ b/specs/juno/proposed/dynamic-logging.rst
@@ -0,0 +1,148 @@
+
+=================================
+Dynamic logging for nova services
+=================================
+
+Launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/dynamic-logging
+
+This blueprint is to allow OpenStack operator dynamically configure logging level
+for nova services on-the-fly, without modifying nova.conf and rebooting services,
+therefore improve system availability.
+
+Problem description
+===================
+
+As it is today, logging level is statically configured inside nova.conf. To modify
+it, operator need to shutdown nova services and restart them. This is an obvious
+limit for both End User and Deployer.
+
+Being End User, I want to see the system always work without interruption.
+
+Being Deployer, I want to the system be available for my customers to keep them
+happy. I also want to troubleshooting and analyze on a live system for many
+reasons, like addressing hard-to-catch bugs that might run away from scene in
+case of rebooting system.
+
+Proposed change
+===============
+
+This blueprint is to extend nova-manage, allow OpenStack Operator issue a request
+over AMQP toward all nova services that are currently running, changing its
+logging level without rebooting those services. This will allow OpenStack operator
+set a much less verbose logging in nova.conf file, but still have a chance to
+collect necessary logs when they are needed for troubleshooting.
+
+Internally, the feature could be implemented as part of BaseRPCAPI which is already
+available in nova, simply a single function call is added. Optionally, it can be
+separated in a new API, like ConfigRPCAPI in the future for in case similar
+features are added. For this particular feature, adding a new API might be over-
+engineering.
+
+Since OpenStack components are sharing similar architecture, this blueprint is
+also applicable to cinder, neutron, and other components. However, only nova
+services are covered in this blueprint.
+
+Please note that the solution dose not persist the logging level after it is
+issued. Because this feature is supposed to use for temporarily increasing logging
+level, but not permanently.
+
+Alternatives
+------------
+
+An alternative is to just make nova services reload their config file upon SIGHUP.
+The limit of this alternative is that it will reset the states of the process and
+may cause interruption to the services. This blueprint is exactly aiming to remove
+this limit.
+
+Another option could be implementing this feature as a common service for all
+OpenStack services through oslo library. However olso is a library, it might be
+good for dealing with "static" configuration, but not flexible enough to cope
+with dynamic scenario. This feature could be extended in the future to pass over
+oslo.config objects as input parameters, and therefore introducing a powerful
+operation and maintenance interface for OpenStack. The change today is obviously
+supporting that to happen.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+This might improve the system performance with limit logging level, and
+only increase it when needed.
+
+Other deployer impact
+---------------------
+
+There will be a new sub-command availabe for nova-manage.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+  liyi-meng
+
+An implementation proposal is available at:
+https://review.openstack.org/#/c/82460/
+
+Work Items
+----------
+
+* Update GetLogCommands class in file nova/cmd/manage.py to add a new command
+  named level, which used to update the logging level of a service/module.
+  Optionally, rename  GetLogCommands into LogCommands
+* Update nova/baserpc.py to add a client side and server side function, which
+  reset the logging to expected level as issued from nova-manage command line
+  interface.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+This is a minor change, a unit test case might be enough.
+
+Documentation Impact
+====================
+
+Update nova-manage printout to feedback to caller, which log level is available.
+Also update nova-manage document if there is any.
+
+References
+==========
+
+* https://blueprints.launchpad.net/oslo/+spec/cfg-reload-config-files
diff --git a/specs/juno/proposed/ec2-volume-filtering.rst b/specs/juno/proposed/ec2-volume-filtering.rst
new file mode 100644
index 0000000..b90ddbf
--- /dev/null
+++ b/specs/juno/proposed/ec2-volume-filtering.rst
@@ -0,0 +1,141 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================
+More filters while listing EC2 volumes
+======================================
+
+https://blueprints.launchpad.net/nova/+spec/ec2-volume-filtering
+
+While listing out volumes using EC2's DescribeVolumes API, our implementation
+allows filtering only based on volume IDs, as of now. Amazon's EC2 API has
+support for around a dozen filters. So this blueprint aims for exposing most
+of them through our EC2 API too.
+
+Problem description
+===================
+
+We would like our EC2 APIs behave as closely to Amazon's as possible. Making
+the EC2 API support more filters is one step towards this goal. After
+this blueprint gets implemented, EC2 API users will be able to filter volumes
+based on these filters, which currently one cannot. The filters which will be
+implemented will be:
+
+ * create-time
+ * size
+ * snapshot-id
+ * status
+ * volume-id
+ * availability-zone
+ * attachment.attach-time
+ * attachment.delete-on-termination
+ * attachment.device
+ * attachment.instance-id
+ * attachment.status
+
+Three more filters, tag-key, tag-value, volume-type will be implemented, once
+their implementations land in EC2 code.
+
+Proposed change
+===============
+
+The process of filtering will be separated into two subparts. The first
+part will be filtering the data directly from Cinder. So basically, Cinder
+supports a few filtering options, e.g. filter volumes by metadata. If the
+DescribeVolume API call asks for filtering by metadata, we'll forward that
+request to Cinder, and then Cinder will return back volumes filtered by
+metadata. But if the DescribeVolumes API wants to filter volume additionally
+by using a filter which is not supported by Cinder, we'll not pass this
+filter to Cinder, but in the next subpart, we'll filter it out from the
+output Cinder provided, before returning it back in the response.
+
+A list of filters supported by Cinder, and a list of filters not supported
+by Cinder will be maintained, which the code will use to iterate over for
+filtering. As Cinder starts supporting more and more filters, we'll keep
+moving them from second list to the first. (The 'lists' will just be
+hardcoded inside the DescribeVolumes API code).
+
+Alternatives
+------------
+
+Can't think of any other method.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+DescribeVolumes API will now be able to filter volumes, as opposed to the
+current way of just ignoring all the filters except volume ID.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Primary assignee:
+  rushiagr (Rushi Agrawal)
+
+Work Items
+----------
+
+* Implement filters for DescribeVolumes (single work-item)
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest tests to test these new filters comprehensively will be added in the
+next iteration.
+
+Documentation Impact
+====================
+
+OpenStack's EC2 API doc will need to be updated to show that DescribeVolumes
+now filters volumes too.
+
+References
+==========
+
+Amazon's doc for DescribeVolumes:
+http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-DescribeVolumes.html
diff --git a/specs/juno/proposed/ec2-volume-type.rst b/specs/juno/proposed/ec2-volume-type.rst
new file mode 100644
index 0000000..0c26f45
--- /dev/null
+++ b/specs/juno/proposed/ec2-volume-type.rst
@@ -0,0 +1,124 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================
+Volume type in EC2 API
+======================
+
+https://blueprints.launchpad.net/nova/+spec/ec2-volume-type
+
+The concept of volume type is not exposed via the OpenStack's EC2 API.
+This blueprint is to address that, and make the EC2 API consistent with
+AWS's.
+
+Problem description
+===================
+
+Volumes created by OpenStack's EC2 API does not have 'volumeType' attribute.
+AWS provides two volumes types currently -- 'standard' and 'io1', where
+'standard' is for normal volumes without any guarantees, and 'io1' volumes
+to which we can attach a value for guaranteed IOPS. We should allow
+a way for deployers to configure the backend such that these volume types are
+exposed, if the backend supports it. If the deployer has only the 'standard'
+volume types, he should be able to expose only that, and then the code should
+raise error if the user requests for a volume of unsupported volume type.
+
+Proposed change
+===============
+
+For standard volumes, we will just return a volume type 'standard' via the
+EC2 API if no volume type is associated with the volume on Cinder side. Also,
+along with this, we'll also provide with a config option in case the deployer
+wants to associate a specific Cinder volume type to 'standard' volume type
+in the EC2 API.
+
+For all other Cinder volume type values, the EC2 API will continue showing
+the value for key 'volumeType' as null. As the aim of this blueprint is
+limited, a future blueprint will be required to make sure we only expose
+'standard' volumes in the EC2 API, and prune out all other volumes.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+No data model impact.
+
+REST API impact
+---------------
+
+The DescribeVolumes API will now return volumes, which will now have
+'volumeType' key a value of 'standard' for standard volumes.
+
+Security impact
+---------------
+
+Can't see any.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  rushiagr (Rushi Agrawal)
+
+Work Items
+----------
+
+* Expose volume type in EC2 API. (Single work item)
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit tests, as usual will be included. Tempest tests need to be written too,
+to test this tweak which this BP is proposing
+
+Documentation Impact
+====================
+
+EC2 API document will be impacted due to this change. We will now need
+to highlight that volume type will be exposed via the API.
+
+References
+==========
+
+* CreateVolume from EC2 docs: http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-CreateVolume.html
+* DescribeVolumes from EC2 docs: http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-DescribeVolumes.html
diff --git a/specs/juno/proposed/emc-sdc-libvirt-driver.rst b/specs/juno/proposed/emc-sdc-libvirt-driver.rst
new file mode 100644
index 0000000..fd06197
--- /dev/null
+++ b/specs/juno/proposed/emc-sdc-libvirt-driver.rst
@@ -0,0 +1,168 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================
+EMC: ScaleIO Data Client (SDC) Libvirt Driver
+=============================================
+
+https://blueprints.launchpad.net/nova/+spec/emc-sdc-libvirt-driver
+
+This blueprint proposes to add a libvirt driver for the support of SDC
+(ScaleIO Data Client) connected volumes. The addition of SDC support to
+libvirt will be leveraged by Nova to attach/detach volumes provided by
+products such as EMC ScaleIO and ECS (Elastic Cloud Storage).
+
+
+Problem description
+===================
+
+This blueprint is being submitted for the EMC ECS (Elastic Cloud Storage)
+hyper scale storage infrastructure system. ECS is an appliance that provides
+block, object, and HDFS capabilities natively and combines commodity
+infrastructure with resilient data services. The EMC ECS appliance will
+support block volume services through the ScaleIO block API.
+
+In order to support mounting such block volumes to Nova instances, a
+libvirt driver for ScaleIO block protocol is needed. This blueprint
+proposes to add such a driver to nova.
+
+
+Proposed change
+===============
+
+A libvirt driver for the ScaleIO protocol used by EMC ECS will be added
+to the nova/virt/libvirt directory.
+
+A section will be added to the list of libvirt volume drivers in the file
+nova/virt/libvirt/volume.py. This will direct volumes of 'volume_driver'
+type 'scaleio' to the correct driver.
+
+A new module called 'scaleio.py' will be introduced. This module will
+contain all the code related to scaleio. There will be minimal changes
+to volume.py.
+
+Refer to the description of ScaleIO technology at:
+http://www.emc.com/storage/scaleio/index.htm
+
+ScaleIO is software-only server based SAN technology. It allows the use
+of commodity disks to deliver affordable and scalable block storage.
+
+To use this driver, the user has to install additional components:
+
+* The SDC Linux driver
+
+* The CLI for ScaleIO
+
+This software can be downloaded from (requires registration):
+https://support.emc.com/products/33925_ScaleIO
+
+Once these dependencies are met, and this driver is installed, nova nodes
+can mount block volumes from the EMC ECS storage system and use them just
+like any other block storage. The SDC Linux driver will direct storage
+traffic over TCP/IP to the right server.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+End users will be able to create block volumes from EMC ECS and use them in
+OpenStack.
+
+This change is accompanied by a cinder driver for EMC ECS (a separate
+cinder blueprint).
+
+Performance Impact
+------------------
+
+This technology takes advantage of massively parallel I/O processing, with
+all servers participating and sharing I/O loads. By adding servers, you can
+increase capacity and processing power linearly, leveraging fast parallel
+rebuild and rebalance without interruption to I/O.
+
+Other deployer impact
+---------------------
+
+The ScaleIO SDC component must be installed on the OpenStack compute node.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  anil-degwekar
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* ScaleIO Libvirt driver -- this is already developed
+
+* An entry in the volume.py file
+
+* a filter for scaleio in nova/rootwrap
+
+Dependencies
+============
+
+Cinder blueprint for EMC ECS driver
+https://blueprints.launchpad.net/cinder/+spec/emc-ecs-driver
+
+Testing
+=======
+
+The cinder driver will be tested using the cinder acceptance tests. Those
+tests will cover this driver as well. A 3rd party CI testing system
+will be used and its results submitted.
+
+Documentation Impact
+====================
+
+This needs to be documented as a new feature.
+
+References
+==========
+
+Product Links:
+EMC ECS:
+http://www.emc.com/storage/ecs-appliance/index.htm
+
+EMC ScaleIO:
+http://www.emc.com/storage/scaleio/index.htm
+
diff --git a/specs/juno/proposed/enchancement-virtio-scsi-support-for-volume.rst b/specs/juno/proposed/enchancement-virtio-scsi-support-for-volume.rst
new file mode 100644
index 0000000..7bedba2
--- /dev/null
+++ b/specs/juno/proposed/enchancement-virtio-scsi-support-for-volume.rst
@@ -0,0 +1,143 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================================
+Enhancement virtio-scsi bus support for volume
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/enchancement-virtio-scsi-support-for-volume
+
+
+VirtIO SCSI is a new para-virtualized SCSI controller device for KVM instances.
+It has been designed to replace virtio-blk, increase it's performance and
+improve scalability. Currently, under some scenario, using virtio-scsi bus is
+not supported when booting from volume.
+
+
+
+Problem description
+===================
+
+VirtIO SCSI is a new para-virtualized SCSI controller device for KVM instances.
+It has been designed to replace virtio-blk, increase it's performance and
+improve scalability. The interface is capable of handling multiple block
+devices per virtual SCSI adapter, keeps the standard scsi device naming
+in the guests (e.x /dev/sda) and support SCSI devices passthrough.
+
+Currently, virtio-scsi bus has been supported when booting from glance image
+with property "hw_scsi_mode=virtio-scsi" or cinder volume created by this type
+of image, which is implemented by BP ([1]) in Icehouse.
+
+However, for volumes which were not created from image, we have no method to
+specify using virtio-scsi controller for it.
+
+The aim of this BP as follows:
+
+For bootable volumes which including virito-scsi driver, user can set specific
+metadata for them. When booting from these volumes with "scsi" bus type, use
+virtio-scsi controller instead of the default lsi controller.
+
+The main use case is to improve performance in I/O-intensive applications.
+
+The prior BP [2] proposed by me has already been merged. However, I had a
+misunderstanding about the virito-scsi and volume. In fact, in icehouse, When
+booting from a volume created by glance image with hw_scsi_mode property, the
+volume will also use virito-scsi controller, really sorry for my misleading.
+
+
+Proposed change
+===============
+
+* Nova retrieve volume's metadata when booting from cinder volume
+
+* Libvirt driver will create the "virtio-scsi" controller if volume has
+  specific metadata, eg. "hw_scsi_model=virtio-scsi"
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Will improve guest's performance.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <zhangleiqiang@huawei.com>
+
+
+Work Items
+----------
+
+* Nova retrieve volume's metadata when booting from cinder volume
+
+* Libvirt driver will create the "virtio-scsi" controller if volume has
+  specific metadata, eg. "hw_scsi_model=virtio-scsi"
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+* [1] https://blueprints.launchpad.net/nova/+spec/libvirt-virtio-scsi-driver
+* [2] https://blueprints.launchpad.net/nova/+spec/add-virtio-scsi-bus-for-bdm
diff --git a/specs/juno/proposed/encrypted-live-migration-nova.rst b/specs/juno/proposed/encrypted-live-migration-nova.rst
new file mode 100644
index 0000000..aa4872f
--- /dev/null
+++ b/specs/juno/proposed/encrypted-live-migration-nova.rst
@@ -0,0 +1,281 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================================
+Force encrypted Live Migration thru Nova API
+============================================
+
+https://blueprints.launchpad.net/nova/+spec/encrypted-live-migration-nova
+
+The goal of this Blueprint is to allow admins to force "encrypted" Live
+Migration (LM) thru Nova API. This will leverage and allow admins to use
+encryption in Live Migrations everywhere; especially for cases where no
+encryption has been set in configuration files and without the need to
+manually update and restart nodes/services.
+
+The proposal is to modify Nova API once for exposing this option for the
+different virtualization drivers, and initially to implement this option for
+libvirt driver making use of libvirt tunneling.
+
+
+Problem description
+===================
+
+As described in OpenStack Security Guide, associated Risks of plain text Live
+migration include (see references Section for details):
+
+* Data Exposure. Memory or disk transfers must be handled securely.
+
+* Data Manipulation. If memory or disk transfers are not handled securely then
+  an attacker could manipulate user data during the migration.
+
+* Code Injection. If memory or disk transfers are not handled securely, then
+  an attacker could manipulate executables, either on disk or in memory,
+  during the migration.
+
+Currently Live Migration encryption can only be configured in nova.conf for
+libvirt tunneling. For cases where the admin does not know/missed/forgot
+configuring encryption in configuration files and encryption desired to be
+used afterwards nodes services are required to be manually modified/restarted.
+Considering a deployment with high number of nodes this may lead to not using
+encryption at all (given possible availability/performance considerations).
+So this new option leverages, facilitates and offers more flexibility in the
+setup and usage of encryption during Live Migration.
+
+
+Proposed change
+===============
+
+For exposing this force encryption option in Live Migration it is required to
+expose this in Nova APIs, by adding a new "force_migration_encryption"
+parameter:
+
+* Add this new parameter in the different method signatures for Nova Compute,
+  Nova Conductor and Nova Client (see API impact section for details). This
+  parameter will be passed all along these components until reaching a
+  specific virt driver where this parameter will be used.
+
+* Add the same "force_migration_encryption" in virt/driver.py abstract
+  live_migration method, setting its default to None.
+
+* libvirt/driver.py _live_migrate method to be modified for setting tunneled
+  option if the "force_migration_encryption" flag is set: "flags" parameter in
+  call to migrateToURI to contain: VIR_MIGRATE_TUNNELLED and
+  VIR_MIGRATE_PEER2PEER.
+
+* If VIR_MIGRATE_TUNNELLED and VIR_MIGRATE_PEER2PEER were set in nova.conf
+  then the encryption flags will be set in the migrateToURI as of today,
+  independently if "force_migration_encryption" parameter is set or not.
+
+* Force encryption option is to be applied where available by the different
+  virt driver and if compatible with other configurations (i.e. libvirt
+  tunneled live migration is only possible if block_migration is not used).
+
+Alternatives
+------------
+
+Currently, such functionality can only be accessed through manual
+configuration of libvirt, as described in OpenStack Security Guide:
+http://docs.openstack.org/security-guide/content/ch055_security-services-for-instances.html#ch055_security-services-for-instances-idp191072
+
+with additional comments in the following OSSN:
+https://wiki.openstack.org/wiki/OSSN/OSSN-0007
+
+Tunneling of migration traffic does not apply to live block migration.
+
+Allowing to force Live Migration Encryption would facilitate and leverage the
+usage of Live Migration thru encrypted communication.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+There are not changes in the URI for live_migration:
+
+/v2/{tenant_id}/servers/{server_id}/action
+/v3/servers/​{server_id}​/action/​{server_id}​/action
+
+In the JSON request a new additional property "force_migration_encryption" is
+added::
+
+    {
+        "os-migrateLive": {
+            "host": "0443e9a1254044d8b99f35eace132080",
+            "block_migration": false,
+            "disk_over_commit": false,
+            "force_migration_encryption": false
+        }
+    }
+
+Initially there have been identified the following API/methods in Nova
+Compute, Conductor, Nova Client and virt/libvirt where the new
+"force_migration_encryption" parameter needs to be added:
+
+Nova Compute:
+
+* nova/compute/api.py: live_migrate()
+
+* nova/compute/rpcapi.py: live_migration()
+
+* nova/compute/manaper.py: live_migration()
+
+Nova Conductor:
+
+* nova/conductor/api.py: live_migrate_instance()
+
+* nova/conductor/manager.py: migrate_server()
+
+* nova.conductor.manager.py: _live_migrate()
+
+* nova/conductor/tasks/live_migrate.py: execute()
+
+virt/libvirt:
+
+* nova/virt/driver.py: live_migration()
+
+* nova/virt/libvirt/driver.py: live_migration()
+
+* nova/virt/libvirt/driver.py: _live_migration()
+
+Nova Client:
+
+* pyhton-novaclient/novaclient/version/server.py: live_migrate()
+
+Parameter "force_migration_encryption" is initially to be added by default as
+None for not broking compatibility with other modules not using it.
+
+At this point there has not been identified a need to modify
+check_can_live_migrate_destination methods to add this new parameter.
+
+Initially no new error codes identified to be added.
+
+
+Security impact
+---------------
+
+This is an enhancement in the Security configuration for Live Migration.
+Nova API is modified for allowing to force encrypted Live Migration in
+case this has not been set in configuration file for the nodes used during
+the Live Migration process.
+
+Encryption thru libvirt tunneling is only enabled initially, but the involved
+abstract driver methods are modified for this being implemented later also for
+other drivers apart from libvirt.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+There is a pair of related Blueprints for allowing admins to force encrypted
+Live Migration thru python-novaclient and Horizon. Both user interfaces would
+make use of this functionality thru the explained changes in Nova API:
+
+*  https://blueprints.launchpad.net/horizon/+spec/encrypted-live-migration-horizon
+
+*  https://blueprints.launchpad.net/python-novaclient/+spec/encrypted-live-migration-novaclient
+
+Performance Impact
+------------------
+
+As explained here: http://libvirt.org/migration.html#transporttunnel a
+tunneled transport involves extra data copies between source and destination
+implying then a degradation in live migration performance (with the benefit of
+Security of course).
+
+Other deployer impact
+---------------------
+
+No specific deployment configuration would be required, but this functionality
+would initially be available only if libvirt is being used and block migration
+is not selected.
+
+Developer impact
+----------------
+
+Initially the proposal is to provide this functionality only for libvirt
+deployments; but to modify the Nova API for being ready for other
+virtualization drivers encryption implementations.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  cristian-fiorentino
+
+Work Items
+----------
+
+* Modification in Nova Compute to include a new optional parameter.
+
+* Modification in Nova Conductor to include a new optional parameter.
+
+* Modification in abstract virt driver to include a new optional parameter.
+
+* Modification in libvirt driver to include a new optional parameter.
+
+* Modification in libvirt driver to interpret this new parameter and
+  to make the respective calls to libvirt API.
+
+* Modification in Nova Client for enaling this new option.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+New tests need to be added on Live Migration for this new option.
+
+
+Documentation Impact
+====================
+
+Document the optional inclusion of the new "force_migration_encryption"
+parameter in respective APIs for Nova Compute, Conductor, virt/libvirt and
+Nova Client.
+
+Comment that this option is currently only available for libvirt and if
+block migration is not used.
+
+Additionally to document the performance considerations of using libvirt
+tunneling as explained in performance section.
+
+
+References
+==========
+
+* Launchpad Blueprint:
+  https://blueprints.launchpad.net/nova/+spec/encrypted-live-migration-nova
+
+* Related Blueprints:
+  https://blueprints.launchpad.net/horizon/+spec/encrypted-live-migration-horizon
+  https://blueprints.launchpad.net/python-novaclient/+spec/encrypted-live-migration-novaclient
+
+* Libvirt tunneling performance considerations:
+  http://libvirt.org/migration.html#transporttunnel
+
+* OpenStack Security Guide - Encrypted Live Migration recommendation:
+  http://docs.openstack.org/security-guide/content/ch055_security-services-for-instances.html#ch055_security-services-for-instances-idp191072
+
+* Encrypted Live Migration comments in related OSSN:
+  https://wiki.openstack.org/wiki/OSSN/OSSN-0007
+
diff --git a/specs/juno/proposed/exclude-cbs-in-snapshot.rst b/specs/juno/proposed/exclude-cbs-in-snapshot.rst
new file mode 100644
index 0000000..145bdde
--- /dev/null
+++ b/specs/juno/proposed/exclude-cbs-in-snapshot.rst
@@ -0,0 +1,189 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================================================
+Allow snapshot of only root volume during snapshot of a volume backed instance
+==============================================================================
+
+https://blueprints.launchpad.net/nova/+spec/exclude-cbs-in-snapshot
+
+This feature is to provide the user with an option to snapshot only the root
+volume during the snapshot instance action. This will not snapshot any
+other volume attached to the instance.
+This will be a feature for volume-backed instances only.
+
+
+Problem description
+===================
+
+Currently, creating a snapshot of an instance which is volume-backed will
+snapshot the root volume as well as all the other volumes attached to it.
+We see issues where the snapshot process fails, while backing up a very large
+volume attached to the instance. We have cases where the snapshot process
+times out.
+
+Also, this feature would help in the case where the customer wants to snapshot
+only the root volume and none of the others.
+
+This feature would provide an option to snapshot only the root volume of the
+instance if that's the only volume that the user is concerned about, while he
+takes the snapshot. This would enhance the flexibility of the feature
+rather than always following the default behavior of taking snapshots of all
+the volumes.
+
+
+Proposed change
+===============
+
+In order to implement this, I propose that we allow the user to specify whether
+only the root volume needs to be backed-up during the instance snapshot
+process. This can be specified as an additional optional parameter to the
+snapshot API call.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+API to specify whether all the volumes or only the root volume needs to be
+backed up during the snapshot process:
+
+Workflow:
+* If the option 'snapshot_only_root_volume' is specified as 'true', snapshot
+only the root volume of the instance.
+* If the option 'snapshot_only_root_volume' is specified as 'false', or not
+specified at all, snapshot all the volumes. (Default behavior)
+
+v3 API specification:
+POST: /v3/servers/{server_id}/action
+
+Request attribute to be added:
+* snapshot_only_root_volume: true/false value to specify whether only the root
+volume needs to be backed up. (Optional)
+
+Example JSON request::
+
+    {
+        "createImage" : {
+            "name" : "new-image",
+            "metadata": {
+                "ImageType": "Gold",
+                "ImageVersion": "2.0"
+            }
+            "snapshot_only_root_volume": true,
+        }
+    }
+
+Response codes:
+HTTP 202 on success
+HTTPBadRequest exception if "snapshot_only_root_volume" is not a valid boolean
+value.
+
+Sample v3 request:
+POST: /v3/servers/7d14f8123/action -d '{"create_image":
+{"name": "image-name", "snapshot_only_root_volume": "true"}
+
+This would snapshot only the root volume of the instance, provided the instance
+is volume backed.
+
+Validation:
+'snapshot_only_root_volume' must be a boolean value, true/false.
+(Optional parameter)
+
+Not implementing the feature in v2, as the idea is to keep v2 changes minimal.
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The create_image call in python-novaclient will have to include the additional
+optional parameter
+
+Optional argument:
+--snapshot_only_root_volume True or false value to specify whether only the root
+volume is to be backed up.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+The parameter will be optional, so no other code needs to be changed.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    aditirav
+
+Work Items
+----------
+
+* Changes to be made to the compute manager 'snapshot_volume_backed' method to
+  use the 'snapshot_only_root_volume' parameter passed in, to decide whether
+  all or only the root volume needs to be backed up.
+* Changes to the V3 API to take in the optional parameter
+  'snapshot_only_root_volume'.
+* Include tests in tempest to check the behavior of create_image of the
+  instance with snapshot only root volume option set.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest tests to be added to check if only the root volume is backed up when
+the parameter 'snapshot_only_root_volume' is set to true in the API call.
+
+
+Documentation Impact
+====================
+
+Changes to be made to the create_image API documentation to include the
+additional parameter 'snapshot_only_root_volume' that can be passed in, and its
+usage information.
+
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/extends-nova-hypervisor.rst b/specs/juno/proposed/extends-nova-hypervisor.rst
new file mode 100644
index 0000000..5da27a2
--- /dev/null
+++ b/specs/juno/proposed/extends-nova-hypervisor.rst
@@ -0,0 +1,171 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================================
+Spec - make nova hypervisor to list available resources for scheduling
+======================================================================
+
+https://blueprints.launchpad.net/nova/+spec/extends-nova-hypervisor
+
+Problem description
+===================
+
+Currently, there is no easy way to check the current available resources on a
+given hypervisor. If we only monitor the actual resource usage, there might be
+cases where real usage is < 75% but the scheduler sees no more capacity
+available to scheduling vm's. For example, we have a 10G RAM on a hypervisor.
+By running ‘proc’ on the compute node, we see the actual RAM usage is only
+3G. With these data point, we cannot say the available RAM is 7G.
+
+By checking nova.compute_nodes table, the current free RAM is -3G. Assuming
+ram_allocation_ratio is 1.5, the correct available RAM seen by scheduler is 2G.
+The correct calculation is as follows:
+
+Physical RAM: 10G
+Free RAM: -3G
+Actual used RAM: 3G
+ram_allocation_ratio: 1.5
+Available RAM seen by scheduler: 10*1.5-(10-(-3)) = 2
+
+We need an easy way to get the current available resources on a given
+hypervisor for both alerting and planning purposes. We should capture the total
+capacity as the scheduler would see it. This should take into consideration
+the overcommit ratios too.
+
+
+Proposed change
+===============
+
+We patch '/os-hypervisors' to also return available resources for
+scheduling, as well as the overcommit ratio.
+
+All the available resources calculations are in the same way as scheduler.
+ram : https://github.com/openstack/nova/blob/master/nova/scheduler/filters/ram_filter.py
+core : https://github.com/openstack/nova/blob/master/nova/scheduler/filters/core_filter.py
+disk : https://github.com/openstack/nova/blob/master/nova/scheduler/filters/disk_filter.py
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+* No new extension needed, the existing hypervisor REST API will be updated to
+  return the resources available for scheduling, as well as the overcommit
+  ratios
+
+* URL: existed hypervisors extension as:
+       * /v2/{tenant_id}/os-hypervisors/{id}
+
+  JSON response body:
+
+    {"hypervisor": {
+            "vcpus_used": 4,
+            "hypervisor_type": "QEMU",
+            "local_gb_used": 80,
+            "host_ip": "172.25.110.34",
+            "hypervisor_hostname": "otccloud06",
+            "memory_mb_used": 8704,
+            "memory_mb": 23638,
+            "current_workload": 0,
+            "vcpus": 16,
+            "cpu_info": {"vendor": "Intel}
+            "running_vms": 2,
+            "free_disk_gb": 439,
+            "hypervisor_version": 1000000,
+            "disk_available_least": 408,
+            "local_gb": 519,
+            "free_ram_mb": 14934,
+            "id": 1,
+            "cpu_allocation_ratio": 16.0,
+            "disk_allocation_ratio": 1.0,
+            "ram_allocation_ratio": 1.5,
+            "available_disk_gb": 439,
+            "available_ram_mb": 26753,
+            "available_vcpus": 252}}
+
+  The new fields are:
+    'cpu_allocation_ratio'
+    'disk_allocation_ratio'
+    'ram_allocation_ratio'
+    'available_disk_gb'
+    'available_ram_mb'
+    'available_vcpus'
+
+Security impact
+---------------
+
+No
+
+Notifications impact
+--------------------
+
+No
+
+Other end user impact
+---------------------
+
+Yes, this will impact the python-novaclient. novaclient should show the new
+fields on the 'nova hypervisor' command.
+
+Performance Impact
+------------------
+
+No
+
+Other deployer impact
+---------------------
+
+No
+
+Developer impact
+----------------
+
+No
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    weidu@yahoo-inc.com
+
+Work Items
+----------
+
+* Changes to V2 API
+* Changes to novaclient
+
+
+Dependencies
+============
+
+No
+
+Testing
+=======
+
+Both unit and Tempest tests will be created to ensure the correct
+implementation.
+
+Documentation Impact
+====================
+
+Document the change to the REST API.
+
+References
+==========
+No
diff --git a/specs/juno/proposed/extension-level-policy-as-default-v3-api.rst b/specs/juno/proposed/extension-level-policy-as-default-v3-api.rst
new file mode 100644
index 0000000..ed63506
--- /dev/null
+++ b/specs/juno/proposed/extension-level-policy-as-default-v3-api.rst
@@ -0,0 +1,150 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=========================================================================
+Enable extension policy rule as default of action policy rule for v3 api
+=========================================================================
+
+https://blueprints.launchpad.net/nova/+spec/extension-level-policy-as-default-v3-api
+
+There are a lot of API in V3, and also a lot of policy rule that can be
+configured by deployer. For ease the maintenance burden of policy rules
+configuration, enable deployer to use extension policy rule instead of all
+the action policy rule in the same extension.
+
+Problem description
+===================
+
+For some extension, it provide policy rule for each API.
+
+For example:
+    "compute_extension:v3:os-aggregates:index": "rule:admin_api",
+    "compute_extension:v3:os-aggregates:create": "rule:admin_api",
+    "compute_extension:v3:os-aggregates:show": "rule:admin_api",
+    "compute_extension:v3:os-aggregates:update": "rule:admin_api",
+    "compute_extension:v3:os-aggregates:delete": "rule:admin_api",
+    "compute_extension:v3:os-aggregates:add_host": "rule:admin_api",
+    "compute_extension:v3:os-aggregates:remove_host": "rule:admin_api",
+    "compute_extension:v3:os-aggregates:set_metadata": "rule:admin_api",
+
+Actually the rule is same for all the API. If Deployer want to change
+the rule, Deployer need modify for each policy rule.
+
+Proposed change
+===============
+
+Extension rule: compute_extension:v3:[extension_name]
+Action rule: compute_extension:v3:[extension_name]:[action_name]
+
+If deployer want to set same rule for all the API in an extension, deployer
+just need write a rule for extension. Then that rule will apply to all the API
+in that extensions.
+
+If deployer want to change rule for one of API in that extension, it only
+need write an action policy rule for that API. Then it will override the rule
+of extension.
+
+For example:
+"compute_extension:v3:os-aggregates": "rule:admin_api",
+"compute_extension:v3:os-aggregates:set_metadata": "role:maintainer"
+
+This means the rule of APIs except set_metadata in os-aggregates extension
+is "rule:admin_api". For set_metadata, the rule is "role:maintainer".
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+If the policy configure file is unchanged, then this is completely backwards
+compatible.
+
+If the policy configure file is changed. There are something need notice:
+When action policy rule is unspecified, without this feature, the default
+rule will be applied. With this feature, the extension policy rule will be
+applied.
+
+Developer impact
+----------------
+A developer should still list every single possible policy setting available
+for the feature they are adding or modifying in policy.json, not just an
+overall policy setting for the extension. A deployer is always free to remove
+more specific settings, but without having an example they would be forced to
+read the nova api code to see what settings are possible.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Alex Xu <xuhj@linux.vnet.ibm.com>
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+Enable extension policy rule as default of action policy rule for
+v3 api.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+Add unittest for this feature to ensure it works.
+
+
+Documentation Impact
+====================
+Upgrade impact documentation will need to be written so deployers are aware of
+this new feature.
+
+Developer documentation will need to notice developer list all the new policy
+rule in the policy configure file, that used to have an example for deployer
+knowing which rule they can use.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/flavor-cpu-overcommit.rst b/specs/juno/proposed/flavor-cpu-overcommit.rst
new file mode 100644
index 0000000..b4b8e64
--- /dev/null
+++ b/specs/juno/proposed/flavor-cpu-overcommit.rst
@@ -0,0 +1,113 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Flavour CPU overcommit
+==========================================
+https://blueprints.launchpad.net/nova/+spec/flavor-cpu-overcommit
+
+The aim of this blueprint is to provide an option to schedule VMS according
+to the CPU overcommit ratio, specified in extra_specs of it's flavour.
+
+
+Problem description
+===================
+Currently there is no way to group vms according to it’s cpu overcommit
+ratio. Such option is required in order to allow the users to group vms
+by its workload characteristics, cpu intensive and not, and by a flavour price
+for cpu usage.
+
+Proposed change
+===============
+Host Capabilities:
+Exposing the cpu_allocation_ratio parameter in host capabilities.
+It will make this parameter be presented to the scheduler filter.
+
+Scheduler:
+2 changes are being proposed.
+
+1.  Add a CpuOvercommitFilter filter. The filter will use
+    the alloc:cpu_ratio provided in the flavour extra_specs,
+    by the administrator, and will return a list of host that exactly match
+    the requested value or, in case such value doesn't exist, return
+    a list of host that closely match the above value.
+
+2. Modify the CpuOvercommitWeigher class, exposing the cpu_allocation_ratio
+   value as a weighting parameter, in order to have the Vms being scheduled
+   on host with the closest to the requested cpu overcommit value.
+
+Setting the overcommit ratio to 1, should effectively set the vm on
+dedicated core.
+
+Alternatives
+------------
+None
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+Although, this is a new filter that is being introduced,
+the filter will only be functional in case alloc:cpu_ratio
+value is being provided in the flavour extra_spec, otherwise
+the filter will be skipped
+
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Vladik Romanovsky <vladik.romanovsky@enovance.com>
+
+Work Items
+----------
+
+
+Dependencies
+============
+None
+
+Testing
+=======
+None
+
+
+Documentation Impact
+====================
+None
+
+
+References
+==========
+None
diff --git a/specs/juno/proposed/flavor-quota-memory.rst b/specs/juno/proposed/flavor-quota-memory.rst
new file mode 100644
index 0000000..0aa223c
--- /dev/null
+++ b/specs/juno/proposed/flavor-quota-memory.rst
@@ -0,0 +1,120 @@
+This work is licensed under a Creative Commons Attribution 3.0 Unported 
+License.                                                                
+                                                                        
+http://creativecommons.org/licenses/by/3.0/legalcode                    
+
+==========================================
+Add memory limit for libvirt
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/flavor-quota-memory
+
+Currently nova has supported setting CPU quota,such as 'quota:cpu_quota' and 
+'quota:cpu_shares',so we can add memory quota too.
+
+KVM has only supported memory limit now,so i think we should support setting 
+memory limit first.Once KVM supports share etc,we can add it later.
+
+
+Problem description
+===================
+
+If we do not control user uses the memory resources, memory resources may be 
+more than the maximum allocation of memory for the guest at boot time, the host
+memory resources cannot control.
+
+
+Proposed change
+===============
+
+Add extra key in flavor,For example:memory:cpu_quota.
+'cpu_quota' stands the maximum memory the guest can use.
+
+Alternatives
+------------
+
+Using the same method as CPU quota.It is easy to be extended and little change.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+It adds key in flavor.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+None
+
+Assignee(s)
+-----------
+
+Primary assignee:  
+   <hs.chen@huawei.com>  
+
+
+Work Items
+----------
+
+1.If the instance has the key 'memory:cpu_quota',we add memory limit config in 
+the libvirt driver.
+2.Add testcase of the limit config.
+
+
+Dependencies
+============
+
+Ability to rely on the KVM support.
+
+
+Testing
+=======
+
+Create a virtual machine, use memory compression tool pressure to a maximum 
+value, check if it can be more than limit value.
+
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
\ No newline at end of file
diff --git a/specs/juno/proposed/freebsd-compute-node.rst b/specs/juno/proposed/freebsd-compute-node.rst
new file mode 100644
index 0000000..d565a9a
--- /dev/null
+++ b/specs/juno/proposed/freebsd-compute-node.rst
@@ -0,0 +1,143 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================
+Add initial support for FreeBSD host as a compute node
+======================================================
+
+https://blueprints.launchpad.net/nova/+spec/freebsd-compute-node
+
+The purpose of this work is to provide support for compute nodes running on
+the FreeBSD operating system. Those will be able to run VMs with either qemu or
+the native FreeBSD hypervisor - bhyve. Also it will include a minimal
+nova-network driver supporting the bridge mode networks.
+
+
+Problem description
+===================
+
+Right now deployers are not able to build their cloud using FreeBSD as a host
+for compute node servers. There is no support for that OS in nova. Consequently,
+recently grown FreeBSD native hypervisor - bhyve can not be utilized.
+
+
+Proposed change
+===============
+
+Extend existing libvirt compute driver so that it supports both qemu and bhyve
+managed VMs on FreeBSD platforms, along with other necessary adjustments for
+the nova.virt subsystem and simple bridge mode networking.
+
+Alternatives
+------------
+
+An alternative approach would be to implement separate, native bhyve compute
+driver for nova. Still, necessity for the freebsd_net driver for nova-network
+would apply here. This path seems to be more complicated and harder to
+accomplish.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <md-sh>
+
+Work Items
+----------
+
+* Adjustments to the libvirt compute driver to support new hypervisor backend
+  - bhyve.
+
+* Adjustments to the libvirt vif driver to support other than linux_net
+  nova-network driver.
+
+* Implementation of libvirt firewall driver as iptables are not available on
+  FreeBSD
+
+* Implementation of freebsd_net driver (equivalent of linux_net) for
+  nova-network to support guest networking.
+
+* Implementation of FreeBSD memory disk support (equivalent of loop device on
+  Linux) for nova.virt.disk.mount module, necessary for file injection into
+  guest's file systems.
+
+* Adjustments to nova.virt.configdrive module to support FreeBSD platform.
+
+
+Dependencies
+============
+
+Primary new dependency is the FreeBSD OS itself. There might be new command
+line tools dependency like 'mdconfig' ('losetup' on Linux), etc.
+
+
+Testing
+=======
+
+New CI for FreeBSD platform has to be introduced, where all unit and integration
+tests will be run.
+
+
+Documentation Impact
+====================
+
+* There has to be new installation guide for FreeBSD.
+
+* Configuration options reference should be updated with new value options.
+
+
+References
+==========
+
+* http://www.freebsd.org/
+
+* http://bhyve.org/
+
+* http://libvirt.org/
diff --git a/specs/juno/proposed/generate-vmstates-graph.rst b/specs/juno/proposed/generate-vmstates-graph.rst
new file mode 100644
index 0000000..a63e2ce
--- /dev/null
+++ b/specs/juno/proposed/generate-vmstates-graph.rst
@@ -0,0 +1,170 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=========================================================================
+Automatically generate the virtual machine states and transitions graph
+=========================================================================
+
+https://blueprints.launchpad.net/nova/+spec/generate-vmstates-graph
+
+The virtual machine states and transitions graph described in
+doc/source/devref/vmstates.rst is not up-to-date with the code. We would like
+to generate this graph directly from the code, so that the code and its
+documentation are always in sync.
+
+
+
+Problem description
+===================
+
+Since the documentation of the virtual machine states and transitions and the
+code tell different stories, the following issues may arise:
+
+* users are not aware of new features
+* users try to use deprecated features
+* users find their VMs in a state that they did not think could be reached and
+  think it is a bug
+
+
+Proposed change
+===============
+
+Generating the virtual machine states and transitions graph from the code would
+solve this issue. This could be done in two steps:
+
+1. Write a program that parses the code and generates the graph using the
+   graphviz syntax.
+2. Remove the current graphviz code in doc/source/devref/vmstates.rst, and
+   inject the output of the script defined above when building the
+   documentation.
+
+The second step is trivial: it is just a matter of tweaking the build system.
+
+The first step is more complex. Using Python's `ast module
+<https://docs.python.org/3/library/ast.html?highlight=ast#module-ast>`_, it is possible to
+parse nova/compute/api.py and compute the pre-conditions for the defined
+transitions. For instance, if we look at the current definition of the "pause"
+operation:::
+
+    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.RESCUED])
+    def pause(self, context, instance):
+        ...
+
+It is possible (and realtively easy) to generate the following two lines of
+graphviz code:::
+
+    active -> pause
+    rescued -> pause
+
+Since the complete graph is a bit hard to read, we will also generate a simple
+table listing all methods and the valid states associated to them.
+
+This does not allow the post-conditions (the states a vm might be in __after__
+performing one of the defined operations) to be retrieved. To do so, we propose
+to add a 'post_vm_state' argument to the 'check_instance_state' decorator, that
+would be used to list all possible states in which a VM might be after the
+operation has been performed.
+
+An implementation can be found at https://review.openstack.org/#/c/97370/ .
+
+Alternatives
+------------
+
+Another way to solve this issue would be to keep the documentation updated
+by hand. But as we all know, this always leads to parts of the documentation
+becoming deprecated.
+
+It could also be automatically done by parsing nova/compute/manager.py and
+determining where the vm states are modified. For instance, in the
+pause_instance() method, it is set to vm_states.PAUSED, so we know that after
+performing the "pause" operation, we may be in the "PAUSED" state. This is just
+like recreating a control flow graph and doing a bit of static analysis.
+Whether this is easy and would allow us to retrieve all the post-conditions is
+not yet determined.
+
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+Generating the documentation will obviously take more time, but it will not be
+noticeable (running the script will take less than one second).
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+Developers will have to manually fill the post-conditions in the
+check_instance_state decorator.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <cyril-roelandt>
+
+
+Work Items
+----------
+
+1) Improve the existing proof of concept so that it handles post-conditions.
+2) Integrate the existing proof of concept into the build system of the
+   documentation.
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+We may want to add unit tests to make sure this scripts works as expected. For
+starters, just looking at the code and the generated graph should be enough and
+should provide a better documentation the what can currently be found in
+doc/source/devref/vmstates.rst.
+
+
+Documentation Impact
+====================
+
+None.
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/get-floatingip-by-all-tenants.rst b/specs/juno/proposed/get-floatingip-by-all-tenants.rst
new file mode 100644
index 0000000..9b59aaa
--- /dev/null
+++ b/specs/juno/proposed/get-floatingip-by-all-tenants.rst
@@ -0,0 +1,155 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Allow admin get floatingips of all tenants
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/get-floatingip-by-all-tenants
+
+Currently, in Nova API, there are several commands support "all-tenants" flag.
+e.g. listing volumes, listing servers, list-secgroup. this blueprint support
+getting floatingips of all tenants.
+
+Problem description
+===================
+
+There is a basic requirement that admin wants to know how much floating ips
+have been created/associated because of 'floating ips' is one type of limited
+resources.
+
+The neutron API supports admin to get all tenants' floating ips now. But if we
+use neutron as network component, admin user(user has associated admin role)
+can not get all tenants' floating ips by Nova API, because nova adds the
+'project_id' as one filter when calling neutronclient to get floatingips.
+This is hard-coding.
+
+This blueprint try to relax the restriction of Nova API about listing
+floatingips.
+
+Proposed change
+===============
+
+* Add an "all_tenants" flag to Nova indexing action of os-floating-ips API.
+
+* Add policy rules and policy validation for all-tenants authority
+
+* Allow nova-network getting floatingips of all tennats, this needs refactor
+  the relevant methods and adds db query methods.
+
+* Add parameter that nova can get floatingips of all tenants and get
+  floatingips of current tenant by calling Neutronclient.
+
+* Novaclient supports "all_tenants" flag of "nova floating-ip-list" command.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+The new 'floatingip-list' rest API in v2 have following choices:
+
+* The original API usage:
+  GET /v2/{tenant_id}/os-floating-ips
+
+* The new API usage support a filter "all_tenants" to get floatingips of all
+  tenants:
+  GET /v2/{tenant_id}/os-floating-ips？all_tenants={True in bool}
+  The new API response body format and response code is same as the original
+  API usage.
+
+Security impact
+---------------
+
+Admin user can get other tenants' floatingip info.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Allow user to specify "all-tenants" flag to get floatingips of all tenants,
+and this change is forward compatible.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    liusheng<liusheng@huawei.com>
+
+Work Items
+----------
+
+* Add "all_tenants" parameter to API layer process of listing floatingip, and
+  the relevant policy rule validation.
+
+* Implement the listing floatingip API with "all_tenants" and without
+  "all_tenants" in nova-network.
+
+* Implement the process that Nova call neutronclient to list floatingips of
+  all tenants.
+
+* Implement Nova CLI supporting "--all-tenants" flag in
+  "nova floating-ip-list" command.
+
+* Add unit tests to test the new methods, and add integration tests to tempest
+  to test the functional API.
+
+* Update the API documentation about this new API.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+This code will be tested in unit tests for this function.
+
+The API will be tested by adding tempest.
+
+Documentation Impact
+====================
+
+The nova API documentation will be updated for the all-tenants flag of listing
+floatingip.
+
+References
+==========
+
+https://blueprints.launchpad.net/nova/+spec/get-floatingip-by-all-tenants
+http://lists.openstack.org/pipermail/openstack-dev/2014-June/037304.html
+http://lists.openstack.org/pipermail/openstack-dev/2014-June/037536.html
diff --git a/specs/juno/proposed/get-lock-status-of-instance.rst b/specs/juno/proposed/get-lock-status-of-instance.rst
new file mode 100644
index 0000000..08db6d5
--- /dev/null
+++ b/specs/juno/proposed/get-lock-status-of-instance.rst
@@ -0,0 +1,171 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+Query lock status of instance
+==================================================
+
+https://blueprints.launchpad.net/nova/+spec/get-lock-status-of-instance
+
+Currently we only support locking/unlocking an instance but we are not able
+to query whether the instance is locked or not.
+This proposal is to add the lock status to the detailed view of an instance.
+
+Problem description
+===================
+
+We are able to lock/unlock an instance through nova API now.
+But we don't return the lock status of the servers.
+
+Proposed change
+===============
+
+Display the lock status as part of the detailed view of an instance
+(that is, 'nova show')
+
+Alternatives
+------------
+
+The lock status can be identified by attempting to lock the instance,
+but if the instance is not already locked this has the side-effect of
+locking it. If another process simultaneously tries to query the lock
+status in the same fashion, it may get a false positive.
+Equally if another process tries to delete the instance while it is
+locked due to a query, it will fail when it shouldn't.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+Add following output to the response body of
+GET /v2/45210fba73d24dd681dc5c292c6b1e7f/
+servers/a9dd1fd6-27fb-4128-92e6-93bcab085a98
+
+Following lock info will be added in addition to
+existing output info.
+
++---------------+---------------------+--------------------------------------+
+| Parameter     |  Type               | Description                          |
++===============+=====================+======================================+
+| locked        | boolean             | whether the instance is locked       |
++---------------+---------------------+--------------------------------------+
+| locked_by     | string              | User locked the instance, current    |
+|               |                     | valid value are 'admin' and 'owner'  |
++---------------+---------------------+--------------------------------------+
+
+If the locked is True, following info will be added into output:
+
++---------------+-----------------------------------------+
+| Parameter     | Data                                    |
++===============+=========================================+
+| locked        | True                                    |
++---------------+-----------------------------------------+
+| locked_by     | 'admin'                                 |
++---------------+-----------------------------------------+
+
+If the locked is false, this will return following info:
+
++---------------+-----------------------------------------+
+| Parameter     | Data                                    |
++===============+=========================================+
+| locked        | False                                   |
++---------------+-----------------------------------------+
+| locked_by     | None                                    |
++---------------+-----------------------------------------+
+
+Both v2 and v3 API will be affected.
+
+* In v2 API, extension os-server-locked-status will be added to
+  advertise the extra information.
+  alias: os-server-locked-status
+  name: ServerLockStatus
+  namespace: http://docs.openstack.org/compute/ext/server_locked_status/api/v2
+  When the new extension "os-server-locked-status" is loaded,
+  2 new fields 'locked', 'locked_by' will be added to
+  the os-hypervisor API.
+
+* In v3 API, locked information will be directly added to extended_status.py
+  since locked_by is already there.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+This will allow user to query the lock status of an instance.
+
+python-novaclient will be updated in order to show the lock status
+in the 'nova show' commands.
+
+If there is no lock status info in the output from older v2 API,
+the new python-novaclient will exclude the lock status,
+locked_by fields.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jichenjc
+
+Work Items
+----------
+
+Nova v2 API update.
+Nova v3 API update.
+Tempest cases update for locked field check.
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest cases will be added, especially the
+lock/unlock related cases will check through the APIs to be added,
+e.g. the new lock status fields will be mandatory required fields.
+
+Documentation Impact
+====================
+
+API document will be updated in order to list the lock status.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/gpfs-instance-store.rst b/specs/juno/proposed/gpfs-instance-store.rst
new file mode 100644
index 0000000..4a56ba3
--- /dev/null
+++ b/specs/juno/proposed/gpfs-instance-store.rst
@@ -0,0 +1,130 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================================
+Leverage the features of IBM GPFS to store cached images and instances
+======================================================================
+
+https://blueprints.launchpad.net/nova/+spec/gpfs-instance-store
+
+In this blueprint we add the IBM GPFS cluster file system as instance store
+and image cached. Leverage the features of GPFS to optimally store cached
+images and instance files. The bock-level format-agnostic Copy-On-Write(COW)
+mechanism enables quick instance provisioning and instance snapshot avoiding
+data copy.
+
+
+Problem description
+===================
+
+When user selete GPFS as instance store and image cached store, GPFS
+Copy-On-Write feture will enhance the performence of spawn instances and
+instances snapshot.
+
+GPFS Copy-On-Write mechanism avoid the data copy when image cached to base
+directory and snapshot instance will be generated regardless of the image
+format.
+
+GPFS as glance image Store blueprint have been approved. The instance snapshot
+image cloud be updated to the same filesystem glance GPFS image store without
+http upload byte by byte.
+
+Proposed change
+===============
+
+Add gpfs image download module for GPFS image cached with GPFS COW avoiding
+date copy
+
+Add gpfs imagebackend for instance gpfs store leaverage GPFS COW to enhance
+instance spawn
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Instance storage should be GPFS filesystem.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Add a new class Gpfs in nova/virt/libvirt/imagebackend.py
+
+Add a new file gpfs.py in nova/image/download/gpfs.py
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  renminmin
+
+Work Items
+----------
+
+Implement gpfs image download module support cached image leavarage the GPFS
+Copy-On-Write feature without data copy.
+
+Implement gpfs imagebckend support instance files generate leaverage the GPFS
+Copy-On-Write feature without date copy regardless image format.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+No tempest changes.
+Comprehensive unit tests to test the functionality have been written
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/horizontally-scalable-scheduling.rst b/specs/juno/proposed/horizontally-scalable-scheduling.rst
new file mode 100644
index 0000000..5c71e7d
--- /dev/null
+++ b/specs/juno/proposed/horizontally-scalable-scheduling.rst
@@ -0,0 +1,248 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Horizontally Scalable Scheduling
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/horizontally-scalable-scheduling
+
+The blueprint introduces an implementation of Nova scheduling that gives
+operators a massively scalable alternative to the existing Nova scheduler,
+and provide constant time performance for very large scale deployments of
+over a hundred thousand nodes.
+
+Problem description
+===================
+
+The existing scheduler implementations allocate resources on what is currently
+considered the optimal location for the given resource. Not only is this a
+costly process, since the data set to be evaluated is potentially very,
+very large, but since the workloads across the cloud is highly variable and
+entirely non-deterministic, the designated optimal location may soon be a
+below average location for the resource.
+
+The scheduler needs to have an updated list of all the active hosts to make
+a decision, whether random, or a filtered set of hosts. Obtaining this list
+on a massive (thousands of) compute deployment can take an unacceptable amount
+of time.
+
+This spec outlines an alternative approach that simply puts resources
+where ever they'll fit with no attempt to make a centralised decision about
+anything at all. The end result is an O(1) scheduler, with less flexibility
+than other approaches, since a richer flavour concept doesn't map well to a
+small set of work queues. It possible that we just don't need this flexibility
+in mega scale deployments.
+
+
+Proposed change
+===============
+
+The proposed solution makes use of the messaging layer for scheduling.
+The desired outcome would be to decentralize scheduling of compute resources
+to individual compute nodes, such that this decision making step is scaled out
+horizontally by the compute infrastructure, and overall complexity is reduced.
+
+The incoming resource request is validated as usual, per the current
+approach by the API. After this they're sent to a scheduler, that simply puts
+it on a queue. Compute nodes poll this queue for work items if they have
+spare capacity.
+
+
+The implementation could be iterated as follows:
+
+* Add a new scheduler class that will send out the scheduling request on the
+  message queue using cast. This keeps Nova scheduler even though it's of very
+  little use.
+  Any compute node that has the capacity to spare will listen on this topic.
+  The messaging layer will "at random" distribute the request to a listening
+  compute node. The compute node will ensure that it can actually accommodate
+  he instance (based on the instance type) and raise an exception if it does
+  not have sufficient capacity after all. This will cause the message to be
+  re-queued (if using the kombu messaging driver) and another node can attempt
+  to fulfill the request. If it does have capacity, it will go ahead with the
+  allocation as usual.
+
+* The next iteration will add per-instance-type topics.
+  Nodes only subscribe to topics for instance types that they can actually
+  accommodate.
+
+* The next iteration will remove the scheduler entirely and make nova-api put
+  resource requests directly on the work queue.
+
+* The final iteration (which depends on Marconi being ready for production)
+  will use Marconi as the queueing back-end.
+  (This may or may not come for free depending on whether a Marconi
+  RPC backend gets added to the RPC layer).
+
+Alternatives
+------------
+
+A number of alternative proposals were discussed:
+
+**1. Broadcast "scheduling"**
+
+Description:
+  While in the proposed solution, the messaging layer picks a single random node
+  for sending the request on a specific topic based on instance type,
+  a mailing list discussion [1]_ suggested that requests could be broadcast to
+  (a subset of) the compute nodes and have them decide if they can accommodate
+  the request. Of the positive responses, pick a winner (at random or based on
+  a suitability score given in the response). First broadcast to idle nodes,
+  then to nodes with 0-10% utilisation, then to nodes with 10%-20% utilisation,
+  etc.
+
+Short Analysis:
+  Offers a constant upper bound on scheduling time (based on time between
+  each broadcast + number brackets that infrastructure is split into
+  (e.g. 0%, 10%, 20%. etc. vs. 0%, 25%, 50%, etc.)).
+  Fewer brackets => faster guaranteed response times.
+  Fewer brackets => more nodes in each bracket = potentially overwhelming for
+  scheduler to deal with responses from nodes.
+
+**2. Split workloads across multiple schedulers**
+
+Description:
+  N schedulers [2]_ could (automatically amongst themselves) split the
+  infrastructure in N equal chunks each of which would be assigned to one
+  scheduler. Leave everything else pretty much as is (use same algorithm for
+  placing VM's, volumes, etc. as the current scheduler does).
+
+Short analysis:
+  The added scalability comes from the fact that schedulers can be added
+  dynamically and each one only has to deal with 1/N of the entire
+  infrastructure. It may prove a difficult to strike a good balance between
+  number (and hence the size) of the chunks vs. the chance that each of them
+  will be able to fulfill a given request.
+  More chunks -> smaller chunks -> less likely to be able to fulfill
+  incoming requests -> more retries.
+
+**3. Move scheduler data to memcache**
+
+Description:
+  Have nodes broadcast their resource stats to all schedulers. Each scheduler
+  has its own (local) memcache [3]_ where this data is stored. Leave everything
+  else pretty much as is (use same algorithm for placing VM's, volumes, etc.
+  as the current scheduler does). The added scalability comes from faster and
+  more local storage.
+
+Short Analysis:
+  This angle of attack makes sense if the biggest problem with the scheduler
+  is collection and retrieval of data. However, if the biggest problem is the
+  scheduling algorithm itself, it's of no use. I personally believe that the
+  problem at scale is the scheduling algorithm itself.
+
+Data model impact
+-----------------
+
+None. The compute nodes would continue to update the compute_node
+tables with their state information, and all updates will be retrieved via
+the conductor.
+
+REST API impact
+---------------
+
+The Compute API v2 extension OS-SCH-HINT cannot be used with this scheduling
+option since the scheduler hints passed in the dictionary are only required
+by the scheduler service.
+this scheduling option enabled.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+Though this change does not remove any existing notifications, notifications
+previously sent by the scheduler service will not longer be sent.
+
+
+Other end user impact
+---------------------
+
+Besides setting a configuration option in nova.conf, an operator does not
+interact with this feature in any other way.
+
+Performance Impact
+------------------
+
+**New Server Creation:**
+A significant performance *improvement* in spawning of new servers would be
+noticed.
+The latency introduced by the scheduler to decide a host
+from a large data set will be eliminated, since the compute host picks up the
+request placed by Nova API directly from the message queue.
+
+**Compute Service startup:**
+Since each compute node will listen to additional instance type based topics
+at start-up, there may be a slight lag in the startup time of Nova Compute.
+
+Other deployer impact
+---------------------
+
+This change is optional and the deployer would need to set a configuration
+parameter in nova.conf before the Compute service starts.
+Nova Compute, as a part of post hook, will check if this option is set and
+subscribe to additional topics.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  rohitk
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+The iterations mentioned in the Proposed Solution section would be used
+as high level work items.
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+The existing Tempest functional tests for Nova compute should continue to pass
+with improved execution performance on a multi node environment. Hence we don't
+need to add more tests to Tempest.
+
+
+Documentation Impact
+====================
+
+Any information that elaborates this approach to scheduling server creation
+for massive scale infrastructure needs to be documented. The relevant
+configuration option that a deployer should use to enable it and it's impact
+should also be documented.
+
+
+References
+==========
+
+.. [1] http://bit.ly/1nvnit8
+
+.. [2] http://lists.openstack.org/pipermail/openstack-dev/2013-July/012428.html
+
+.. [3] https://blueprints.launchpad.net/nova/+spec/no-db-scheduler
diff --git a/specs/juno/proposed/host-metric-hook.rst b/specs/juno/proposed/host-metric-hook.rst
new file mode 100644
index 0000000..c1bcf3b
--- /dev/null
+++ b/specs/juno/proposed/host-metric-hook.rst
@@ -0,0 +1,207 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Host Metric Hook
+==========================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/host-metric-hook
+
+Provide a hook into the HV host stats periodic_task
+
+Problem description
+===================
+
+The compute.manager has a large number of periodic tasks that collect data
+about the running host. The typical solution for monitoring hypervisors is
+to install agents on the host to collect and relay this data. This seems
+like a duplication of effort and makes deployment complicated.
+
+In large deployments it would be unwise to emit
+this data for every instance on the host as it would quickly saturate the
+system. Instead we are proposing to provide a means where an in-service
+plugin can be called to process the collected data locally (and in-memory)
+and only report on exceptional cases.
+
+The typical use-case for this would be QoS and Alarming. If we were to see
+customer that has a 5Gb pipe has been running at 20Gb, we'd like to catch
+that early. Likewise, if a customer is running at 100% CPU on a 4 core
+image, that should be reported. Alternatively, the plugin could be as
+simple as taking this collected data and emitting it to a reporting
+tool like statsd/graphite via UDP.
+
+The existing notification system would be a good place for these metrics
+to be passed on. The recent addition of the `Routing Notifier`_ and the `SAMPLE
+notification priority`_ gives us all the means we need to not clutter up
+the usage/audit notifications with metering data.
+
+.. _Routing Notifier: https://blueprints.launchpad.net/oslo.messaging/+spec/configurable-notification
+.. _SAMPLE notification priority: http://docs.openstack.org/developer/oslo.messaging/#a3
+
+Periodic Tasks in compute.manager
+---------------------------------
+
+* update_available_resource() - yes
+
+  * potentially also call with resource_tracker information that
+    includes requested resources (not just what hypervisor is reporting)
+
+  * actually called in resource_tracker.update_available_resource()
+
+* _cleanup_running_deleted_instances - Should emit notification
+
+* _run_image_cache_manager_pass - Should emit notification
+
+* _run_pending_deletes - Should emit notification
+
+* _check_instance_build_time - Should emit notification
+
+* _heal_instance_info_cache - maybe?
+
+* _poll_rebooting_instances - Might emit notification on anomalies
+
+* _poll_rescued_instances - Might emit notification on anomalies
+
+* _poll_unconfirmed_resizes - Might emit notification on anomalies
+
+* _poll_shelved_instances - meh
+
+* _instance_usage_audit - maybe, but unlikely
+
+* _poll_bandwidth_usage - yes
+
+* _poll_volume_usage - yes
+
+* _sync_power_states - calls virt.get_info() which has mem/max_mem info ... dunno?
+
+* _reclaim_queued_deletes - meh
+
+
+Proposed change
+===============
+
+The above period tasks will emit notifications at the SAMPLE priority so
+they can be picked up by the Routing Notifier and sent to the appropriate
+downstream systems.
+
+Most of the heavy lifting has been done for this feature and the hooks
+were already established in a previous branch. This previous attempt
+was abandoned because it used a plugin approach and not the routing
+notifier. But the hooks were proven to be viable.
+
+Alternatives
+------------
+
+Hypervisor meter collection can currently be performed by many different
+existing open source tools, such as Diamond or the Ceilometer pollsters.
+But, these approaches require agents to be installed on the compute nodes
+and the don't associate the hypervisor information with the Nova metadata.
+By putting this feature in the core nova code (optionally enabled) we give
+one less thing the deployer has to deal with. And, we're already collecting
+this data anyway ... DRY.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+The collected hypervisor data has to be scrubbed to remove any sensitive
+information, just as with any notification. Since we're leveraging the
+existing notification framework, any efforts here can be shared with
+other notifications.
+
+Notifications impact
+--------------------
+
+New notifications will be generated with a SAMPLE priority. These can
+be ignored or processed using the Routing Notifier.
+
+Other end user impact
+---------------------
+
+This is not an end-user facing feature. Its benefits will be felt by
+downstream metering/monitoring systems such as Ceilometer, Heat and
+StackTach (to name a few).
+
+Performance Impact
+------------------
+
+The routing notifier will need to inspect each notification for
+suitability and, depending on the routing path chosen, could
+cause an impact to performance. However, it is not recommended
+that any slow export of notification data be used. Instead,
+AMQP or UDP-based notification drivers should be selected.
+
+Other deployer impact
+---------------------
+
+Setting up the Routing Notifier requires editting a json
+configuration file, which could be a little complicated for
+some deployers. However, all notifications are disabled by
+default, so it won't affect the majority of users.
+
+Developer impact
+----------------
+
+New periodic tasks should consider publishing collected data via
+this SAMPLE priority notification as well, if applicable.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  sandy.walsh
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+Convert https://review.openstack.org/#/c/51249/ to use the SAMPLE
+priority notification and ditch the existing plugin scheme.
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit tests are already established in the above branch. They will be fixed up
+to cover the new approach.
+
+
+Documentation Impact
+====================
+
+None - the Routing Notifier is already documented.
+
+May need to update the `System Usage Data`_ wiki page.
+
+.. _System Usage Data: https://wiki.openstack.org/wiki/SystemUsageData
+
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/host-servers-live-migrate.rst b/specs/juno/proposed/host-servers-live-migrate.rst
new file mode 100644
index 0000000..39a69de
--- /dev/null
+++ b/specs/juno/proposed/host-servers-live-migrate.rst
@@ -0,0 +1,156 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================================
+Live migrate all running instances from the host
+================================================
+
+https://blueprints.launchpad.net/python-novaclient/+spec/host-servers-live-migrate
+
+The aim of this feature in python-novaclient is to allow Operators in only
+one command to live migrate all running instances from one host to other hosts.
+
+
+Problem description
+===================
+
+Currently, in order to perform a maintenance operation on a host without
+service interruption for customers, Openstack Operator can live-migrate one
+by one each running instance from the host.
+
+To facilitate this operation, we propose a wrapper in python-novaclient
+that would, in only one command, live-migrate all hypervisor running instances
+to other hosts.
+
+The major difference between this new feature and two existing novaclient
+commands, namely host-evacuate [1] and host-servers-migrate [2], is that
+host-servers-live-migrate command will be transparent for customer,
+without any downtime for running instances :
+
+* ``host-evacute`` command is targeting failed compute nodes, and will
+  fail for compute nodes that are up
+
+* ``host-servers-migrate`` will **suspend** all running instances on a
+  host in order to migrate the instances on new host.
+
+
+Proposed change
+===============
+
+The new command will internally use the existing server live-migrate API,
+but since Nova API V3 brings some changes in the manner to query the hypervisor
+information, the proposed change will add two feature implementations in
+python-novaclient.
+
+The existing live-migration command in python-novaclient uses several
+optional arguments:
+
+* ``--block-migrate``   True in case of block_migration.
+
+* ``--disk-over-commit`` Allow disk overcommit.
+
+And positional arguments:
+
+* ``<server>``          Name or ID of server.
+
+* ``<host>``            Destination host name.
+
+Proposed new command, host-servers-live-migrate, will keep the
+same optional arguments: ``--block-migrate`` and ``--disk-over-commit``.
+The optional positional argument ``host`` will be renamed to
+``--target_host <target_host>`` and a new positional argument ``host``
+will be used in order to designate the compute node that will be taken
+out of action.
+
+
+Alternatives
+------------
+
+In order to avoid multiple calls from client, the alternative would be to
+implement this feature in Nova.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The user can trigger this feature by:
+``nova host-servers-live-migrate host``
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  arezmerita
+
+Work Items
+----------
+
+* Implement the feature for Nova API V2
+
+* Implement the feature for Nova API V3
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Since live-migration feature is already well covered by tempest tests, we
+think that unit tests are sufficient.
+
+
+Documentation Impact
+====================
+
+OpenStack Command-Line Interface Reference must be updated in order to
+add new command in Compute command-line client.
+
+
+References
+==========
+
+.. [1] https://blueprints.launchpad.net/python-novaclient/+spec/evacuate-host
+
+.. [2] https://blueprints.launchpad.net/python-novaclient/+spec/host-servers-migrate
diff --git a/specs/juno/proposed/hot-resize.rst b/specs/juno/proposed/hot-resize.rst
new file mode 100644
index 0000000..ac49c9c
--- /dev/null
+++ b/specs/juno/proposed/hot-resize.rst
@@ -0,0 +1,430 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+API: Live Resize
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/hot-resize
+
+'Cold-resize' had been implemented previously. But instance needs to be
+rebooted during the period.
+
+So this plan aims to add 'live-resize'(or 'hot-resize'?) function into Nova.
+This is useful and convenient for users.
+
+Cpu hot-add feature has been supported by qemu(since version 1.6.2)
+and libvirt(since version 1.2).
+VMware can support cpu-hot-add/remove, mem-hot-add/remove.
+It will be implemented if this function merged.
+
+Problem description
+===================
+
+There is no current API that can resize servers online.
+The original resize API is equal to cold-resize, or offline resize.
+That's not convenient for end users.
+
+The use cases that are driving this API extension are derived from a user's
+experience.
+
+Proposed change
+===============
+
+0. Only images contain 'QEMU Guest Agent'(qga) could support this function
+   in libvirt. It uses 'hw_qemu_guest_agent' tag in image's metadata
+   to indicate(Implemented in Havana [1]).
+
+   Image without qga in libvirt will be raised 'NotSupportedException'
+   for "live-resize".
+
+1. Add a new key-value pair in image's metadata like {'max_vcpus_limit':<int>}.
+   This value will be effective in spawn/rebuild/rescue/resize/live-resize.
+
+2. Add a new extension API "live-resize" which the owner and admin can use it.
+   The API can specify whether execute "live_migration"
+   when original host doesn't have enough resources to hotplug or not.
+   See details in the API part.
+
+3. Add a new 'LIVE_RESIZING' to task states.
+   It means the request is sent to compute node to execute.
+
+4. Add a new "live-resize" ComputeTaskAPI & related logic in nova-conductor
+   to decide whether need/how to select a new target host.
+
+5. Add new "check_live_resize_supported" & "live-resize" APIs
+   in base virt driver.
+
+   The former one allows each hypervisor can check its supported
+   for live-resizing by comparing the requested flavor with the old one.
+   And the latter one will implement specific execution for live-resizing.
+   The default of the two APIs is to raise 'NotImplementedError'
+   in base virt driver.
+
+6. Add a new field called 'max_vcpus_limit' in 'Instance.system_metadata'
+   (or Instance?) table in DB to record max vCPUs of instances.
+   It aims to ensure max vCPUs number correct even if the image's metadata
+   was modified.
+
+   If image does not have the 'max_vcpus_limit' tag in its metadata,
+   the value of 'max_vcpus_limit' will be 0('NO LIMIT'). Besides, all instances
+   upgraded from lower version, this value will also be equals to 0.
+
+7. Change one element in the config-xml of instance in libvirt driver
+   from "<vcpu>X</vcpu>" style to "<vcpu current='X'>'Y'</vcpu>"
+   if 'max_vcpus_limit' is specified in image's metadata.
+
+   Here, the 'X' will be the value of flavor's 'vcpus',
+   and the 'Y' will be equal to 'max_vcpus_limit' we added in the step1.
+
+8. The API 'DescribeInstance' needs to add one key-pair property
+   into the response body like <'max_vcpus_limit':4> to show the
+   max vcpus limit of this instance which can be obtained from DB table.
+   (Now only vcpus live-resize will be implemented in libvirt driver)
+
+
+Alternatives
+------------
+
+1. Support image without QEMU guest agent(qga)
+
+   In the above design, only the image installed qga can support this feature
+   in libvirt.
+
+   Actually, if image doesn't contain gqa, after "live-resize" action
+   in libvirt, the user can execute some commands manually in the instance
+   to activate the new plugged resources(choose vcpus to explain):
+
+   'echo 1 > /sys/devices/system/cpu/cpuX/online'
+
+   * The 'X' means which vCPU you want to activate.
+   * The range of 'X' is from 0 to (amount of vCPUs - 1).
+   * For example, the 'cpu0' means your instance only has one vCPU.
+   * The count of the executions relies on the vCPUs number your instance has.
+
+   The reason why this proposal isn't chosen is
+   the manual operations is not friendly enough to end-users.
+
+   * If normal images need to be supported, the API response ought to
+     give two different answers to end-users based on your image attribute,
+     like 'Your operation took effort.' or 'Your operation is applied, but you
+     need to execute some commands in your instance...'.
+
+   * Moreover, if one instance plugged new 4 vCPUs,
+     end-user needs to execute the command four times.
+
+   So this is not a good idea IMO.
+
+2. Don't touch 'DescribeInstance'
+
+   Why I add a new param in response body of 'DescribeInstance' is
+   I want to provide a mechanism for user to check the 'max_vcpus_limit'
+   of instances.
+
+   * If not, the only way to get this info is to check it in DB,
+     and that's not a convenient and acceptable way for normal users.
+
+   * Image's metadata could be changed therefore it can't be the evidence.
+
+   * Moreover, I don't think it's necessary to add a new API
+     for describing the max vcpus limit of instances.
+
+   Therefore, I modified the response body of 'DescribeInstance'.
+
+3. The 'disk-over-commit' in live_migration
+
+   The 'disk-over-commit' means whether allow disk overcommit
+   in "live_migration".
+   The default value of it is 'False', and I'm also preferred to use 'False'.
+
+   And I don't think it's clear to expose this param in the 'live-resize'
+   because this param doesn't suit all conditions
+   (only effective for one condition).
+
+   But I paste it here to see other reviewers' opinion.
+
+4. The behaviour of 'resize' API
+
+   In the above design, we want to keep the consistency between
+   "resize" & "live-resize".
+
+   * If images without 'max_vcpus_limit', the instances will be handled
+     as usual, no restriction in "resize", but no "live-resize" available.
+
+   * If images include 'max_vcpus_limit', the instance will have a restriction
+     that new flavor's item can't be greater than the limitation(like vcpus),
+     both in "resize" & "live-resize".
+
+   * But the 'max_vcpus_limit' of instance can't be modified
+     after initialization. It's not convenient.
+
+   So, should we need to remove the 'max_vcpus_limit' restriction in "resize"?
+
+   * If item value(like 'vcpus') of new flavor in "resize" is greater than the
+     'max_vcpus_limit' in DB, we change the DB value to this new value,
+     instead raising an exception.
+
+   * The advantage is we'll have a method to change the 'max_vcpus_limit'
+     in DB table.
+   * The disadvantage is it'll brings difference between "resize" &
+     "live-resize". Because "live_resize" needs to follow the restriction.
+
+   Therefore, I'm still preferred to the original design.
+
+5. 'max_vcpus_limit' stored in flavor's extra_specc, not in image's metadata
+
+   In the earlier discussion in one deprecated nova-spec for 'cpu hot-add' [2],
+   I use flavor's extra_spec to restore the 'max_vcpus_limit' to "live-resize".
+
+   Right now, I think image's metadata is a better place to store the tag.
+   There are three reasons for my choice:
+
+   * Firstly, storing the tag in flavor's extra_spec is not friendly for users.
+     The modifications may confuse original flavor's scope for understanding,
+     especially when user wants to resize their instances, what should they
+     choose to use? Resize with new flavor's vpus or live-resize with old
+     flavor's 'max_vcpus_limit' in extra_spec?
+
+   * Secondly, many commercial systems use flavor to be the foundation of their
+     billing systems. The modification of flavor may impact their exist billing
+     principles. We need to cautious about that.
+
+   * Lastly but importantly, this feature is strongly associated with
+     each guestOS' abilities. One OS may has distinguish behaviours under
+     different versions the rather that we need to support the feature
+     under different OSs.
+
+   Therefore, I choose to store the 'max_vcpus_limit' in each image's metadata.
+
+
+Data model impact
+-----------------
+
+Table 'instance_system_metadata'(or Instances?) needs to add one filed:
++-----------------+--------------+------+-----+---------+----------------+
+| Field           | Type         | Null | Key | Default | Extra          |
++-----------------+--------------+------+-----+---------+----------------+
+| max_vcpus_limit | int(11)      | YES  |     | NULL    |                |
++-----------------+--------------+------+-----+---------+----------------+
+
+* This value is initialled in the procession of instance creation,
+  and can't be modified afterwards.
+* The value of 'max_vcpus_limit' will be equal to the 'max_vcpus_limit'
+  in image's metadata.
+* But if image doesn't have the tag, the value of it will be 0('No Limit').
+
+The related DB modification will also be merged
+into /nova/db/sqlalchemy/migrate_repo/versions/.
+
+REST API impact
+---------------
+
+1. Add one API "live_resize"
+
+The rest API look like this in v2::
+ /v2/{project_id}/servers/{server_id}/action
+
+    {
+        'live_resize':{
+        'flavor_id':2,
+        'allow_live_migration':True
+        }
+
+    }
+
+and in v3 it is like::
+ /v3/servers/{server_id}/action
+
+    {
+        'live_resize':{
+        'flavor_id':2,
+        'allow_live_migration':True
+        }
+
+    }
+
+  * The <int> 'flavor_id' means the new flavor you want to change to.
+    No default value.
+  * The <boolean> 'allow_live_migration' means whether execute "live_migration"
+    if instance's original host doesn't have enough resources.
+    The default value should be 'False'.
+
+The response body of it is like::
+
+    {
+        'task_states':'LIVE_RESIZING'(/'MIGRATING')
+    }
+
+  * The 'task_states' will be 'LIVE_RESIZING'/'MIGRATING'.
+
+    * The 'LIVE_RESIZING' means the request is sent to compute node to execute
+      the real-logic.
+    * The 'MIGRATING' means the original host doesn't have enough resources;
+      we need to "live_migrate" instance to another suitable host and
+      execute "live-resize" later if 'allow_live_migration' equals to 'True'.
+
+The status code will be HTTP 202 when the request has succeeded.
+
+If the request fails, corresponding exception will be raised.
+The related states/information will be roll back.
+
+* The state of instance will always be ACTIVE during the procession.
+
+* If flavor or instance doesn't exist, the request will fail.(HTTP 404)
+
+* If 'max_vcpus_limit' of the instance is smaller than the request
+  flavor's vcpus, one new exception like 'VCPUsLimitExceeded' will be raised.
+  (HTTP 400)
+
+* If instance's original host doesn't have enough resources to "live-resize"
+  and 'allow_live_migration' equals to 'False' in request body,
+  then the request will fail and 'ComputeResourcesUnavailable' exception
+  will be raised.(HTTP 400)
+
+* The task_state will be roll backed if fails happened during the verification.
+  (validation in API/host selection in conductor/supported check in compute)
+
+
+2. Modify API "DescribeInstance"
+
+Add one optional key-pair property into the response body to show
+the 'max_vcpus_limit' of this instance. This value will not be None or '':
+
+{
+    'server':{
+    ...
+    'max_vcpus_limit':4,
+    ...
+    }
+
+}
+
+* The 'max_vcpus_limit' can be captured from 'max_vcpus_limit' in DB
+  'instance_system_metadata' table.
+
+3. Change in "CreateInstances"
+
+If tag 'max_vcpus_limit' includes in image's metadata, its value will be
+recorded in DB 'instance_system_metadata' table for instance.
+
+If not, the value of 'max_vcpus_limit' in DB will be 0. It means 'No Limit'.
+
+4. Change in "resize"
+
+Now we add 'max_vcpus_limit' for every instance, so we need to restrict
+all related APIs to obey the same rule.
+
+* If images without 'max_vcpus_limit', "resize" action will be handled
+  as usual.
+
+* If images includes 'max_vcpus_limit', "resize" action will have a new
+  restriction that new flavor's item can't be greater than
+  the limitation(like 'max_vcpus_limit') in DB.
+  Or a new exception will be raised.
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+Needs to add two new types of notification:
+1. live_resize.start
+2. live_resize.end
+
+Other end user impact
+---------------------
+
+This function will be added into python-novaclient when the work finish.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+* The 'max_vcpus_limit' of all instances upgraded from lower version will be 0.
+  They'll be treated as the instances without 'max_vcpus_limit' specified.
+
+* The images contain 'QEMU Guest Agent'(qga) are required by this function
+   in libvirt.
+
+* A new key-value pair in image's metadata like {'max_vcpus_limit': <int>}
+  is required.
+
+Developer impact
+----------------
+
+Now only cpu-hot-add will be implemented in libvirt driver.
+Memory & disk functions will be added when QEMU supports them.
+
+Later, the VMware Driver support will be added if this function merged [3].
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Assignee: wingwj <wingwj@gmail.com>
+
+Work Items
+----------
+
+1. Code this function in v2/v3
+
+1.1 DB modification
+1.2 API "live-resize" Implementation in v2/v3
+1.3 Related APIs(create/resize/..) modification
+
+2. Finish tempest tests
+3. Code this function in python-novaclient
+
+
+Dependencies
+============
+
+Now only vcpus live-resize will be implemented in libvirt driver firstly
+
+1. QEMU >= 1.5.0
+2. libvirt >= 1.1.0
+
+
+Testing
+=======
+
+Unit tests and tempest tests will verify this function.
+UTs for API/DB/libvirt driver will be examined separately.
+In integrated tests and tempest, it will be tested together.
+
+
+Documentation Impact
+====================
+
+A description of this function will be added into Compute API V2
+and V3 Reference.
+
+Moreover, a "live-resize" support matrix of GuestOSs under each hypervisor
+is planed to add into the Reference.
+
+
+References
+==========
+
+A "live-resize" support matrix of GuestOSs under each hypervisor
+is planed to add into a new wiki page in wiki.openstack.org.
+(Under Construction)
+
+* [1] https://blueprints.launchpad.net/nova/+spec/qemu-guest-agent-support
+
+* [2] https://review.openstack.org/#/c/86273/
+
+* [3] http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=2020993
diff --git a/specs/juno/proposed/hyper-v-generation-2-vms.rst b/specs/juno/proposed/hyper-v-generation-2-vms.rst
new file mode 100644
index 0000000..41745b0
--- /dev/null
+++ b/specs/juno/proposed/hyper-v-generation-2-vms.rst
@@ -0,0 +1,146 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Hyper-V generation 2 VMs
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/hyper-v-generation-2-vms
+
+Introduction of Hyper-V generation 2 VMs support in the Nova Hyper-V
+compute driver.
+
+Problem description
+===================
+
+Hyper-V Server 2012 R2 introduces a new feature for virtual machines named
+"generation 2" , consisting mainly in a new virtual firmware and better support
+for synthetic devices.
+
+The main advantages are:
+
+* secureboot support
+* reduced boot time
+* virtual devices completely synthetic (no emulation)
+* UEFI firmware in place of BIOS
+* support for live resize of boot disks (expand)
+
+Operating systems supporting generation 2:
+
+* Windows Server 2012 / Windows 8 and above
+* Newer Linux kernels
+
+Other operating systems not supporting generation 2, including previous
+versions of Windows won't install or boot, so generation 1 needs to be retained
+as the default.
+
+The image must be in VHDX format.
+
+Proposed change
+===============
+
+The Hyper-V compute driver creates a generation 2 VM based on a property
+defined in the instance image, defaulting to generation 1.
+
+The compute driver reverts to generation 1 if the image format is VHD,
+generating a warning.
+
+Generation 2 VMs don't support IDE devices, which means that local boot and
+ephemeral disks must be attached to a SCSI controller, while retaining IDE
+support for generation 1 instances (where SCSI boot is not supported).
+
+Proposed image property to identify the desired generation and related values:
+
+hw_machine_type={hyperv-gen1,hyperv-gen2}
+
+Alternatives
+------------
+
+Generation 1 VMs are currently supported.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  cbelu
+
+Other contributors:
+  alexpilotti
+
+Work Items
+----------
+
+* Nova Hyper-V driver implementation
+* Unit tests
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit tests. Hyper-V specific Tempest tests can be added as well.
+
+Documentation Impact
+====================
+
+The Nova driver documentation should include an entry about this topic
+including when to use and when not to use generation 2 VMs. A note on the
+relevant Glance image property should be added as well.
+
+References
+==========
+
+* Initial discussion (Juno design summit):
+  https://etherpad.openstack.org/p/nova-hyperv-juno
+
+* Hyper-V Generation 2 VMs
+  http://blogs.technet.com/b/jhoward/archive/2013/11/04/hyper-v-generation-2-virtual-machines-part-7.aspx
diff --git a/specs/juno/proposed/hyper-v-host-power-actions.rst b/specs/juno/proposed/hyper-v-host-power-actions.rst
new file mode 100644
index 0000000..a543e65
--- /dev/null
+++ b/specs/juno/proposed/hyper-v-host-power-actions.rst
@@ -0,0 +1,116 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Hyper-V host power actions
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/hyper-v-host-power-actions
+
+This blueprint introduces host power off and reboot support in the Nova Hyper-V
+driver.
+
+Problem description
+===================
+
+The Nova Hyper-V driver is currently not implementing host power actions as
+specified in the driver interface.
+
+Proposed change
+===============
+
+The actions that can be implemented are limited to power off and reboot.
+Power on can not be implemented as the driver runs on the host itself.
+
+Any running instance will be shut down by hyper-v before shutting down or
+powering off the host.
+
+The feature can be implemented by invoking the "Win32Shutdown" of the
+"Win32_OperatingSystem" WMI class with the appropriate parameter for either
+forced shutdown or forced reboot.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  alexpilotti
+
+Work Items
+----------
+
+* Hyper-V Nova driver feature implementation
+* Unit tests
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+* Unit tests
+* Will be tested in the Hyper-V third party CI
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+* Initial discussion (Juno design summit):
+  https://etherpad.openstack.org/p/nova-hyperv-juno
diff --git a/specs/juno/proposed/hyper-v-remotefx.rst b/specs/juno/proposed/hyper-v-remotefx.rst
new file mode 100644
index 0000000..b8afea4
--- /dev/null
+++ b/specs/juno/proposed/hyper-v-remotefx.rst
@@ -0,0 +1,159 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Hyper-V RemoteFX support
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/hyper-v-remotefx
+
+This blueprint introduces RemoteFX support in the Nova Hyper-V driver.
+
+
+Problem description
+===================
+
+OpenStack VDI support on Hyper-V can greatly benefit from enabling RemoteFX
+in the Nova driver.
+
+RemoteFX allows a guest VM to access the hypervisor's GPU features for 3D
+graphical acceleration.
+
+Guest configuration is performed by specifying the number of virtual desktops
+to be used by the VM and the maximum resolution, with a given set of predefined
+values:
+
+"1024x768"
+"1280x1024"
+"1600x1200"
+"1920x1200"
+"2560x1600"
+
+The hypervisor available resources include the total video memory provided by
+the combined video adapters configured for RemoteFX.
+
+
+Proposed change
+===============
+
+The RemoteFX resources are provided by the hypervisor and need to be handled
+in a way similar to other compute resources. In particular:
+
+* The Nova Hyper-V driver needs to report the amount of available RemoteFX
+  video memory as part of the host capabilities
+* The Nova Hyper-V driver needs to evaluate video memory requirements specified
+  in the flavor extra specs when spawning images
+* The Nova scheduler needs a filter to allocate instances based on video memory
+  flavor requirements. The filter needs also to be taken into account for live
+  migration / cold migration / resize operations.
+
+Example flavor with RemoteFx support:
+
+    nova flavor-key remotefx1 set "hyperv:remotefx=1280x1024,2"
+
+The amount of video memory consumed by an instance based on a given resolution
+and monitor count can be calculated as:
+
+video_memory = 64*horizontal_res*vertical_res*monitor_count
+e.g.: 64*1280*1024*2 = 167772160 bytes
+
+During the configuration of Hyper-V instances, RemoteFX resources can be
+assigned by using the Msvm_Synthetic3DDisplayControllerSettingData WMI class.
+
+On each compute host, the "Remote Desktop Services Virtualization Host" (RDVH)
+role must be enabled on the host and one or more physical GPUs with RemoteFX
+support need to be available.
+
+RDVH can be enabled with:
+
+    Add-WindowsFeature –Name RDS-Virtualization
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  alexpilotti
+
+Work Items
+----------
+
+* Hyper-V Nova driver feature implementation
+* Unit tests
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+* Unit tests
+* Additional Tempest tests can be evaluated
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+* Initial discussion (Juno design summit):
+  https://etherpad.openstack.org/p/nova-hyperv-juno
+
+* Partial implementation submitted during the Havana cycle:
+  https://review.openstack.org/#/c/42529/
diff --git a/specs/juno/proposed/hyper-v-rescue.rst b/specs/juno/proposed/hyper-v-rescue.rst
new file mode 100644
index 0000000..7a564f0
--- /dev/null
+++ b/specs/juno/proposed/hyper-v-rescue.rst
@@ -0,0 +1,119 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================
+Instance rescue support in Hyper-V
+==================================
+
+https://blueprints.launchpad.net/nova/+spec/hyper-v-rescue
+
+This blueprint introduces rescue instance support in the Nova Hyper-V driver.
+
+
+Problem description
+===================
+
+The Hyper-V Nova driver is currently not supporting Nova "rescue" commands,
+unlike other hypervisor drivers (e.g. libvirt).
+
+
+Proposed change
+===============
+
+The Hyper-V Nova driver can be extended to support the "rescue" feature,
+supporting both Linux and Windows images.
+
+Hyper-V uses VHD/VHDX images, not AMI/AKI/ARI. The Nova rescue command will
+result in a new temporary image spawned using the same image as the instance to
+be rescued, attaching the root disk of the original image as secondary local
+disk.
+
+The "unrescue" command will result in the temporary instance being deleted and
+the original instance restarted.
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  alexpilotti
+
+Work Items
+----------
+
+* Hyper-V Nova driver feature implementation
+* Unit tests
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+* Unit tests
+* Will be tested in the Hyper-V third party CI
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+* Initial discussion (Juno design summit):
+  https://etherpad.openstack.org/p/nova-hyperv-juno
diff --git a/specs/juno/proposed/hyper-v-smbfs-volume-support.rst b/specs/juno/proposed/hyper-v-smbfs-volume-support.rst
new file mode 100644
index 0000000..ab487ad
--- /dev/null
+++ b/specs/juno/proposed/hyper-v-smbfs-volume-support.rst
@@ -0,0 +1,146 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================
+Hyper-V: Support for attaching volumes via SMB
+==============================================
+
+https://blueprints.launchpad.net/nova/+spec/hyper-v-smbfs-volume-support
+
+Currently, the Hyper-V driver allows attaching volumes only via iSCSI. The
+purpose of this blueprint is adding support for attaching volumes hosted on
+a SMB share.
+
+Problem description
+===================
+
+For the moment, there are drivers which support distributed file systems such
+as Gluster or NFS as volume backends. SMB is another widely used protocol,
+especially in the Microsoft world.
+
+Its simplicity along with the big improvements that were introduced in SMB 3
+make this type of volume backend a very good alternative.
+
+SMB 3 brings features such as transparent failover, multichanneling using
+multiple NICs, encrypted communication, and RDMA. Note that in order
+to use live migration, SMB 3 is required.
+
+This feature will be backwards compatible, supporting older versions of SMB
+for simple tasks. It will support using any type of SMB share, including:
+
+	- from Scale-Out file servers to basic Windows shares;
+
+	- Linux SMB shares using Samba;
+
+	- vendor specific hardware exporting SMB shares.
+
+Proposed change
+===============
+
+A new volume driver will be added in order to support attaching volumes
+hosted on SMB shares. The Hyper-V driver will be able to choose between the
+volume drivers using the volume type stored in the connection info.
+
+The SMB volume driver will mount the share on which a volume is hosted using
+credentials specified in the volume connection info, then attach the volume
+to a VM using its UNC path. This way, the Hyper-V driver will be able to
+attach vhd or vhdx images hosted on SMB shares.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+The credentials recieved in the volume connection_info will be used in
+order to mount the according SMB share.
+
+Note that the Hyper-V VMMS user account needs access to the remote image file
+in order to be able to attach it to instances. As a best practice, the Cinder
+and Nova nodes should be part of the same AD domain, using AD credentials
+and giving the required permissions to Hyper-V VMMS.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <lpetrut@cloudbasesolutions.com>
+
+Other contributors:
+  <gsamfira@cloudbasesolutions.com>
+
+Work Items
+----------
+
+Add SMB Volume driver.
+
+Adapt Hyper-V Driver in order to be able to use multiple volume drivers
+according to volume types.
+
+Adapt existing volume related operations such as lookups in order to support
+disk resources stored on SMB shares.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+This feature should be tested along with the upcoming SMB Cinder driver.
+
+Documentation Impact
+====================
+
+Using the SMB backend will be documented.
+
+References
+==========
+
+Cinder SMB Driver blueprint:
+https://blueprints.launchpad.net/cinder/+spec/smbfs-driver
+
diff --git a/specs/juno/proposed/idempotentcy-client-token.rst b/specs/juno/proposed/idempotentcy-client-token.rst
new file mode 100644
index 0000000..1562f35
--- /dev/null
+++ b/specs/juno/proposed/idempotentcy-client-token.rst
@@ -0,0 +1,276 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Idempotency for OpenStack API
+==========================================
+
+
+https://blueprints.launchpad.net/nova/+spec/idempotentcy-client-token
+
+When user failed to receive the API response from Nova, it is difficult to
+confirm whether the API request is successfully processed or not.  As a result,
+user may create unnecessary resources by unintentionally sending the same API
+request again and again (retry).
+In order to avoid such scenarios, this blueprint introduces the idempotency
+feature to OpenStack Nova API.
+Thereby, regardless of those unintended API requests made by user retries, user
+can obtain the same result as the initial API request and it prevents the
+creation of unnecessary resource.
+
+
+Problem description
+===================
+
+* Current API feature does not consider the request retry. (Current API
+  feature treats each request retry as a new request.)
+
+    As a brief description, POST requests like "CreateServer" do not contain
+    ID in its request URL.
+    However API response has "Resource ID" generated by Nova.
+    Therefore, if API response failed to receive for some reason, client who
+    has sent the API request does not have any method to know the resource id
+    or whether it is successfully created or not.
+    The user retries it by resending the same API request again and again.
+    In such case, unnecessary resources might be created without
+    the knowledge of the user.
+    Current situation, OpenStack does not have any method to recognize whether
+    the received API request is a result of user retry or totally new
+    API request.
+
+* End user use-case
+
+    Case 1.  Orchestration system execution like Heat
+
+    1.  While the client launches 3 multiple servers using templates,
+        and then the client process is suddenly down due to some reason before
+        it receives the API response from Nova.
+
+    2.  After a while, process started up.
+
+    3.  Now, the client process does not have any method to identify the
+        progress of Nova procedure and Heat side also cannot delete them.
+
+    4.  In this case, the client side interprets its error and retries
+        resource creation.
+
+    5.  As a result, some unnecessary virtual resources remain on Nova,
+        and Heat cannot manage those resources.
+
+    6.  Finally if retry resumed successfully, total 4 or more servers
+        have been created and one or more of them are unnecessary.
+
+    Case 2. Personal execution
+
+    On this case, assume that the end user is a cloud user
+    on commercialized service:
+
+    1.  User requests a server launch by API request, and Nova receives
+        launch request successfully.
+
+    2.  After then, some problem occurs like a network error.
+
+    3.  At this moment, the client could not receive the API response from
+        the Nova, but the launch request has been received successfully.
+
+    4.  When the user retires, Nova creates a new server for each API
+        request that sent by the retry.
+
+    5.  Finally, as a result of the user retry, 2 or more servers are
+        created and one or more of them are unnecessary.
+
+
+Proposed change
+===============
+
+The simplest solution is that client appends a uniquely identifiable "Mark" to
+the API request.
+Therefore, resource can be identified by the created API request.
+Meanwhile, current resource specification depends on API response.
+In other words, those API executions are identifiable and the client could
+trace the errors or check whether the API request has executed
+successfully or not.
+Similar feature has been implemented as "ClientToken" on Amazon EC2.
+
+ClientToken is a functionality that is able to append identifier called
+ClientToken to the API execution.
+If the client could not identify API execution result for some system trouble,
+then the client can use to call same API using same ClientToken.
+The Amazon EC2 cloud side will behave as:
+
+    * If a target API request has NOT been received, then the server executes
+      API and returns the result.
+
+    * If a target API request has been received, then the server skips
+      execution and returns the result.
+
+Therefore, appending of ClientToken makes the response same if the client
+calls duplicate requests(retries).
+Amazon EC2 calls this functionality "Ensuring Idempotency".
+It ensures that the instance creation is always unique when run instance is
+executed with ClientToken.
+
+Using the ClientToken functionality, the previous use case could be resolved:
+
+Resolve 1.
+    When orchestration system like Heat is down, caller side could
+    still keep ClientToken information.
+    It just needs re-request a server launch simply because Nova knows whether
+    launch an instance or skip the request.
+
+Resolve 2.
+    Just re-request a server launch when API response is unknown.
+    Nova recognizes duplicate request and will NOT create unnecessary servers.
+
+The solution using ClientToken provides that simple and safe retry
+method to OpenStack.
+
+* Description
+
+  Append an header tag like 'X-Client-Token: foo'
+
+  ClientToken specifications are required:
+
+  -  64 letters
+  -  ASCII encoding
+  -  Identify characters CAPITAL or not
+  -  ClientToken is unique for each tenant
+  -  Clients must create an unique ClientToken by themselves
+
+Alternatives
+------------
+
+* TaskAPI
+
+  https://blueprints.launchpad.net/nova/+spec/instance-tasks-api
+
+  At the first, I considered to implement this feature to TaskAPI.
+
+  TaskAPI is a feature that it gives id to request(equal to task), and
+  returns this id as response of request.
+  So user can check status of his request later.
+  I intended to implement tag field(user can set this tag value) to TaskAPI,
+  and treat this as this blueprint's ClientToken.
+
+  But to use TaskAPI, it is necessary to set instance_id as required parameter.
+  So, if user fails to get response from Nova, user can't use TaskAPI.
+  I thought that is not appropriate to implement idempotency feature
+  to TaskAPI.
+
+Data model impact
+-----------------
+
+This feature is implemented as decorator function.
+If you want to use idempotency, you may just simply set this decorator
+to any POST API you want to apply.
+
+* To implement this feature, following data is needed to save at least,
+    client_token/project_id/request_url/request_parameter
+
+REST API impact
+---------------
+
+This decorator can be used with v3 POST API.
+
+* For the first time of POST request, Nova returns 202 as result of
+  create server request.
+
+* For the second POST request, if you do same request with same ClientToken,
+  Nova also returns 202.
+
+* For the second POST request, if you do different request with same
+  ClientToken, Nova returns 409.
+  It means that 'The ClientToken is already used.'.
+
+Security impact
+---------------
+
+ClientToken is intended to be unique for each tenant.
+So it is possible that different users belonging to different tenants use
+the same ClientToken.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+Even if client does not specify the request ClientToken or not, user don't
+take any effect from it.
+If you use python-novaclient, 'X-Client-Token' is needed to be implemented.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  haruka tanizawa(h-tanizawa)
+Other contributors:
+  None
+
+Work Items
+----------
+
+1. With below PoC, second request's response is equal to content of GET.
+   In the near term, I re-implement this as POST response.
+2. There needs to be more robust features decorator.
+3. I have implemented to idempotent.py this feature temporarily.
+   It is needed to be consider appropriate file and filepath.
+
+Here is the concrete PoC link：
+https://github.com/ntt-sic/nova/commit/c9bc157b122907d7bd7e98b364137b7ecd47bd0f
+
+
+Dependencies
+============
+One of Heat blueprint depends on this blueprint.
+* Support API retry function with Idempotency in creating/updating a stack
+
+  https://blueprints.launchpad.net/heat/+spec/support-retry-with-idempotency
+
+
+Testing
+=======
+
+In tempest, below tests will be needed.
+
+* Tests that to post requests several times with ClientToken.
+* Valiation tests of client_token/project_id/request_url/request_parameter .
+
+
+Documentation Impact
+====================
+
+There are some documentation impacts.
+
+First, new request header will be added.
+Second, every POST response would be same if request has same ClientToken.
+
+
+References
+==========
+
+Mailing list discussions
+
+- https://lists.launchpad.net/openstack/msg13082.html
+- http://lists.openstack.org/pipermail/openstack-dev/2013-October/017691.html
diff --git a/specs/juno/proposed/image-precacher.rst b/specs/juno/proposed/image-precacher.rst
new file mode 100644
index 0000000..b9ff0a6
--- /dev/null
+++ b/specs/juno/proposed/image-precacher.rst
@@ -0,0 +1,161 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================
+Add a new image preloading service
+====================================================
+
+https://blueprints.launchpad.net/nova/+spec/image-precacher
+
+Introduce a new, optional nova service to pre-cache high priority images on
+compute hosts prior to the image being requested during the normal instance
+creation process.
+
+Problem description
+===================
+
+One of the most common operations in OpenStack is to build servers.  As part
+of builds, hosts need to download images.  These images can be reasonably
+small (under 200MB) to very large (160GB).  The size of the image can
+directly affect how long it takes to build a server.  To improve the
+experience, prepopulating or caching images will allow servers to become
+active more quickly in the common cases.
+
+Also solving the problem where a captive customer has a special image which
+needs quick build times for scaling is desired.  This document looks at
+building the necessary component infrastructure in the compute nodes to help
+solve these types of problems.
+
+In short:
+
+- The first build of a server can be very slow with long image download times.
+- Building many servers at once can potentially throttle the network with many
+  image downlods.
+
+Proposed change
+===============
+
+To rectify the current limitations, we can add a simple nova service to aid in
+efficient distribution of high priority images to compute hosts out of the
+control path of the instance creation process.
+
+The service would consist of a periodic task performing the following tasks:
+
+List
+  Determine a list of images to pre-cache.
+Fetch
+  Obtain a copy of each image's data.
+Serve
+  Setup each image's data for distribution to hosts.
+Distribute
+  Trigger downloads of each image by the hosts.
+
+Alternatives
+------------
+
+- Change nothing - Let image be downloaded and cached on-demand.  The initial
+  build on a host may be slow depedning on image size.  Network throughput may
+  also suffer if many instances are built together.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Could significantly speed up the problem of distributing images to many hosts.
+
+Using the new service combined with BitTorrent could also network bandwidth
+saturation.
+
+Some rate-limiting controls should be provided to work in concert with the
+type of distribution mechanism selected.
+
+Other deployer impact
+---------------------
+
+An extra (optional) nova service to deploy and manage.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+For each of the tasks, there would be a pluggable strategy for performing the
+given task.
+
+- Lister - A module for determining a list of image to pre-populate on compute
+  hosts.  A sensible default might be to query Glance for base images and
+  pre-load those.
+- Fetcher - A module for fetching the selected images from their backing storage.
+  A sensible default might be to fetch image data via Glance.
+- Server - A module for the setup and serving of image data for distribution
+  to hosts.  If BitTorrent is used, this would mean seeding the image data.
+- Distributor - A module to trigger download by the compute hosts.  A sensible
+  default may be to drop a message on the compute RPC topic indicating that an
+  image should be pre-loaded.
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  belliott
+
+Work Items
+----------
+
+- Create new service
+- Create default implementations for each component. (e.g. lister, fetcher,
+  etc)
+
+Dependencies
+============
+
+This blueprint has a dependency on the following to handle compute-side work:
+
+- https://blueprints.launchpad.net/nova/+spec/compute-image-cache
+
+Testing
+=======
+
+- Unit tests
+- devstack integration
+- tempest?
+
+Documentation Impact
+====================
+
+Need to document new service and interactions with nova-compute.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/image-upload-module-plugin.rst b/specs/juno/proposed/image-upload-module-plugin.rst
new file mode 100644
index 0000000..237ea71
--- /dev/null
+++ b/specs/juno/proposed/image-upload-module-plugin.rst
@@ -0,0 +1,183 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================================
+Add plug-in modules for direct uploads to glance locations
+==========================================================
+
+https://blueprints.launchpad.net/nova/+spec/image-upload-module-plugin
+
+To make image/snapshot upload to the glance more efficient by providing
+a plug-in for other protocols to implement.
+
+
+Problem description
+===================
+
+The image needs to download from glance to nova when a VM is launched,
+and the image/snapshot needs to upload from nova to glance when an VM
+image/snapshot is created. By default, HTTP is used to transfer the image
+data, even if it quite huge, even the nova-compute and the glance share
+the same disk, etc. Nova has already implemented a download plug-in
+to allow direct file access to the image in glance and potentially provide
+a chance for other transfer protocols to implement. This proposal will
+introduce an upload plug-in for nova, so that direct file access to the
+VM image/snapshot becomes possible, and other protocols are also possible
+to implement for the image/snapshot upload.
+
+The use cases can be:
+1. If nova-compute and glance share the same disk, it will save a lot
+of time to use the direct access when the image data are huge.
+
+2. If nova-compute does not share the disk with glance and the image
+data are very large, transferring data via other protocols, like ftp,
+bitTorrent, etc, can improve the efficiency. One new protocol may bring in
+a new plug-in by implementing the interface this plug-in provides.
+
+
+Proposed change
+===============
+
+This blueprint will start the implementation for libvirt. When it
+gets mature, we can considering to generalize it for other hypervisors.
+
+* The nova.conf file needs to have a new configuration,
+  like glance_upload_handler, to indicate the transfer protocol.
+  For example, glance_upload_handler=file means to transfer with direct
+  file access.
+
+* How to implement the update method for the glance api(image_service) needs to
+  change. Take the libvirt driver for instance, the image data is saved into
+  image_file, then call image_service.update(context, image_href, metadata,
+  image_file). We can check if a direct file access is allowed. If so,
+  instead of give the image_file, We can add one parameter,
+  like image_path, to this update method. Within this update method, we do not
+  set image_meta['data'] = data, but set image_meta['data'] = None.
+  In this case, the metadata is created, but the image file is missing for
+  glance. To prepare the image for glance, we can call get_location to retrieve
+  the path for this glance image and directly copy the VM image/snapshot into
+  it. If glance does not provide the location for the empty image data, we can
+  upload a dummy image and then replace it with the real image data afterwards.
+  In summary, we separate the HTTP transfer of image data and image metadata
+  into three steps:
+  1. Update the image status with the metadata, telling glance that the image
+  is reserved first and the image data will be uploaded via other protocols.
+  All the image information can be prepared in this step and the status is
+  set to 'saving'.
+  2. Upload the image data via other protocols.
+  3. When the image is ready in the glance store, update the image status with
+  the metadata to set it to 'active'.
+
+
+
+Alternatives
+------------
+
+We can make glance actively "take" the image/snapshot as well. Nova upload the
+image metadata with empty image data, glance receives the VM image path, so it
+can directly "take" the VM image via the specified protocol. However, this
+will bring change to glance and glance needs to be enabled with other transfer
+protocols within the code.
+
+Data model impact
+-----------------
+
+No changes to the current data model. One new package to be loaded with
+stevedore is added.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None for the direct file access. However, for other protocols, the
+authentication needs to be shared with Keystone. Otherwise we need to maintain
+several security mechanism.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+I think the image download and upload will have better performance to bypass
+the HTTP.
+
+Other deployer impact
+---------------------
+
+The deployer can configure the image transfer in the way, which is more
+appropriate besides HTTP.
+
+Developer impact
+----------------
+
+The developer will have other choices rather than to put the data into the
+HTTP request, when one needs to transfer data between glance and nova.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  houshengbo(Vincent Hou)
+
+Other contributors:
+  TBD
+
+Work Items
+----------
+
+
+* Add a new configuration direct_url_upload_schemes for nova.conf.
+
+* Add a new package "upload" nova.image to be loaded with stevedore.
+
+* Implement the direct file access for the image upload.
+
+* Implement the direct upload to swift for the image upload.
+
+* Implement the ftp protocol for the image upload.
+
+* Separate the metadata update and the image upload into two steps.
+
+
+Dependencies
+============
+
+The implementation needs to use glance v2:
+https://blueprints.launchpad.net/nova/+spec/use-glance-v2-api
+
+Testing
+=======
+
+To test the direct file access, direct access to swift and ftp for
+upload.
+
+Documentation Impact
+====================
+
+New documentation need to explain how this upload plug-in is configured.
+
+
+References
+==========
+
+* http://tropicaldevel.wordpress.com/2013/08/26/http-get-outta-here
+
diff --git a/specs/juno/proposed/incremental-instance-snapshot.rst b/specs/juno/proposed/incremental-instance-snapshot.rst
new file mode 100644
index 0000000..89447a1
--- /dev/null
+++ b/specs/juno/proposed/incremental-instance-snapshot.rst
@@ -0,0 +1,221 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Support incremental snapshot
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/incremental-instance-snapshot
+
+Currently nova support create snapshot for instance. For each snapshot, nova
+will take a full-copy of instance's image. The full-copy will waste a lot of
+network bandwidth and storage. This BP propose to support incremental snapshot
+of instance's image.
+
+Problem description
+===================
+
+The full-copy snapshot isn't effcient. It waste more network bandwidth and
+storage. The user need waiting more time for snapshost instance.
+
+
+Proposed change
+===============
+
+Nova REST API
+-------------
+
+Add new paramenter 'incremental' for createImage API. 'incremental' is
+boolean. When 'incremental' is False, nova will take full-copy snapshot.
+When 'incremental' is True, nova will take incremental snapshot. Before
+doing incremental snapshot, the instance at least need a full-copy snapshot
+first.
+
+
+Incremental snapshot image
+--------------------------
+
+Nova will add system_metadata 'last_snapshot_image_ref' to instance that
+indicates the parent image for next incremental snapshot. When doing
+incremental snapshot, nova add property 'parent_image_ref' to snapshot image
+with the value of instance's system_metadata 'last_snapshot_image_ref'.
+
+When booting instance with image, nova can check the image's property
+'parent_image_ref', and then can find out all the parent image to combine them
+as a whole image.
+
+Libvirt incremental snapshot
+----------------------------
+
+This BP propose add incremental snapshot to libvirt driver. And only support
+'qcow2' image first. The instance need to be create with qcow2 image, the
+snapshot image uploaded as qcow2 format too.
+
+Before doing the first time incremental snapshot, the images store in local
+storage as:
+
+base_image->active_image
+
+Then doing external snapshot. Then images looks like as below:
+
+base_image->active_image->snap1
+
+Then the active_image is the incremental snapshot, nova will uploaded it into
+glance with property.
+
+For follow-up incremental snapshot, nova will doing external snapshot first.
+
+base_image->active_image->snap1->snap2
+
+The snap1 is the incremental snapshot image, then upload it into glance image.
+For performance, nova need reduce the image chain, nova will commit snap1
+into active_image, then images chain back to as below:
+
+base_image->active_image->snap2
+
+snap1 become useless, nova will remove it.
+
+Libvirt boot instance with incremental snapshot image
+-----------------------------------------------------
+
+When boot new instance, nova will check the 'parent_image_ref' property for
+image. If image has parent_image_ref, nova will try to download parent image.
+Nova will iterate the image chain, and rebase those images as order. Then
+commit then into one image.
+
+Alternatives
+------------
+
+* Incremental image
+
+Let glance support incremental image. From nova side, nova will only see one
+image, the image is combine by glance. With this way, glance need to implement
+combine image for different image type. This way is more complex.
+
+* Incremental snapshot with dirty map for qemu
+
+This way won't increase the image chain, that's pretty good for performance.
+But it didn't implement in qemu yet. After it implement, it also need waiting
+for libvirt support. So that will be long time.
+https://lists.gnu.org/archive/html/qemu-devel/2013-12/msg00152.html
+
+Data model impact
+-----------------
+
+There isn't any DB model change. This propose utilize system_metadata to
+store last time snapshot. And glance image property to store the parent
+image for incremental snapshot image.
+
+REST API impact
+---------------
+
+This propose add new paramenter 'incremental' to createImage action both for
+nova V2 and V3 REST API.
+
+For V2 API, the new extension 'os-incremental-snapshot'will be added for user
+can detection the new parameter enabled.
+
+For V3 API, the new parameter will be added directly.
+
+The json schema as below::
+
+    create_image = {
+        'type': 'object',
+        'properties': {
+            'create_image': {
+                type: 'object',
+                properties: {
+                    'name': parameter_types.name,
+                    'incremental': parameter_types.boolean,
+                    'metadata': {'type': 'object'}
+                },
+                'required': ['name'],
+                'additionalProperties': False,
+             }
+         }
+         'required': ['create_image'],
+         'additionalProperties': False,
+    }
+
+If 'incremental' didn't specific, then the default value is 'False'.
+
+If 'incremental' is True, but instance haven't 'last_snapshot_image_ref' in
+system_metadata, API will return 400 (BadRequest).
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The incremental snapshot will speed up the snapshot of instance.
+
+Performance Impact
+------------------
+
+After doing incremental snapshot, there will be one more level in image chain.
+That will affect the instance performance. After qemu support incremental
+back-up with dirty-map, the performance will be resolve.
+
+Other deployer impact
+---------------------
+
+The incremental snapshot will reduce the network bandwidth and save more
+storage.
+
+Developer impact
+----------------
+
+The image chain changed after support incremental snapshot. Developer shouldn't
+suppose the image only have one level backing_file.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Alex Xu (xuhj@linux.vnet.ibm.com)
+
+Work Items
+----------
+
+* Add incremental snapshot for libvirt driver
+
+* Add new parameter for REST API
+
+[TODO...]
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+[TODO]
+
+Documentation Impact
+====================
+
+The API paramenter will be document. And doc how to do incremental snapshot in
+admin doc.
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/instance-boot.rst b/specs/juno/proposed/instance-boot.rst
new file mode 100644
index 0000000..9ba239f
--- /dev/null
+++ b/specs/juno/proposed/instance-boot.rst
@@ -0,0 +1,113 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================
+Improve instance boot performance
+=================================
+
+https://blueprints.launchpad.net/nova/+spec/improve-instance-boot-time
+
+This is an umbrella spec that will include various performance enhancements
+for booting a instance.
+
+Problem description
+===================
+
+Current booting an instance has a large number of database accesses. These are
+costly and have a considerable price for large clouds.
+
+Proposed change
+===============
+
+Reduce the amount of database access when booting an instance. This will
+include the following:
+
+ * Pass the instance type to the spawn virt method. This will save reading it
+   from the database
+
+ * Pass config drive information
+
+Alternatives
+------------
+
+None really. Unless we want to bear the brunt of bad instance launch times.
+
+Data model impact
+-----------------
+
+In some cases we will change some virt method signatures, for example adding
+the instance type to the spawn method.
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+This will improve performance. :)
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  garyk
+
+Work Items
+----------
+
+* Pass instance type to the spawn method
+
+* Pass config drive information to the spwan method
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Consider using Rally to show the advantages.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/instance-level-snapshots.rst b/specs/juno/proposed/instance-level-snapshots.rst
new file mode 100644
index 0000000..e0aad0b
--- /dev/null
+++ b/specs/juno/proposed/instance-level-snapshots.rst
@@ -0,0 +1,169 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Instance-Level Snapshots
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/instance-level-snapshots
+
+Moving towards the ability to take a transactional snapshot of all volumes
+attached to an instance.
+
+Problem description
+===================
+
+This blueprint covers adding support for taking a transactional snapshot of all
+volumes attached to an instance.  I/O on the guest is quiesced to achieve
+filesystem consistency for each snapshot in the set.  This gives the user the
+ability to capture the entire disk state of an instance with a single
+operation.  The result can be stored in Glance and used later as a starting
+point for instantiating a new instance or provide a known point for rolling
+back an existing instance.
+
+This will require recent changes in libvirt and the pending artifact
+implementation in Glance.  As these changes will likely not be available in
+Juno, this blueprint covers the initial changes that can be made without
+needing these requirements.
+
+There are currently four ways in Nova to create a snapshot of one, some, or all
+of the volumes attached to a particular instance.  But none of these options
+allow a user to create volume snapshots after I/O is quiesced as a single
+transaction.
+
+Existing Behaviour
+------------------
+
+These are four ways of creating a snapshot in Nova:
+
+1. `create_image` - takes a snapshot of the root volume and may take snapshots
+   of the attached volumes depending on the volume type of the root volume.
+   I/O is not quiesced.
+
+2. `create_backup` - takes a snapshot of the root volume with options to
+   specify how often to repeat and how many previous snapshots to keep around.
+   I/O is not quiesced.
+
+3. `volume_snapshot_create` - takes a snapshot of a single cinder volume.  I/O
+   is not quiesced.
+
+4. `os-assisted-snapshot` - takes a snapshot of a single cinder volume.  The
+   volume is first quiesced before the snapshot is initiated.
+
+Proposed change
+===============
+
+Although libvirt has recently acquired the ability to quiese an instance as
+a first-class operation, this version must be present in the gate in order to
+make use this feature in Nova.  Additionally, the artifact implementation for
+Glance is still under development.  So here I propose that we make as much
+progress towards the end goal as possible without relying on these dependencies
+for Juno.
+
+As a first step, I propose to modify `create_image` to behave consistently
+regardless of the type of root volume.  In its current state, this call will
+create volume snapshots only if the root volume is persistent.  I would like
+this behaviour to be applied to ephemeral root volumes as well.  So the result
+would be a snapshot of the root volume (either in glance or cinder) and volume
+snapshots of all attached cinder volumes.
+
+Alternatives
+------------
+
+In general, I think the `create_image` API call is perhaps being misused in
+this context.  Instead, we could remove the volume-iteration logic so that
+`create_image` becomes simply a function that creates a glance image from an
+instance's root volume.  The extended capability of a transactional snapshot of
+all volumes could be implemented in a new API call.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+For this initial piece, no changes required.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Working with the resulting snapshots is not the most intuitive thing, and given
+the snapshot naming scheme, working with numerous snapshots of a single
+instance can be difficult to manage.  However, the proposed change here does
+not worsen that experience.  And hopefully in next iteratations this can be
+improved.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jbernard
+
+Work Items
+----------
+
+The proposed change should be nicely contained in the api layer and only
+requires modifying the snapshot logic to not discriminate based on root volume
+type.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Similar to the proposed change, the tests will be updated to expect the same
+behaviour from `create_image` regardless of root volume type.  So all attached
+cinder volumes will be snapshotted as a result of the call to `create_image`.
+Otherwise, the feature is not working as expected.
+
+Documentation Impact
+====================
+
+Update `create_image` behaviour when the root volume is ephemeral.
+
+References
+==========
+
+Mailing list discussion:
+
+* http://lists.openstack.org/pipermail/openstack-dev/2014-January/025382.html
+
+Summit discussion:
+
+* https://etherpad.openstack.org/p/juno-nova-multi-volume-snapshots
diff --git a/specs/juno/proposed/instance-tasks-api.rst b/specs/juno/proposed/instance-tasks-api.rst
new file mode 100644
index 0000000..3f97f42
--- /dev/null
+++ b/specs/juno/proposed/instance-tasks-api.rst
@@ -0,0 +1,278 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================
+Add tasks to the API
+====================
+
+https://blueprints.launchpad.net/nova/+spec/instance-tasks-api
+
+In order to be more clear and explicit about what is happening to an instance a
+task resource is being introduced into the API.  This will allow better
+reporting on what is happening during an instance boot or when an action is
+requested on an instance like reboot, rescue, rebuild, etc...
+
+
+Problem description
+===================
+
+One of the challenges in the API up to this point has been signalling a failure
+of an action without affecting the instance itself. For example, if a reboot
+fails, it may leave the instance untouched and usable so it's not appropriate
+to set the vm_state to ERROR.  But without doing so there's no way to indicate
+that the reboot failed.
+
+Another challenge is accounting for what has happened to an instance.  The
+details of who has performed which action on an instance is stored and
+presented by the instance-actions extension but few users know of its
+existence.  This is useful information and should be clearly visible in the
+API.
+
+
+Proposed change
+===============
+
+A POST to ``/v3/servers/`` to create an instance or POSTs to
+``/v3/servers/<uuid>/action`` to perform actions like reboot/rescue/rebuild/etc
+would return a task object as the response body.  In the case of creating > 1
+server with a single request mutiple task objects will be returned, one for
+each server being built.  The API impact will be described more fully in the
+REST API impact section below.
+
+The task model will initially be made up of the following components:
+
+ * uuid:
+ * name: The name of the task i.e. rebooting, rescue, rebuild, pause, etc...
+         These will re-use the currently defined task states.
+ * state: 'active', 'error', 'complete', 'timeout', 'aborted' is a
+          non-exhaustive list of examples.
+ * resource_uuid: uuid of the instance in motion.
+ * request_id: context.request_id
+ * user_id: context.user_id
+ * project_id: context.project_id
+ * created_at: initialized by the database on creation
+ * last_updated: Can be used to periodically update a long running task to show
+                 that it has not stalled.
+
+Task objects will be created in the API method that handles the request, and
+will be sent to the appropriate compute.api methods along with the instance
+that is currently passed there.  They will also be passed via RPC to compute
+nodes so that they can be updated there without needing to pull them from the
+database.  Basically any method that along a code path that updates tasks will
+be passed a task object along with the instance object.
+
+There are some things that are out of scope for this change but should be
+mentioned as they may affect some design decisions or answer questions that may
+come up later.
+
+ * task_state isn't being modified in any way.  This will ensure the v2 API and
+   the current task_state state machine won't be affected.
+
+ * Two desirable use cases are the ability to abort tasks when a delete is
+   initiated and the ability to resume tasks after a service restart.  Future
+   work should allow this so decisions will be made with this idea in mind.
+
+ * Tasks could eventually spawn sub-tasks in order to more granularly represent
+   what is happening withing Nova.  For example the boot process involves
+   downloading an image or connecting a volume, setting up networking, and many
+   other steps.  Some of these may end up being their own task.
+
+Alternatives
+------------
+
+The instance-actions extension was an alternative approach to solving some of
+these problems.  But it is not extensible enough to fully deprecate the
+instance task_state.  It is constrained by only being able to have one
+instance-action per request_id.  And for multiple server create, or future
+sub-tasks that is too limiting.  Tasks is an attempt to achieve what it could
+not.
+
+Since instance-actions are an alternative approach to solving the problems that
+tasks are addressing, they should be deprecated in favor of tasks.  Tasks
+provide the same capabilities that instance-actions do, or can be expanded to
+do so.  Initially the deprecation could be accomplished by not including them
+in API V3.  Further work to remove instance-actions will be somewhat reliant on
+how API V2 is handled once V3 is available.
+
+Data model impact
+-----------------
+
+A new table will be added to store tasks.  The schema will be created such that
+it can store the tasks as described above.  This table will initially be empty
+so a schema migration will be necessary but not a data migration.
+
+The sqlalchemy definition of the table may look like:
+
+    columns = [Column('created_at', DateTime(timezone=False)),
+               Column('updated_at', DateTime(timezone=False)),
+               Column('deleted_at', DateTime(timezone=False)),
+               Column('id', Integer, primary_key=True),
+               Column('uuid', String(36), nullable=False),
+               Column('name', String(255), nullable=False),
+               Column('state', String(255), nullable=False),
+               Column('resource_uuid', String(36), nullable=False),
+               Column('request_id', String(length=255), nullable=False),
+               Column('user_id', String(length=255), nullable=False),
+               Column('project_id', String(length=255), nullable=False),
+               Column('last_updated', DateTime(timezone=False)),
+               Column('deleted', Integer, default=0)]
+
+
+REST API impact
+---------------
+
+The requests for creating an instance or acting on an instance will not change
+but the responses will.  The success response code of 202 will not change and
+error codes will not change, though the messages may change for task related
+failures.
+
+The reponse object returned for requests which return a task will have the
+following structure (jsonschema)::
+
+    task = {
+        'type': 'object',
+        'properties': {
+            'id': { 'type': 'string', 'format': 'uuid' },
+            'name': { 'type': 'string' },
+            'state': { 'type': 'string' },
+            'resource_uuid': {'type': 'string' },
+            'request_id': { 'type': 'string' },
+            'user_id': { 'type': 'string' },
+            'project_id': { 'type': 'string' },
+            'created_at': { 'type': 'string' },
+            'last_updated': { 'type': 'string', 'format': 'date-time' }
+        }
+        'additionalProperties': True
+    }
+
+additionalProperties is True because there may be times that it makes sense to
+return a copy of the server as well and there may be other additions over time.
+
+It's worth mentioning the multiple server create case specifically.  When a
+user requests multiple servers to be created with a single request a separate
+task will track each server.  The response for this case will be a list of
+tasks with the server, or just server_id, within each one.  There is another
+proposal[1] open that gets into more specifics on the response format for
+multiple server create.
+
+
+Additionally there will be two new URLs for requesting task data.  The details
+of the v3 API are slightly unclear at this point, but building on what's in
+place at this moment the new URLs would be
+``http://<host>/v3/servers/<server-uuid>/tasks`` and
+``http://<host>/v3/tasks``.
+
+The tasks URL under servers will allow listing tasks for a server with a GET on
+that URL, or getting details of a particular task with a GET on
+``/servers/<server-uuid>/tasks/<task-uuid>``.
+
+The ``http://<host>/v3/tasks`` URL will allow listing all tasks within a
+project_id, or all tasks as an admin, and then filtering by various criteria.
+The initial plan is to include filtering by task, state, request_id, and
+project_id(admin only).
+
+And it should be noted that the lifetime of a task is expected to match the
+server that it's connected with.  So tasks should be visible until the server
+is deleted at which point the associated tasks will be deleted.  And any db
+pruning should handle tasks the same as other data, like metadata, attached to
+a server.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+python-novaclient will need to adapt to work with tasks that are returned as
+API responses.  This includes displaying tasks in the CLI and returning tasks
+to scripts using python-novaclient as a library.
+
+Performance Impact
+------------------
+
+This change will involve more writes to the database.  The number of writes
+should end up being approximately equal to the number of task_state updates
+that currently occur.
+
+Other deployer impact
+---------------------
+
+Nova supports compute nodes running at a version behind the rest of the
+infrastructure which poses some challenges for this work.  Computes have some
+responsibility for updating tasks but Icehouse computes will not have any code
+to accomplish that.  This means that the full capabilities of tasks will not
+be present until computes are upgraded to Juno.  They will be present in the
+API but will not provide accurate representations of work done on an instance.
+
+Developer impact
+----------------
+
+If a new action is added to the API it will need to create and return a task
+object.  Otherwise there should be no impact.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  alaski
+
+Other contributors:
+  harlowja
+
+Work Items
+----------
+
+ * Create db model and migration for tasks.
+
+ * Create task object.
+
+ * Add task API resources for querying.
+
+ * Create and return tasks with instance POST requests.
+
+ * Identify remaining task_state modifications that could be a server task.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest tests should ensure that tasks are being returned in the proper places
+and that they're ending up in a proper complete or fail state.
+
+
+Documentation Impact
+====================
+
+The REST API additions, as represented by the jsonschema definition above, will
+need to be documented.  The additions are limited to the response from a server
+create or the response to a request to take an action on a server.  Definitive
+examples of the API additions will be available as API samples generated from
+testing.
+
+
+References
+==========
+
+[1] https://review.openstack.org/#/c/91907/
+
+https://etherpad.openstack.org/p/midcycle-instance-tasks
diff --git a/specs/juno/proposed/internal-dns-resolution.rst b/specs/juno/proposed/internal-dns-resolution.rst
new file mode 100644
index 0000000..517b38b
--- /dev/null
+++ b/specs/juno/proposed/internal-dns-resolution.rst
@@ -0,0 +1,297 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================
+Neutron DNS Using Nova Hostname
+===============================
+
+:Author: Carl Baldwin <carl.baldwin@hp.com>
+:Copyright: 2014 Hewlett-Packard Development Company, L.P.
+
+https://blueprints.launchpad.net/nova/+spec/neutron-hostname-dns
+
+Users of an OpenStack cloud would like to look up their instances by name in an
+intuitive way using the Domain Name System (DNS).  They boot an instance using
+Nova and they give that instance an "Instance Name" as it is called in the
+Horizon interface.  That name is used as the hostname from the perspective of
+the operating system running in the instance.  It is reasonable to expect some
+integration of this name with DNS.
+
+Problem description
+===================
+
+Neutron already enables DNS lookup for instances using an internal dnsmasq
+instance.  It generates a generic hostname based on the private IP address
+assigned to the system.  For example, if the instance is booted with
+*10.224.36.4* then the hostname generated is *10-224-36-4.openstacklocal.*
+
+The first problem with this implementation is that the name used in DNS does
+not match what the instance knows as its host name.  This creates problems with
+sudo and other software that expect to be able to look up its own hostname
+using DNS [#]_.
+
+.. [#] https://bugs.launchpad.net/nova/+bug/1175211
+
+The second problem with this implementation is that the end user does not know
+this name without some special knowledge.  The generated name is not presented
+anywhere in the API and therefore cannot be presented in any UI either.
+
+Proposed change
+===============
+
+This blueprint will reconcile the DNS name between Nova and Neutron.  Nova will
+pass the *hostname* to the Neutron API as part of any port create or update
+using a new *hostname* field in the Neutron API.  Neutron DHCP offers will use
+the instance name as hostname.  Neutron DNS will reply to queries for the new
+hostname.
+
+To handle existing installations, Neutron will fall back completely to the
+current behavior in the event that a hostname is not supplied on the port.
+
+Nova will pass its sanitized hostname when it boots using an existing Neutron
+port by updating the port with the hostname field.  An implementation has
+already been started [#]_.  This implementation will need to be augmented in
+two ways:
+
+.. [#] https://review.openstack.org/#/c/36133/
+
+#. Nova will pass the VM name using a new *hostname* field in the port rather
+   than the *name* field on create or update.
+#. Nova will recognize an error from the Neutron API server and retry without
+   *hostname* if it is received.  This error will be returned if Neutron has
+   not been upgraded to handle the hostname field.  This check will be
+   well-documented in the code as a short-term issue and will be deprecated in
+   a following release.  Adding this check will save deployers from having to
+   coordinate deployment of Nova and Neutron.
+#. Neutron will validate the hostname passed to it for dns label validitity and
+   also for uniqueness with the scope of the configured domain name.  If it
+   fails then the port create will fail and the instance boot will fail.
+   Neutron will *only* begin to fail port creations *after* it has been
+   upgraded with the corresponding changes *and* the user has enabled DNS
+   resolution on the network by associating a domain name other than the
+   default openstack.local.  This will avoid breaking existing work-flows that
+   might use unacceptable DNS names.
+
+.. NOTE:: If the user updates the hostname on the Neutron port after the VM has
+   already booted then there will be an inconsistency between the hostname in
+   DNS and the instance hostname.  This blueprint will not do any special
+   handling of this case.  The user should not be managing the hostname through
+   both Nova and Neutron.  I don't see this as a big concern for user
+   experience.
+
+Following is a description of scenarios where the Nova *hostname* may fail
+validation when passed to Neutron.
+
+Host names may not be Valid DNS Names.
+
+    Violations of this type will be rare.
+
+    Nova sanitizes the hostname to conform to rules for hostnames [#]_ [#]_.
+    These rules are close to the rules for DNS labels [#]_.  The main
+    differences are that Nova allows a hostname to begin with a digit and
+    exceed 63 characters.
+
+    In the event that Nova passes an illegal DNS label in creating the port,
+    Neutron API will return an error that should fail the VM create.
+
+.. [#] https://tools.ietf.org/html/rfc952
+.. [#] https://tools.ietf.org/html/rfc1123
+.. [#] http://tools.ietf.org/html/rfc1035
+
+Nova Allows Duplicate Names.
+
+    The Nova API does not enforce that host names are unique.  Adding this
+    enforcement has similar complications to adding validation discussed in the
+    previous section.
+
+    Additionally, Neutron will enforce that hostnames are unique using a case
+    insensitive comparison [#]_.  The Neutron companion to this blueprint
+    discusses this constraint in more detail.
+
+    Neutron will enforce this in port create and port update APIs using a
+    *first-come-first-served* policy.  The database will have a unique
+    constraint on this new name.  If that constraint is violated then Neutron
+    will return an error that should fail the VM create.
+
+.. [#] http://tools.ietf.org/html/rfc4343
+
+Nova Allows Fully Qualified Names
+
+    A FQDN can be passed in as the VM name.  In this case, the Nova sanitized
+    version is still a fully qualified name.  For example, if I pass
+    "vm.example.com" or "vm.example.com." as the name of the VM, the hostname
+    will be set to "vm.example.com".
+
+    In this case, Nova will pass the entire hostname to Neutron.  Neutron may
+    be able to make use of the full name.  More about this is discussed in the
+    Neutron blueprint.  It is sufficient to say that Nova should not worry
+    about truncating this or doing further validation.  If Neutron does return
+    an error then the VM create should fail like with the other errors.
+
+Alternatives
+------------
+
+Move Validation to Nova
+~~~~~~~~~~~~~~~~~~~~~~~
+
+It might be reasonable to modify Nova's hostname sanitization so that it
+produces a valid DNS label.  The problem with doing this would be potential
+disruption to existing workflows.  The differences might be subtle enough that
+this is not a concern.  Nova would not allow hostnames starting with a digit or
+exceeding 63 characters in length.  It would also normalize to lower-case.
+
+Duplicate name detection could be attempted in Nova.  This is a bigger issue.
+I've seen duplicate names in the wild.  Nova likely does not have the
+information necessary to check for duplicate names within the appropriate
+scope.  For example, I would like to check duplicate names per domain across
+networks, this will be difficult for Nova.
+
+Move Port Creation Earlier
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+It may be better if Nova could attempt port creation with Neutron before the
+API operation completes so that the API operation will fail if the port
+creation fails.  In the current design, the Nova API call will succeed and the
+port creation failure will cause the instance to go to an error state.  I
+believe the thing preventing this is the use case where a bare-metal instance
+is being booted.  In that use case, Nova must wait until the instance has been
+scheduled before it can get the mac address of the interface to give to port
+creation.
+
+This change will make for a better user experience in the long run.  However,
+this work is out of the scope of this blueprint and can be done as follow up
+work independently.  One possibility that should be explored is to allow
+updating the Neutron port with the mac address when it is known.
+
+Send Neutron DNS name back to Nova
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+I briefly considered a design where instead of returning an error to Nova,
+Neutron would accept whatever Nova passed as the hostname.  If it failed
+validation then Neutron would fall back to its old behavior and generate a DNS
+name based on the IP address.  This IP address would've been fed back to Nova
+through the existing port status notifications that Neutron already sends back
+to Nova.  It would then be written in to the Nova database so that it can be
+shown to the user.
+
+Feedback from the community told me that this would create a poor use
+experience because the system would be making a decision to ignore user input
+without a good mechanism for communicating that back to the user.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+This will provide a better user experience overall.  With the hostname being
+fed to Neutron, it will be available in the DNS in Neutron and optionally -- in
+the future -- in DNSaaS externally.  This improves the integration of these
+services from the user's point of view.
+
+Performance Impact
+------------------
+
+If the Nova upgrade is deployed before the corresponding Neutron upgrade then
+there will be a period of time where Nova will make two calls in to Neutron for
+every port create.  The first call will fail and then Nova will make a second
+call without the *hostname* field which will be expected to pass like before.
+
+Other deployer impact
+---------------------
+
+This change was carefully designed to allow new Nova and Neutron code to be
+deployed independently.  The new feature will be available when both upgrades
+are complete.
+
+DNS names will only be passed for new instances after this feature is enabled.
+Nova will begin passing hostname to Neutron after an upgrade only for new
+instances.
+
+If Neutron is upgraded before Nova, there is no problem because the hostname
+field is not required and behavior defaults to old behavior.
+
+If Nova is upgraded before Neutron then Nova will see errors from the
+Neutron API when it tries passing the hostname field.  Once again, Nova
+should recognize this error and retry the operation without the hostname.
+
+The deployer should be aware of the `Performance Impact`_ discussed.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  `carl-baldwin <https://launchpad.net/~carl-baldwin>`_
+
+Other contributors:
+  `zack-feldstein <https://launchpad.net/~zack-feldstein>`_
+
+Work Items
+----------
+
+#. Modify existing proposal to pass hostname to use *hostname* field rather
+   than *host*.
+#. Handle expected errors by retrying without hostname set.
+#. Extra credit:  Move port creation up so that the API fails with a bad DNS
+   name.
+
+Dependencies
+============
+
+In order to for this to work end to end, we need coordinated change in Neutron.
+
+https://blueprints.launchpad.net/neutron/+spec/internal-dns-resolution
+
+
+Testing
+=======
+
+Tempest tests should be added or modified for the following use cases
+
+- An instance created using the nova API can be looked up using the instance
+  name.
+
+Documentation Impact
+====================
+
+Mention in the documentation that instance names will be used for DNS.  Be
+clear that it will be the Nova *hostname* that will be used.  Also, detail the
+scenarios where instance creation will fail.
+
+#. It will only fail when DNS has been enabled for the Neutron network by
+   associating a domain other than openstack.local.
+#. An invalid DNS label was given.
+#. Duplicate names were found on the same domain.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/isnot-operator.rst b/specs/juno/proposed/isnot-operator.rst
new file mode 100644
index 0000000..b8b8821
--- /dev/null
+++ b/specs/juno/proposed/isnot-operator.rst
@@ -0,0 +1,118 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================
+IsNot operator for boolean specs
+================================
+
+https://blueprints.launchpad.net/nova/+spec/isnot-operator
+
+This blueprint introduces an new operator in the extra specs filter to have
+an easy way to define a "default" behavior.
+
+Problem description
+===================
+
+Defining a group of hosts in a host aggregate group with a boolean extra spec
+set:
+
+  nova aggregate-create fast-io nova
+  nova aggregate-set-metadata fast-io ssd=true
+  nova aggregate-add-host fast-io node1
+
+Defining a flavor that matches ssd=true will filter (using the extra-spec
+filter) only node1. In order to have a list of all other nodes (without
+ssd=true) it's needed to have a a second group that defined ssh=false.
+
+
+Proposed change
+===============
+
+Add a "isNot" operator to extra_spec filter with that it is possible to check
+if the a certain extra_spec is not set.
+
+
+Alternatives
+------------
+
+Have a "default" value field for specs.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None. The user can specify the new operator within the nova-manage command.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+
+Primary assignee:
+  Marc Koderer <m-koderer>
+
+
+Work Items
+----------
+
+TBD
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+Should be tested with unit test and inside Tempest.
+
+Documentation Impact
+====================
+
+New operator need to get documented.
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/isolate-scheduler-db.rst b/specs/juno/proposed/isolate-scheduler-db.rst
new file mode 100644
index 0000000..e187dd5
--- /dev/null
+++ b/specs/juno/proposed/isolate-scheduler-db.rst
@@ -0,0 +1,282 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================
+Isolate Scheduler Database
+==========================
+
+https://blueprints.launchpad.net/nova/+spec/isolate-scheduler-db
+
+We want to split out nova-scheduler into gantt. To do this, this blueprint is
+the second stage after scheduler-lib split. These two blueprints are
+independent however.
+
+In this blueprint, we need to isolate all accesses to the database that
+Scheduler is doing and refactor code (manager, filters,
+weighters) so that scheduler is only internally accessing scheduler-related
+tables or resources.
+
+
+Problem description
+===================
+
+In order to make decisions, the scheduler needs to know the state of very
+different Nova objects depending on which filter to run, like aggregates or
+instance groups. At the moment, the scheduler is directly reading values from
+DB or conductor while it should not, if we want to do a clean split.
+
+Here we can consider two distinct concerns :
+
+* things that scheduler should know (and persist) which are related to compute
+  nodes should by updated by ResourceTracker using update_available_resource()
+  or when instance_claim() (like aggregates whose host belongs to)
+
+* things the scheduler needs about the wider context of a specific scheduling
+  request need to be passed into select_destinations scheduler lib call, e.g.
+  instance groups
+
+
+Proposed change
+===============
+
+As said, this blueprint will focus on removing all accesses to other Nova
+objects for the Scheduler code. Filters will only either look at request_spec,
+filter_properties and compute_nodes table in order to make decisions.
+
+As of now, we identified so far four external Nova objects that are accessed
+externally by the Scheduler : aggregates (including AZ metadata), instance
+groups, instances and servicegroup API.
+
+Note: This blueprint is *not* targeting to replace calls to other Nova
+libraries such as nova.pci.pci_request or nova.compute.vm_states if
+these libraries only manipulate local dicts that are not persisted to DB or
+calling conductor.
+
+Below is the summary of where each distinct Nova object is identified in
+nova.scheduler namespace :
+
+* Aggregates (and AZs) (calls n.db.aggregate_metadata_get_by_host):
+
+  * AggregateImagePropertiesIsolation,
+  * AggregateInstanceExtraSpecsFilter,
+  * AggregateMultiTenancyIsolation,
+  * AvailabilityZoneFilter,
+  * AggregateCoreFilter (calls n.objects.aggregate.AggregateList.get_by_host)
+  * AggregateRamFilter (calls n.objects.aggregate.AggregateList.get_by_host)
+  * AggregateTypeAffinityFilter (calls
+    n.objects.aggregate.AggregateList.get_by_host)
+
+* Instance Groups:
+
+  * FilterScheduler._setup_instance_group (calls
+    nova.objects.instance_group_obj.InstanceGroup.get_by_hint)
+
+* Compute API for Instances:
+
+  * TypeAffinityFilter
+  * SameHostFilter
+  * DifferentHostFilter
+
+* ServiceGroup API:
+
+  * nova.scheduler.driver.Scheduler.hosts_up()
+  * ComputeFilter
+
+0. For aggregates allocation ratios (AggregateCoreFilter and
+   AggregateRamFilter), it will be covered by
+   bp/allocation-ratio-to-resource-tracker from jaypipes
+
+1. For InstanceGroups, proposal is made to move _setup_instance_group in
+   scheduler.utils and call it by conductor.manager.build_instances.
+
+2. Regarding accesses to aggregates table for other filters (see above) which
+   lookup aggregates of the host to get metadata corresponding to the filter,
+   we propose the following :
+
+  - Compute host reports to Scheduler (thanks to update_resource_stats) a extra
+    set of details (thanks to Extensible Resource Tracker blueprint) like :
+
+    - metadata "filter_tenant_id" from aggregates he is part of
+    - metadata "availability_zone" from aggregates he is part of
+    - all the metadata from aggregates he is part of (for extra specs filter)
+    - etc.
+
+  - Filters will look into HostState (populated thanks to Extensible RT
+    counterside in Scheduler) to access what they need
+
+    ie. the idea is to move the call to Aggregates object made at the moment
+    in the filter into a resource plugin called by Extensible Resource Tracker.
+
+    The current proposal implies that if a resource plugin is disabled for
+    performance reasons, the corresponding filter won't be able to decide.
+    That approach allows to reduce dramatically the number of calls to the
+    necessary ones defined by the operator.
+
+    On a mid-term view, we don't avoid ourselves to think about versioning
+    capabilities for plugins and possibly discovery features either from
+    Scheduler or from Computes to make sure both are talking the same
+    language.
+
+3. For instances, we propose a very close approach :
+
+  - RT.update_available_resource calls Extensible RT plugins, whom one is
+    responsible for reporting either instance UUIDs running on the host (for
+    SameHost and DifferentHost filters) and another one is responsible for
+    reporting all instance flavors on the host
+
+  - filters will look into HostState to know if host runs the instance asked in
+    the hint
+
+4. For ServiceGroups, nothing will be done in this blueprint (considering that
+   it will only be modified in Gantt).
+
+
+In order to provide compatibility in between distinct releases of compute nodes
+and scheduler, the idea is, for each filter, to check HostState if the field
+is there, and if not failback to calling the corresponding Nova object.
+So, Juno scheduler filters will have compatibility filters and upgrade to Kilo
+will see the failback mode removed (because Juno computes will provide stats)
+
+
+Alternatives
+------------
+
+Instead of passing data to the scheduler either thru RT or request property, we
+could let the scheduler place a call to conductor for having these values. That
+said, the problem here is that it increases response time (external call
+instead of direct DB access) and also creates an external dependency with Nova
+objects that would require to be turned into python-novaclient call once Gantt
+forklifted.
+Reporting aggregates capabilities (eg. metadata) can also be designed in
+various different ways, like having the conductor placing a call to aggregates
+and providing it in request_spec, or copying all the aggregate data in
+compute_nodes or moving the whole aggregate concept into the scheduler for its
+solely use, but we think the proposal we made is the most performance-wise.
+
+
+Data model impact
+-----------------
+
+None, as no other data except JSON will be changed in compute_nodes. There is
+no extra information to persist, as we will provide extra required data for
+filters either in request_spec or filter_properties.
+
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None.
+
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Possibly some little increases in performance as the scheduler won't no longer
+access other Nova objects thru conductor but rather will look at
+filter_properties or request_spec that are passed thru RPC payload.
+
+
+Other deployer impact
+---------------------
+
+As said above, it's up to the deployer to synchronize which plugins are
+reporting stats to the scheduler with which filters he wants to run.
+
+Developer impact
+----------------
+
+Ideally:
+
+* Filters should no longer place calls to other bits of code except Scheduler.
+  This will be done by modifying Scheduler component to proxy conductor calls
+  to a Singleton which will refuse anything but scheduler-related objects.
+  See footnote [1] as example. As said above, we will still provide a failback
+  mode for Juno release in order to have compatibility with N-1 release.
+
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  sylvain-bauza
+
+Other contributors:
+  None
+
+
+Work Items
+----------
+
+* Write RT plugins for reporting stats to Scheduler, one per type of stats
+  to report (tenancy metadata, AZ metadata, list of instances, etc.)
+
+* Write Scheduler plugins for adding those stats into HostState
+
+* Modify filters to look at HostState (and failback to Nova object)
+
+* Move _setup_instance_group to scheduler.utils
+
+* Modify scheduler entrypoint to block conductor accesses to other Nova objects
+  (once K release development will be open)
+
+
+Dependencies
+============
+
+* https://blueprints.launchpad.net/nova/+spec/scheduler-lib
+
+* https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
+
+
+Testing
+=======
+
+Covered by existing tempest tests and CIs.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+* Extensible ResourceTracker could avoid DB change for adding 'aggregates' and
+  'instances' field to compute_nodes
+  https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
+  (pmurray)
+
+* Other effort related to RT using objects is not mandatory for this blueprint
+  but both efforts can mutually benefit
+  https://blueprints.launchpad.net/nova/+spec/make-resource-tracker-use-objects
+  (pmurray)
+
+* https://etherpad.openstack.org/p/icehouse-external-scheduler
+
+* http://eavesdrop.openstack.org/meetings/gantt/2014/gantt.2014-03-18-15.00.html
+
+[1] http://git.openstack.org/cgit/openstack/nova/commit/?id=e5cbbcfc6a5fa31565d21e6c0ea260faca3b253d
diff --git a/specs/juno/proposed/keypair-x509-certificates.rst b/specs/juno/proposed/keypair-x509-certificates.rst
new file mode 100644
index 0000000..734df53
--- /dev/null
+++ b/specs/juno/proposed/keypair-x509-certificates.rst
@@ -0,0 +1,182 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================================
+Keypair support for X509 public key certificates
+================================================
+
+https://blueprints.launchpad.net/nova/+spec/keypair-x509-certificates
+
+Introduction of X509 public key certificates keypair support in the Nova API.
+
+
+Problem description
+===================
+
+Nova keypairs are mostly used by Linux guests to handle user authentication via
+SSH public key authentication without incurring in the management and security
+overhead that passwords require.
+
+Public keys are provided to the guests as part of the metadata and included in
+the guest configuration by tools like cloud-init.
+
+Windows operating systems don't support natively SSH and thus authentication
+requires the usage of passwords unless the image deployer chooses to include a
+3rd party unsupported SSH service port, which implies incurring in potential
+security and support issues.
+
+Windows supports natively password-less authentication for WinRM by using X509
+certificates in place of SSH keys, including PowerShell remoting as detailed
+here: http://www.cloudbase.it/windows-without-passwords-in-openstack/
+
+From a practical perspective, X509 certificates are used by WinRM in a way
+which can be considered consistent with the usage of SSH keys on Linux, as
+both are based on public / private keypairs. For simplicity, since WinRM can be
+configured to accept self signed certificates, we will omit the implications of
+certificate chain validation.
+
+
+Proposed change
+===============
+
+While Nova currently supports SSH keypairs only, the API can be extended to
+support x509 certificates, while maintaining full backwards compatibility.
+
+The areas that require changes are:
+
+* Nova API (v2 and v3), to accept the certificate type as an option (defaulting
+  to SSH if not provided)
+* The nova.compute.api.KeypairAPI class to be able to generate and manage X509
+  certificates beside the existing SSH case
+* nova.objects.Keypair class to add a type property
+* Related database migration
+* Nova metadata API to support an additional x509 content in
+  nova.api.metadata.base.InstanceMetadata()
+* python-novaclient to add the optional keypair type option
+
+Usage examples
+
+Generating an x509 keypair via CLI and saving the private certificate locally
+(in PKCS#12 format):
+
+    nova keypair-add keypair1 --keypair-type x509
+
+Importing an x509 public key certificate:
+
+    nova keypair-add keypair1 --keypair-type x509 --pub-key \
+    /path/to/PEM_encoded_public_certificate
+
+The certificate type could be discovered by examining the content without
+requiring the user to provide it explicitly.
+
+For backwards compatibility, omitting the certificate type results in a SSH
+certificate:
+
+    nova keypair-add keypair2 --pub-key /path/to/SSH_pub_key
+
+equivalent to:
+
+    nova keypair-add keypair2 --keypair-type SSH --pub-key /path/to/SSH_pub_key
+
+On the metadata side ssh keys and x509 keys are provided separately for
+backwards compatibility:
+
+"public_keys": {"keypair2": "ssh-rsa AAAAB3...cyJvOQ== Generated by Nova\n"},
+"x509": {"keypair1": "MIIDaTC...ysk8w==\n"},
+
+The -----BEGIN CERTIFICATE----- / -----END CERTIFICATE----- header and footer
+can be omitted
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+nova.objects.Keypair requires an additional type property
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  cbelu
+
+Other contributors:
+  alexpilotti
+
+Work Items
+----------
+
+* Nova feature implementation
+* Unit tests
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit tests. Specific Tempest tests can be added as well.
+
+Documentation Impact
+====================
+
+The Nova driver documentation for nova keypair API and CLI commands usage need
+to be updated.
+
+References
+==========
+
+* Initial discussion (Juno design summit):
+  https://etherpad.openstack.org/p/nova-hyperv-juno
+
+* Windows authentication without passwords in OpenStack
+  http://www.cloudbase.it/windows-without-passwords-in-openstack/
+
+* An introduction to WinRM basics
+  http://blogs.technet.com/b/askperf/archive/2010/09/24/an-introduction-to-winrm-basics.aspx
diff --git a/specs/juno/proposed/libvirt-hugepage.rst b/specs/juno/proposed/libvirt-hugepage.rst
new file mode 100644
index 0000000..c859707
--- /dev/null
+++ b/specs/juno/proposed/libvirt-hugepage.rst
@@ -0,0 +1,134 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Libvirt hugepage backed memory support
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-hugepage
+
+This blueprint gives the cloud deployer the option of providing guests with
+access to the hosts hugepage memory pages instead of the default sized memory
+pages. A config parameter in nova.conf will determine if guests can access the
+hosts default sized memory pages or hugepage memory pages.
+
+Problem description
+===================
+
+User space vhost (usvhost) allows a userspace process to communicate directly
+with a vswitch. In the virtual world this means the qemu process that hosts
+the guest has direct access to the user space vswitch, without the overhead
+of guest traffic passing through the kernel.
+
+To use usvhost with Intel(R) DPDK vSwitch there is a dependency that memory
+used is allocated from hugepages. Currently nova does not configure libvirt
+to provide guests access to the hosts hugepage memory.
+
+The overhead of the host operating systems virtual to physical memory
+translations can be reduced by providing guest access to larger memory
+pages on the hosts system.
+
+Currently the libvirt driver has the ability to enable guests to use hugepage
+backed memory, but nova needs enhancements to add this capability.
+
+Proposed change
+===============
+
+Depending on configuration flag in nova.conf, libvirt will add a
+"memoryBacking" element to the guest xml definition file.
+File: nova/virt/libvirt/config.py
+
+The LibvirtDriver will be modified to become aware of the hugepage memory
+availablility. This information can be used by the scheduler for guest
+placement.
+File: nova/virt/libvirt/driver.py
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Guests that have access to a hosts hugepage memory configuration will see a
+significant reduction in memory access latencies. Particulary for guests with
+memory intensive workloads.
+
+Other deployer impact
+---------------------
+
+To avail of this feature the cloud deployer will need to configure hugepages
+on the node and set the (use_hugepages = True) config flag in nova.conf.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <James Chapman>
+
+Other contributors:
+  <Przemyslaw Czesnowicz>
+
+Work Items
+----------
+
+Define "memoryBacking" element for the guest xml definition file.
+
+Add awareness to the libvirt driver of the hosts hugepage memory availability.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Covered by existing tempest test
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/libvirt-linux-net-refactor-for-freebsd.rst b/specs/juno/proposed/libvirt-linux-net-refactor-for-freebsd.rst
new file mode 100644
index 0000000..3c86326
--- /dev/null
+++ b/specs/juno/proposed/libvirt-linux-net-refactor-for-freebsd.rst
@@ -0,0 +1,197 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================================================
+Refactor libvirt/linux_net integration to be portable to FreeBSD
+================================================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-linux-net-refactor-for-freebsd
+
+The ultimate goal is to get Openstack working on FreeBSD host using
+the libvirt driver. Libvirt on FreeBSD supports Qemu and Bhyve.
+Independently of which driver will be used, it is still required
+to bring in the networking support for nova's libvirt driver.
+
+Problem description
+===================
+
+Currently nova.virt.libvirt.vif imports and uses nova.network.linux_net
+directly. It is not flexible enough if we target non-Linux systems as
+a host, so it should be configurable which network driver will be used.
+
+Proposed change
+===============
+
+The proposed appoach would be:
+
+Currently the usage of linux_net in libvirt.vif looks this way:
+
+ * The following methods are used:
+   - create_tap_dev()
+   - create_ovs_vif_port()
+   - create_ivs_vif_port()
+   - device_exists()
+   - delete_net_dev()
+
+ * Usage of linux_net.LinuxBridgeInterfaceDriver methods:
+   - ensure_vlan_bridge()
+   - ensure_bridge()
+
+
+One could notice this interface is pretty complex and actually
+it is responsible for two things at the same time:
+
+ * Providing a Nova network API logic
+ * Providing helpers for OS-level network device management
+
+In order to make it more portable the proposal is to split out
+the OS-level helpers into its own entity and allow custom
+implementations for specific platform.
+
+For example, it would look this way::
+
+
+        """
+        nova.network.netdev module
+        """
+
+        def get_driver():
+            "Method returning platfrom specific implementation"
+            if our_os == "Linux":
+                return LinuxNetDevDriver
+            else
+                # not implemented
+
+        # network device helpers
+        def create_bridge(brname):
+            return get_driver().create_bridge(brname)
+
+        # other methods go here
+
+        """
+        nova.network.netdev.driver
+        """
+
+        class NetDevDriver(object):
+            """A class that defines an interface for
+            OS-level network device manipulation"""
+
+            def create_bridge(self, brname):
+                raise NotImplementedError
+
+            # other methods go here
+
+
+        """
+        nova.netowrk.netdev.linux
+        """
+
+        class LinuxNetDevDriver(NetDevDriver):
+            """A class that implements NetDevDriver
+            interface for Linux"""
+
+            def create_bridge(self, brname):
+                # Linux impl goes here
+
+            # other methods
+
+
+The plan is:
+
+ - Move out helper functions from linux_net to netdev
+ - Convert consumers of these helper functions from linux_net
+   to use the new netdev helpers
+ - Drop the old implementation of helpers from linux_net
+ - Add netdev.freebsd implementation
+
+Alternatives
+------------
+
+The more straight-forward approach would be to just rename
+linux_net.py to host_net.py and do if linux: elif bsd ...
+check all over the place but it'll be hard to read and hard to
+maintain code.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Deployers will be able to deploy compute on FreeBSD hosts.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  novel
+
+
+Work Items
+----------
+
+ - Make nova test suite pass on FreeBSD
+ - Refactor linux_net to the interface described above
+ - Implement subclasses with the FreeBSD support
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Once the feature is complete, it would be useful to add gate
+tests on FreeBSD.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+Blueprint created based on discussion of this change:
+
+https://review.openstack.org/#/c/85119/
+
diff --git a/specs/juno/proposed/libvirt-multiple-image-backends.rst b/specs/juno/proposed/libvirt-multiple-image-backends.rst
new file mode 100644
index 0000000..dd70799
--- /dev/null
+++ b/specs/juno/proposed/libvirt-multiple-image-backends.rst
@@ -0,0 +1,181 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================================
+support multiple image backends for libvirt driver
+===============================================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-multiple-image-backends
+
+Currently, the cloud administrator can not choose ephemeral storage backend
+device (e.g. SSD or non-SSD) and driver (e.g. qcow2 or ceph) for per flavor.
+The lack of these features restrict the price strategy for different
+performance and functional requirements. So we should add support to
+configure multiple libvirt image backends, and then in the flavor
+indicate which backend should be used for per instance.
+
+Problem description
+===================
+
+1. The administrator have no ability to create flavor with ephemeral
+   storage assigned on SSD as some Amazon EC2 instance types (e.g. m3.large).
+
+2. The administrator have no ability to choose among different libvirt
+   image types for instance disks. To strive for high performance, local disk
+   (e.g. raw/qcow2) may be better. To strive for live migration or single
+   storage backend for glance, nova and cinder, choosing ceph is more 
+   reasonable.
+
+3. The administrator have no ability about more fine grained configuration
+   for ephemeral storage backend, i.e. separately set which backend to be 
+   used for root disk, ephemeral disk and swap disk. For example, there is 
+   the case where the administrator wants some flavours with root disk and
+   swap disk on SSD, and other flavors with only swap disk on SSD.
+
+Proposed change
+===============
+
+1. The <libvirt_image_type> in nova.conf would need to allow a list of image
+   backend names + types, and we use existing config parameters:
+   raw/qcow2-<instances_path>,
+   lvm-<libvirt_images_volume_group>,
+   rbd-<libvirt_images_rbd_pool>.
+
+   these parameters need to be extended to allow a list of values, instead of
+   a single value. For example, if we want to do a choice of local qcow2 and 
+   two rbd pools, one of which is fast ssd backend, we should configure:
+   libvirt_image_type=default:qcow2, fast:qcow2, shared:rbd, sharedfast:rbd
+   instance_path=default:/var/nova/images/hdd, fast:/var/nova/imges/ssd
+   libvirt_images_rbd_pool=shared:main,sharedfast:mainssd
+
+   The names 'default', 'fast', 'shared', 'sharedfast' are set by deployer
+   freely, and would be used to tag ephemeral storage backend in the flavour.
+
+2. In periodic task about update_available_resource, resource tracker retrieve
+   the detail info of each ephemeral storage back-ends. These info include 
+   backend name, free size, used size and total size.
+
+3. Modify the disk_filter, and then based on ephemeral storage backend type and
+   available size, nova-scheduler choose compute node reasonably.
+
+4. Modify the imagebackend, and then based on ephemeral storage backend type,
+   create_image for root disk, ephemeral disk and swap disk, which involves
+   four image type: raw, qcow2, lvm and rbd.
+
+Alternatives
+------------
+
+none
+
+Data model impact
+-----------------
+
+Add a table compute_node_storage_resource, its fields contain:
+compute_node_id, backend_name, total_size, free_size, used_size
+disk_available_least.
+
+
+REST API impact
+---------------
+
+none
+
+Security impact
+---------------
+
+none
+
+Notifications impact
+--------------------
+
+none
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Modify a scheduler filter (i.e. disk filter) and a periodic task
+(i.e. update_available_resource), but not add any new scheduler filters and 
+periodic tasks. So there are little impact on nova. 
+
+
+Other deployer impact
+---------------------
+
+The <libvirt_image_type> in nova.conf allow a list of image backend names
++ types, and we use existing config parameters:
+raw/qcow2-<instances_path>,
+lvm-<libvirt_images_volume_group>,
+rbd-<libvirt_images_rbd_pool>
+
+but these parameters have been extended to allow a list of values, instead of
+a single value. For example, if we want to do a choice of local qcow2 and 
+two rbd pools, one of which is fast ssd backend, we should configure:
+libvirt_image_type=default:qcow2, fast:qcow2, shared:rbd, sharedfast:rbd
+instance_path=default:/var/nova/images/hdd, fast:/var/nova/imges/ssd
+libvirt_images_rbd_pool=shared:main,sharedfast:mainssd
+
+The names 'default', 'fast', 'shared', 'sharedfast' are set by deployer
+freely, and would be used to tag ephemeral storage backend in the flavour.
+
+Developer impact
+----------------
+
+none
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Zhou Yu <vitas.yuzhou@huawei.com>
+
+Work Items
+----------
+
+1. In periodic task about update_available_resource, resource tracker retrieve
+   the detail info of each ephemeral storage back-ends. These info include 
+   backend name, free size, used size and total size.
+
+2. Modify the disk_filter, and then based on ephemeral storage backend type and
+   available size, nova-scheduler choose compute node reasonably.
+
+3. Modify the imagebackend, and then based on ephemeral storage backend type,
+   create_image for root disk, ephemeral disk and swap disk, which involves
+   four image type: raw, qcow2, lvm and rbd.
+
+
+Dependencies
+============
+
+none
+
+Testing
+=======
+
+none
+
+
+Documentation Impact
+====================
+
+1. In the installation and configuration guide, we need to add content
+   about how to configure the parameters of libvirt multiple image
+   backends.
+
+2. In the admin guide, we need to add content about how to specify
+   ephemeral storage backend in flavor.
+
+References
+==========
+
+https://www.mail-archive.com/openstack-dev%40lists.openstack.org/msg22152.html
\ No newline at end of file
diff --git a/specs/juno/proposed/libvirt-ovs-use-usvhost.rst b/specs/juno/proposed/libvirt-ovs-use-usvhost.rst
new file mode 100644
index 0000000..89876bd
--- /dev/null
+++ b/specs/juno/proposed/libvirt-ovs-use-usvhost.rst
@@ -0,0 +1,162 @@
+============================================
+Support dpdkvhost in LibvirtGenericVIFDriver
+============================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-ovs-use-usvhost
+
+Open vSwitch based virtual switches that use Intel(R) DPDK offer dpdkvhost
+mechanism to connect VM's to the switch.
+The existing ovs vif bindings in LibvirtGenericVIFDriver are not compatible
+with this mechanism. We propose to add support for dpdkvhost by creating a new
+vif binding.
+
+Problem description
+===================
+
+Intel(R) DPDK UserSpace vhost is a high-throughput implementation of the
+standard qemu vhost interface for qemu versions <2.0
+
+The Intel(R) DPDK Userspace implemention of the vHost backend uses the CUSE
+kernel module to replace the standard /dev/vhost-net character device
+with a userspace character device to intercept ioctls from QEMU.
+This allows the standard vhost interface to be used
+with userspace only vswitches.
+
+To avoid confusion with other userspace vhost implementations,
+Intel(R) DPDK userspace vhost will be called dpdkvhost
+
+Existing libvirt ovs vif bindings don't support dpdkvhost on
+Intel(R) DPDK enabled virtual switches (ie. Open vSwitch,
+Intel(R) DPDK Accelerated vSwitch). To allow use of dpdkvhost via nova a
+new vif type is required.
+
+Proposed change
+===============
+A new vif_type VIF_TYPE_VHOST_CUSE will be added to support dpdkvhost.
+
+Add {get_config, plug, unplug}_ovs_vhost_cuse methods to
+LibvirtGenericVIFDriver.
+{get_config, plug, unplug}_ovs_vhost_cuse will be called
+when the vif_type is VIF_TYPE_VHOST_CUSE.
+
+The {get_config, plug, unplug}_ovs_vhost_cuse will implement dpdkvhost
+support.
+
+To support dpdkvhost in Nova and Neutron enhancements to
+mech_openvswitch and mechanism_odl will be made to detect if the new vif_type
+should be used.
+
+For full details of the neutron changes see the following blueprint:
+https://blueprints.launchpad.net/neutron/+spec/ml2-use-dpdkvhost
+
+Alternatives
+------------
+1:
+in addtion to the above changes the LibvirtGenericVIFDriver
+can be extended to pass a list of supported vif_types to neutron.
+this removes the need for neutron to determine if the hypervisor supports
+the new vif_type but requires nova to maintian a list of all vif_types
+supprted by each hypervisor.
+
+see the following post for more details:
+http://lists.openstack.org/pipermail/openstack-dev/2014-July/039853.html
+
+2:
+Add {get_config, plug, unplug}_ovs_vhost_cuse methods to
+LibvirtGenericVIFDriver. Those methods would be called from
+{get_config, plug, unplug}_ovs  depending on the value of ovs_use_vhost_cuse
+variable. ovs_use_dpdkvhost will be passed in binding:vif_details by neutron.
+The {get_config, plug, unplug}_ovs_vhost_cuse would implement the dpdkvhost
+support.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Userspace vHost was developed to enable high-throughput workloads without
+modifications to QEMU or the guest VMs.
+
+For performance measurements based on Intel(R) DPDK Accelerated vSwitch
+please refer to:
+https://01.org/sites/default/files/page/intel_dpdk_vswitch_performance_figures_0.10.0_0.pdf
+
+Other deployer impact
+---------------------
+
+To use this feature an Intel(R) DPDK enabled vSwitch is required.
+
+note that supporting side by side deployments of any combination of
+kernel open vswitch, dpdk enabled open vswitch and the intel(R) dpdk vswitch
+on the same compute node is out of scope of the current change.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  przemyslaw-czesnowicz
+
+Secondary assignee:
+  sean-k-mooney
+
+Work Items
+----------
+
+Implement new vif type in nova.virt.libvirt.vif
+
+Dependencies
+============
+
+Neutron:
+https://blueprints.launchpad.net/neutron/+spec/ml2-use-dpdkvhost
+
+Testing
+=======
+
+Unit tests will be added to test added code.
+
+Documentation Impact
+====================
+
+References
+==========
+
+Intel(R) DPDK accelerated vSwitch
+https://github.com/01org/dpdk-ovs
+
+dpdkvhost support in openvswitch.org
+TBA
+
+dpdkvhost description (section 22.0)
+http://www.intel.com/content/dam/www/public/us/en/documents/guides/intel-dpdk-sample-applications-user-guide.pdf
diff --git a/specs/juno/proposed/libvirt-separate-virt-types-to-classes.rst b/specs/juno/proposed/libvirt-separate-virt-types-to-classes.rst
new file mode 100644
index 0000000..3de04ba
--- /dev/null
+++ b/specs/juno/proposed/libvirt-separate-virt-types-to-classes.rst
@@ -0,0 +1,131 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================================
+Libvirt - Separate class for each hypervisor type
+=================================================
+
+https://blueprints.launchpad.net/nova/+spec/
+                                 libvirt-separate-virt-types-to-classes
+
+Separate each hypervisor type functionality to it's own class and refactor
+the base class to support inheritance.
+
+
+Problem description
+===================
+
+Libvirt driver handles several hypervisor types.
+It's code base has grown to the point where it is difficult to see
+how a single change will affect the rest of the hypervisors, without
+introduction side-effects. Recently, such issues rose quite often.
+
+
+Proposed change
+===============
+
+Separating the hypervisor specific functionality to their own classes.
+Introduce a factory class that will deal with the initialization
+of the relevant hypervisor instance, according to the provided
+libvirt.virt_type configuration option. Create a base class that will hold
+the common implementation of each functionality. Introduce hypervisor specific
+tests against each of the classes. This work would also improve the test
+coverage with the tests already in place.
+
+Alternatives
+------------
+
+Continue developing according to the existing model, introducing much more
+tests that would cover all of the supported virt types.
+
+Data model impact
+-----------------
+
+No impact.
+
+REST API impact
+---------------
+
+No impact.
+
+Security impact
+---------------
+
+No impact.
+
+Notifications impact
+--------------------
+
+No impact.
+
+Other end user impact
+---------------------
+
+No impact.
+
+Performance Impact
+------------------
+
+No impact.
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+Developers introducing changes or adding new functionality,
+would have to add it to the relevant virt type class or to a base class
+if it common.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+
+      Vladik Romanovsky
+           <vladik.romanovsky@enovance.com>
+
+
+Work Items
+----------
+
+* Separate virt type specific functionality into helper methods
+  for it to be overridden in the hypervisor specific classes.
+
+* Introduce hypervisor specific classes and consolidate common
+  functionality in the base class. The classes would be:
+  LibvirtDriverBase(driver.ComputeDriver)
+  LibvirtDriverUml(LibvirtDriverBase)
+  LibvirtDriverLxc(LibvirtDriverBase)
+  LibvirtDriverXen(LibvirtDriverBase)
+  LibvirtDriverQemu(LibvirtDriverBase)
+  LibvirtDriverKvm(LibvirtDriverQemu)
+
+* Introduce a factory class that would initialize the relevant
+  hypervisor instance.
+
+Dependencies
+============
+None
+
+Testing
+=======
+Adjust the existing tests to cover the new hypervisor classes.
+Introduce more hypervisor specific tests.
+
+Documentation Impact
+====================
+None
+
+
+References
+==========
+None
diff --git a/specs/juno/proposed/libvirt-smbfs-volume-support.rst b/specs/juno/proposed/libvirt-smbfs-volume-support.rst
new file mode 100644
index 0000000..8c7d2ed
--- /dev/null
+++ b/specs/juno/proposed/libvirt-smbfs-volume-support.rst
@@ -0,0 +1,142 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================
+Libvirt: Support for attaching volumes via SMB
+==============================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-smbfs-volume-support
+
+Currently, there are Libvirt volume drivers that support network-attached
+file systems such as Gluster of NFS. The purpose of this blueprint is adding
+support for attaching volumes hosted on a SMB share.
+
+Problem description
+===================
+
+SMB is another widely used protocol, especially in the Microsoft world. Its
+simplicity along with the big improvements that were introduced in SMB 3
+make this type of volume backend a very good alternative.
+
+SMB 3 brings features such as transparent failover, multichanneling using
+multiple NICs, encrypted communication, and RDMA. Newer versions of Samba
+are getting better support for the SMB 3 features, as well as supporting
+Active Directory membership. For this reason, this driver will support
+scenarios where the node is part of an Active Directory Domain.
+
+This feature will be backwards compatible, supporting older versions of SMB
+for simple tasks. It will support using any type of SMB share, including:
+
+	- from Scale-Out file servers to basic Windows shares;
+
+	- Linux SMB shares using Samba;
+
+	- vendor specific hardware exporting SMB shares.
+
+Proposed change
+===============
+
+A new volume driver will be added in order to support attaching volumes
+hosted on SMB shares. The volume driver will have a similiar worflow with
+the NFS volume driver.
+
+The SMB volume driver will mount the SMB share on which a volume is hosted
+using credentials and other flags specified in the volume connection info.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+The share credentials will be parsed in the volume connection info and used
+when mounting a SMB share.
+
+Also, the driver will support Active Directory integration (as long as the
+Samba version supports it) so that it will be able to use AD credentials.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+The deployer will be able to configure the path where the SMB shares will be
+mounted, as well as setting mount flags.
+
+Also, the Libvirt-qemu uid and gid will have to be specified as mount flags
+in order to support attaching volumes because of Libvirt trying to change
+the owner of the volume.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <lpetrut@cloudbasesolutions.com>
+
+Other contributors:
+  <gsamfira@cloudbasesolutions.com>
+
+Work Items
+----------
+
+Add support for mounting SMB shares.
+
+Provide support for local shares.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+This feature should be tested along with the upcoming SMB Cinder driver.
+
+Documentation Impact
+====================
+
+Using the SMB backend will be documented.
+
+References
+==========
+
+Cinder SMB Driver blueprint:
+https://blueprints.launchpad.net/cinder/+spec/smbfs-driver
+
diff --git a/specs/juno/proposed/libvirt-storpool-volume-attach.rst b/specs/juno/proposed/libvirt-storpool-volume-attach.rst
new file mode 100644
index 0000000..c9f631c
--- /dev/null
+++ b/specs/juno/proposed/libvirt-storpool-volume-attach.rst
@@ -0,0 +1,128 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+StorPool Volume Attachment
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-storpool-volume-attach
+
+There are various Nova volume drivers providing access to Cinder volumes using
+specific types of storage, such as LVM, RBD, etc.  The purpose of this
+blueprint is to add a driver supporting the volumes defined in a StorPool
+cluster.
+
+Problem description
+===================
+
+StorPool is distributed data storage software running on standard x86 servers.
+StorPool aggregates the performance and capacity of all drives into a shared
+pool of storage distributed among the servers.  Within this storage pool the
+user creates thin-provisioned volumes that are exposed to the clients as block
+devices.  StorPool consists of two parts wrapped in one package - a server and
+a client.  The StorPool server allows a hypervisor to act as a storage node,
+while the StorPool client allows a hypervisor node to access the storage pool
+and act as a compute node.  In OpenStack terms the StorPool solution allows
+each hypervisor node to be both a storage and a compute node simultaneously.
+
+Proposed change
+===============
+
+The proposed driver will make use of the StorPool API (based on JSON over HTTP)
+to attach and detach volumes defined in the StorPool cluster and already known
+to Cinder.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+The requests to attach or detach a volume will be passed on to the StorPool
+JSON-over-HTTP API.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Peter Penchev <openstack-dev@storpool.com>
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* Write the nova.virt.libvirt.storpool driver to attach and detach volumes.
+
+* Write tests for the StorPool driver.
+
+* Provide a CI setup for the StorPool driver.
+
+Dependencies
+============
+
+The StorPool driver for Cinder for handling StorPool volumes:
+https://blueprints.launchpad.net/cinder/+spec/storpool-block-driver
+
+Testing
+=======
+
+Since the test setup requires an operational StorPool cluster, the unit tests
+will mostly use mocking to simulate the operations.  A separate continuous
+integration environment will be set up by StorPool and access to it will be
+provided for running automated CI tests.
+
+Documentation Impact
+====================
+
+Using the StorPool driver will be documented.
+
+References
+==========
+
+The StorPool distributed storage software: http://storpool.com/
diff --git a/specs/juno/proposed/libvirt-support-tpm-passthrough.rst b/specs/juno/proposed/libvirt-support-tpm-passthrough.rst
new file mode 100644
index 0000000..3c339d1
--- /dev/null
+++ b/specs/juno/proposed/libvirt-support-tpm-passthrough.rst
@@ -0,0 +1,126 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+libvirt-support-tpm-passthrough
+==========================================
+https://blueprints.launchpad.net/nova/+spec/libvirt-support-tpm-passthrough
+
+The aim of the blueprint is to provide a support for TPM devcie passthrough
+in libvirt.
+The Trusted Platform Module (TPM) is a crypto device that has been built
+into many modern servers, laptops and even handheld devices. Operating Systems
+have been extended with device driver support for the TPM. 
+
+
+Problem description
+===================
+* Host TPM device passthrough
+
+
+Proposed change
+===============
+* Filter the host with TPM device
+* DB table to record TPM device on the hosts and it's usage
+* Report TPM device info when nova-compute start(because TPM device
+  info is stable.)
+* TPM device whitelist in nova.conf(like the PCI passthrough implement)
+* flavor support specify TPM passthrough
+
+
+Alternatives
+------------
+Maybe we can provide a API for user to add available passthrough TPM device
+for a host.I feel it is more convenience and flexible.
+But current other passthrouth(for example PCI passthrough) use the implement
+as described in above "Proposed change".
+
+
+Data model impact
+-----------------
+A table to record TPM device and usage info.
++-----------------+--------------+------+-----+---------+----------------+
+| Field           | Type         | Null | Key | Default | Extra          |
++-----------------+--------------+------+-----+---------+----------------+
+| created_at      | datetime     | YES  |     | NULL    |                |
+| updated_at      | datetime     | YES  |     | NULL    |                |
+| deleted_at      | datetime     | YES  |     | NULL    |                |
+| deleted         | int(11)      | NO   |     | NULL    |                |
+| id              | int(11)      | NO   | PRI | NULL    | auto_increment |
+| compute_node_id | int(11)      | NO   | MUL | NULL    |                |
+| dev_address     | varchar(255) | NO   |     | NULL    |                |
+| instance_uuid   | varchar(36)  | YES  | MUL | NULL    |                |
++-----------------+--------------+------+-----+---------+----------------+
+
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Primary assignee:
+  boh.ricky
+
+Work Items
+----------
+* Filter the host with TPM device
+* DB table to record TPM device on the hosts and it's usage
+* Report TPM device info when nova-compute start(because TPM device
+  info is stable.)
+* TPM device whitelist in nova.conf(like the PCI passthrough implement)
+* flavor support specify TPM passthrough
+
+
+Dependencies
+============
+libvirt(since 1.0.5) Qemu(1.5)
+
+
+Testing
+=======
+Unit test is sufficient.
+
+Documentation Impact
+====================
+Need to add the usage description of TPM passthrough in the document.
+
+
+References
+==========
+TPM in libvirt:
+http://libvirt.org/formatdomain.html#elementsTpm
+
+TPM in qemu:
+http://wiki.qemu.org/Features/TPM
diff --git a/specs/juno/proposed/lock-free-quota-management.rst b/specs/juno/proposed/lock-free-quota-management.rst
new file mode 100644
index 0000000..29ef255
--- /dev/null
+++ b/specs/juno/proposed/lock-free-quota-management.rst
@@ -0,0 +1,218 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================
+A lock-free quota implementation
+================================
+
+https://blueprints.launchpad.net/nova/+spec/lock-free-quota-management
+
+Implement a lock-free quota management algorithm that removes the use of
+the SELECT FOR UPDATE in the database API.
+
+Problem description
+===================
+
+When launching a single instance in Nova, more than 120 database queries may
+be made against the Nova, Neutron, Glance and/or Cinder databases. Of these
+queries, a significant portion of them involve quota management tasks --
+checking for existing quotas, checking for existing project and user usage
+records, claiming the resources used in a reservation, and either committing
+or rolling back those reservations once the launch sequence succeeds or fails.
+
+The `SELECT FOR UPDATE` SQL construct is used in Nova in a couple places, to
+ensure that no two concurrent threads attempt to update the same rows in the
+database. When a thread selects records with `SELECT FOR UPDATE`, the thread
+is announcing that it intends to modify the records it is reading from the
+table -- this is called a write-intent lock. If another thread wants to update
+the same set of records, it will issue a `SELECT FOR UPDATE` call, and this
+call will wait until the first thread has completed the transaction and
+either issued a `COMMIT` or a `ROLLBACK`.
+
+In the case of traditional RDBMS systems like PostgreSQL or MySQL, calls to
+`SELECT FOR UPDATE` are, by nature, a detriment to scalability, since only
+a single thread may hold the write-intent lock on the same rows in the table
+at any given time. All other threads must wait while the single writer thread
+finishes what it is doing. In the case of Nova, the use of `SELECT FOR UPDATE`
+is predominantly in two areas: quota management and assignment of free IP
+addresses in nova-network's IPAM layer.
+
+In addition to `SELECT FOR UPDATE`'s inherent scalability issues, a popular
+replication variant of MySQL, called MySQL Galera, does not support the
+write-intent locks that `SELECT FOR UPDATE` requires. What this means, in
+practice, for deployers of Nova with MySQL Galera, is that occasionally the
+MySQL client will return a deadlock error when two threads simultaneously
+attempt to change the same set of rows. A deadlock in the traditional sense
+of the term does not actually occur, but Galera raises the error code for
+an InnoDB lock wait timeout (deadlock has occurred) when something called a
+certification failure happens. A certification failure happens when two
+threads writing to two different nodes in a Galera cluster attempt to
+`UPDATE` the same set of rows in the same table during the same time interval.
+Instead of causing MySQL to contain inconsistent data (two nodes having a
+different idea of the underlying data), Galera simply causes both threads to
+fail and thus issue a `ROLLBACK` of the containing transaction. This is
+different behavior from standard MySQL, in which a similar situation would
+cause just one of the threads to `ROLLBACK` after receiving a lock wait
+timeout error, and the other thread's `UPDATE` would succeed. The reason that
+this "deadlock" is not actually a deadlock in the Galera case is that the
+entire process happens without any actual waits or timeout loops. Each
+conflicting thread is simply sent an error and that thread issues a
+`ROLLBACK` of the current SQL transaction.
+
+Since MySQL Galera is, by far, the most popular high-availability database
+deployment option currently in the operator ecosystem, some changes are
+required in the quota management code to replace the use of
+`SELECT FOR UPDATE` with a lock-free implementation that suffers neither
+scalability problems nor the Galera-specific quasi-deadlock problems.
+
+Proposed change
+===============
+
+The proposed solution is to borrow a page from obstruction-free and lock-free
+algorithm design and use a "compare and swap" method that allows the thread
+that intends to change the quota usage records for a user or project to issue
+a standard `SELECT` statement for those records, and when that thread goes
+to update those records, it first checks that the state of records is what
+the thread previously knew to exist. The `UPDATE` statement will include a
+`WHERE` condition that will ensure that the rows are *only* updated in the
+table IFF the current row values are what the thread thought they were when
+previously reading the rows with the `SELECT` statement. The thread will
+check the number of rows affected by the `UPDATE` statement. If the number of
+rows affected is 0, then a randomized exponential backoff loop will be hit and
+the process of reading and then `UPDATE` ing with the `WHERE` condition will
+repeat until a pre-defined number of tries has been attempted.
+
+This algoritm is lock-free, in that no record locks of any kind are taken at
+any point in the quota management transactions.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+We will work with the Rally developer team to identify some pre and post
+benchmarks that should demonstrate better concurrency with this lock-free
+implementation under standard MySQL and PostgreSQL deployments.
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+Implementation
+==============
+
+We will implement the lock-free algorithm entirely in the quota_reserve(),
+quota_rollback() and quota_commit() DB API methods.
+
+The use of `with_lockmode('update')` shall be removed from the query object
+construction in the `_get_project_user_quota_usages()` method in
+`nova.db.sqlalchemy.api`. Within `quota_reserve()`, `quota_commit()` and
+`quota_rollback()`, we will change the algorithm from this *simplified*
+pseudo-code for `quota_reserve()`:
+
+.. code-block:: none
+
+    start_transaction:
+
+        usage_records = get_and_lock_usage_records()
+
+        reservations = []
+        for resource, amoount in requested_resource_changes:
+
+            reservation = reservation_record_create(resource, amount)
+            reservations.append(reservation)
+
+    commit_transaction
+    return reservations
+
+to this:
+
+.. code-block:: none
+
+    start_transaction:
+
+        current_usage_records = get_usage_records()
+
+        for resource, amoount in requested_resource_changes:
+
+            while num_attempts < max_attempts:
+                if usage_records_update(resource, amount,
+                                        current_usage_records):
+                    break
+                num_attempts++
+                current_usage_records = get_usage_records()
+
+    commit_transaction
+    return requested_resource_changes
+
+where the `usage_records_update()` method would look like this, again,
+in pseudo-code:
+
+.. code-block:: none
+
+    def usage_records_update(resource, amount, current_records):
+
+        sql = "UPDATE quota_usage SET used = used + amount
+               WHERE resource = $resource
+               AND used = $current_records.used"
+        execute_sql()
+        return num_affected_rows() > 0
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jaypipes
+
+Other assignees:
+  AlexFrolov
+  pkholkin
+
+Work Items
+----------
+
+TODO
+
+Dependencies
+============
+None
+
+Testing
+=======
+None
+
+Documentation Impact
+====================
+None
+
+References
+==========
+None
diff --git a/specs/juno/proposed/log-guidelines.rst b/specs/juno/proposed/log-guidelines.rst
new file mode 100644
index 0000000..79bfad7
--- /dev/null
+++ b/specs/juno/proposed/log-guidelines.rst
@@ -0,0 +1,415 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Logging Guidelines
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/log-guidelines
+
+Problem description
+===================
+
+The current state of logging both within, and between OpenStack
+components is inconsistent to the point of being somewhat harmful by
+obscuring the current state, function, and real cause of errors in an
+OpenStack cloud. In order to make OpenStack clouds possible for non
+super powered enabled humans to debug, which should make this better.
+
+Before we can address this in OpenStack, we first need to come up with
+a set of guidelines that we can get broad agreement on. This is
+expected to happen in waves, and this is the first iteration to gather
+agreement on.
+
+Proposed change
+===============
+
+Definition of Log Levels
+------------------------
+
+http://stackoverflow.com/a/2031209
+This is a nice writeup about when to use each log level. Here is a
+brief description:
+
+- Debug: Shows everything and is likely not suitable for normal
+  production operation due to the sheer size of logs generated
+- Info: Usually indicates successful service start/stop, versions and
+  such non-error related data. This should include largely positive
+  units of work that are accomplished (such as starting a compute,
+  creating a user, deleting a volume, etc.)
+- Audit: REMOVE - (all previous Audit messages should be put as INFO)
+- Warning: Indicates that there might be a systemic issue; potential
+  predictive failure notice
+- Error: An error has occurred and an administrator should research
+  the event
+- Critical: An error has occurred and the system might be unstable;
+  immediately get administrator assistance
+
+We can think of this from an operator perspective the following ways
+(Note: we are not specifying operator policy here, just trying to set
+tone for developers that aren't familiar with how these messages will
+be interpretted):
+
+- Critical : ZOMG! Cluster on FIRE! Call all pagers, wake up
+  everyone. This is an unrecoverable error with a service that has or
+  probably will lead to service death or massive degredation.
+- Error: Serious issue with cloud, administrator should be notified
+  immediately via email/pager. On call people expected to respond.
+- Warning: Something is not right, should get looked into during the
+  next work week. Administrators should be working through eliminating
+  warnings as part of normal work.
+- Info: normal status messages showing measureable units of positive
+  work passing through under normal functioning of the system. Should
+  not be so verbose as to overwhelm real signal with noise. Should not
+  be continuous "I'm alive!" messages.
+
+Proposed Changes From Status Quo
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+- Deprecate and remove AUDIT level
+
+Rationale, AUDIT is confusing, and people use it for entirely the
+wrong purposes. The origin of AUDIT was a NASA specific requirement
+which is not longer really relevant to the current code.
+
+Information that was previously being emitted at AUDIT should instead
+be sent as notifications to a notification queue. *Note: Notification formats
+and frequency are beyond the scope of this spec.*
+
+Open Question: Logger Name Standardization
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+In OpenStack code today, most loggers are created via:
+
+::
+   LOG = logging.getLogger(__name__)
+
+While this is useful for DEBUG levels, it's actually quite confusing
+for INFO and above levels and leads to output like:
+
+::
+   2014-04-25 18:49:21.546 INFO nova.virt.libvirt.firewall
+   [req-87280286-8964-46f3-9e8d-d55d7ecf6ebd
+   ServersAdminTestXML-1606495100 ServersAdminTestXML-1617692500]
+   [instance: 6e51a557-cde5-43e5-9e0f-5321158bee2a] Ensuring static
+   filters
+
+   2014-04-25 18:49:22.217 INFO nova.virt.libvirt.driver
+   [req-b20fe944-28f4-4076-8394-0a8e33554e14
+   ServersAdminTestJSON-492015978 ServersAdminTestJSON-253173963]
+   [instance: b3efe38b-82fd-4798-b36f-b1a0c302a00d] Creating image
+
+   2014-04-25 18:49:22.400 INFO nova.virt.libvirt.driver
+   [req-b20fe944-28f4-4076-8394-0a8e33554e14
+   ServersAdminTestJSON-492015978 ServersAdminTestJSON-253173963]
+   [instance: b3efe38b-82fd-4798-b36f-b1a0c302a00d] Using config drive
+
+   2014-04-25 18:49:22.409 INFO nova.compute.manager
+   [req-c234703f-60ac-4233-8d5b-0ee297288900
+   FixedIPsTestJson-1879416564 FixedIPsTestJson-1390666706] [instance:
+   ae31b4f6-18d2-46b6-94d9-2f116d825db0] Terminating instance
+
+Where the code implementation details on what module is used leak
+through to the operator. This may or may not be considered useful.
+
+This remains an open question about whether python module
+implementations should be in the logs vs. service names
+(i.e. nova-compute, nova-sched, nova-os-api, nova-ec2-api).
+
+
+Overall Logging Rules
+---------------------
+The following principles should apply to all messages
+
+Log messages should be a unit of work
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+OpenStack services are typically multiworker so the immediate lines
+above and below a log message may not be related at all to the message
+that was printed. As such all messages should be a complete unit of
+work with enough context to know what happened with just that message.
+
+DEBUG messages may be more trace oriented, however they still need to
+provide enough context that their operation can be determined from a
+single log message. In a real environment there may be tens, hundreds,
+or even thousands of workers all processing content at the same time
+to the same log stores, and without fully contained message context
+piecing the flows back together may be very difficult.
+
+**Good**
+
+::
+   2014-01-26 15:36:10.597 28297 INFO nova.virt.libvirt.driver [-]
+   [instance: b1b8e5c7-12f0-4092-84f6-297fe7642070] Instance spawned
+   successfully.
+
+   2014-01-26 15:36:14.307 28297 INFO nova.virt.libvirt.driver [-]
+   [instance: b1b8e5c7-12f0-4092-84f6-297fe7642070] Instance destroyed
+   successfully.
+
+**Bad**
+
+::
+   2014-01-26 15:36:11.198 INFO nova.virt.libvirt.driver
+   [req-ded67509-1e5d-4fb2-a0e2-92932bba9271
+   FixedIPsNegativeTestXml-1426989627 FixedIPsNegativeTestXml-38506689]
+   [instance: fd027464-6e15-4f5d-8b1f-c389bdb8772a] Creating image
+
+   2014-01-26 15:36:11.525 INFO nova.virt.libvirt.driver
+   [req-ded67509-1e5d-4fb2-a0e2-92932bba9271
+   FixedIPsNegativeTestXml-1426989627 FixedIPsNegativeTestXml-38506689]
+   [instance: fd027464-6e15-4f5d-8b1f-c389bdb8772a] Using config drive
+
+   2014-01-26 15:36:12.326 AUDIT nova.compute.manager
+   [req-714315e2-6318-4005-8f8f-05d7796ff45d FixedIPsTestXml-911165017
+   FixedIPsTestXml-1315774890] [instance:
+   b1b8e5c7-12f0-4092-84f6-297fe7642070] Terminating instance
+
+   2014-01-26 15:36:12.570 INFO nova.virt.libvirt.driver
+   [req-ded67509-1e5d-4fb2-a0e2-92932bba9271
+   FixedIPsNegativeTestXml-1426989627 FixedIPsNegativeTestXml-38506689]
+   [instance: fd027464-6e15-4f5d-8b1f-c389bdb8772a] Creating config
+   drive at
+   /opt/stack/data/nova/instances/fd027464-6e15-4f5d-8b1f
+   -c389bdb8772a/disk.config
+
+This is mostly an overshare issue. At Info these are stages that don't
+really need to be fully communicated.
+
+Messages shouldn't need a secret decoder ring
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+**Bad**
+
+::
+   2014-01-26 15:36:14.256 28297 INFO nova.compute.manager [-]
+   Lifecycle event 1 on VM b1b8e5c7-12f0-4092-84f6-297fe7642070
+
+General rule, when using constants or enums ensure they are translated
+back to user strings prior to being sent to the user.
+
+Specific Event Types
+--------------------
+
+Inbound WSGI requests
+~~~~~~~~~~~~~~~~~~~~~
+
+Should be:
+
+- Logged at **INFO** level
+- Logged exactly once per request
+- Include enough information to know what the request was
+
+Operator Deprecation Warnings
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Should be:
+
+- Logged at **WARN** level
+- Logged exactly once per service start (not on every request through
+  code)
+- Include directions on what to do to migrate from the deprecated
+  state
+
+REST API Deprecation Warnings
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+Should be:
+
+- **Not** logged any higher than DEBUG (these are not operator facing
+  messages)
+- Logged no more than once per REST API usage / tenant. Definitely
+  not on *every* REST API call.
+
+Stacktraces in Logs
+~~~~~~~~~~~~~~~~~~~
+Stacktraces in logs should be an exceptional event for a completely
+unforeseeable circumstance that is not yet recoverable by the system.
+
+As such during normal behavior there should be no stack traces in the
+system. Any that arise should be filed as high priority bugs and be
+fixed as soon as possible.
+
+Logging by non-OpenStack Components
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+OpenStack uses a ton of libraries, which have their own definitions of
+logging. This causes a lot of extraneous information in normal logs by
+wildly different definitions of those libraries.
+
+As such, all 3rd party libraries should have their logging levels
+adjusted so only real errors are logged.
+
+Currently proposed settings for 3rd party libraries:
+
+- amqp=WARN
+- boto=WARN
+- qpid=WARN
+- sqlalchemy=WARN
+- suds=INFO
+- iso8601=WARN
+- requests.packages.urllib3.connectionpool=WARN
+
+Security Guidelines
+~~~~~~~~~~~~~~~~~~~
+Logs should have a format that enables grouping of confidential data
+especially when logging data such as:
+
+- Passwords: Never log plain text passwords
+- Private Keys: Never log plain text private keys
+
+If the log message will also be used in an end user response (via a
+REST response of some sort, or logged in a database where the end user
+will have access to it), then additional measures should be taken,
+such as:
+
+- Exceptions: Unless the developer is sure that an exception will
+  never contain confidential information, exceptions should be
+  identified as confidential. This has historically been especially
+  problematic with database exceptions which may contain real field
+  data.
+- Recommend parsing the specific exception or error and providing an
+  abstracted/safe version back to the user
+- PII: Minimize Personally Identifiable Information (PII) logging
+  where possible
+- Tenant/Project ID Checking: If a user identifier (tenant/project ID)
+  is not present in the log record or does not match the current
+  authenticated user, do not show this log data to the user
+- Log Insecure Configurations: If a configuration option causes the
+  system to enter a potentially less secure state, log a message to
+  this effect for operators to see
+
+OpenStack's Oslo Log is capable of creating formatted logs with a
+section for confidential data. The following example contains two
+pieces of variable data: key_name which is not confidential data (and
+will equal 'ssh') and key_value which is a confidential key that
+should not be visible to anyone but admins/operators.
+
+Note: This is a contrived example for simplicity. If the key_value is
+a public ssh key, it probably isn't critical to hide it in the logs
+from the authorized user that it belongs to. If the key_value is a
+private ssh key, it shouldn't be logged to begin with.
+
+Bad Example:
+
+::
+   LOG.debug("User set %s key to value %s" % [key_name, key_value])
+
+Revised/Good Example:
+
+::
+   LOG.debug("User set %s key" % [key_name],
+   extra={private={value=key_value}})
+
+
+Note that the extra->private structure is used to hold all
+confidential data within logs so that it may be filtered out later
+before a user views logs. In this example, the key value is moved to a
+'private' dictionary which makes filtering out confidential data from
+logs easier as there will be a single keyword to locate in log entries
+if these guidelines are followed. An authenticated user may see that
+an ssh key has been changed but an operator may see the actual ssh key
+value in the logs.
+
+Alternatives
+------------
+
+Continue to have terribly confusing logs
+
+Data model impact
+-----------------
+
+NA
+
+REST API impact
+---------------
+
+NA
+
+Security impact
+---------------
+
+NA
+
+Notifications impact
+--------------------
+
+NA
+
+Other end user impact
+---------------------
+
+NA
+
+Performance Impact
+------------------
+
+NA
+
+Other deployer impact
+---------------------
+
+Should provide a much more standard way to determine what's going on
+in the system.
+
+Developer impact
+----------------
+
+Developers will need to be cognizant of these guidelines in creating
+new code or reviewing code.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Assignee is for moving these guidelines through the review process to
+something that we all agree on. The expectation is that these become
+review criteria that we can reference and are implemented by a large
+number of people. Once approved, will also drive collecting volunteers
+to help fix in multiple projects.
+
+Primary assignee:
+  Sean Dague <sean@dague.net>
+
+Work Items
+----------
+Using this section to highlight things we need to decide that aren't
+settled as of yet.
+
+Proposed changes with general consensus
+
+- Drop AUDIT log level, move AUDIT message to INFO
+- Begin adjusting log levels within projects to match the severity
+  guidelines.
+
+Proposed changes *without* general consensus
+
+- Logger naming. There just wasn't much feedback yet.
+
+
+Dependencies
+============
+
+NA
+
+Testing
+=======
+
+See tests provided by
+https://blueprints.launchpad.net/nova/+spec/clean-logs
+
+Documentation Impact
+====================
+
+Once agreed upon this should form a more permanent document on logging
+specifications.
+
+References
+==========
+
+- Security Log Guidelines -
+  https://wiki.openstack.org/wiki/Security/Guidelines/logging_guidelines
+- Wiki page for basic logging standards proposal developed early in
+  Icehouse - https://wiki.openstack.org/wiki/LoggingStandards
diff --git a/specs/juno/proposed/log-translation-hints.rst b/specs/juno/proposed/log-translation-hints.rst
new file mode 100644
index 0000000..40cdd2b
--- /dev/null
+++ b/specs/juno/proposed/log-translation-hints.rst
@@ -0,0 +1,192 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================
+Add log translation hints for Nova
+==================================
+
+https://blueprints.launchpad.net/Nova/+spec/log-translation-hints
+
+To update Nova log messages to take advantage of oslo's new feature of
+supporting translating log messages using different translation domains.
+
+Problem description
+===================
+
+Current oslo libraries support translating log messages using different
+translation domains and oslo would like to see hints in all of our code
+by the end of juno. So Nova should handle the changes out over the release.
+
+Proposed change
+===============
+
+Since there are too many files need to change, so divide this bp into dozens of
+patche according to Nova directories(which need applying this change).
+
+For each directory's files, we change all the log messages as follows.
+
+1. Change "LOG.exception(_(" to "LOG.exception(_LE".
+
+2. Change "LOG.warning(_(" to "LOG.warning(_LW(".
+
+3. Change "LOG.info(_(" to "LOG.info(_LI(".
+
+4. Change "LOG.critical(_(" to "LOG.info(_LC(".
+
+Note that this spec and associated blueprint are not to address the problem of
+removing translation of debug msgs.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  liusheng<liusheng@huawei.com>
+
+Other contributors:
+  shihanzhang<shihanzhang@huawei.com>
+
+Work Items
+----------
+
+For each directory's files, we change all the log messages as follows.
+
+1. Change "LOG.exception(_(" to "LOG.exception(_LE".
+
+2. Change "LOG.warning(_(" to "LOG.warning(_LW(".
+
+3. Change "LOG.info(_(" to "LOG.info(_LI(".
+
+4. Change "LOG.critical(_(" to "LOG.info(_LC(".
+
+We handle these changes in the following order::
+
+    ├── nova                  #TODO1
+    │   ├── api              #TODO2
+    │   ├── CA
+    │   ├── cells            #TODO3
+    │   ├── cert
+    │   ├── cloudpipe
+    │   ├── cmd              #TODO4
+    │   ├── compute          #TODO5
+    │   ├── conductor        #TODO6
+    │   ├── console
+    │   ├── consoleauth
+    │   ├── db               #TODO7
+    │   ├── hacking
+    │   ├── image            #TODO8
+    │   ├── ipv6
+    │   ├── keymgr
+    │   ├── locale
+    │   ├── network          #TODO9
+    │   ├── objects          #TODO10
+    │   ├── objectstore
+    │   ├── openstack
+    │   ├── pci
+    │   ├── rdp
+    │   ├── scheduler        #TODO11
+    │   ├── servicegroup
+    │   ├── spice
+    │   ├── storage
+    │   ├── tests
+    │   ├── virt
+    │   │   ├── baremetal   #TODO12
+    │   │   ├── disk        #TODO13
+    │   │   ├── hyperv      #TODO14
+    │   │   ├── libvirt     #TODO15
+    │   │   ├── vmwareapi   #TODO16
+    │   │   ├── xenapi      #TODO17
+    │   │.........             #TODO18
+    │   ├── vnc
+    │   ├── volume
+    │......(others1)            #TODO19
+    │......(others2)            #TODO20
+
+Add a HACKING check rule to ensure that log messages to relative domain.
+Using regular expression to check whether log messages with relative _L*
+function.
+
+::
+
+    log_translation_domain_error = re.compile(
+        r"(.)*LOG\.error\(\s*\_LE('|\")")
+    log_translation_domain_warning = re.compile(
+        r"(.)*LOG\.(warning|warn)\(\s*\_LW('|\")")
+    log_translation_domain_info = re.compile(
+        r"(.)*LOG\.(info)\(\s*\_LI('|\")")
+    log_translation_domain_critical = re.compile(
+        r"(.)*LOG\.(critical)\(\s*\_LC('|\")")
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+[1]https://blueprints.launchpad.net/oslo/+spec/log-messages-translation-domain-rollout
+
+[3]https://wiki.openstack.org/wiki/LoggingStandards
diff --git a/specs/juno/proposed/lvm-driver-for-shared-storage.rst b/specs/juno/proposed/lvm-driver-for-shared-storage.rst
new file mode 100644
index 0000000..a36df37
--- /dev/null
+++ b/specs/juno/proposed/lvm-driver-for-shared-storage.rst
@@ -0,0 +1,197 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Support LVM on a shared storage volume
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/lvm-driver-for-shared-storage
+
+Current LVM cinder driver only supports iSCSI topology and a guest instance
+on nova compute is connected to a cinder volume(logical volume) on a
+volume-group via iSCSI target.
+
+This proposal add a feature to connect cinder volume to an instance directly
+using LVM on a shared storage volume.
+
+As a result, an instance which uses cinder volume can directly issue I/O to
+volumes via FC and this provides better I/O performance to a instance.
+
+In this feature, both cinder and nova pieces are necessary, and blueprint
+for cinder piece was already approved.
+
+https://blueprints.launchpad.net/cinder/+spec/lvm-driver-for-shared-storage
+
+
+Problem description
+===================
+
+Here is a use case for this proposal.
+
+Conventionally, operations to an enterprise storage such as volume creation,
+deletion, snapshot, etc are only permitted system administrator and they
+handle these operations after carefully examining.
+
+In OpenStack cloud environment, workloads of storages have been increasing
+and it is difficult to manage the workloads because every user have a
+permission to execute storage operations via cinder.
+
+In order to use expensive storage more efficiently, I think it is better to
+reduce hardware based storage workload by offloading the workload to software
+based volume operation on a case by case basis.
+
+If we have two drivers in regards to a storage, we can provide volumes
+both way as the situation demands. For example,
+
+- As for "Standard" type storage, use proposed software based LVM
+  cinder driver.
+- As for "High performance" type storage, use hardware based cinder driver.
+  (Ex. Higher charge than "Standard" volume)
+
+For more detail of this proposal such as benefits, use-cases, please refer
+URLs in section "Reference".
+
+Proposed change
+===============
+
+Introduce LibvirtSharedLvmDriver class to virt.libvirt.volume and add the
+class to list of volume_drivers to handle in case of "lvm".
+
+In this class, connect_volume() and disconnect_volume() will be implemented
+to handle LVM on a shared storage volume.
+
+- connect_volume()
+
+  When a storage volume is shared to multiple nodes, volume activation to
+  created logical volume(LV) is necessary before attaching a created volume
+  to an instance.
+  After volume creation at cinder node, only cinder node knows the volume and
+  other compute nodes do not know about the created LV. Therefore, these two
+  steps are necessary to attach the created volume.
+
+  - Update LV list and status using "lvs" at a compute node.
+  - Activate LV using "lvchange -ay" is required at a compute node.
+
+  After volume activation, access device path /dev/VG1/LV1 is created at
+  compute node. Libvirt and qemu can handle device path and attach a volume
+  using this.
+
+- disconnect_volume()
+  After detaching a cinder volume from an instance, the compute node does
+  not have to access a cinder volume. In order to keep a consistency of
+  cinder volume, volume deactivation using "lvchange -an" is required at a
+  compute node. This operation will remove access device path /dev/VG1/LV1
+  so that the compute node can't access to the volume using the device
+  path until the volume will be attached to an instance again.
+  In this method, underlying connection to LVM on shared storage volume
+  is continuously maintained.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+The user attaches and detaches cinder volumes which are created from
+LVM on a shared storage volume in the same way that any user does.
+
+Performance Impact
+------------------
+
+There is performance impact of IOPs by using LVM compared to using
+a single underlying volume but the impact is subtle.
+
+Other deployer impact
+---------------------
+
+To apply this feature to OpenStack environment, storage admin needs to
+prepare a storage volume which is shared between cinder node and compute
+nodes and also needs to create a volume group on top of the storage volume.
+
+And then, storage admin needs to configure cinder.conf to use sharedLVM
+driver and the created storage volume as a cinder backend-storage.
+
+Here is a sample of cinder.conf::
+
+ enabled_backends=SharedLVM
+ [SharedLVM]
+ volume_group=cinder-shared-volume
+ volume_driver=cinder.volume.drivers.lvm.SharedLVMDriver
+ volume_backend_name=SharedLVM
+
+Configuration of nova driver is not required in this feature.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+   mitsuhiro-tanino
+
+Work Items
+----------
+
+None.
+
+
+Dependencies
+============
+
+This feature depends on following cinder blueprint.
+ https://blueprints.launchpad.net/cinder/+spec/lvm-driver-for-shared-storage
+
+Testing
+=======
+
+Unit tests will be added in nova.
+
+Documentation Impact
+====================
+
+Cinder driver documentation will be added.
+There is no impact to nova documentation.
+
+References
+==========
+
+Blueprints:
+ https://blueprints.launchpad.net/nova/+spec/lvm-driver-for-shared-storage
+ https://blueprints.launchpad.net/cinder/+spec/lvm-driver-for-shared-storage
+
+Wiki:
+ https://wiki.openstack.org/wiki/Cinder/NewLVMbasedDriverForSharedStorageInCinder
+
+openstack-dev discussion:
+ [openstack-dev] [Cinder] Support LVM on a shared LU
diff --git a/specs/juno/proposed/message-in-update-notifications.rst b/specs/juno/proposed/message-in-update-notifications.rst
new file mode 100644
index 0000000..d2a7776
--- /dev/null
+++ b/specs/juno/proposed/message-in-update-notifications.rst
@@ -0,0 +1,120 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================
+Add a message field to instance update notifs
+====================================================
+
+https://blueprints.launchpad.net/nova/+spec/message-in-update-notifications
+
+Add a string field, 'message', to compute instance update notifications.
+This 'message' field will make it easier for external notification
+consumers to deduce the purpose of the update.
+
+Problem description
+===================
+
+Nova emits a number of instance update notifications for various state changes
+and VM spawning steps.  It can be difficult to reconstruct the purpose of the
+update from the message payload.
+
+The xenapi driver, for example, emits update notifications for many instance
+spawning sub-steps.  These do not always modify the vm or task state so a
+message describing the purpose of the notification. (e.g. devices_attached)
+can help with tasks like writing a notification consumer that is able to
+report on the performance of these steps.
+
+Proposed change
+===============
+
+Allow for extra fields in the compute.instance.update payload, including
+'message'.  The message will be a description of the type of update being
+generated.  Possible values for the field will be enumerated in
+nova.compute.update_messages.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+Additional field(s) in payload
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Minimal
+
+Other deployer impact
+---------------------
+
+Notification consumers must be able to cope with extra payload fields.
+
+A boolean flag will be added to toggle the use of these extra payload fields.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  belliott
+
+Work Items
+----------
+
+* Define possible message values
+* Add extra fields to notification. (with boolean toggle flag)
+* Update virtapi to take extra_notification_info
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit tests
+
+Documentation Impact
+====================
+
+* 'message' field added to compute.instance.update notif
+* config flag to disable extra payload fields
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/metadata-service-callbacks.rst b/specs/juno/proposed/metadata-service-callbacks.rst
new file mode 100644
index 0000000..0ae1cb8
--- /dev/null
+++ b/specs/juno/proposed/metadata-service-callbacks.rst
@@ -0,0 +1,219 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================
+API: Metadata Service Callbacks
+===============================
+
+https://blueprints.launchpad.net/nova/+spec/metadata-service-callbacks
+In this blueprint we aim to extend the metadata service API to allow guest
+instances to set (but not overwrite) server metadata. An example use of this
+would be for cloud-init to report to the outside world that server-setup has
+been completed.
+
+Problem description
+===================
+
+Currently in-guest services cannot communicate externally without depending on
+hypervisor specific implementations. Cloud-init has a mechanism for
+communicating encrypted passwords externally using its own metadata API. The
+encrypted password is stored in instance_system_metadata. An API to set server
+metadata will allow a similar channel of communication for in-guest agents
+but in a more generic fashion.
+
+Use case 1:
+* add a script in cloud-init to report back to the metadata service when
+cloud-init has completed.
+* user can poll server metadata to see when their server setup is complete.
+* In the case of a rebuild all the keys within the namespace will be dropped in
+order to enable cloud-init to report completion when the server comes back up
+again.
+
+Proposed change
+===============
+
+Add an API to allow guest instances to set server metadata.
+We already allow posts to set-password, this extends it to a more general
+mechanism.
+
+To ensure it is safe and not for high amounts of traffic:
+* add a namespace to all keys from\_server\_
+* only set metadata keys that are not already set
+i.e. it is a single write only API
+* only set metadata if it is within the quota of the user.
+
+Alternatives
+------------
+
+We could give access to the nova-api directly, and give the server some kind
+of token that allows it to set metadata for just its own server. However, with
+private networking, servers don't always have access to the nova-api, but do
+always have access to the metadata service.
+
+We could store the metadata without a namespace, and allow it to overwrite
+existing metadata, but this seemed to encourage using the metadata service as
+a high bandwidth interface (which would be a bad idea), and increases the
+attach surface if the wrong person can post to the metadata service.
+
+Users could use something outside of Nova
+
+We could add new APIs for each kind of new interaction like "setup-complete"
+and "client-agent-response", etc.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+API for updating metadata
+
+POST: http://169.254.169.254/openstack/2014-10-17/meta_data
+
+Normal http response code for a request containing only new keys: 200
+Normal http response code for a request containing new and existing keys:400
+Response code for a request containing only existing keys:400
+Response code if operation exceeds quota: 413
+
+Request parameters:
+* meta: a json dictionary of the new metadata keys and values
+
+JSON Request::
+
+    {
+        "meta": {
+             "key_1": "value1",
+             "key_2": "value2",
+        }
+    }
+
+JSON Response::
+
+    {
+        "availability_zone": "nova",
+        "hostname": "test.novalocal",
+        "launch_index": 0,
+        "meta": {
+            priority": "low",
+            "role": "webserver",
+            "from_server_key_1": "value1",
+            "from_server_key_2": "value2",
+        },
+        "name": "test",
+        "public_keys": {
+            "mykey": "ssh-rsa AAAAB3NzaC1yc2EAAMdEX8Q== Generated by Nova\n"
+       },
+       "uuid": "d8e02d56-2648-49a3-bf97-6be8f1204f38"
+    }
+
+Security impact
+---------------
+Although this opens up a write channel to the metadata service, there should
+be no security impact because
+
+* Rate limiting will continue to be applied to all the requests to the metadata
+  service just as it is today
+* Quotas for the number of keys will be enforced just as it is done today.
+* Restrictions on the size of the keys and values will be enforced just as it
+  is done today.
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    aneesh-udumbath
+
+
+Other contributors:
+    aditirav
+
+
+Work Items
+----------
+
+* Add a route for /meta_data under http://169.254.169.254/openstack/latest
+* Add a module under nova/api/metadata to handle POST requests to the above
+  URL
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest tests need to be written to check that keys are updated in metadata
+service.
+
+Documentation Impact
+====================
+
+Metadata service documentation needs to be updated to include the extension to
+the metadata API.
+
+JSON Request::
+
+    {
+        "meta": {
+             "key_1": "value1",
+             "key_2": "value2",
+        }
+    }
+
+JSON Response::
+
+    {
+        "availability_zone": "nova",
+        "hostname": "test.novalocal",
+        "launch_index": 0,
+        "meta": {
+            priority": "low",
+            "role": "webserver",
+            "from_server_key_1": "value1",
+            "from_server_key_2": "value2",
+        },
+        "name": "test",
+        "public_keys": {
+            "mykey": "ssh-rsa AAAAB3NzaC1yc2EAAMdEX8Q== Generated by Nova\n"
+       },
+
+References
+==========
+
+https://etherpad.openstack.org/IcehouseNovaMetadataService
diff --git a/specs/juno/proposed/metadata-service-network-info.rst b/specs/juno/proposed/metadata-service-network-info.rst
new file mode 100644
index 0000000..c995396
--- /dev/null
+++ b/specs/juno/proposed/metadata-service-network-info.rst
@@ -0,0 +1,291 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+API: Proxy neutron configuration to guest instance
+==================================================
+
+https://blueprints.launchpad.net/nova/+spec/metadata-service-network-info
+
+Improve the networking info given in both config drive and the metadata
+service to instances.
+
+Problem description
+===================
+
+Currently, cloud-init takes the Debian-style interfaces file that is
+generated from a template and has to convert it to an interfaces file for
+other OSs such as RHEL or Windows. This becomes more and more challenging as
+network configurations get more complex.
+
+Ironic is working with baremetal hardware. Their network configs might
+require more complex network configurations such as multiple VLANs over bonded
+interfaces. Translating network templates to multiple OS's then becomes much
+more challenging than today. These aren't supported in Neutron as of today,
+but there are multiple proposed changes to add support. Using a flexible
+design will allow new network configurations much more easily.
+
+Alternate Use Cases:
+Consider a VM with the first interface configured by DHCP, and all other
+interfaces on private networks where the interfaces are statically configured,
+but you are not using config drive, just the metadata service, and not
+cheating by doing file injection, presenting the data in a guest agnostic
+format.
+
+Setting up static routes without declaring a global route in the interfaces
+template.
+
+For Future Expansion:
+Future use cases would be using this format to create bonded interfaces,
+either of physical or virtual interfaces. Many hypervisors are deployed on
+hardware with bonded interfaces, so it is sensible for Ironic/TripleO
+to require bonds. To create these bonds today, assumptions have to be made
+about the interface names that are being bonded, which can change depending
+on the OS. With this change, the bonds can be described generically and
+implemented in a consistent way for the OS.
+
+Proposed change
+===============
+
+* Create a versioned network_data (like user_data and vendor_data already in
+  the metadata service and configdrive) providing more detailed network info
+* A flexible JSON schema to deal with complex network layouts,
+  and which can be extended easily as Neutron supports more configurations
+* Information comes from current network_info for instance
+* Some things like bonds won't be supported until Neutron supports them
+* We only really need concrete info: mac address, fixed IP address, subnet,
+  gateway, host routes, neutron port-id, neutron network-id, neutron subnet-id
+* Links should be split out separate from network information to make tiered
+  structures like bonds more easily implemented
+* VIFs will be supported as a Link
+* Physical links will be supported when the neutron-external-attachment-points
+  blueprint is completed. [1]
+* VLANs will be supported as another type of Link
+* Add a "services" section for network services that aren't related to a
+  particular network or interface. The primary use will be DNS servers.
+* In the future, bonds can be supported as another type of Link, pointing at
+  multiple other Links
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+Sample API for getting network information from metadata service
+
+GET: http://169.254.169.254/openstack/latest/metadata/network_data.json
+
+JSON Response::
+
+    {
+    "links": [
+        { // Example of VIF
+            "id": "interface2", // Generic, generated ID
+            "type": "vif", // Can be 'vif', 'phy' or (future) 'bond'
+            "ethernet_mac_address": "a0:36:9f:2c:e8:70", // MAC from Neutron
+            "vif_id": "E1C90E9F-EAFC-4E2D-8EC9-58B91CEBB53D",
+            "mtu": 1500 // MTU for links
+        },
+        { // Example of physical NICs
+            "id": "interface0",
+            "type": "phy",
+            "ethernet_mac_address": "a0:36:9f:2c:e8:80",
+            "mtu": 9000
+        },
+        {
+            "id": "interface1",
+            "type": "phy",
+            "ethernet_mac_address": "a0:36:9f:2c:e8:81",
+            "mtu": 9000
+        },
+        { // Bonding two NICs together (future support)
+            "id": "bond0",
+            "type": "bond",
+            "bond_links": [
+                "interface0",
+                "interface1"
+            ],
+            "ethernet_mac_address": "a0:36:9f:2c:e8:82",
+            "bond_mode": "802.1ad",
+            "bond_xmit_hash_policy": "layer3+4",
+            "bond_miimon": 100
+
+        },
+        { // Overlaying a VLAN on a bond (future support)
+            "id": "vlan0",
+            "type": "vlan",
+            "vlan_link": "bond0",
+            "vlan_id": 101,
+            "vlan_mac_address": "a0:36:9f:2c:e8:80",
+            "neutron_port_id": "E1C90E9F-EAFC-4E2D-8EC9-58B91CEBB53F"
+        },
+    ],
+    "networks": [
+        { // Standard VM VIF networking
+            "id": "private-ipv4",
+            "type": "ipv4",
+            "link": "interface0",
+            "ip_address": "10.184.0.244",
+            "netmask": "255.255.240.0",
+            "routes": [
+                {
+                    "network": "10.0.0.0",
+                    "netmask": "255.0.0.0",
+                    "gateway": "11.0.0.1"
+                },
+                {
+                    "network": "0.0.0.0",
+                    "netmask": "0.0.0.0",
+                    "gateway": "23.253.157.1"
+                }
+            ],
+            "neutron_network_id": "DA5BB487-5193-4A65-A3DF-4A0055A8C0D7"
+        },
+        { // IPv6
+            "id": "private-ipv4",
+            "type": "ipv6",
+            "link": "interface0",
+            // supports condensed IPv6 with CIDR netmask
+            "ip_address": "2001:cdba::3257:9652/24",
+            "routes": [
+                {
+                    "network": "::",
+                    "netmask": "::",
+                    "gateway": "fd00::1"
+                },
+                {
+                    "network": "::",
+                    "netmask": "ffff:ffff:ffff::",
+                    "gateway": "fd00::1:1"
+                },
+            ],
+            "neutron_network_id": "DA5BB487-5193-4A65-A3DF-4A0055A8C0D8"
+        },
+        { // One IP on a VLAN over a bond of two physical NICs (future support)
+            "id": "publicnet-ipv4",
+            "type": "ipv4",
+            "link": "vlan0",
+            "ip_address": "23.253.157.244",
+            "netmask": "255.255.255.0",
+            "dns_nameservers": [
+                "69.20.0.164",
+                "69.20.0.196"
+            ],
+            "routes": [
+                {
+                    "network": "0.0.0.0",
+                    "netmask": "0.0.0.0",
+                    "gateway": "23.253.157.1"
+                }
+            ],
+            "neutron_network_id": "62611D6F-66CB-4270-8B1F-503EF0DD4736"
+        }
+    ],
+    "services": [
+        {
+            "type": "dns",
+            "address": "8.8.8.8"
+        },
+        {
+            "type": "dns",
+            "address": "8.8.4.4"
+        }
+    ]
+    }
+
+
+The same JSON will be stored in the configdrive under
+openstack/$VERSION/network_data.json
+
+Security impact
+---------------
+
+The JSON data could give more insight into the network than would be
+available otherwise to a guest instance. In a locked down environment,
+a user may be able see more network details in the metadata service than they
+could otherwise discover. An example could be a hardened SELinux VM. A
+security note should be documented.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+The intention is that this network metadata can be used by cloud-init and
+other in-instance agents to configure the network in more advanced ways. It
+is possible that, depending on the agent's implementation,
+the network config could change slightly compared to configs generated prior
+to this new metadata. An example is network interfaces being named slightly
+differently than the OS would name them. This will be highly dependent on
+changes to agents like cloud-init.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  JoshNang
+
+Other contributors:
+  claxton
+
+Work Items
+----------
+
+* Get basic networking info from neutron into Metadata Service
+  (list of: mac, IP, subnet, gateway, neutron-port-id, host-routes)
+* Add above information into ConfigDrive as "network_data"
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest tests to be added to check if network data is returned.
+
+Documentation Impact
+====================
+
+Changes to the Metadata Service api to ask and return network data.
+
+References
+==========
+
+[1] https://blueprints.launchpad.net/neutron/+spec/neutron-external-attachment-points
+
+[2] https://etherpad.openstack.org/p/IcehouseNovaMetadataService
+
diff --git a/specs/juno/proposed/migrate-non-active-instances.rst b/specs/juno/proposed/migrate-non-active-instances.rst
new file mode 100644
index 0000000..37f470d
--- /dev/null
+++ b/specs/juno/proposed/migrate-non-active-instances.rst
@@ -0,0 +1,140 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================
+Live Migrate instances in non-active states
+===========================================
+
+https://blueprints.launchpad.net/nova/+spec/migrate-non-active-instances
+
+Currently only instances in active states can be live-migrated. In this
+blueprint we propose a method to migrate instances in stable non-active states
+such as SHUTOFF or SUSPENDED.
+
+
+Problem description
+===================
+
+For an admin, being able to live-migrate an instance with a single command
+without depending on the instance state can be convenient:
+
+* The driving scripts do not need to check instance state and call different
+  commands.
+* Reading of instance state and issuing a live-migration command do not need to
+  be atomic. No need to invoke any instance locking mechanism. Without this, a
+  user can change the state of an instance just before a live-migration call and
+  the command can become invalidated.
+* Limit the number of secure channels required. For instance, libvirt TLS setup
+  configured for live-migration of active instances can be used for all
+  instances.
+
+
+Proposed change
+===============
+
+We propose a libvirt specific solution and provide configurability to turn it
+off in the case of other hypervisors. The main changes are:
+
+* Start non-active instance in SUSPENDED or SHUTOFF in PAUSED state in the
+  libvirt driver. (dom.createWithFlags(libvirt.VIR_DOMAIN_START_PAUSED))
+* Configure libvirt to VIR_MIGRATE_PAUSED. This will ensure that the instance
+  will migrate and stay in PAUSED state in the destination.
+* In post-migration use the vm_state as a hint to the intended state and make
+  the relevant power_state changes to the instance.
+* Change compute API to accept SUSPENDED and SHUTOFF vm_states as valid initial
+  states for a migration.
+* Introduce a flag live-migrate-non-active-states to turn on this feature.
+
+Alternatives
+------------
+
+* Offline migration framework exists
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+There won't be any changes to the API schema.
+
+However, the response will now differ in that less states will result in an
+ERROR to live-migrate. We will document this changes.
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+
+Currently the duration an active VM is paused for a very short period of time
+during which the final dirty pages are moved and the source VM stopped. Our work
+will add another small duration of time that would take for nova to do the
+following.
+
+1. Source compute thread detects VM moved
+2. Post live-migration on source
+3. Post live-migration on destination
+4. PAUSED->intended power state
+
+From our implementation we didn't see any significant degradation in the
+duration the VM went offline, but this is an area we will need to test more.
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  parthipan
+
+Work Items
+----------
+
+* Compute API changes to allow non-active states enabled by a flag
+* Require VIR_MIGRATE_PAUSED if this flag is set for a libvirt setup
+* Changes to make VM paused and revert power_state to match vm_state in the
+  post-migration
+
+Dependencies
+============
+None
+
+Testing
+=======
+
+Tempest tests to cover the following:
+
+* Migrating shutoff and suspended instances expecting success
+* <TODO> need more work to come up with failure cases
+
+Documentation Impact
+====================
+* The behaviour changes
+* The proposed flag live-migrate-non-active-states to turn on the feature
+
+References
+==========
+None
+
diff --git a/specs/juno/proposed/monitoring-ip-availability-proposal-1.rst b/specs/juno/proposed/monitoring-ip-availability-proposal-1.rst
new file mode 100644
index 0000000..93ce31e
--- /dev/null
+++ b/specs/juno/proposed/monitoring-ip-availability-proposal-1.rst
@@ -0,0 +1,168 @@
+================================================
+List all the available IP present in all subnets
+================================================
+
+https://blueprints.launchpad.net/nova/+spec/monitoring-ip-availability
+
+Problem description
+===================
+
+In an Openstack deployment monitoring usage of resource like compute,
+storage, network is critical.
+Having a way to keep track of these resources can be important so
+that proper actions can be taken
+in case a threshold is reached in resource consumption of these
+resources. This feature
+can be used to monitor IP availability in all the available subnets.
+In short, this feature can be used to get a count of the available ips,
+used ips and total ips across all the subnets.
+
+Proposed change
+===============
+
+A new command nova fixed-ip-list will be introduced with the
+parameter ‘available’ to display the
+list of available IP’s present in all the subnets. This can
+be a useful utility to monitor IP availability.
+The target is for nova-network. This command can be run by 
+the operator or the administrator of the cloud who have 
+have admin proviledges.
+
+1. The change will consist of modification at the nova-api
+layer and the nova client side.
+2. At the DB layer and the SQLAlchemy a new API will be
+added to list available IP’s.
+
+Alternatives
+------------
+The standard solution for this will be on the operator side or
+the administrator side. Doing a direct DB call is also an
+option which we are trying to avoid with this proposal. There
+are possible downsides to direct query as in if the underlying
+schema changes the DB query might need a change so its better
+to have an API in such case. Also direct DB query can be prone
+to human errors hence an extension of this sort makes more sense.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+API for specifying available fixed ips.
+
+V2 API specification:
+POST: v2/{tenant_id}/os-fixed-ips/action
+
+V2.1 API specification:
+POST: v2.1/{tenant_id}/os-fixed-ips/action
+
+V3 API specification:
+POST: v3/{tenant_id}/os-fixed-ips/action
+
+Request parameters:
+* tenant_id: The ID for the tenant or account in a multi-tenancy cloud.
+* action : available
+
+Sample v2 request:
+POST: v2/1b4a75e0c54049518438ee3e514e1844/os-fixed-ips/available
+Where 1b4a75e0c54049518438ee3e514e1844 : tenant-id
+
+HTTP response codes:
+Normal HTTP Response Code: 200 on success
+
+In the response we will get the count of available
+ips present and the total ips present
+in all the subnets.
+INFO (connectionpool:203) Starting new HTTP connection (1):
+API-host DEBUG (connectionpool:295)
+"GET /v2/1b4a75e0c54049518438ee3e514e1844/os-fixed-ips/available
+HTTP/1.1" 200 490028
+RESP: [200] {'date': 'Wed, 21 May 2014 22:48:30 GMT',
+'content-length': '490028',
+'content-type': 'application/json',
+'x-compute-request-id': 'req-161c898a-ca69-4e9f-9b43-fc3af12b9a47'}
+RESP BODY: {"fixed_ips_info": [{"allocated": false,
+"availableipcount": 4202,
+"reserved": false, "totalipcount": 4608, "address": "10.220.32.8"}.
+The response body will have the detail of the available ip
+count and totalipcount
+and the list of IP's which are available.
+
+Validation:
+‘action’ must be ‘available’
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+The parameter will be optional, so no other code needs to be changed.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    Vilobh Meshram (vilobhmm@yahoo-inc.com)
+
+Work Items
+----------
+
+Add an API to get the list of available IP’s for all subnets.
+At the SQLAlchemy layer add an API to get the list of available
+IP’s for all subnets.
+Changes at the nova-api layer to handle the request to list IP
+and invoke DB layer API.
+Nova client changes to parse the command line options and
+invoke Nova API.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest test to be added to verify available IP list.
+
+Documentation Impact
+====================
+
+Changes to be made to the Nova API documentation to include
+ the new command
+fixed-ip-list and additional parameter ‘available' that
+ can be passed in.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/monitoring-ip-availability-proposal-2.rst b/specs/juno/proposed/monitoring-ip-availability-proposal-2.rst
new file mode 100644
index 0000000..649ed13
--- /dev/null
+++ b/specs/juno/proposed/monitoring-ip-availability-proposal-2.rst
@@ -0,0 +1,152 @@
+================================================
+List all the available IP present in all subnets
+================================================
+
+https://blueprints.launchpad.net/nova/+spec/monitoring-ip-availability
+
+Problem description
+===================
+
+In an Openstack deployment monitoring usage of resource like
+compute, storage, network is critical. Having a way to keep
+track of these resources can be important so that proper actions
+can be taken in case a threshold is reached in resource consumption
+of these resources. This feature
+can be used to monitor IP availability in all the available subnets.
+
+Proposed change
+===============
+
+A new command nova fixed-ip-list will be introduced with the
+parameter ‘available’ to display the
+list of available IP’s present in all the subnets. This can be
+a useful utility to monitor IP availability.
+The target is for nova-network and if needed can be
+applied to neutron as well.
+
+1. The change will consist of modification at the nova-api
+layer and the nova client side.
+2. At the DB layer and the SQLAlchemy a new API will be
+added to list available IP’s.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+API for specifying available fixed ips.
+
+V2 API specification:
+POST: v2/{tenant_id}/os-fixed-ips/action
+
+V2.1 API specification:
+POST: v2.1/{tenant_id}/os-fixed-ips/action
+
+V3 API specification:
+POST: v3/{tenant_id}/os-fixed-ips/action
+
+Request parameters:
+* tenant_id: The ID for the tenant or account in a multi-tenancy cloud.
+* action : available
+
+Sample v2 request:
+POST: v2/1b4a75e0c54049518438ee3e514e1844/os-fixed-ips/available
+Where 1b4a75e0c54049518438ee3e514e1844 : tenant-id
+
+HTTP response codes:
+Normal HTTP Response Code: 200 on success
+
+In the response we will get the list of all the available IP's present
+in all the subnets.
+INFO (connectionpool:203) Starting new HTTP connection (1):
+API-host DEBUG (connectionpool:295) 
+"GET /v2/1b4a75e0c54049518438ee3e514e1844/os-fixed-ips/available
+HTTP/1.1" 200 490028
+RESP: [200] {'date': 'Wed, 21 May 2014 22:48:30 GMT',
+'content-length': '490028',
+'content-type': 'application/json',
+'x-compute-request-id': 'req-161c898a-ca69-4e9f-9b43-fc3af12b9a47'}
+RESP BODY: {"fixed_ips_info": [{"allocated": false, "availableipcount": 4202,
+"reserved": false, "totalipcount": 4608, "address": "10.220.32.8"}.
+The response body will have the detail of the available ip count and
+totalipcount and the list of IP's which are available.
+
+Validation:
+‘action’ must be ‘available’
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+The parameter will be optional, so no other code needs to be changed.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    Vilobh Meshram (vilobhmm@yahoo-inc.com)
+
+Work Items
+----------
+
+* Add an API to get the list of available IP’s for all subnets.
+* At the SQLAlchemy layer add an API to get the list of available IP’s for all subnets.
+* Changes at the nova-api layer to handle the request to list IP and invoke DB layer API.
+* Nova client changes to parse the command line options and invoke Nova API.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest test to be added to verify available IP list.
+
+Documentation Impact
+====================
+
+Changes to be made to the Nova API documentation to include
+the new command fixed-ip-list and additional parameter ‘available' that
+can be passed in.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/monitoring-ip-availability-proposal-3.rst b/specs/juno/proposed/monitoring-ip-availability-proposal-3.rst
new file mode 100644
index 0000000..2ac2e43
--- /dev/null
+++ b/specs/juno/proposed/monitoring-ip-availability-proposal-3.rst
@@ -0,0 +1,153 @@
+================================================
+List all the available IP present in all subnets
+================================================
+
+https://blueprints.launchpad.net/nova/+spec/monitoring-ip-availability
+
+Problem description
+===================
+
+In an Openstack deployment monitoring usage of resource like
+compute, storage, network is critical. Having a way to keep
+track of these resources can be important so that proper actions
+can be taken in case a threshold is reached in resource consumption
+of these resources. This feature
+can be used to monitor IP availability in all the available subnets.
+
+Proposed change
+===============
+
+A new command nova fixed-ip-list will be introduced with the
+parameter ‘available’ to display the
+list of available IP’s present in all the subnets. This can be
+a useful utility to monitor IP availability.
+The target is for nova-network and if needed can be
+applied to neutron as well.
+
+1. The change will consist of modification at the nova-api
+layer and the nova client side.
+2. At the DB layer and the SQLAlchemy a new API will be
+added to list available IP’s.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+API for specifying available fixed ips.
+
+V2 API specification:
+POST: v2/{tenant_id}/os-fixed-ips/action
+
+V2.1 API specification:
+POST: v2.1/{tenant_id}/os-fixed-ips/action
+
+V3 API specification:
+POST: v3/{tenant_id}/os-fixed-ips/action
+
+Request parameters:
+* tenant_id: The ID for the tenant or account in a multi-tenancy cloud.
+* action : available
+
+Sample v2 request:
+POST: v2/1b4a75e0c54049518438ee3e514e1844/os-fixed-ips/available
+Where 1b4a75e0c54049518438ee3e514e1844 : tenant-id
+
+HTTP response codes:
+Normal HTTP Response Code: 200 on success
+
+In the response we will get the list of all the available IP's present
+in all the subnets.
+INFO (connectionpool:203) Starting new HTTP connection (1):
+API-host DEBUG (connectionpool:295)
+"GET /v2/1b4a75e0c54049518438ee3e514e1844/os-fixed-ips/available
+HTTP/1.1" 200 490028
+RESP: [200] {'date': 'Wed, 21 May 2014 22:48:30 GMT',
+'content-length': '490028',
+'content-type': 'application/json',
+'x-compute-request-id': 'req-161c898a-ca69-4e9f-9b43-fc3af12b9a47'}
+RESP BODY: {"fixed_ips_info": [{"allocated": false, "availableipcount": 4202,
+"reserved": false, "totalipcount": 4608, "address": "10.220.32.8"}.
+The response body will have the detail of the available ip count and
+totalipcount and the list of IP's which are available.
+
+Validation:
+‘action’ must be ‘available’
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+The parameter will be optional, so no other code needs to be changed.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    Vilobh Meshram (vilobhmm@yahoo-inc.com)
+
+Work Items
+----------
+
+* Add an API to get the list of available IP’s for all subnets.
+* At the SQLAlchemy layer add an API to get the list of available IP’s for all subnets.
+* Changes at the nova-api layer to handle the request to list IP and invoke DB layer API.
+* Nova client changes to parse the command line options and invoke Nova API.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest test to be added to verify available IP list.
+
+Documentation Impact
+====================
+
+Changes to be made to the Nova API documentation to include
+the new command
+fixed-ip-list and additional parameter ‘available' that
+can be passed in.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/multi-attach-volume.rst b/specs/juno/proposed/multi-attach-volume.rst
new file mode 100644
index 0000000..3d40c5b
--- /dev/null
+++ b/specs/juno/proposed/multi-attach-volume.rst
@@ -0,0 +1,154 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Support Cinder Volume Multi-attach
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/multi-attach-volume
+
+Currently, Cinder only allows a volume to be attached to a single
+host or instance.  There are times when a user may want to be able
+to attach the same volume to multiple instances.
+
+Problem description
+===================
+
+Currently, Cinder only allows a volume to be attached to one instance
+and or host at a time.  Nova makes an assumption in a number of places
+that assumes the limitation of a single volume to a single instance.
+
+* cinderclient only has volume as a parameter to the detach() call.  This
+  makes the assumption that a volume is only attached once.
+
+* nova assumes that if a volume is attached, it can't be attached again.
+  see nova/volume/cinder.py: check_attach()
+
+
+Proposed change
+===============
+
+The Changes needed in Nova are related to attach time and detach time.
+
+At attach time, nova has to remove the assumption that it can only attach
+a volume if it's not 'in-use'.  A Cinder volume can now be attached if it's
+'available' and/or 'in-use'.  Cinder will only allow a volume to be attached
+more than once if it's 'shareable' flag is set on the volume at create time.
+
+At detach time, nova needs to pass a new parameter to the cinderclient
+to tell cinder which specific attachment it's requesting cinder to detach.
+Since a volume can be attached to an instance and/or a host, a new
+attachment uuid is added at detach time.  Passing only an instance uuid
+is insufficient.  The attachment_id will be optional in the cinderclient.
+If it isn't passed in and there are multiple attachments, then cinder will
+fail because it won't know which attachment to detach.
+
+Alternatives
+------------
+
+The only alternative is for a user to clone a volume and attach the clone
+to the second instance.   The downside to this is any changes to the original
+volume don't show up in the mounted clone.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The command line will now allow you to call nova volume-attach for a volume
+to multiple instances.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+Any time new code is added to Nova that requires a call to detach
+a volume, the developer must get the volume attachment uuid for
+the instance.  This information is embedded in the cinder volume
+volume_attachments list.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  walter-boring
+
+Other contributors:
+  Charlie Zhou
+
+Work Items
+----------
+
+1. Update the use of cinderclient to extract the new list of volume
+   attachments when Nova fetches a volume.
+2. Update all calls to cinderclient.detach() to include the attachment uuid.
+
+
+Dependencies
+============
+
+* This requires a new version of the python-cinderclient.  The changes in the
+  client include the new detach API.
+  https://blueprints.launchpad.net/python-cinderclient/+spec/multi-attach-volume
+
+* This also requires a patch in cinder to support the ability to attach to
+  multiple instances.
+  https://blueprints.launchpad.net/cinder/+spec/multi-attach-volume
+
+
+Testing
+=======
+
+We'll have to add new Tempest tests to support the new Cinder volume shareable
+flag.  The new cinder shareable flag is what allows a volume to be attached
+more than once or not.  Have to look into a tempest test for attaching the
+same volume to multiple instances.
+
+
+Documentation Impact
+====================
+
+We will have to update the docs to discuss the new ability to attach a
+volume to multiple instances if the cinder shareable flag is set on a
+volume.
+
+
+References
+==========
+
+* This is the cinder wiki page that discusses the approach to multi-attach
+  https://wiki.openstack.org/wiki/Cinder/blueprints/multi-attach-volume
diff --git a/specs/juno/proposed/nested-quota-driver-api-proposal-1.rst b/specs/juno/proposed/nested-quota-driver-api-proposal-1.rst
new file mode 100644
index 0000000..f93616d
--- /dev/null
+++ b/specs/juno/proposed/nested-quota-driver-api-proposal-1.rst
@@ -0,0 +1,116 @@
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================
+nested-quota-driver-api
+=======================
+
+https://blueprints.launchpad.net/nova/+spec/nested-quota-driver-api
+
+Nested quota driver for Nested Quota Management will enable  OpenStack projects to enforce quota in nested projects.The current openstack nova implementation can support only one level of hierarchy,ie projects and users.The nested projects is having a hierarchical structure, where each project contains projects and users inside it ,except those at the leaf nodes,which contain only users.
+
+Problem description
+===================
+Use Case 1 :
+Consider the use case of a special effects company named Industrial Light And Magic(ILM). The parent project ILM ,will be having different projects like ANIMATION,ACTION and FICTION,to deal with different types of films.So, under FICTION there will be projects like MATRIX,AVATAR etc ,under ACTION there will be MI,SKYFALL etc and ANIMATION will be having projects like SHREK,TINTIN etc.Consider the hierarchy ILM->FICTION->MATRIX.Suppose, George is having admin role in ILM,John is having admin role in FICTION and Peter is having admin role in MATRIX.Through role inheritance,George and John will be having the same role in their child projects.
+Suppose the free instance cores in ILM and FICTION are 500 and 100,respectively.And MATRIX, which has a total allocation of 50 cores,needs an additional 150 cores.If the user is George ,FICTION will take 50 cores from ILM ,and a total of 150 cores will be given to MATRIX.But John cannot provide 150 cores to MATRIX, as he doesn't have any role in ILM and FICTION is having only 100 free cores with it.
+Suppose after some time, MATRIX doesn't require 75 cores and it needs to be allocated back to the parent project.If John is doing the deletion,the 75 cores will add to the free quota of FICTION and if George is doing the deletion,75 cores will add to the free quota of ILM.
+For efficient utilization of resources,it is ensured that more free quota is available at the higher levels,so that maximum number of projects can make use of it.
+
+Proposed change
+===============
+Quota Allocation:
+When the user tries to do an allocation in a project,all the hierarchy up to the topmost parent where the user is having a role is found out.The free quota is accumulated by traversing up the order, till the sufficient amount of quota is found and the allocation will be done top down the order.
+Quota Deletion:
+If the quota is deleted,then that quota will add to the free quota of the top most project where the user who is deleting is having the same role.
+Project Deletion:
+A project can be deleted if it doesn't have allocation in the child project.The quota of the deleted project ,will add to the free quota of the top most project where the user is having the same role.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+
+If the sufficient free quota is not available with the immediate parent,it has to be taken from the projects further up in the order ,which requires more database operations
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Primary assignee:
+Sajeesh
+Other contributors:
+vishy
+shwicke
+raildo
+tellesnobrega
+morganfainberg
+
+
+Work Items
+----------
+
+1.The current dbquotadriver should be extended for nested projects
+2.When the immediate parent is not having sufficient quota for the child ,there should be an iterative mechanism to go up the hierarchy till sufficient quota is found.
+3.While deletion of quota and projects,the free quota should add to the topmost parent project,where the user who is deleting is having the same rule.
+
+Dependencies
+============
+
+Depends on keystone to  get the hierarchy of projects,and the users and their corresponding roles in those projects
+
+
+Testing
+=======
+
+Integration and unit tests should be added.It should be verified that ,whether the quota is properly allocated from the parent and if sufficient quota is not available with the immediate parent,it is taken from higher  levels.Also,it should be verfied that ,when there is a deletion of quota or project,the free quota is getting added to the top most project,where the user is having the same role.
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+https://wiki.openstack.org/wiki/HierarchicalMultitenancy
diff --git a/specs/juno/proposed/nested-quota-driver-api-proposal-2.rst b/specs/juno/proposed/nested-quota-driver-api-proposal-2.rst
new file mode 100644
index 0000000..206cebd
--- /dev/null
+++ b/specs/juno/proposed/nested-quota-driver-api-proposal-2.rst
@@ -0,0 +1,218 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================
+nested-quota-driver-api
+=======================
+
+https://blueprints.launchpad.net/nova/+spec/nested-quota-driver-api
+
+Nested quota driver will enable  OpenStack projects to enforce quota
+in nested projects.The current openstack nova implementation can support
+only one level of hierarchy,ie projects and users.The nested projects
+are having a hierarchical structure, where each project contains projects
+and users inside it ,except those at the leaf nodes,which contain only
+users.
+
+Problem description
+===================
+
+Use Case 1 :
+
+Consider the use case of a special effects company named Industrial Light
+And Magic(ILM). The parent project ILM ,will be having different projects
+like ANIMATION,ACTION and FICTION,to deal with different types of films.
+So,under FICTION there will be projects like MATRIX,AVATAR etc ,under
+ACTION there will be MI,SKYFALL etc,and ANIMATION will be having projects
+like SHREK,TINTIN etc.
+
+Consider the hierarchy ILM->FICTION->MATRIX.Suppose, George is having
+manager role in ILM,John is having manager role in FICTION and Peter is
+having manager role in MATRIX.Through role inheritance,George and John
+will be having the same role in their child projects.
+
+Suppose the free instance cores in ILM and FICTION are 500 and 100,
+respectively.And MATRIX, which has a total allocation of 50 cores,needs
+an additional 150 cores.If the user is George ,FICTION will take 50 cores
+from ILM ,and a total of 150 cores will be given to MATRIX.But John
+cannot provide 150 cores to MATRIX, as he doesn't have any role in ILM,
+and FICTION is having only 100 free cores with it.
+
+Suppose after some time, MATRIX doesn't require 75 cores and it needs to
+be allocated back to the parent project.If John is doing the deletion,
+the 75 cores will add to the free quota of FICTION and if George is doing
+the deletion,75 cores will add to the free quota of ILM.For efficient
+utilization of resources,it is ensured that more free quota is available
+at the higher levels,so that maximum number of projects can make use of it.
+It can be configured whether the free quota should go to the immediate
+parent or the topmost parent where the user is having the same role.
+
+
+Proposed change
+===============
+
+
+The total quota of a project falls in four categories,
+1.Used Quota.
+2.Reserved Quota
+3.Allocated Quota
+4.Free Quota
+Note:Allocated quota is the quota which is allocated to child projects.
+
+Project Creation:
+While creating the project,it will be having either the default quota
+or the free quota of the parent,whichever is smaller.
+
+Project Deletion:
+A project can be deleted if it doesn't have any allocation in the child
+project.The quota of the deleted project ,will add to the free quota of
+the topmost project where the user is having the same role,  or to the
+immediate parent project,which is configurable.
+
+Quota Expansion:
+While quota expansion,upper limit is determined by the free quota available
+with the parent project.If the requirement of the child  crosses the free
+quota of the parent,free quota of the parent is raised by taking free quota
+from the project, which is the parent of the parent project,and the process
+continues till the requirement is met.
+
+Quota Deletion:
+Quota deletion is equaivalent to upating the quota to a smaller value.The
+lower limit of quota is the sum of  used,reserved and allocated quotas.
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+One column named "allocated" needs to be added to the quota_usages table.
+It denotes the quota which is allocated to the child projects.
+
+
+REST API impact
+---------------
+
+There will be no change in the API calls.For example ,consider the command,
+PUT /v2/{tenant_id3}/os-quota-set/{tenant_id3}
+-d '{"quota_set":{"ram": 2048}}'.
+The keystone will give the hierarchy from bottom to the top,like
+tenant_id3.tenant_id2.tenant_id1.If sufficient amount of ram is not
+available with tenant_id2,then free quota of tenant_id2 will be raised by
+allocating quota from tenant_id1.
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+During quota expansion,if the sufficient amount of quota is not found,even
+after traversing till the topmost project,where the user is having the same
+role, "Quota Insufficient" message will be given.During quota deletion,if
+the quota goes below the sum of "in_use","reserved" and "allocated" quotas,
+then "Operation Not Permitted" message will be given.
+
+
+
+Other end user impact
+---------------------
+
+None
+
+
+Performance Impact
+------------------
+
+If the sufficient free quota is not available with the immediate parent,
+it has to be taken from the projects further up in the order ,which
+requires more database operations
+
+
+Other deployer impact
+---------------------
+
+None
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Primary assignee:
+
+Sajeesh
+
+Other contributors:
+
+vishy
+shwicke
+vinod
+raildo
+tellesnobrega
+morganfainberg
+
+
+Work Items
+----------
+
+1.The current dbquotadriver should be extended for nested projects
+
+2.When the immediate parent is not having sufficient quota for the child,
+there should be an iterative mechanism to go up the hierarchy till
+sufficient quota is found and allocated.
+
+3.While deletion of  projects,the free quota should add to the
+topmost parent project,where the user who is deleting is having the same
+rule or it should go to the immediate parent.The choice of action can
+be configured.
+
+4.While reducing the quota,lower limit should be considered.
+
+
+Dependencies
+============
+
+Depends on keystone to  get the hierarchy of projects,and the users
+and their corresponding roles in those projects
+
+
+Testing
+=======
+
+Integration and unit tests should be added.It should be verified that ,
+whether the quota is properly allocated from the parent,and if sufficient
+quota is not available with the immediate parent,it is taken from higher
+levels.Also,it should be verfied that ,when there is a deletion of quota or
+project,the free quota is getting added to the topmost project,where the
+user is having the same role.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+https://wiki.openstack.org/wiki/HierarchicalMultitenancy
+
diff --git a/specs/juno/proposed/nested-quota-driver-api-proposal-3.rst b/specs/juno/proposed/nested-quota-driver-api-proposal-3.rst
new file mode 100644
index 0000000..84fea65
--- /dev/null
+++ b/specs/juno/proposed/nested-quota-driver-api-proposal-3.rst
@@ -0,0 +1,266 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================
+nested-quota-driver-api
+=======================
+
+https://blueprints.launchpad.net/nova/+spec/nested-quota-driver-api
+
+Nested quota driver will enable OpenStack projects to enforce quota in nested
+projects. The nested projects are having a hierarchical structure, where each
+project may contain users and projects (can be called sub-projects), except for
+projects at the last level in the hierarchy called leaf nodes, which contain
+only users.
+
+Users can have different roles inside each project: A normal user can make use
+of resources of a project. A project admin is a user who in addition is allowed
+to create sub-projects, assign quota on resources to these sub-projects and
+assign the project admin role to individual users of the sub-projects.
+Resources quotas of the root project can only be set by the cloud admin.The
+users roles can be set as inherited, and if set, then an admin of a project is
+automatically an admin of all projects in the tree below.
+
+Problem description
+===================
+
+OpenStack is moving towards to support for hierarchical ownership of objects.
+In this regard, the Keystone will change the organizational structure of
+Openstack, creating nested projects in Keystone.
+
+The existing Quota Driver in Nova called "DbQuotaDriver" is useful to enforce
+quotas at both the project and the project-user level provided that all the
+projects are at the same level (i.e hierarchy level canot be greater than 1).
+
+The proposal is to develop a new Quota Driver called "NestedQuotaDriver", which
+will allow enforcing quotas in hierarchical multitenancy of Openstack.
+
+Use Case:
+
+**Actors**
+* Martha - Admin (i.e role:cloud-admin) of ProductionIT
+
+* Joe - Manager (i.e role: project-admin) of Project CMS
+
+* Sam - Manager (i.e role: project-admin) of Project ATLAS
+
+* Bill : is one important CMS user and needs access to the bulk of computing
+         resources of CMS for a physics analysis
+
+Martha is an infrastructure provider and offers cloud services to Joe for
+Project CMS, and Sam for Project ATLAS. Martha needs to be able to set the
+quotas for both CMS and ATLAS, and manage quotas across the entire system.
+Joe has multiple sub-projects with many users under CMS. Joe needs the ability
+to create quotas, as well as the ability to list and delete resources across
+CMS. Sam and Joe cannot see or manipulate the resources owned by each other,
+only Martha has the ability to manage the whole cloud. Joe can approve the
+request of Bill for an increase of quota for his activity, and changes the
+quota of the Bills projects without having to involve the cloud admin.
+
+
+Proposed change
+===============
+
+1. The default quota (hard limit) for any newly created project is set to 0.
+The neutral value of zero ensures consistency of data in the case of race
+conditions when several project are created by admins  at the same time.
+
+2. A project is allowed to create a server(VM) only after setting the quota is
+set to a non-zero value (as default value is 0). After the creation of a new
+project Quota values must be set explicitly by a Nova API call to a value which
+ensures availability of free quota before resources can be claimed in the
+project.
+
+3. A user with role "cloud-admin" is permitted to do quota operations
+across the entire hierarchy, including the top level project. Cloud-Admins are
+the only users who are allowed to set the quota of the root project in a tree,
+and assign the Project-admin role to users in the root project.
+
+4. A person with role "project-admin" in a project is permitted to do quota
+operations on its siblings (sub-projects and users) in the hierarchy. If the
+role "projet-admin" on a project is set as inheritable in Keystone, then the
+user with this role is permitted to do quota operations starting from that
+project to the last level project/user under the project hierarchy.
+
+5. The total resources consumed by a project is dividied into
+     a.Used Quota  - Resources used by the project users
+                     (excluding child-projects)
+
+     b.Reserved Quota - Resources reserved for future use by the project
+
+     c.Allocated Quota - Sum of the quota "hard_limit" value of immediate child
+                         projects
+
+6. The "free" quota availabile with a project is calculated as
+         free quota = hard_limit - (used + reserved + allocated)
+
+   Free quota is not stored in database; it is calculated for each project
+   on the fly.
+
+7. An increase in the quota value of a project is allowed only if its parent
+   has sufficient free quota available. If there is free quota available at
+   the parent, then the quota update operation will result in update of the
+   "hard_limit" value of the project and "allocated" value update of parent
+   project. That's why, it should be noted that updating the quota of a project
+   requires the token to be scoped at the parent level.
+
+    * Hierarchy of Projects is as A->B->C
+      Project A (hard_limit = 100, used = 0, reserved = 0, allocated = 50)
+      Project B (hard_limit = 50, used = 20, reserved = 0, allocated = 10)
+      Project C (hard_limit = 10, used = 10, reserved = 0, allocated = 0)
+
+      Free quota for projects would be:
+
+      A:Free Quota = 100 {A:hard_limit} - ( 0 {A:used} + 0 {A:reserved} +
+                          50 {A:Allocated to B})
+
+      A:Free Quota = 50
+
+      B:Free Quota = 50  {B:hard_limit} - ( 20 {B:used} + 0 {B:reserved} +
+                          10 {B:Allocated to C})
+
+      B:Free Quota = 20
+
+      C:Free Quota = 10  {C:hard_limit} - ( 10 {C:used} + 0 {C:reserved} +
+                          0 {C:Allocated})
+
+      C:Free Quota = 0
+
+      If Project C hard_limit is increased by 10, then this change results in:
+      Project A (hard_limit = 100, used = 0, reserved = 0, allocated = 50)
+      Project B (hard_limit = 50, used = 20, reserved = 0, allocated = 20)
+      Project C (hard_limit = 20, used = 10, reserved = 0, allocated = 0)
+
+      If Project C hard_limit needs to be increased further by 20, then this
+      operation will be aborted, because the free quota available with its
+      parent i.e Project B is only 10. So, first project-admin of A should
+      increase the "hard_limit" of Project B (using scoped token to Project A,
+      because of action at level A) and then increase the "hard_limit" of
+      Project C (again scoped token to Project B)
+
+8.A decrease in the quota value of a project is allowed only if it has free
+  quota available
+     free quota > 0 (zero)
+
+  And hence the maximum decrese in quota value is limited to free quota value.
+
+  * Hierarchy of Projects is A->B->C,where A is the root project
+      Project A (hard_limit = 100, used = 0, reserved = 0, allocated = 50)
+      Project B (hard_limit = 50, used = 20, reserved = 0, allocated = 10)
+      Project C (hard_limit = 10, used = 10, reserved = 0, allocated = 0)
+
+      If Project B hard_limit is reduced by 10, then this change results in
+      Project A (hard_limit = 100, used = 0, reserved = 0, allocated = 40)
+      Project B (hard_limit = 40, used = 20, reserved = 0, allocated = 10)
+      Project C (hard_limit = 10, used = 10, reserved = 0, allocated = 0)
+
+      If Project B's hard_limit needs to be reduced further by 20, then this
+      operation will be aborted, because the free quota of Project B should
+      be greater than or equal to (20+0+10)
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+Create a new column "allocated" in table "quota_usages" with default value 0.
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  * sajeesh
+
+Other contributors:
+  * vishy
+  * schwicke
+  * raildo
+  * vinod
+  * nirbhay
+  * morganfainberg
+  * tellesnobrega
+  * rodrigodsousa
+  * afaranha
+
+Work Items
+----------
+
+1. Two new roles will be used "cloud-admin" and "project-admin". Users with
+   role "cloud-admin" will be able to do quota operations on any project/user
+   in the hierarchy. The user with "project-admin" role on a project will be
+   able to do quota operations on the  child projects which he created.
+
+2. A new Quota Driver called "NestedQuotaDriver" will be implemented to enforce
+   quotas in hierarchical multitenancy of OpenStack.
+
+
+Dependencies
+============
+
+Depends on bp Hierarchical Multitenancy
+  * https://blueprints.launchpad.net/keystone/+spec/hierarchical-multitenancy
+
+
+Testing
+=======
+
+* Add unit tests for the REST APIs calls.
+
+* Add unit tests for integration with other services.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+* Wiki <https://wiki.openstack.org/wiki/HierarchicalMultitenancy>
diff --git a/specs/juno/proposed/neutron-migration.rst b/specs/juno/proposed/neutron-migration.rst
new file mode 100644
index 0000000..3f4af56
--- /dev/null
+++ b/specs/juno/proposed/neutron-migration.rst
@@ -0,0 +1,326 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================================================
+Nova-network to Neutron migration for an instance
+================================================================
+
+https://blueprints.launchpad.net/nova/+spec/neutron-migration
+
+This blueprint proposes a way to migrate running instances from Nova-network
+to Neutron.
+
+
+Problem description
+===================
+
+Nova-network deprecation process requires Neutron to reach full feature parity
+with Nova-network and as part of the parity plan there needs to be a way to
+automatically migrate existing deployments to Neutron. This will likely require
+changes to both Nova and Neutron as well as support at the orchestration
+level.
+
+
+Proposed change
+===============
+
+Glossary
+--------
+
+* nova-net instance - an instance whose networking is configured and controlled
+  by Nova-network service
+* neutron instance - an instance whose networking is configured and controlled
+  by Neutron service
+* nova-net node - a compute node using Nova-network as a primary network API
+* neutron node - a compute node using Neutron as a primary network API
+
+The current proposal describes an approach to perform a migration of one
+running nova-net instance from a nova-net node to the specified neutron
+node converting it to the neutron instance at the same time. Once implemented,
+this capability can then be consumed by an orchestration mechanism for bulk
+migrations.  Such migrations can be performed on a host-by-host basis.
+Creating and configuring the proper Neutron networks is out of scope of this
+document.
+
+Migrating state
+---------------
+
+In order to avoid the need for freezing the whole deployment we need to
+synchronize ip allocations between Nova-network and Neutron during the
+migration:
+
+* each neutron node should:
+
+ * continue using Nova-network to allocate/deallocate IPs until migration (on
+   all compute nodes) is complete
+ * use Neutron to actually set up new instances, using the IP that Nova
+   allocated
+
+Neutron nodes should be aware of migrating state in order to use nova-net for
+ip address management. Current proposal would be adding a new config option to
+define "migrating_to_neutron" state on a node.
+
+* all new instances should be created on neutron nodes
+
+This should be achievable by disabling all nova-compute services that are not
+neutron nodes. Thus the scheduler will not choose those nodes for new
+instances, while leaving them still accessible and running. It will be
+considered as the responsibility of the operator to disable those nodes before
+migrating to Neutron.
+
+Single instance migration
+-------------------------
+
+Once having a new empty neutron node an operator may migrate (live or
+cold) an instance there from a nova-net node. Target neutron
+node should be properly configured:
+
+* nova_net_neutron_network_map (DictOpt) - Mapping between Nova-network
+  networks and corresponding Neutron networks
+* nova_net_neutron_secgroup_map (DictOpt) - Mapping between Nova-network
+  security groups and corresponding Neutron ones
+
+Neutron migration as part of live migration will include following steps:
+
+* pre_live_migration at destination:
+
+  * for a given instance, retrieve its current network data - vif, security
+    groups, floating_ips
+  * ensure security groups and floating ips for the instance are mirrored from
+    Nova to Neutron
+  * for each vif associated with the instance, create a new port. The new port
+    should have the same ip/mac addresses, security groups,
+    floating ip as the VIF defined in Nova
+  * plug port
+  * send info about new VIFs' configuration to the source node
+
+* on source nova-net-node:
+
+  * perform live-migation for the instance updating it's interface(s)
+    config using info provided by dest node (libvirt "migrateToURI2" should be
+    able to do this)
+
+* post live migration at destination:
+
+  * update instance info cache with neutron vif data
+
+While live migration lets migrate instances with almost no downtime, not all
+valid nova configurations have support for it. Cold migration works for all
+hypervisors and is less risky than live migration.
+
+Neutron migration as part of cold migration:
+
+* prep_resize step at destination:
+
+  * get current network info of the instance
+  * for each VIF create a port on a corresponding Neutron network. The new port
+    should have the same ip/mac addresses, security groups, floating ips
+  * update instance info cache with new network info
+
+After instance is moved to the destination host it will be plugged to proper
+Neutron network(s) automatically (using new network info)
+
+Possible failures:
+
+* could not allocate a neutron port with target ip - ip is already allocated
+  (dhcp port)
+* could not allocate a neutron floating ip - ip is already allocated on
+  external net (router gateway)
+
+Dealing with such failures suggests adding the ability to specify alternative
+fixed and floating ips to use when migrating an instance. This might reduce
+the value of a live migration, since it may impact connectivity to a given
+instance and clients of that instance, so it will be considered out-of-scope
+for this spec. It will be the responsibility of the operator to detect and
+resolve these kinds of conflicts in advance of the migration method proposed
+by this spec.
+
+Alternatives
+------------
+
+1. Rather than convert instances during migration between nodes do migration
+within one host. This will need a new API call and may be achieved by:
+
+* reconfiguring compute service to use Neutron as the primary network API and
+  move it to "migrating_to_neutron" state
+* for a given instance, retrieve its current network data - vif, security
+  groups, floating_ips
+* ensure security groups and floating ips for the instance are mirrored from
+  Nova to Neutron
+* for each vif associated with the instance, create a new port using the
+  Neutron API
+* plug port
+* use libvirt to move the tap device between the nova network-managed
+  linux bridge and the neutron-managed linux bridge while the instance
+  remains running
+* update instance info cache with neutron vif data
+* all Nova API operations that utilize the network API should be routed
+  appropriately according to whether the instance is migrated (or created
+  during the migration process) or non-migrated
+
+However the preference was given to a migration between hosts as it is
+less disruptive and is actually the established pattern for big
+migrations/upgrades on real deployments.
+
+2. Add nova-network compatible mode for linux-bridge-agent:
+
+* Bridge naming br-<port-id> -->  br-<segmentation_id>
+  (so we can keep using exsisting br100 stuff in nova-network)
+* Iptables migration: we should re-generate new neutron based iptables
+  with new bridge name before this migration happens
+
+This is also a migration within one host. The benefit here is that this
+approach will likely allow migration with no dataplane downtime. However it
+is based on Linux Bridge plugin and one of major parts of Neutron-Nova parity -
+the DVR (Distributed Virtual Router) feature - a Neutron replacement for
+Nova-network multi-host functionality - will be based on OVS, so that was
+the primary reason to not go with this option.
+
+3. An alternative to "migrating_to_neutron" config option is having the new API
+wrapper which will be setting/unsetting "migrating" state for a compute node,
+removing the necessity of restarting the service. This adds operational
+flexibility which could be useful for a large cluster. This alternative
+however is not a part of current blueprint and can be implemented at any
+point later.
+
+
+Data model impact
+-----------------
+
+None
+
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+* a connection reset for all existing TCP connections on the migrated instance
+  is expected
+* some time will be needed in order to refresh the arp tables, so data-plane
+  outages are possible for the following services during migration of L3
+  services:
+
+  * Routing
+  * DHCP
+
+
+Performance Impact
+------------------
+
+As described above in the mixed environment where some compute nodes use
+nova-net and some use neutron (migrating state) all neutron nodes should
+still use nova-net for ip-address management. This will likely slow down API
+calls that involve IP allocations/deallocations. After all instances are
+migrated this will not be needed.
+
+Other deployer impact
+---------------------
+
+In order to perform migration one should do the following:
+
+* start Neutron service and agents
+* create target Neutron networks, external networks, routers, security groups
+* prepare an empty compute node(s) with Neutron as default network API,
+  "migrating_to_neutron" set to True and nova_net_neutron_network/secgroup_map
+  properly configured
+* migrate instances from nova-net nodes to neutron nodes via live or cold
+  migration
+* restart Nova-compute service on each neutron node with "migrating_to_neutron"
+  config option set to False
+* stop Nova-network
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  obondarev
+
+Other contributors:
+
+
+Work Items
+----------
+
+* ip allocations should be synchronized with nova-net on neutron nodes
+* update existing cold migration mechanism
+* update existing live migration mechanism
+* handle instance security groups migration
+* handle instance floating ip migration
+
+
+Dependencies
+============
+
+* https://blueprints.launchpad.net/neutron/+spec/specify-router-ext-ip
+
+* https://blueprints.launchpad.net/neutron/+spec/allow-specific-floating-ip-address
+
+
+Testing
+=======
+
+Along with unit and tempest testing Grenade Nova-net to Neutron migration test
+should be created which should be able to set up a multinode OpenStack
+deployment with several tenants/networks/instances, at least one nova-net
+compute and one neutron compute nodes. Then perform a full migration to Neutron
+and run all existing tempest network scenario tests (before and after the
+migration), including:
+
+* internal connectivity
+* external connectivity
+* security groups functionality
+* floating ips functionality
+
+This should be tested for all three network managers available in Nova-network.
+The target Neutron configurations should be ml2+ovs+flat for
+Flat/FlatDHCPManagers and ml2+ovs+vlan for VlanManager.
+
+
+Documentation Impact
+====================
+
+Documentation should be created/updated describing the whole neutron migration
+process in details.
+
+
+References
+==========
+
+etherpad from Juno summit:
+
+* https://etherpad.openstack.org/p/novanet-neutron-migration
+
+Related blueprint:
+
+* https://blueprints.launchpad.net/neutron/+spec/nova-to-quantum-upgrade
+
+Documents above contain some other useful links.
diff --git a/specs/juno/proposed/new-libvirt-volume-driver-for-Huawei-SDSHypervisor.rst b/specs/juno/proposed/new-libvirt-volume-driver-for-Huawei-SDSHypervisor.rst
new file mode 100644
index 0000000..093709d
--- /dev/null
+++ b/specs/juno/proposed/new-libvirt-volume-driver-for-Huawei-SDSHypervisor.rst
@@ -0,0 +1,185 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+New libvirt volume driver for Huawei SDSHypervisor
+==================================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/huawei-sdshypervisor-volume-driver
+
+Huawei SDSHypervisor is a storage virtualization solution, is a software
+running in host, just focus on the data plane, the goal is to facilitate
+the reuse of customer's existing old and low-end devices. SDSHypervisor does
+not interact with the storage device in control plane, such as create volume,
+create snapshot, do not need third-party device manufacturers to develop any
+driver. Administrator just attaches Lun to the hypervisor node as hypervisor
+storage entity and hypervisor will provide virtual volume based user's QoS
+and patch advance feature to these Lun such as snapshot, link clone, cache,
+thin provision and so on.
+
+Huawei SDSHypervisor cinder driver has been merged into cinder in this url
+https://review.openstack.org/#/c/101688/
+
+The purpose of this blue print is to add a new libvirt volume driver for
+Huawei SDSHypervisor.
+
+
+Problem description
+===================
+
+Currently, user can access Huawei SDShypervisor by Openstack Cinder, but can't
+attach volume from SDSHypervisor to nova instance.
+
+Proposed change
+===============
+
+SDShypervisor data panel using private Key Value protocal, so we also add a
+new libvirt volume driver to realize attach/detach volume.
+
+The following diagram shows the command and data paths.
+
+````
+
+    +------------------+                +------------------+
+    |                  |                |                  |
+    |  Nova +          |    Rest API    |  Cinder +        |
+    |  Nova compute    | -------------- |  Cinder Volume   |
+    |  (qeumu-kvm)     |                |                  |
+    |                  |                |                  |
+    +------------------+                +------------------+
+
+            |                           |                  |
+            |                           |                  |
+            |                           |                  |
+            |                           |                  |
+
++-----------------------+ +-----------------------+ +-----------------------+
+|                       | |                       | |                       |
+|                       | |                       | |                       |
+|     SDShypervisor     | |     SDShypervisor     | | SDShypervisor Driver  |
+| libvirt volume driver | |       connector       | |                       |
+|                       | |                       | |                       |
++-----------------------+ +-----------------------+ +-----------------------+
+
+                       |               |                |
+                       |               |                |
+                       |               |                |
+
+                      CLI             CLI         Socket API
+
+                       |               |                |
+                       |               |                |
+                       |               |                |
+
+                    +--------------------------------------+
+                    |                                      |
+                    |                                      |
+                    |            SDShypervisor             |
+                    |              storage                 |
+                    |                                      |
+                    +--------------------------------------+
+
+````
+
+
+Add a new volume driver in nova/virt/libvirt/volume.py file to realize
+attach/detach volume to qemu-kvm instance, and realize abstract volume dirver
+methods:
+* connect_volume
+* disconnect_volume
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+User will be able to attach Huawei SDSHypervisor volume to Nova instance.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  zhangni <zhangni@huawei.com>
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+Realize new libvirt volume driver using CLI.
+Add unit test for Huawei SDShypervisor libvirt volume driver.
+
+
+Dependencies
+============
+
+Cinder can access Huawei SDSHypervisor to create volume/snapshot and so on.
+This has been merged to cinder in this review url
+https://review.openstack.org/#/c/101688/
+
+
+Testing
+=======
+
+Add unit test for Huawei SDShypervisor libvirt volume driver.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+https://wiki.openstack.org/wiki/Cinder/HuaweiSDSHypervisorDriver
diff --git a/specs/juno/proposed/nic-state-aware-scheduling.rst b/specs/juno/proposed/nic-state-aware-scheduling.rst
new file mode 100644
index 0000000..5497814
--- /dev/null
+++ b/specs/juno/proposed/nic-state-aware-scheduling.rst
@@ -0,0 +1,183 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+NIC state aware scheduling
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/nic-state-aware-scheduling
+
+As an End User I want that my instance are placed on a compute host where the
+network connectivity of my instances are ensured.
+As a Deployer I want to configure my deployment to check the state of the
+network adapters used by the instances periodically and do not schedule
+instances on compute host where the network connectivity is not ensured.
+
+
+Problem description
+===================
+
+In various OpenStack deployments the control network used by the OpenStack
+services are different than the data network used by the instances therefore
+having a connectivity between nova-compute and nova-scheduler is not enough to
+validate that there is network connetivity available for the instance.
+Therefore nova-scheduler needs information about the link state of the network
+adapters on the compute hosts and nova-scheduler needs to use this information
+to select the proper host for an instance.
+
+The servicegroup API is currently used to maintain service statuses like
+the status of nova-compute service. The status information is limited to the
+last hearthbeat timestamp of the service so the scheduler does not have
+detailed information about the fine grained state of the service
+nova-compute provides.
+
+
+Proposed change
+===============
+
+The servicegroup drivers will be extened to handle more fine grained status of
+the services than the last hearthbeat timestamp.
+
+An internal interface will be added to the Service class that will allow
+services to provide more fine grained information in the status report
+towards the servicegroup.
+
+The nova-compute service will be extended to periodically collect the state of
+the NICs according to the configuration and include the collected information
+into the status report sent to the servicegroup.
+The deployer will be able to define which NIC needs to be monitored via a
+new configuration option.
+
+The servicegroup API will be extended to allow querying the fine grained
+status of the services.
+
+A new NetworkConnectivityFilter will be implemented for the FilterScheduler
+that can be used to filter compute hosts that does not have proper
+network connectivity. The filter will make decision based on the state of
+the NICs on the given host if the fine grained state is present in the
+servicegroup. The deployer will be able to configure the list of NICs
+that needs to be in connected state via configuration options.
+
+In the future the fine grained state can be extended to provide other
+informations like link speed to the scheduler.
+
+
+Alternatives
+------------
+
+Alternatively administrator can set up a thrid party montoring tool in its
+deployment and configure it in a way that the monitoring tool disables the
+nova-compute service on the compute hosts where certain network adatpers are
+in a wrong state. However this solution is not flexible enough in the future
+because it cannot be extended to provide fine grain data for example link
+speed to the scheduler.
+
+Another alternative would be to extend the resource tracker to collect
+the inforamtion and report it as part of the host_state. This alternative
+is rejected as the NIC state is not considered as a resource.
+
+Data model impact
+-----------------
+
+For the db driver of the servicegroup feature the services table needs to be
+extended with a new field to store the fine grained status information of the
+services in a json blob.
+
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+A new config option compute_service_network_adapters will be added to make it
+possible for the deployer to specify the device name of those network
+adapters which will be monitored by the nova-compute service and which state
+will be avalible for the NetworkConnectivityFilter.
+The NetworkConnectivityFilter will use the same configuration option to decide
+which NICs needs to be in connected stated.
+The compute_service_network_adapters option will be defaulted to [].
+
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  balazs-gibizer
+
+Other contributors:
+  adrian-hoban
+  alan-kavanag
+  liyi-meng
+
+Work Items
+----------
+
+* Extend the services table with a new field and create the necessary
+  migration scripts and extend db driver for the servicegroup to use
+  the new field to store the fine grained state
+* Extend memcached driver to handle fine grained state
+* Extend zookeeper driver to handle fine grained state
+* Extend the servicegroup implementation to allow providing fine grained
+  status information
+* Extend the nova-compute service to include the NIC states into the
+  status reporting towards the servicegroups
+* Implement a new NetworkConnectivityFilter that filters out host where the
+  configured network adapters are not in connected state
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Today there is no separate test for different scheduler filters in tempest so
+I am not considering adding filter specific tests
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/no-db-scheduler.rst b/specs/juno/proposed/no-db-scheduler.rst
new file mode 100644
index 0000000..61f53f4
--- /dev/null
+++ b/specs/juno/proposed/no-db-scheduler.rst
@@ -0,0 +1,238 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================
+ Decouple scheduler from DB
+============================
+
+Launchpad entry: https://blueprints.launchpad.net/nova/+spec/no-db-scheduler
+
+This blueprint considers two major problems with current implementations of
+scheduler in Nova:
+
+* New features that impact scheduling have to either store their data in one
+  Text column (compute_nodes.stats) or add new colums to compute_node table
+  itself (which requires new migrations). This limits opportunities for
+  cross-project scheduling as it makes scheduler bound to DB schema.
+
+* Current implementation uses one big request to DB to retrieve all data about
+  compute nodes that resides there. These requests becomes a performance
+  bottleneck in large enough deployments.
+
+Problem description
+===================
+
+Currently DB is used for scheduler in these steps:
+
+* once in a minute (by default) every compute node updates its state in
+  database in compute_nodes tables in on JSON blob;
+* every time a new instance is to be scheduled, DB is queried for all existing
+  nodes and all their parameters;
+* after that scheduler selects appropriate host for a VM from aquired list.
+
+Flexibility of scheduler
+------------------------
+
+The main problem in direct storing of schedulling data in DB is limiting data
+available for scheduling to one project. For example, user might want to deploy
+VMs near existing Cinder volumes. This requires scheduler to hold information
+from different projects which is not possible with a single DB schema.
+
+Upcoming performance issues
+---------------------------
+
+compute_nodes table is mainly being accessed from two places:
+
+* every compute node issues an UPDATE once every minute (by default);
+* scheduler does global SELECT for all records for every VM boot request.
+
+One problem is a heavy load on one table: for 10000 of compute nodes the first
+part alone would produce hundreds of UPDATE requests per second while the
+second one would generate a lot of unnecessary DB->Python traffic.
+
+Proposed change
+===============
+
+These problems can be solved in the following steps (in any order):
+
+1. use schedulers' memory as a primary storage for scheduling-related
+   parameters and synchronize them through external backend;
+2. abstract out pushing data to scheduler to one RCP call which in future can
+   be called from different projects;
+3. store all data required for scheduling in one JSON for each node (avoid
+   limiting the data that can be used by DB schema) - mostly done by
+   https://blueprints.launchpad.net/nova/+spec/compute-node-stats-as-resource-tracker-extension
+
+These steps provide the following benefits:
+
+1. With this step we avoid re-requesting data for all nodes for each scheduling
+   operation. Each host state is updated rather rare (once in a minute by
+   default) so there's no reason to fetch mostly-static data from DB. We should
+   fetch data for hosts that provided new data since our last request instead.
+2. Using one single RPC call to scheduler instead of a number of direct DB
+   calls allows us to decouple scheduler from Nova and allow in future other
+   projects to send their data to one central scheduler.
+3. The first part allows to store any strucrured data in schedulers so that
+   different projects can provide their data that can be used for scheduling
+   without need to provide any schema updates.
+
+Synchronizer implementation details
+-----------------------------------
+
+The core for the new approach for host state storage is a new mechanism to
+provide schedulers with actual data about all compute nodes.
+
+Synchronizer uses these models:
+
+* Records
+
+  All data is stored in impersonal records consiting of ID and some JSON data.
+  In our case one record represents current state of compute node.
+
+* Updates
+
+  Update represents the act of updating one record and holds ID of updated
+  record and a timestamp. Updates also provide a token - a monotonic increasing
+  integer that differs for each update. Token is backend-dependent.
+
+* Namespaces
+
+  As our first goal is to allow storage for data from different projects, we
+  incapsulate records and updates to named namespaces. For host states in nova
+  "nova-hoststate" namespace is used.
+
+These models can be stored in any backend. Initial implementation proposes 2
+backends: SQLAlchemy and Memcached.
+
+Wherever the data is needed to be read or written, Synchronizer object is
+instantiated. It's initialized with a namespace name and a backend class. It
+provides 3 basic operations:
+
+* get all records;
+* put up new record (or update an existing one);
+* delete existing record.
+
+For every operation internal state is synchronized with backend.
+Synchronization process consits of the following steps:
+
+* latest fetched update's token is compared with the latest update's token
+  stored in the namespace;
+* if there are new updates, appropriate records are fetched from the backend
+  (note that the record will be fetched once even if there're a number of new
+  updates for it);
+* in case of some error (e.g. token sequence is broken or backend got flushed),
+  a timer is set to flush all state in 1 minute (by default) to allow hosts to
+  report their new state in the mean time;
+* once in a while (10 seconds by default) all old updates are purged from
+  backend.
+
+After synchronization get_all method returns data from internal state while put
+and delete methods send requests to backends.
+
+Alternatives
+------------
+
+Fanout all host states to all schedulers
+""""""""""""""""""""""""""""""""""""""""
+
+Compute nodes can fanout their state to all schedulers but it would put a
+significant load to message queue while still requiring to store states in DB
+to bootstrap new scheduler nodes.
+
+Data model impact
+-----------------
+
+SQLAlchemy (default) backend requires 3 new tables created in DB. Old
+compude_nodes.stats column should be deleted after migration.
+
+Note that since stats data is transient, no data migration is necessary.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None with default SQL backend. Memcached backend would need to be secured
+separately.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+Performance benefit for large deployments (~10k nodes), no penalty for smaller
+clouds.
+
+Other deployer impact
+---------------------
+
+SQLAlchemy backend merely requires migrations to be run.
+
+Memcached backend would require deploying memcached alongside Nova and
+configure synchronizer to use its endpoints.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  yorik-sar
+
+Other contributors:
+  aovchinnikov (original developer)
+
+Work Items
+----------
+
+* implementation of new synchronizer (mostly done);
+* move as much scheduling-related data to host states as possible (eliminate
+  compute_nodes?)
+
+Dependencies
+============
+
+For memcached backend python-memcached is required.
+
+Testing
+=======
+
+This change doesn't change Nova behavior so it doesn't require changes in
+Tempest.
+
+Documentation Impact
+====================
+
+New configuration options should be documented.
+
+References
+==========
+
+Original etherpad description:
+https://etherpad.openstack.org/p/scheduler-design-proposal
+
+Discussion of original reasons for this change:
+https://docs.google.com/a/mirantis.com/document/d/1_DRv7it_mwalEZzLy5WO92TJcummpmWL4NWsWf0UWiQ/view
+
+Patches submitted for review:
+https://review.openstack.org/#/q/topic:bp/no-db-scheduler,n,z
diff --git a/specs/juno/proposed/no-downward-resize.rst b/specs/juno/proposed/no-downward-resize.rst
new file mode 100644
index 0000000..e04d786
--- /dev/null
+++ b/specs/juno/proposed/no-downward-resize.rst
@@ -0,0 +1,152 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================
+Don't allow resize to smaller disks
+===================================
+
+https://blueprints.launchpad.net/nova/+spec/no-downward-resize
+
+Current hypervisors have a range of checks against reduction in disk size for
+resize operations, ranging from checking root and ephemeral to not checking at
+all. Blocking any resize operation to a flavor with less disk capacity will
+provide a better and more consistent user experience.
+
+Problem description
+===================
+
+Hypervisor checks on resize to a smaller disc are currently inconsistent:
+
+Baremetal:  No support for resize
+Hyperv: Checks and fails root disk only
+Libvirt: Checks and fails for root and ephemeral
+Vmware: Currently no checks, patch submitted to check and fail root disk
+Xen: Allows root disk resize, but fails ephemeral disk
+
+Because these checks are only implemented at the driver failures cannot be
+easily detected by the user (the instance remains active).
+
+Even where implemented resize operations will not work if the source disk
+already contains more data that the new flavor allows.
+
+This gives an inconsistent user experience.
+
+
+Proposed change
+===============
+
+Consensus on the mailing list was that resize operations where the root or
+ephemeral disk capacity is reduced should be treated as an error and blocked
+at the API layer.
+
+The new check will be implemented as an API extension, which if loaded will
+return an error if either of the root or ephemeral disks in the target flavor
+are smaller than the current sizes used by the instance.
+
+
+Alternatives
+------------
+
+As the current behavior is either that checks exist (but are buried in the
+system), or is inconsistent (the operation may fail under some conditions) an
+alternative implantation would be to considered this as enhanced validation in
+the API rather than a change in behavior, and so add the check to the existing
+API code rather than as an extension.   However that would prevent users from
+being able to detect if the new check was being enforced, and prevent
+operators who do have a hypervisor that partially implements the resize from
+continuing to do so.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+Adds a new API extension:
+
+Name = "NoResizeDown"
+Alias = "os-no-resize-down"
+
+Loading this extension will modify the behavior of the resize operation so
+that if the root disk size or ephemeral disk size are less than the current
+sizes used by the instance it will return 400
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Users will get an error result for operations that would have previously
+been accepted (but may not have been successful)
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Deployers will need to decide whether to load this new extension or not,
+taking into account the capabilities of their hypervisor.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Phil Day
+
+Work Items
+----------
+
+Will be implemented as a single change
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest will not be affected as it currently only includes a resize upwards
+(resize to a smaller flavor would have failed silently on libvirt).
+
+The new additional validation check can be adequate covered by unit tests.
+
+
+Documentation Impact
+====================
+
+The new API extension will need to be documented.
+
+
+References
+==========
+
+The issue and approach was discussed on openstack-dev:
+http://lists.openstack.org/pipermail/openstack-dev/2014-June/037609.html
+
diff --git a/specs/juno/proposed/no-migration-resize.rst b/specs/juno/proposed/no-migration-resize.rst
new file mode 100644
index 0000000..728dc8f
--- /dev/null
+++ b/specs/juno/proposed/no-migration-resize.rst
@@ -0,0 +1,96 @@
+====================================================
+no-disk-resize - allow resize without host migration
+====================================================
+
+https://blueprints.launchpad.net/nova/+spec/no-migration-resize
+
+Allow a user who does not want to configure server migration to resize on local host only.
+
+Problem Description
+====================
+
+following bug https://bugs.launchpad.net/nova/+bug/1323578, the current option -  allow_resize_to_same_host=True only adds the localhost to the pool of computes used to migrate during resize. 
+
+Proposed change
+===============
+
+I am not sure the current allow_resize_to_same_host usage is clear to the user and I think we should change allow_resize_to_same_host behavior to resize on localhost only. 
+
+Alternative
+-----------
+
+1. if we do not change allow_resize_to_same_host to only allow resize to the localhost I think we should create a new option to configure resize without migration. 
+2. if we see a need in adding localhost to the hosts pool on resize, than I would also suggest that we add optional argument of manual host selection for resize. 
+
+(--host or --target)
+
+Data model Impact
+------------------
+
+None
+
+REST API impact
+----------------
+
+depends on the solution selected 
+
+Security Impact
+---------------
+
+None
+
+Notification impact
+--------------------
+
+depending on solution selected
+
+Other end user impact
+----------------------
+
+depending on solution selected
+
+Other deployer impact
+----------------------
+
+depending on solution selected
+
+Developer impact
+-----------------
+
+depending on solution selected
+
+
+Implementation
+===============
+
+Assignee(s)
+-----------
+
+None
+
+Work Items
+----------
+
+depending on solution selected
+
+
+Dependencies
+============
+None
+
+Testing
+=======
+
+depending on solution selected
+
+Documentation Impact
+=====================
+
+depending on solution selected
+
+References
+==========
+
+https://bugs.launchpad.net/nova/+bug/1323578
+
+
diff --git a/specs/juno/proposed/nodename-in-pci-device.rst b/specs/juno/proposed/nodename-in-pci-device.rst
new file mode 100644
index 0000000..6cd2b05
--- /dev/null
+++ b/specs/juno/proposed/nodename-in-pci-device.rst
@@ -0,0 +1,208 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Nodename in pci device
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/nodename-in-pci-device
+
+PCI device tracker manages the PCI devices in a compute node. Currently the
+PCI device tracker identifies the compute node with compute_node_id, this
+arrangement causes several issues because the compute_node_id is created
+in very late stage. It should be changed to be host/nodename.
+
+Problem description
+===================
+PCI device tracker manages the PCI devices in a compute node. Currently the
+PCI device tracker identifies the compute node with compute_node_id.
+
+However, the compute_node_id is created and valid only after the compute
+node entry is created in database, while the PCI device tracker, and the
+corresponding PCI devices, are created and used before that. We have to
+set up the compute_node_id later on each PCI device after compute node is
+created in database, and we also need make sure the PCI device will not be
+saved to database before the compute_node_id is setup. It's really bad.
+
+It also make scheduler client library effort difficult because the
+scheduler client library has to send back the compute_node_id to the
+resource tracker.
+
+We should use the host/nodename to identify the compute node that the PCI
+device is hosted, which will make things much easier.
+
+Proposed change
+===============
+
+The PciDevice table will be updated to use the host/nodename and keep the
+compute_node_id field deprecated. The corresponding unique constraint and index
+will be removed.
+
+The PCI device object will be updated to use host/nodename. The
+compute_node_id field will be deprecated. The PCI device object will
+be enhanced to translate the compute_node_id to host/nodename in run
+time for legacy API. See details discussion on the 'Data model impact'
+section on how to update the PCI device object and the conductor.
+
+
+The PCI device tracker will be updated to use host/nodename to identify the
+compute node. New API is provided to select PCI devices based on compute node.
+
+A migration script will be created to populate the host/nodename field in the
+database schema.
+
+Alternatives
+------------
+
+This is an enhancement to current implementation, so one of alternative is
+the current poor implementation.
+
+There are several alternative for the migration solution. Currently we are
+trying the additive migration, i.e. the migration script will only change
+the schema, while the conductor will update the data additively. Another
+alternative is to change the data in the migration script. We select the
+additive migration method because it's suggested as new solution, also
+because the potential big number of the PCI devices in the cloud.
+
+Data model impact
+-----------------
+
+The schema for the PciDevice table will be changed. The compute_node_id
+column will be deprecated. Two new columns will be added as host/node to
+identify the corresponding compute node's host and nodename.
+
+Below is a work flow of the migration process:
+
+1. Database migration:
+A migration script will change the schema. As it only change the schema, it
+will hopefully be quick.
+After the database migration, the compute node id will be deprecated and the
+host/node row will be populated later when new conductor is alive.
+At this time, all the PCI devices in the DB will have invalid host/node
+information, which will be updated later by conductor.
+
+New DB API will be added to query PCI device table using host/nodename, and
+the legacy query using compute_node_id will be kept for backward
+compatibility.
+
+2. Conductor/API service migration:
+When the conductor is updated, it will use the new DB api code to access the
+database.
+
+New APIs using host/nodename will be added to the PCI device object. Legacy
+API will be kept for backward compatibility. The implementations of legacy API
+will be changed to use new DB API. They will firstly translate the
+compute_node_id to host/nodename, and then use the new DB API to access the DB.
+
+The API layer will now use the new PCI device object code to access the DB.
+
+Some consideration needed to handle the stale data in DB. Stale data means the
+PCI device in DB that has not been updated by compute node yet. As the stale
+data is not updated, so the host/nodename columns are not populated. If
+conductor does not get any PCI devices using new DB API, it has to turn to
+legacy DB API, to check if any stale data left. If there are really any stale
+data, the conductor will update the stale data to avoid legacy API for this
+row anymore.
+
+However, because the conductor can't distinguish compute node w/o assignable
+PCI devices and compute node w/ stale data, it will always turn to legacy
+API for compute node w/o assignable PCI devices. This is a performance hit
+if most of the compute nodes have no assignable PCI devices.
+
+One solution is two-steps update. The first step is to update the conductor
+and support stale data. Later, after all compute nodes are updated in step 3,
+we will update the conductor again to remove the support the stale data, as
+it's assumed that all stale data has been updated by compute node and no stale
+data anymore.
+
+3. Compute migration:
+The updated compute node will use the new PCI device object code to access the
+conductor.
+
+4. Conductor service migration again
+As discussed in step 2, we need migrate the conductor service again, to
+remove the support of the stale data. This migration will not change the API
+layer, but it will not access stale data using legacy API, so the version will
+still be bumped.
+
+REST API impact
+---------------
+
+N/A.
+
+Security impact
+---------------
+
+N/A
+
+Notifications impact
+--------------------
+
+N/A
+
+Other end user impact
+---------------------
+
+N/A
+
+Performance Impact
+------------------
+
+One performance consideration here:
+The PCI device object legacy API need translate the compute_node_id to
+host/nodename before DB access. Hopefully this will be ok because it happens
+only temply when system update. After upgrade, it should not happen.
+
+Other deployer impact
+---------------------
+
+As stated in the "Data model impact" section, there will be a window that
+the PCI api can't get the request PCI devices.
+
+The two-step update will make the update process tricky, not sure if it will
+be easy for operator to handle it.
+
+Developer impact
+----------------
+
+N/A
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  yunhong-jiang
+
+
+Work Items
+----------
+
+PCI device object changes
+PCI device tracker changes
+Database changes.
+
+Dependencies
+============
+
+N/A
+
+Testing
+=======
+
+We will add unit test. As there is no gate for PCI device yet and the PCI 3rd
+party test is not published yet, we can't enhance the tempest.
+
+Documentation Impact
+====================
+
+N/A
+
+References
+==========
+N/A
diff --git a/specs/juno/proposed/normalize-scheduler-weights-2.rst b/specs/juno/proposed/normalize-scheduler-weights-2.rst
new file mode 100644
index 0000000..a2f0b9f
--- /dev/null
+++ b/specs/juno/proposed/normalize-scheduler-weights-2.rst
@@ -0,0 +1,191 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================
+ Normalize Weights (adapt weighers)
+====================================
+
+https://blueprints.launchpad.net/nova/+spec/normalize-scheduler-weights-2
+
+This blueprint is just a continuation of a blueprint already approved for
+Havana that missed feature freeze:
+https://blueprints.launchpad.net/nova/+spec/normalize-scheduler-weights
+
+Nova's scheduler used the raw values returned by each of the weighers to
+compute the final weight of a compute host or cell. This made difficult for
+operators to setup and use multipliers to stablish the relative importance
+between weighers since they weren't able to know the maximim weight for an
+object in advance, as it was a variable value. Moreover, in order to make a
+weigher prevail among others in some cases it was needed to artificially
+inflate either the weigher's returned value or the weigher's multiplier.
+
+The blueprint that was already implemented introduced weight normalization.
+This mechanism maps all the values returned from a weigher between 0 and 1,
+thus they have an even relative influence in the final weight for a host.
+Therefore, an operator knows a-priori that a host with a weight of 1 is the
+winner of that weighing process, and a host with 0 is the loser, making easier
+to setup multipliers in order to stablish the relative importance between
+weighers.
+
+This new blueprint is targeted into an homogenization, cleanup and improvement
+of existing weighers to profit from that normalization process.
+
+
+Problem description
+===================
+
+With weight normalization in place there are weigher options that are not
+needed anymore. These are focused on artificially inflating the values of the
+weigher so that it could prevail against any other weigher. This is not needed
+anymore, since the final maximum weight of the host will be '1' and is
+confusing since setting them has no effect at all.
+
+Lastly, some weighers can be improved and they can support the usage of
+relative values instead of the absolute ones. This is useful in weighers
+that take into account the usage of resources, such as the usage of RAM.
+
+
+Proposed change
+===============
+
+This change will review the existing weighers and adapt them to profit from the
+changes introduced by weight normalization in commit e5ba8494. In that commit
+the values returned by the weighers are normalized between 1.0 and 0.0. Before
+the raw values where used, therefore there were some weighers that used flags
+to artificiually inflate their values to make them prevail agaist another
+weigher that could return a large value. This is no longer true, so this kind
+of flags are misleading. Moreover, normalization opens the door to the usage
+of relative weights for some weighers (for example the RAM weigher).
+
+Namely, the weighers that will be adapted and the proposed changes are the
+following
+
+nova.cells.weights.mute_child
+-----------------------------
+
+Deprecate the usage of 'mute_weight_value' configuration option. Currently
+this option has no effect at all and thus it is misleading: The option can be
+set, but has no effect because the value is being normalized, so a muted child
+will get a final weight of 1.0, regardless of its value. This option must be
+deprecated for Juno, and removed for the K release.
+
+nova.scheduler.weights.ram
+--------------------------
+
+This weigher will be adapted to use relative values (i.e. percentage based).
+A new flag will be introduced to select this behaviour, thus it will remain
+compatible with its current status.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+A new configuration option will be included for the 'RAMWeigher':
+'ram_weight_percentage=False' so as to use the percentage of free RAM available
+instead of the usage of the absolute values.
+
+A configuration option will be set as deprecated, namely 'mute_weight_value'
+for the 'MuteChildWeigher' weigher. This configuration option was used to
+artificially inflate the returned weight for a cell that was unavailable, but
+it is not needed anymore and a multiplier should be used instead. Since the
+normalization process is already in place, this variable has no effect at all
+and a muted child will get a weight of 1.0 instead of the applied value.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  aloga
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+- Mark as deprecated 'mute_weight_value' from nova.cells.weights.mute_child
+- Modify RamWeigher so that percentages can be used instead of absolute values
+  (new flag 'ram_weight_percentage').
+- Update both the developer documentation and the Cloud Admin Guide with
+  clarifications about the scheduling and weighing process, and with
+  information about the behaviour of each of the individual weighers with the
+  updated options.
+
+Dependencies
+============
+
+This blueprint is a continuation of the following (already implemented):
+https://blueprints.launchpad.net/nova/+spec/normalize-scheduler-weights
+
+
+Testing
+=======
+
+There is no need for new tempest tests.
+
+
+Documentation Impact
+====================
+
+A new configuration option 'ram_weight_percentage' will be introduced, so
+that the RAM weighing will take into account the percentage of free ram
+instead of the absolute values.
+
+The 'mute_weight_value' option will be marked as deprecated so the
+operators need to adjust the 'mute_weight_multiplier' to a proper
+value if they weren't using the defaults.
+
+These configuration changes will be noted in the Release Notes and the Cloud
+Admin Guide will be updated along with this blueprint.
+
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/nova-api-extension-to-list-available-resources.rst b/specs/juno/proposed/nova-api-extension-to-list-available-resources.rst
new file mode 100644
index 0000000..6b5d2ba
--- /dev/null
+++ b/specs/juno/proposed/nova-api-extension-to-list-available-resources.rst
@@ -0,0 +1,194 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================================================
+Spec - nova-api extension to list available resources seen by scheduler
+=======================================================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/nova-api-extension-to-list-available-resources
+
+Currently, there is no easy way to check the current available resources on a
+given cluster. This work provides a nova client extension, and a corresponding
+nova api implementaion. Through this nova client command, admin users can
+easily get the current resource availability data, which is same as those
+seen by scheduler.
+
+Problem description
+===================
+
+Currently, there is no easy way to check the current available resources on a
+given cluster. If we only monitor the actual resource usage, there might be
+cases where real usage is < 75% but the scheduler sees no more capacity
+available to scheduling vm's. For example, we have a 10G RAM on a hypervisor.
+By running ‘proc’ on the compute node, we see the actual RAM usage is only 3G.
+With these data point, we cannot say the available RAM is 7G.
+ 
+By checking nova.compute_nodes table, the current free RAM is -3G. Assuming
+ram_allocation_ratio is 1.5, the correct available RAM seen by scheduler is 2G.
+The correct calculation is as follows:
+
+Physical RAM: 10G
+Free RAM: -3G
+Actual used RAM: 3G
+ram_allocation_ratio: 1.5
+Available RAM seen by scheduler: 10*1.5-(10-(-3)) = 2
+
+We need an easy way to get the current available resources on a given cluster
+for both alerting and planning purposes. We should capture the total cluster
+capacity as the scheduler would see it. This should take into consideration
+the overcommit ratios too.
+
+
+Proposed change
+===============
+
+We implement a nova api extension 'os-available-resources'. An admin user can invoke it
+through novaclient extension like 'nova available-resources'.
+
+All the available resources calculations are in the same way as scheduler.
+ram : https://github.com/openstack/nova/blob/master/nova/scheduler/filters/ram_filter.py
+core : https://github.com/openstack/nova/blob/master/nova/scheduler/filters/core_filter.py
+disk : https://github.com/openstack/nova/blob/master/nova/scheduler/filters/disk_filter.py
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+* New v2 API extension:
+
+   * Name: AvailableResources
+   * Alias: os-available-resources
+
+* Description : List current available resources for a cluster
+
+* Method type : GET
+
+* Normal http response code : 200
+
+* Expected error http response code(s)
+
+   * serviceUnavailable (503)
+   * forbidden (403)
+
+* URL for the resource : v2/{tenant_id}/os-available-resources
+
+* Parameters which can be passed via the url : None
+
+* JSON schema definition for the response data :
+
+.. code-block:: none
+
+   response := {'available_resources': nodes}
+   nodes := [node]
+   node := {key : value}
+   key := 'hypervisor_hostname' | 'vcpus' | 'vcpus_used' |
+          'cpu_allocation_ratio' | 'available_vcpus' |
+          'memory_mb' | 'free_ram_mb' |
+          'ram_allocation_ratio' | 'available_ram_mb' |
+          'local_gb' | 'free_disk_gb' |
+          'disk_allocation_ratio' | 'available_disk_gb 
+
+* Example :
+
+.. code-block:: none
+
+   request : GET /v2/{tenant_id}/os-available-resources
+   response : 
+       RESP: [200] {'date': 'Thu, 26 Jun 2014 00:04:47 GMT', 'content-length': 
+                    '371', 'content-type': 'application/json', 
+                    'x-compute-request-id': 
+                    'req-2daaca17-4008-48d5-b277-60c21659de01'}
+       RESP BODY: {"available_resources": 
+                   [{"available_disk_gb": 479.0, 
+                     "available_ram_mb": 30849.0, 
+                     "vcpus_used": 2, 
+                     "cpu_allocation_ratio": 16.0, 
+                     "available_vcpus": 254.0, 
+                     "hypervisor_hostname": "hostname.domain.com", 
+                     "memory_mb": 23638, 
+                     "vcpus": 16, 
+                     "disk_allocation_ratio": 1.0, 
+                     "free_disk_gb": 479, 
+                     "local_gb": 519, 
+                     "free_ram_mb": 19030, 
+                     "ram_allocation_ratio": 1.5}]}
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+There is also a novaclient extension to invoke this new api, which is
+'nova available-resource'.
+
+Performance Impact
+------------------
+None -- This new API is not introducing any new DB joins that would affect
+performance.
+
+Other deployer impact
+---------------------
+
+None
+
+----------------
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+weidu@yahoo-inc.com
+
+
+Work Items
+----------
+
+1. nova api extension implementation
+2. novaclient extension implementation
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Both unit and Tempest tests need to be created to ensure that the returned data
+is accurate for clusters.
+
+Documentation Impact
+====================
+
+Document the API extension (see "REST API impact" section for details).
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/nova-api-policy.rst b/specs/juno/proposed/nova-api-policy.rst
new file mode 100644
index 0000000..1fb62f7
--- /dev/null
+++ b/specs/juno/proposed/nova-api-policy.rst
@@ -0,0 +1,226 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================================
+Policy should be enforced at API REST layer where possible
+===========================================================
+
+https://blueprints.launchpad.net/nova/+spec/v3-api-policy
+
+This BP proposes enforcing all policy checks only at the Nova REST API
+layer. The extra permission checks at the lower layers of Nova will be
+removed. There will be consistent policy naming for the V2.1/V3 API and
+backwards compatibility will be retained for existing policy checks
+related to the V2 API so that the policy checks remain effectively the
+same, even though they may in practice be implemented at different points.
+
+This BP is already discussed at Icehouse summit:
+https://etherpad.openstack.org/p/icehouse-summit-nova-v3-api
+
+Problem description
+===================
+
+Currently policy permission checking is spread through the various
+levels of the Nova code.  There are also duplicated checks where
+effectively the same sort of policy check under different names is
+done at different levels such as both at the Nova REST API layer
+and the Nova Compute API layer. In addition to this there are also
+some cases where there are hard coded permission checks in the db
+layer.
+
+This situation makes it much harder for operators to correctly
+configure the policy settings that they want and because of the multi
+layered complexity of permission the implementation itself is more
+vulnerable to security bugs.
+
+A detailed description of the problem:
+
+* Permission checking spread in different level of nova code
+  Example:
+
+  * REST API layer: pause server "compute_extension:admin_actions:pause"
+  * Compute API layer: pause in compute API "compute:pause"
+  * DB layer: require_admin_context decorator for db API service_get
+
+* Duplicated policy checking for same API. Example:
+
+  * For server's pause action:
+  * REST API layer:
+        "compute_extension:admin_actions:pause": "rule:admin_or_owner"
+  * Compute API layer: "compute:pause": ""
+
+* Hard code policy check at db layer
+  Example: https://github.com/openstack/nova/blob/master/nova/db/sqlalchemy/api.py#L445
+  This means it won't have any effect after you modify the policy at REST
+  API layer, it always enforced as admin at db layer.
+
+Proposed change
+===============
+
+Enforce policy at REST API layer. Because REST API will access
+different internal APIs, like compute API, DB API or other internal API, the
+REST API layer is the place to enforce policy consistently.
+
+* Remove policy check from compute API layer
+
+  * For V2.1/V3 API, there will only be policy checks in the nova REST API
+    layer. There will be a parameter 'skip_policy_check' for compute API to
+    control whether doing the policy checks. For V2.1/V3 API,
+    skip_policy_check will be True.
+
+    https://review.openstack.org/#/c/100408/2/nova/api/openstack/compute/plugins/v3/shelve.py
+
+  * For Ec2, We also want to keep backwards compatibility. we will move the
+    compute API layer policy checking into REST API layer, the same as
+    V2.1/V3 API.
+
+  * For V2 API, we want to keep the backwards-compatibility. So we won't move
+    the compute API layer policy checking into REST API layer. We will set
+    compute API's parameter skip_policy_check to False, that means still
+    doing policy checking at compute API layer. It's because V2 API will be
+    depreciated. After V2.1 released, we needn't take risk of breaking existed
+    code.
+
+    https://review.openstack.org/#/c/100408/2/nova/compute/api.py
+
+* Remove hard-code permission check from db layer
+
+  * Example: https://review.openstack.org/#/c/73490/
+  * For the v3 API, we remove all the hard-code permission check from DB layer.
+    And we should ensure we have policy check at REST API layer.
+  * For the v2 API, we remove all the hard-code permission check from DB layer.
+    And with UpgradeImpact flags that notify deployer to update their
+    development configuration.
+  * Update policy configuration file to match the existing behavior.
+
+* Correct the policy rule name specification for the v2.1/v3 api
+
+  The policy naming style as below:
+    api:[extension_alias]:[action]
+
+  * We won't use 'compute' and 'compute_extension' to distingish the core and
+    extension API. Because the core API may be changed in the future.
+  * We also remove the API verison from the policy rule. Because after we have
+    Micro-version, the version will be changed often.
+
+* For volume related extensions, it won't be added any policy rule. Those
+  should be enforce by cinder API.
+
+* For network related extensions, we will doing same improvement for
+  nova-network. And skip any policy enforcement for neutron, that should
+  be enforced at neutron API. And because network related extensions didn't
+  port into v2.1/v3 yet, so it will be improved after those extensions ported.
+
+Alternatives
+------------
+The alternative is the status quo which is confusing for both deployers as
+well as developers having to maintain the current implementation
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+This BP will remove the policy permission checks in the compute API layer
+and DB layer.
+
+These patches will require very rigorous double checking and high
+quality reviews to ensure that security bugs are not introduced as the
+nova internal calls can be called from quite a few different code
+paths (Ec2, V2 API, V2.1/V3 API and other internals).
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+This BP will improve the error handling performance. Because the permission
+checking occurs at the API level rather than at a lower level in Nova less
+processing will occur before a request is rejected. Also potentially for newer
+versions of the API redundant policy checks are removed which will also
+improve performance.
+
+Other deployer impact
+---------------------
+
+Every effort will be made to keep all existing policy permission
+settings backwards compatible. Where this is not possible or too
+impractical the policy file changes required to keep the existing
+behaviour will be very clearly documented in upgrade documentation.
+
+
+Developer impact
+----------------
+
+When a developer adds a new REST API for nova policy permission checks
+will only be added at the REST API layer.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Alex Xu <xuhj@linux.vnet.ibm.com>
+
+Other contributors:
+  Ivan Zhu <bozhu@linux.vnet.ibm.com>
+  Ji Chen <jichenjc@cn.ibm.com>
+  Shuangtai Tian <shuangtai.tian@intel.com>
+  Chris Yeoh <cyeoh@au1.ibm.com>
+
+Work Items
+----------
+* Move compute API layer policy checking into REST API layer.
+* Remove the db layer permission checking.
+
+(Nova network related extension didn't move into V2.1 yet, will do
+after those extension ported)
+* Move nova network API layer policy checking into REST API layer.
+* Remove db layer policy checking for nova network related db API.
+
+Working list:
+https://etherpad.openstack.org/p/apipolicycheck
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+No tempest changes. All the policy checks tests will be test by unittest,
+as this is mostly an internal nova blueprint.
+
+Documentation Impact
+====================
+The db layer permission checks will be deleted, this should be document at
+upgrade documentation.
+
+All the policy should enforce at API layer, this should be document at
+developer documentation.
+
+For the consistent configuration of policy rule, this should be document at
+Cloud Admin documentation.
+
+References
+==========
+
+https://etherpad.openstack.org/p/icehouse-summit-nova-v3-api
diff --git a/specs/juno/proposed/nova-compute-multi-backend-support.rst b/specs/juno/proposed/nova-compute-multi-backend-support.rst
new file mode 100644
index 0000000..123d22a
--- /dev/null
+++ b/specs/juno/proposed/nova-compute-multi-backend-support.rst
@@ -0,0 +1,200 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================
+Nova-compute multiple backend drivers support
+=============================================
+
+https://blueprints.launchpad.net/nova/+spec/multi-back-ends-for-nova-compute
+
+This blueprint enables to run multiple nova-compute process from a given
+Node where nova-compute is installed.  And is similar to
+Cinder feature https://wiki.openstack.org/wiki/Cinder-multi-backend
+
+Problem description
+===================
+
+Following are the issues exist with nova-compute:
+
+1.node is under utilized: Nova-compute is designed to configure with only
+one type of nova compute driver like kvm, vmware, citrix,etc. And for
+hypervisors like kvm, nova-compute is installed on the hypervisor node
+itself and here CPU, memory will be shared load will be high as
+hypervisor is running on the nova-compute. But for the vmware
+hypervisors, nova-compute is installed on the separate node and its
+called proxy for nova-compute. So the load on the proxy-compute node will
+be comparatively lesser and it will be more efficient if nova-compute node
+is allowed to configure multiple compute proxy driver similar to the
+cinder-volume multi-backend architecture.
+
+2. Scalability is  big challenge: By enabling the multiple back-end
+drivers, it helps to improve the scale-ability of the given nova-compute as
+it can handle more number of instance action across multiple backend
+drivers.
+
+3. One nova compute per hyervisor: It helps to model one nova-compute per
+hypervisor as decided in juno release onwards.
+
+
+Proposed change
+===============
+
+Cinder already solved the same issue by means of multi-backend and same can
+be leveraged for nova-compute  to launch one nova-compute process per backend
+driver as detailed below:
+
+Update the nova.conf such that all common configuration will go in the
+[DEFAULT] section and for each backend drirver, create one section as below:
+
+[DEAFULT]
+#back_ends will have comma separated set of compute drivers section
+#Each compute driver could be same or different
+back_ends = vcdriver1, vcdriver2, xen1
+
+[vcdriver1]
+...
+
+[vcdriver2]
+...
+
+[xen1]
+...
+
+With these configuration in place, when nova-compute starts, it will spawn
+3 nova-compute one for each of the configured driver.
+
+NOTE: This helps to launch different hypervisor drivers from the same node
+similar to cinder.
+
+Functionality wise there is no change and its only the service launching
+process change to address the drawbacks mentioned above.
+
+Alternatives
+------------
+
+For example, Consider the exemple of vmware VC driver.
+one nova-compute was supporting as many clusters in the
+given vcenter. Assume that admin wants to use a vcenter with 30 clusters,
+then in icehouse, only one nova-compute is sufficient to handle all  clusters
+But in juno design summit, it was decided that, there should be only one vmware
+cluster per nova-compute. so to support 30 clusters, admin needs to follow one
+of the work-around given below:
+
+1. Run 30 nova-compute nodes(servers) and each node for one cluster. Usually
+admin runs nova-compute on high-end servers, and it is an under utilization of
+server as only one nova-compute runs per server.
+
+2. In a given high-end node, run 30 compute-node process and each process
+with independent copy of nova.conf. Here admin needs to run nova-compute from
+the command line, as there can be only upstart service named 'nova-compute'
+per node.
+
+In both the case, admin needs to maintain the same nova.conf parameters in all
+30 nodes except the vmware->cluser_name parameter. This is more of error prone
+and for updating any of a given parameter is cumbersome, as it needs to be
+Updated across 30 node's nova.conf.
+
+Apart from this issue, running 30 compute process would imply that 30 different
+cache directories are required on the datastore. This would effectively kill
+the storage. By using multi backend support, this can be avoided as nova lock
+mechanism is file based in the image management
+.
+
+Data model impact
+-----------------
+No change
+
+REST API impact
+---------------
+
+No change
+
+Security impact
+---------------
+
+No change
+
+Notifications impact
+--------------------
+
+No change
+
+Other end user impact
+---------------------
+
+No change
+
+Performance Impact
+------------------
+
+As each backend will be exposed as separate nova-compute process as
+existing today, there won't be no performance impact.
+
+Other deployer impact
+---------------------
+
+Only change is nova.conf as mentioned in the "proposed change" page.
+
+Developer impact
+----------------
+
+No Change.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+
+Primary assignee:
+  kanagaraj-manickam
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+1. Nova compute module update to launch one nova-compute per driver
+
+2. Enable drivers to retrieve the driver parameters from respective section
+of nova.conf.
+
+3. Update the queue name.
+a. Current format: compute.<nova-compute-node-host-name>.
+b. New format: computee.<nova-compute-node-host-name>.<backend-name>
+
+4. Make sure that scheduler is able to identify the nova-computes launched
+in the multi backend modes and status updates are happening properly.
+
+5. Make sure that "nova-manage service list" command provides the correct
+details as configured in the multi backend mode.
+
+
+Dependencies
+============
+
+It will refer the existing functionality of cinder-volume
+
+Testing
+=======
+
+Here unit testing of all compute.py, and its counter part nova.service module,
+compute manager modules should be updated with new test cases for the updated
+codes.Make sure existing tests successfully passed
+
+
+Documentation Impact
+====================
+
+As mentioned the "proposed change" section, the nova.conf will be updated.
+
+
+References
+==========
+cinder multi-backed: https://wiki.openstack.org/wiki/Cinder-multi-backend
diff --git a/specs/juno/proposed/nova-ephemeral-cinder.rst b/specs/juno/proposed/nova-ephemeral-cinder.rst
new file mode 100644
index 0000000..9d97ad1
--- /dev/null
+++ b/specs/juno/proposed/nova-ephemeral-cinder.rst
@@ -0,0 +1,137 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================
+Nova using ephemeral storage with cinder
+===========================================
+
+https://blueprints.launchpad.net/nova/+spec/nova-ephemeral-cinder
+
+As a cloud provider I would like to be able to serve nova's ephemeral storage
+by cinder. For certain deployment scenarios this would ease management and 
+maintenance effort when one entity manages all storage related items.
+
+Problem description
+===================
+
+As a cloud provider I would like to manage all the storage related activities
+through one entity: cinder.
+A lot of people want to ease management and maintenance of their virtual 
+machines and their storage.
+With cinder serving the needs for nova's ephemeral storage, glance image 
+storage and cinder block storage there would be one entity to manage all 
+the storage related items (also for blade systems without local storage). 
+Things like live-migration could be made easier.
+As there is development ongoing in glance to use cinder as image backend,
+we have to address the usage in nova to use ephemeral storage with cinder
+as backend in this blueprint.
+When talking about ephemeral, it has to be noted that the rootfs is also
+ephemeral.
+
+
+Proposed change
+===============
+
+Proposal
+- VM boot requested by the tenant
+- Nova tries to find an "ephemeral" volume for the VM
+- Today nova will use the local disk.
+This blueprint suggests to change nova to be able to use
+a cinder volume as ephemeral storage instead of local disk.
+
+See Implementation for more details.
+
+Alternatives
+------------
+
+The current block device mapping implementation can be used which is
+not that easy to use than the proposed solution here.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+Introduce a new configuration parameter force_ephemeral_on_cinder
+in nova.conf.
+When force_ephemeral_on_cinder=True, no ephemeral storage should be used.
+This is checked by nova before creating ephemeral storage, instead
+nova sends request to cinder to create a bootable volume in the size of the
+root disk size of the flavor. When the flavor defines a secondary ephemeral
+data disk, a second volume in the size of the second ephemral data disk shall
+be created by nova sending this request to cinder and shall be attached to 
+the virtual machine after it has booted.
+
+Assignee(s)
+-----------
+
+Primary assignee:
+ <eedten>
+ 
+
+Work Items
+----------
+TODO
+
+Dependencies
+============
+
+Related Topic:
+https://blueprints.launchpad.net/nova/+spec/nova-ephemeral-storage-with-cinder
+
+Testing
+=======
+
+Unit test code has to be added.
+
+Documentation Impact
+====================
+
+Configuration option has to be described.
+
+References
+==========
+
+Related Topic:
+https://blueprints.launchpad.net/nova/+spec/nova-ephemeral-storage-with-cinder
diff --git a/specs/juno/proposed/nova-vmware-vcdriver-nfs-image-copy.rst b/specs/juno/proposed/nova-vmware-vcdriver-nfs-image-copy.rst
new file mode 100644
index 0000000..f95448b
--- /dev/null
+++ b/specs/juno/proposed/nova-vmware-vcdriver-nfs-image-copy.rst
@@ -0,0 +1,144 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================
+Enable VmwareVCDriver with NFS glance image datastore
+======================================================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-vc-driver-nfs-datastore-image-copy
+
+This blueprint enables VmwareVCDrvier to copy the image from the glance by
+using NFS image store configured in the glance.
+
+Problem description
+===================
+
+While instance is booted from an image, the VmwareVCDriver copies the image
+from glance into 'vmware_temp' caching by means of glance REST API. It makes
+image copying process to be slower and consumes the nova-compute CPU and
+memory resource. This can be improved by using NFS image store on the glance as
+mentioned in below section.
+
+
+Proposed change
+===============
+Assume that admin has made following setup:
+1. Once Glance is installed with file system as image store, install and
+configure NFS on the glance image store
+2. On the vCenter cluster, which is consumed as compute node, create the NFS
+datastore from the glance image store configured in above step and let the
+name be 'glance_image_store'.
+
+Then following are the changes in the nova:
+1. In nova.conf, under [vmware] section, add a new config parameter named,
+'nfs_glance_datastore' and set the value as 'glance_image_store'
+2. update the driver code to copy the image from the NFS glance datastore
+instead of using glance API.
+
+Alternatives
+------------
+
+The existing logic is alternative to this blueprint
+
+Data model impact
+-----------------
+No change
+
+REST API impact
+---------------
+
+No change
+
+Security impact
+---------------
+
+No change
+
+Notifications impact
+--------------------
+
+No change
+
+Other end user impact
+---------------------
+
+No change
+
+Performance Impact
+------------------
+
+If both the glance store and the destination temporary directory are on NFS,
+then the copy can happen in vCenter itself, instead of going through
+nova-compute and its very faster approach compared to existing one.
+
+With MS Windows vCenter, followings are the improvement shown up during the
+testing phase:
+1. For 800 MB disk size, nova instance boot time has reduced from 16 minutes to
+6 minutes
+2. For 1 GB disk szie, nova instance boot time has reduced from 18 minutes to
+7.2 minutes
+3. For 1.5 GB disk szie, nova instance boot time has reduced from 21 minutes to
+8.1 minutes
+
+Other deployer impact
+---------------------
+
+No change
+
+Developer impact
+----------------
+
+No Change.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+
+Primary assignee:
+  kanagaraj-manickam
+
+Other contributors:
+  johnson.raj@hp.com
+
+Work Items
+----------
+
+1. Add new variable 'nfs_glance_datastore' to to nova.conf and set default to
+None.
+2. Add a new utility method in vim_utils to copy the image file from
+'nfs_glance_datastore' to 'vmware_temp'
+3. update the image copying logic to use the above defined new method
+if 'nfs_glance_datastore' is configured, otherwise use existing logic
+
+
+Dependencies
+============
+
+There is no dependency.
+
+Testing
+=======
+
+1. provide required unit tests to test the image copy with and without
+nfs_glance_datastore in place
+2. add required tempest test cases.
+3. Vmware minesweeper to be updated with required NFS setup in place in
+both glance and vCenter.
+
+
+Documentation Impact
+====================
+
+As mentioned the "proposed change" section, the nova.conf will be updated.
+
+
+References
+==========
+None
diff --git a/specs/juno/proposed/online-schema-changes.rst b/specs/juno/proposed/online-schema-changes.rst
new file mode 100644
index 0000000..6aa62d5
--- /dev/null
+++ b/specs/juno/proposed/online-schema-changes.rst
@@ -0,0 +1,282 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================
+Online Schema Changes
+=====================
+
+https://blueprints.launchpad.net/nova/+spec/online-schema-changes
+
+Make schema changes execute online (ie while services are running) when
+safely and semantically possible. This will allow operators to reduce the
+amount of downtime currently required during deploys by running most
+database schema changes when services are running.
+
+
+Problem description
+===================
+
+* All database migrations are currently required to be run offline.
+
+* Database migrations have historically been a source of lengthy downtime
+  during deployments.
+
+
+Proposed change
+===============
+
+The existing 'db sync' nova-manage command will be split into three
+separate phases:
+
+#. Expand. This would apply changes that are compatible with old running
+   code.
+#. Migrate. This would apply changes that are necessary to be run offline.
+#. Contract. This would apply changes that are compatible with new
+   running code.
+
+Those schema changes that can be safely and semantically applied while
+running online will be moved from sqlalchemy-migrate managed migrations
+to a new schema synchronizer. Data migrations will continue being managed
+by sqlalchemy-migrate.
+
+The schema synchronizer will generate DDL statements to bring an existing
+database (mostly) into line with the model defined in
+nova/db/sqlalchemy/model.py.
+
+Only schema changes that can be safely and semantically applied online
+will be done during the expand and contract phases. This will be an
+inclusive list that is intended to be initially conservative at first.
+The automatation can become more sophisticated in the future.
+
+Schema changes that will be automatically performed during expand:
+- Table creates
+- Column additions
+- Non-Unique Index additions
+
+Schema changes that will be automatically performed during migrate:
+- Unique Index additions/drops
+- Foreign Key additions/drops
+
+Schema changes that will be automatically performed during contract:
+- Table drops
+- Column drops
+- Non-Unique Index drops
+
+Those schema changes that are not listed will need to be manually made
+as part of an sqlalchemy-migrate migration. As an example, column type
+changes will need to be done as part of an sqlalchemy-migrate migration
+because it is generally not safe to be executed online and it is
+difficult to automate on all databases.
+
+The list of schema changes that can be safely applied online can be
+different depending on the database software and version used. As an
+example, MySQL will acquire a table lock for index additions in MySQL
+5.1. If the schema change isn't safe to run online for the running
+database, they would be executed during the migrate phase (ie offline).
+
+As a result, the list of schema changes that can run online is a subset
+of the list of schema changes that can be automated.
+
+Alembic will be used for it's DDL generating module. Alembic will not
+replace the current use of sqlalchemy-migrate as part of this blueprint.
+
+The migrate and contract phases would verify that the previous phases
+(expand in the case of migrate, expand and migrate in the case of
+contract) no longer need to be executed before continuing.
+
+This would be performed by generating the list of needed changes for
+the previous phases and verifying they are empty. This indicates the
+previous phases were either run or unnecessary.
+
+The existing 'db sync' command would be reimplemented to effectively
+run the 'db expand', 'db migrate' and 'db contract' commands. This
+would provide a backwards compatible and simpler way to upgrade the
+database for those that don't wish to run migrations online.
+
+A new '--dryrun' argument would print, instead of execute, each
+generated DDL statement. This could be used by database administrators
+to see what would be executed for a particular phase. These can be
+optionally executed manually if desired. The schema synchronizer will
+not generate that DDL statement since the running schema does not
+have that difference anymore.
+
+Also, a 'db compare' command would show the differences between the
+running schema and the model in an easier to read and more comprehensive
+format.
+
+
+Alternatives
+------------
+
+Splitting the existing single stream of migrations into three separate
+streams of migrations. This would allow some schema changes to be
+executed online.
+
+This limits the schema changes that can be safely executed online to
+that of the lowest common denominator of databases supported by Nova.
+
+This would also require changes to sqlalchemy-migrate to be able to
+manage seperate streams of migrations.
+
+Alembic has an autogenerate module that can compare a model against
+a running schema and generate a list of differences. It currently has
+a variety of deficiencies in ordering of operations (wrt to foreign
+keys), incomplete features (TODOs for constraints, etc) and database
+specific issues (type aliasing in MySQL confuses it).
+
+In the future, after the problems with Alembic and SQLAlchemy are
+resolved, the autogenerate module could be a good engine to use.
+However, it's unclear when it will be ready and even if it was ready
+today, it would not be possible to use an extremely new version of
+either module in Nova so quickly.
+
+
+Data model impact
+-----------------
+
+The existing model needs to be brought in line with changes migrations
+make. These are limited to a handful of cases:
+
+- PostgreSQL index name limitations
+- PostgreSQL Enum type naming
+- MySQL index length restrictions
+- Foreign key names
+
+
+REST API impact
+---------------
+
+None
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+None
+
+
+Performance Impact
+------------------
+
+Running online DDL changes can affect the performance of a running system.
+This is optional and is only done when the deployer explicitly requests
+it.
+
+This can mitigated by the deployer by scheduling the expand and contract
+phases to be run during periods of low activity. The expand phase can
+be run an arbitrary amount of time before the migrate phase. Likewise,
+the contract phase can be run an arbitrary amount of time after the
+migrate phase.
+
+
+Other deployer impact
+---------------------
+
+The new 'db compare' command provides a means of viewing differences
+between the current schema and the model.
+
+The expand and contract phases are optional. If not explicitly run
+otherwise, the existing call to 'nova-manage db sync' would execute all
+necessary schema changes, as is the existing behavior.
+
+Those deployers that want to take advantage of the online schema changes
+will need to run the 'db expand', 'db migrate' and 'db contract' commands
+at the appropriate steps in their deployment process.
+
+Deployers that have made local schema changes (extra indexes, columns,
+tables, etc) will need to update the model to ensure those additions
+aren't dropped during the contract phase.
+
+
+Developer impact
+----------------
+
+Since the model will now be used as the authoritative source of
+information for schema changes, it is required to keep it updated
+as new sqlalchemy-migrate migrations are added. A new unit test will
+enforce this.
+
+Less work developing database migrations. Most schema changes will be
+handled automatically by the schema synchronizer.
+
+No more migration compaction. The initial creation of tables for a
+database is handled completely by the schema synchronizer.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  johannes.erdfelt
+
+Other contributors:
+  None
+
+
+Work Items
+----------
+
+- Bring model into line with existing migrations
+- Implement schema synchronizer
+- Implement new 'expand', 'migrate', 'contract' and 'compare' commands
+  to 'nova-manage db'
+- Reimplement 'db sync' to use the new expand, migrate and contract calls
+- Drop schema changes from existing migrations that are now done
+  automatically as part of the schema synchronizer
+
+
+Dependencies
+============
+
+The schema synchronizer is implemented on top of alembic for its DDL
+generating functionality. This is already in the OpenStack global
+requirements list, but will be a new addition for Nova.
+
+
+Testing
+=======
+
+No tempest tests will be added since tempest does not do any upgrade
+testing.
+
+Grenade currently tests upgrades from older versions of Nova. No changes
+are necessary since it currently uses 'nova-manage db sync' and that
+will ensure that the expand, migrate and contract steps are executed.
+
+turbo-hipster tests upgrades using production database snapshots. No
+changes are necessary since it currently uses 'nova-manage db sync' as
+well.
+
+
+Documentation Impact
+====================
+
+Documentation will need to be updated to include the new 'expand',
+'migrate', 'contract' and 'compare' commands to 'nova-manage db'.
+
+Release Notes will need to be updated to warn that the model will need
+to be updated with local schema changes.
+
+
+References
+==========
+
+https://etherpad.openstack.org/p/juno-nova-live-upgrade
diff --git a/specs/juno/proposed/online-volume-extend-extension.rst b/specs/juno/proposed/online-volume-extend-extension.rst
new file mode 100644
index 0000000..7befd08
--- /dev/null
+++ b/specs/juno/proposed/online-volume-extend-extension.rst
@@ -0,0 +1,230 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================
+Enable online extend of in-use Cinder volumes
+=============================================
+
+https://blueprints.launchpad.net/nova/+spec/online-volume-extend-extension
+
+Online volume extend allows volumes that are attached to instances (in-use)
+to be extended without first detaching them. This spec defines a Nova
+extension that is required by Cinder to issue an online volume extend,
+specifically this requires iSCSI rescan and the ability to return the size of
+the volume block device from the compute host that the instance is running on.
+
+Problem description
+===================
+
+Currently Cinder volumes must be detached from instances before they can be
+extended. In cases where services are using the volume (e.g., a Trove
+instance running MySQL), these services must be stopped, causing downtime
+for the user. For Cinder to extend in-use volumes, it needs the compute host
+that the instance is running on to be able to issue an iSCSI rescan and
+return the size of the volume block device so Cinder can poll until the block
+device is actually the new size. Online extend also requires support from the
+underlying hypervisor and volume backend.
+
+Proposed change
+===============
+
+To support Cinder online extend this spec proposes a corresponding Nova
+extension to run the following on the underlying compute host (via the virt
+driver):
+
+* Rescan iSCSI. This will allow the underlying compute host to see the new
+  size of the volume block device after it has been extended by Cinder.
+
+* Get volume block device size. This will allow Cinder to poll until the
+  underlying block device is actually the new size on the compute host before
+  Cinder completes the online extend.
+
+This initial proposal only covers the minimum required components for some
+hypervisors, specifically those that do not require any additional hooks
+to expose the updated size of the volume block device inside instances.
+Additional work would be required to support this extension for other virt
+drivers (see additional comments in Developer impact).
+
+The Cinder portion of the online extend feature is proposed in [1]_.
+
+Alternatives
+------------
+
+The main alternative is simply to detach the volume for extend, which is
+already supported, and requires services using the volume to be offline.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+Add os-online_extend_volume extension with the following API calls:
+
+PUT v2/{tenant_id}/servers/{server_id}/os-online_extend_volume
+
+* Issues an iSCSI rescan on the compute host with the instance and volume
+* Normal http response code: 202
+* Expected error http response code: 404: Instance not found
+* JSON schema definition for the body data::
+
+    {
+        "volume_id": "string"
+    }
+
+* JSON schema definition for the response data: None
+* Example use case:
+
+  Request:
+
+  PUT v2/fa83baf8315542229f39af36aa643206/servers/822b2c1b-e4c4-4b12-\
+      b28e-35ca7f036edb/os-online_extend_volume
+
+  Body::
+
+    {
+        "volume_id": "e5f556d2-3cfe-4ef1-9ddd-6ddb07d9b9b4",
+    }
+
+  Response: HTTP 202/Accepted
+
+* Policy changes: Requires admin API access or it must be explicitly
+  enabled by an admin in policy.json
+
+GET v2/{tenant_id}/servers/{server_id}/os-online_extend_volume/{volume_id}
+
+* Get the block device size of a volume on the compute host where it is
+  attached to a server
+* Normal http response code: 200
+* Expected error http response code: 404: Instance not found
+* JSON schema definition for the body data: None
+* JSON schema definition for the response data::
+
+    {
+        "blockdev": {
+            "volume_id": "string",
+            "bytes": integer
+        }
+    }
+
+* Example use case:
+
+  Request:
+
+  GET v2/fa83baf8315542229f39af36aa643206/servers/822b2c1b-e4c4-4b12-\
+      b28e-35ca7f036edb/os-online_extend_volume/e5f556d2-3cfe-4ef1-9ddd-\
+      6ddb07d9b9b4
+
+  Response: HTTP 200/OK
+
+  ::
+
+    {
+        "blockdev":{
+            "volume_id": "e5f556d2-3cfe-4ef1-9ddd-6ddb07d9b9b4",
+            "bytes": 4294967296
+        }
+    }
+
+* Policy changes: Requires admin API access or it must be explicitly
+  enabled by an admin in policy.json
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+These Nova operations are included in novaclient for Cinder, however, users
+will simply perform an online extend via Cinder if a deployer has it enabled.
+
+Performance Impact
+------------------
+
+Both of the calls in this extension require calls to the underlying compute
+host. Rescan is non-blocking and getting block device size is blocking.
+However, these calls should only be made when a user has issued an online
+extend from Cinder.
+
+Other deployer impact
+---------------------
+
+The online extend extension is disabled by default for users and therefore
+it must be explicitly enabled in Cinder and Nova by a deployer if they
+have the required hypervisor and volume backend support.
+
+Developer impact
+----------------
+
+In addition to backend volume support, other virt drivers may support this
+extension as-is if they do not require any additional mechanisms to notify
+running instances of updated block device sizes. Libvirt versions that
+support block resize would likely need an additional call to notify the
+running instance of the new volume block device size.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  paulvmarshall
+
+Work Items
+----------
+
+Add online extend extension with two API calls: rescan and get block device
+size.
+
+Add rescan to compute manager, compute rpc api, and the virt driver interface.
+
+Add get block device size to compute manager, compute rpc api, and the virt
+driver interface.
+
+Add unit tests for both rescan and get block device size.
+
+Add rescan and get block device size calls to novaclient.
+
+
+Dependencies
+============
+
+The initial implementation requires backend volume support as well as
+hypervisors that do not require additional mechanisms to expose updated
+block device sizes to instances, such as OpenVZ:
+https://github.com/stackforge/openvz-nova-driver
+
+
+Testing
+=======
+
+The initial implementation of this extension is disabled by default and
+requires special hypervisor and volume backend support (e.g., OpenVZ and HP
+LH SAN), therefore, it will only be tested by unit tests and non-gate 3rd
+party tests.
+
+
+Documentation Impact
+====================
+
+None. Documentation will be included for Cinder.
+
+
+References
+==========
+
+.. [1] https://blueprints.launchpad.net/cinder/+spec/inuse-extend-volume-extension
diff --git a/specs/juno/proposed/only-allow-admins-to-do-local-delete.rst b/specs/juno/proposed/only-allow-admins-to-do-local-delete.rst
new file mode 100644
index 0000000..c7e57fe
--- /dev/null
+++ b/specs/juno/proposed/only-allow-admins-to-do-local-delete.rst
@@ -0,0 +1,153 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================================
+Only allow admins to delete VMs from down compute nodes
+========================================================
+
+https://blueprints.launchpad.net/nova/+spec/only-allow-admins-to-do-local-delete
+
+Currently any user is allowed to do a local delete (delete a VM from a
+compute node which is currently down). This is useful functionality but
+we've seen first hand that there is a potential issue. We locally patch
+the code to restrict local deletes to only admins. Since we find this
+change extremely useful, it may be of benefit to the community.
+
+Problem description
+===================
+
+Consider the following scenario.
+
+* A compute node goes down for some unknown cause. All of the VMs currently
+  running on it are unavailable to users.
+
+* The users do 'nova delete' on their VMs. The local delete processing
+  cleans up things and releases the IPs from those VMs back into the pool.
+
+* New VMs are started which may reuse some or all of those IPs.
+
+* Through the valiant efforts of the operations team, the compute node is
+  brought back and all of the VMs hosted on it are restarted.
+
+* We now have multiple VMs with the same IP.
+
+Proposed change
+===============
+
+The local delete functionality is still extremely important to have in
+order to clean up VMs from down compute nodes. Our proposal (and what
+we do with a local patch) is to only allow admins to use it since they're the
+ones who know if the compute node has a chance of coming back or not.
+
+The exact chance would be to modify nova/compute/api.py before _local_delete
+is called to check if the current user is an admin. If so, go ahead and
+do the local delete as before. If the user is not an admin, then clear
+the current task state and raise an error stating that the delete can't
+happen because the compute node is down, so please contact your local
+administrator.  Note that as part of raising the error, a call to
+QUOTAS.rollback will happen to make sure that the delete quota change does
+not go through.
+
+
+Alternatives
+------------
+
+We could leave it as it is, but as we've seen, this can be a dangerous
+situation.  We could provide a config setting to retain the old behavior
+for folks who want to go ahead and let their users keep the ability to
+do local deletes.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+A new error message will be generated to tell a non-admin user that
+they aren't being allowed to delete their VM because the compute node
+is down and that they should contact their administrator.
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+As mentioned above, a new config setting could be added to keep the old
+behavior for folks who like living on the edge. If this config setting is
+added, I strongly recommend that the default setting be to use the new
+more restrictive (and safer) behavior.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  krt
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* Modify nova/compute/api.py
+* Modify/write unit tests (if possible)
+
+
+Dependencies
+============
+
+* None
+
+Testing
+=======
+
+* To prevent from breaking any existing local delete tests, we'll need to
+  make sure that the user being used is an admin, or we that the config
+  setting is set to use the old behavior.
+
+* If possible, a new test for non-admin user doing local delete will be
+  written and check that the expected error is returned.
+
+
+Documentation Impact
+====================
+
+We may need to add a note to nova delete that it may be rejected by non-admin
+users if the compute node is down.
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/pci-device-capability-aware-scheduling.rst b/specs/juno/proposed/pci-device-capability-aware-scheduling.rst
new file mode 100644
index 0000000..1837055
--- /dev/null
+++ b/specs/juno/proposed/pci-device-capability-aware-scheduling.rst
@@ -0,0 +1,167 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================
+PCI and PCIe device capability aware scheduling
+===============================================
+
+https://blueprints.launchpad.net/nova/+spec/pci-device-capability-aware-scheduling
+
+In most Data Centers they have a heterogeneous compute host environment
+which is sourced from various manufacturers and these hosts contain various
+components such as PCI and PCIe devices. As the data center expands and more
+hosts are added and removed over a period of time the PCI cards on these
+compute hosts will differ from different vendors and different versions
+will support different capabilities over a period of time in this
+heterogeneous environment. Similarly not all compute hosts
+in the data center will have the same PCI cards throughout and
+typically some hosts will carry specialized PCIe devices to support hardware
+acceleration services such as encryption etc. while others will not.
+
+When a tenant requests for an instance to be provisioned there are certain
+applications that benefit and in some cases require being placed on a
+compute host that supports a specific PCI/PCIe capabilities and
+in such a case nova db should contain the inventory of which PCI/PCIe
+devices are installed in which hosts to assist in this fine grain placement
+of an instance where the application requires specific capability such as
+High Order packet throughput on a Network Interface Card (NIC) card as an
+example.
+
+
+Problem description
+===================
+
+As an End User I want to have my instances scheduled on a host where some
+PCI/PCIe capability exists so that I can get better performance.
+
+As a Deployer I want to automate the discovery and classification of the
+capabilities of my compute hosts so that I don't have to manually configure
+nova to use the capability information for instance scheduling.
+
+Use Case
+--------
+
+* Deployer installs couple of new compute blades with 40G NICs
+* Nova detects the new capabilities of the new hosts and creates new
+  host aggregate for 40G NICs capability and puts the new hosts
+  into the proper aggregate
+* Deployer checks the new host aggregates and decide which capability
+  sets he want to publish for his tenants. He creates the proper
+  flavors.
+* Tenant uses the new flavors to boot instances
+* Nova scheduler will use the existing AggregateInstanceExtraSpecsFilter
+  to match the requirements with capabilities
+
+
+Proposed change
+===============
+
+Introduce a new nova-capability agent that runs at the start up of the
+compute host. This agent will use the standard Linux tool set to discover
+available PCI/PCIe devices and their capabilities. In this blueprint we will
+cover only NICs but later on the idea can be extended with other devices.
+In this blueprint we will handle the existence of such devices and in case
+of NICs we will use the link speed of the device as a capability,
+but later on the solution can be extened with other capabilities.
+
+This agent will use nova public API to put the compute host automatically into
+host aggregates according to the discovered capabilities. Also the agent will
+create new host aggregate if it finds a totally new host capability.
+
+
+Alternatives
+------------
+
+Flavor creation is still a manual step. We can avoid this manual step by
+allowing the tenants to express the requirements via scheduler hints.
+This alternative is not choosen because deployer would loose the control
+of what kind of capability sets are used. Also deployer wouldn't have
+out of the box billing support in this sollution.
+
+Current proposal uses nova public API. Alternatively the new nova-capability
+agent could use the message bus and nova internal API to achieve the same
+goal.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+New nova-capability agent might need sudo right on the compute hosts to be able to
+discover every capability. Needs further investigation.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Large amount of host aggregates might affect scheduler perfomance
+
+Other deployer impact
+---------------------
+
+The new nova-capability agent needs to be started on the compute host
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  balazs-gibizer
+
+Other contributors:
+  alan-kavanagh
+  liyi-meng
+
+Work Items
+----------
+
+To be defined!
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+To be defined!
+
+Documentation Impact
+====================
+
+OpenStack Intallation Guide is impacted with the new service
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/pci-extra-info.rst b/specs/juno/proposed/pci-extra-info.rst
new file mode 100644
index 0000000..4b3b9b2
--- /dev/null
+++ b/specs/juno/proposed/pci-extra-info.rst
@@ -0,0 +1,193 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================
+PCI passthrough Extra information of PCI device
+===============================================
+
+https://blueprints.launchpad.net/nova/+spec/pci-extra-info-provisioning
+
+for better deploying and use Nova PCI passthrough, there is need a facility
+to provisioning extra PCI device information to Nova besides the vendor_id
+product_id.
+
+Problem description
+===================
+
+in current PCI passthrough the pci device only have few standard pci property
+like the product_id, vendor_id. there is lack of a method to provisioning the
+PCI device's related information to cloud. this make hard to use the pci
+passthrough for tenant.
+
+
+use case this spec try to cover:
+
+  * Tenant want a GPU which support the OpenGL and DirectX 11.
+    Tenant really don't care the vendor_id and product_id only if the GPU
+    suport the OpenGL and DirectX 11.
+
+    current solution:
+    admin need to create a alias which contain all posssible vendor/product.
+    i.e.
+    pci_alias={"vendor_id":"8086", "product_id":"1520", "name":"DirectX11"}
+    pci_alias={"vendor_id":"10DE", "product_id":"0028", "name":"DirectX11"}
+    note: same name's alias means 'OR' operation.
+
+    proposal for better solution:
+    1. when depoly, attach to each GPU with it's suported DirectX and OpenGL
+    capability
+    pci_information = [{"vendor_id":"8086", "product_id":"1520"},
+    {"e.DirectX11":"True"} ]
+    pci_information = [{"vendor_id":"10DE", "product_id":"0028"},
+    {"e.DirectX11":"True"} ]
+    2. admin just create a alias to request the OpenGL and DirectX 11
+    directly.
+    pci_alias={"name":"DirectX11", "e.DirectX11":"True"}
+
+  * Tenant want a PCI offloading card which suport 3DES, AES, and ssl
+    Tenant really don't care the vendor_id and product_id only if the PCI card
+    suport the 3DES, AES, and ssl algorithm.
+    Note: this use case share same solution with first one.
+
+
+Proposed change
+===============
+
+to address the current pci passthrough problem, this spec introduce a flexable
+extra information provisioning for pci device.
+
+PCI devices have PCI standard properties like address (BDF), vendor_id,
+product_id, etc, other none standard pci device infomation like GPU supported
+DirectX version,PCI offloading card supported algorithms are the extra
+information of a pci device.
+
+to support extra information provisioning, extra infomation based scheduling,
+here are changes to current pci passthrough.
+
+1. add new config option for pci support extra information
+   currently compute node the whit-list define a set of filters. PCI device
+   passed filtering will be available for allocation.
+
+   for supporting attach extra info to pci device, add a new configuration item
+   pci_information:
+
+  * pci_information = [ { pci-specs } ,{pci-extra-attrs } ]
+  * pci-specs is a dict of { string-key: string-value } pairs , this only match
+    standard device properties, like vendor_id, address, product_id,etc.
+  * pci-extra-attrs is a dict of { string-key: string-value } pairs. these
+    values can be arbitrary. all this extra attrs will be store in the pci
+    device table's extra info field. and the extra attrs should use this naming
+    schema:
+    e.attrname
+
+2. PCI stats report PCI device pools base on both standard properties and extra
+   information.
+   current PCI stats report device pools based on vendor_id, product_id and
+   pf_address.
+   The stats pool should contains all the PCI properties which pci_information
+   provided.
+
+3. Currently PCI alias support only vendor_id/product_id as keys, and we need
+   enhance it to support all keys from pci_information.
+
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+* Does this change involve cryptography or hashing?
+
+* Does this change require the use of sudo or any elevated privileges?
+
+* Does this change involve using or parsing user-provided data? This could
+  be directly at the API level or indirectly such as changes to a cache layer.
+
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+
+Aside from the API, are there other ways a user will interact with this
+feature?
+
+* Does this change have an impact on python-novaclient? What does the user
+  interface there look like?
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  yongli.he@intel.com
+
+
+Work Items
+----------
+
+None
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+the pci passthrough now need run OS on bearmetal compute node, not a VM based
+test environment, there is needed to providing 3rd test platform for PCI
+passthrough testing.
+
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/pci-hotplug-juno.rst b/specs/juno/proposed/pci-hotplug-juno.rst
new file mode 100644
index 0000000..e812e4d
--- /dev/null
+++ b/specs/juno/proposed/pci-hotplug-juno.rst
@@ -0,0 +1,218 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Libvirt support PCI passthrough hotplug
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/pci-hotplug
+
+Currently, openstack only support pci passthrough statically before the
+instance is booted.
+This blueprint is aim to provide the ability to allow user to hotplug
+the pci passthrough device when the instance is running for libvirt.
+
+Problem description
+===================
+
+* An instance called "green", "green" has one or more passthrough pci device.
+  Someday, one pci device becomes into failed. User can unplug the passthrough
+  pci device and plug a normal one.
+
+* An instance called "blue", "blue" has a passthrough pci device(nic).
+  Someday, we found one nic is not enough. We need more nics for more
+  network throughout. User can plug another passthrough pci device(nic).
+  Or maybe, we found two nics is a waste for the instance. The instance
+  doesn't has so large network throughout, we can unplug one passthrough pci
+  device(nic).
+
+Proposed change
+===============
+Initially, only the libvirt driver will support this function, and
+only with qemu/kvm as the hypervisor.
+
+* api to hot plug/unplug pci passthrough devices for an instance
+* api to query attached pci passthrough devices for an instance
+* add plug/unplug RPC method
+* add plug/unplug method in the libvirt driver 
+
+Alternatives
+------------
+None
+
+Data model impact
+-----------------
+* add two new columns in table "pci_devices"
+  "inner_address" : the device address in the instance like "0000:00:00.0"
+  "attachment_id" : an uuid to identify a pci attachment of the instance
+
+* migration
+  upgrade : we need to add the two new columns
+  downgrade : we need to remove the two new columns
+
+* initial the column data
+  "attachment_id" can be generated in migration. It's just an UUID for
+  user to specify to unplug which attachment of the pci passthrough device
+  "inner_address" is better to be updated when an pci passthrough device
+  is pluged/unpluged to the instance or on instance startup
+
+REST API impact
+---------------
+API for hotplug a pci device
+
+V2 API specification:
+POST: v2/{tenant_id}/servers/{server_id}/os-pci_attachments
+V3 API specification:
+POST: v3/servers/{server_id}/os-pci_attachments
+
+JSON Request::
+    {
+        "pciAttachment":{
+            "pci_passthrough:alias":"a1:2"
+
+        }
+
+    }
+
+JSON Response::
+    {
+        "pci_devices": [
+            {
+                "attachment_id":"a26887c6-c47b-4654-abb5-dfadf7d3f803",
+                "serverId": "4d8c3732-a248-40ed-bebc-539a6ffd25c0",
+                "dev_id":"pci_0000_02_00_0",
+                "address":"0000:02:00.0",
+                "inner_address":"0000:03:00.0",
+                "vendor_id":"8086",
+                "product_id":"10c9"
+            },{
+                "attachment_id":"a26887c6-c47b-4654-abb5-dfadf7d3f704",
+                "serverId": "4d8c3732-a248-40ed-bebc-539a6ffd25c0",
+                "dev_id":"pci_0000_02_00_1",
+                "address":"0000:02:00.1",
+                "inner_address":"0000:03:01.0",
+                "vendor_id":"8086",
+                "product_id":"10c9"
+
+            }
+
+        ]
+
+    }
+
+API for hotunplug a pci device
+
+V2 API specification:
+DELETE: v2/{tenant_id}/servers/{server_id}/os-pci_attachments/{attachment_id}
+V3 API specification:
+DELETE: v3/servers/{server_id}/os-pci_attachments/{attachment_id}
+
+HTTP response codes:
+v2:
+Normal HTTP Response Code: 200 on success
+v3:
+Normal HTTP Response Code: 202 on success
+
+API for query plugged pci devices of an instance
+
+V2 API specification:
+GET: v2/{tenant_id}/servers/{server_id}/os-pci_attachments
+V3 API specification:
+GET: v3/servers/{server_id}/os-pci_attachments
+
+JSON Response::
+    {
+        "pciAttachment": [
+            {
+                "attachment_id":"a26887c6-c47b-4654-abb5-dfadf7d3f803",
+                "serverId": "4d8c3732-a248-40ed-bebc-539a6ffd25c0",
+                "dev_id":"pci_0000_02_00_0",
+                "address":"0000:02:00.0",
+                "inner_address":"0000:03:00.0",
+                "vendor_id":"8086",
+                "product_id":"10c9"
+
+            },{
+
+                "attachment_id":"a26887c6-c47b-4654-abb5-dfadf7d3f803",
+                "serverId": "4d8c3732-a248-40ed-bebc-539a6ffd25c0",
+                "dev_id":"pci_0000_02_00_1",
+                "address":"0000:02:00.1",
+                "inner_address":"0000:03:01.0",
+                "vendor_id":"8086",
+                "product_id":"10c9"
+
+            }
+
+        ]
+
+    }
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+Add notification for the operation of plug/unplug pci passthrough device.
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+Initially, only the libvirt driver will support this function, and
+only with qemu/kvm as the hypervisor.
+
+Developer impact
+----------------
+Need to add two APIs in driver.
+* plug a pci device to instance
+* unplug a pci device from instance
+This blueprint will implement them for libvirt.
+
+Data to pass.
+* plug RPC method : instance id, pci alias name and number of pci devices
+* unplug RPC method : instance id and pci device's attachment id
+* plug driver method : instance id and pci device address
+* unplug driver method : instance id and pci device address in instance
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  boh.ricky
+
+Work Items
+----------
+* query instance's pci passthrough device
+* hotplug pci passthrough device to an instance
+* hotunplug pci passthrough device to an instance
+
+Dependencies
+============
+* https://blueprints.launchpad.net/nova/+spec/pci-passthrough-base
+* https://wiki.openstack.org/wiki/Pci_passthrough
+
+Testing
+=======
+unit tests.
+
+Documentation Impact
+====================
+Need to document the APIs.
+
+References
+==========
+None
diff --git a/specs/juno/proposed/pcs-support.rst b/specs/juno/proposed/pcs-support.rst
new file mode 100644
index 0000000..e9fdcf9
--- /dev/null
+++ b/specs/juno/proposed/pcs-support.rst
@@ -0,0 +1,117 @@
+=====================================================
+Parallels Cloud Server support in nova/libvirt driver
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/pcs-support
+
+This specification proposes to make changes in nova/libvirt driver in order to
+support Parallels Cloud Server (http://www.parallels.com/products/pcs/).
+
+Problem description
+===================
+
+Parallels Cloud Server (PCS) is a virtualization solution, which enables
+hosters to use container and hypervisor virtualization over the same API.
+PCS is supported by libvirt, but OpenStack can't use it because of some
+differences in domains configration and supported features.
+
+
+Proposed change
+===============
+
+To implement this feature we need to make a set of small changes in
+nova/libvirt driver so that it will create PCS domains correctly. The end
+user will be able to configure nova to use PCS by setting libvirt.virt_type
+option to "parallels".
+
+Alternatives
+------------
+
+The alternate way is to use separate nova driver
+https://github.com/parallels/pcs-nova-driver
+
+pros:
+* There is no middle layer between OpenStack and PCS, pcs-nova-driver uses
+PCS's python API.
+* Changes in pcs-nova-driver will not affect nova/libvirt's code.
+
+cons:
+* It's hard to maintain out-of-tree driver.
+* pcs-nova-driver is unlikely to be accepted into nova's tree.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+In order to use PCS as Openstack compute node, deployer must install
+nova-compute packages on PCS node and set libvirt.virt_type config option
+to "parallels".
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  dguryanov
+
+Work Items
+----------
+
+To be filled
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+To be filled
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+Parallels Cloud Server: http://www.parallels.com/products/pcs/.
diff --git a/specs/juno/proposed/per-flavor-quotas.rst b/specs/juno/proposed/per-flavor-quotas.rst
new file mode 100644
index 0000000..b2d061f
--- /dev/null
+++ b/specs/juno/proposed/per-flavor-quotas.rst
@@ -0,0 +1,350 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================
+per-flavor-quotas
+=================
+
+https://blueprints.launchpad.net/nova/+spec/per-flavor-quotas
+
+This blueprint allows the operators to define quotas per flavor.
+
+
+Problem description
+===================
+
+Currently when an operator has to set up quotas, is not able to define the
+quotas based on the number of a given flavor, for instance, define the user
+quotas as 3 x m1.medium flavor.
+
+Then, if a user boots a new instance and the limit for the flavor associated
+to this is exceeded, the system won't create this instance.
+
+This blueprint comes from the operators feedback:
+https://etherpad.openstack.org/p/operators-feedback-mar14 -> line 448
+
+Use Cases:
+
+1. The operator wants to apply a default set of quotas to all flavors in
+   the system which are applied in addition to any other quotas to control the
+   profile of flavors that users can create. For example:
+
+   . ram: 16284
+
+   . m1.tiny: 32
+
+   . m1.small: 8
+
+   . m1.meduim: 2
+
+   . m1.large: 1
+
+2. The operator wants to be able to introduce a new flavor and control
+   access to a small group of users (in a way more granular than just
+   allow/deny via an ACL). For example you might during the early access stage
+   want to limit to 1 or 2 instances only of the new flavor). So the steps here
+   are like:
+
+   1. There are no quota flavors defined (i.e. current state and default state
+      after this change with default quota for flavors = -1.
+   2. I define a new flavor with a default quota for this flavor only of 0.
+      Other flavors remain unlimited.
+   3. I define quota values for the new flavor just for a subset of projects.
+   4. When I want to make it generally available, I either remove the few per
+      project quotas of the early adopters, and/or apply a default quota for
+      the new flavor.
+
+Proposed change
+===============
+
+To address this request I propose to add a new resource for the flavors that
+can be use when the operator sets up the user quotas.
+
+So when a user attempts to create an instance, the quotas system will check if
+there is a limit for the involved flavor and will block the creation in case
+the number of instances already created + 1 is bigger than the current quota.
+
+As any other resource, the unlimited value will be represented with (-1).
+
+The quota resources based on flavors will be named "flavor_<flavor_id>". The
+resource named 'flavor' which will be defined as default in the quota-class,
+will be used as default for flavors which don't have a quota defined.
+
+As any other resource any flavor_* can be set for any user and project, and
+also as default through quota-class.
+
+In case a flavor is deleted, all the references to this flavor in the quotas
+will be deleted as well. The instances that were created based on this flavor
+which are running will continue running, but the usages and quotas for this
+resource will not be measured any more.
+
+There aren't restrictions between flavors and other resources.
+
+The order followed when quotas are either applied or set is the following:
+
+1. Quotas for the current resources such as ram, instances, cores,
+   floating_ips, etc.
+2. Quotas for flavors.
+
+
+Alternatives
+------------
+
+An alternative is to define quota templates and use them to associate it with
+flavors. This mechanism is similar to the solution proposed but doesn't
+meet all the scenarios.
+
+Data model impact
+-----------------
+
+Tables are not impacted.
+
+The flavor resources will be stored with a name as: flavor_<flavorid>.
+
+The flavor resources will be updated once a flavor is created or deleted.
+
+A Migration script has to be created to:
+
+. Update the usages table with the information related to the flavors in use.
+. Add the resource 'flavor' with value -1 to the defaults.
+
+
+REST API impact
+---------------
+
+Change in the response when getting the quotas for a user/tenant.
+* Method: GET
+* Path: /os-quota-sets/{tenant_id}
+* Resp: Normal Response Codes 200
+
+JSON response
+
+{
+ "quota_set": {
+  "cores": 20,
+  "fixed_ips": -1,
+  "floating_ips": 10,
+  "id": "fake_tenant",
+  "injected_file_content_bytes": 10240,
+  "injected_file_path_bytes": 255,
+  "injected_files": 5,
+  "instances": 10,
+  "key_pairs": 100,
+  "metadata_items": 128,
+  "ram": 51200,
+  "security_group_rules": 20,
+  "security_groups": 10,
+  "flavor_3": 2,
+  "flavor_4": 1
+
+ }
+
+}
+
+Change in the response when getting the default quotas.
+* Method: GET
+* Path: /os-quota-sets/defaults
+* Resp: Normal Response Codes 200
+
+JSON response
+
+{
+ "quota_set": {
+  "cores": 20,
+  "fixed_ips": -1,
+  "floating_ips": 10,
+  "id": "fake_tenant",
+  "injected_file_content_bytes": 10240,
+  "injected_file_path_bytes": 255,
+  "injected_files": 5,
+  "instances": 10,
+  "key_pairs": 100,
+  "metadata_items": 128,
+  "ram": 51200,
+  "security_group_rules": 20,
+  "security_groups": 10,
+  "flavor_3": 2,
+  "flavor": -1
+
+ }
+
+}
+
+Change in the request when updating the quotas for a user/tenant.
+* Method: POST
+* Path: /os-quota-sets/{tenant_id}/{user_id}
+* Resp: Normal Response Codes 200
+
+JSON response:
+
+{
+ "quota_set": {
+  "force": "True",
+  "instances": 9,
+  "flavor_3": 4
+
+ }
+
+}
+
+JSON Schema:
+
+"quota_set" = {
+ "type":"object",
+ "required":false,
+ "properties": {
+
+  "quota_set": {
+   "type":"object",
+   "required":false,
+   "properties":{
+
+    "cores": {"type":"number", "required":false},
+    "fixed_ips": {"type":"number", "required":false},
+    "flavor_<flavor_id>": {"type":"number", "required":false},
+    "floating_ips": {"type":"number", "required":false},
+    "injected_file_content_bytes": {"type":"number", "required":false},
+    "injected_file_path_bytes": {"type":"number", "required":false},
+    "injected_files": {"type":"number", "required":false},
+    "instances": {"type":"number", "required":false},
+    "key_pairs": {"type":"number", "required":false},
+    "metadata_items": {"type":"number", "required":false},
+    "ram": {"type":"number", "required":false},
+    "security_group_rules": {"type":"number", "required":false},
+    "security_groups": {"type":"number", "required":false}
+
+   }
+
+  }
+
+ }
+
+}
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+This change affects the client user interface adding new optional values when
+update a quota:
+
+quota-update
+    --tenant-id <tenant_id>
+    --instances <instances>
+    --cores <cores>
+    --ram <ram>
+    --floating-ips
+    --fixed-ips <fixed-ips>
+    --metadata-items <metadata-items>
+    --injected-files <injected-files>
+    --injected-file-content-bytes <injected-file-content-bytes>
+    --injected-file-path-bytes <injected-file-path-bytes>
+    --key-pairs <key-pairs>
+    --security-groups <security-groups>
+    --security-group-rules <security-group-rules>
+    --flavor_<flavor-id> <flavors>
+    <name>
+
+quota-class-update
+    --instances <instances>
+    --cores <cores>
+    --ram <ram>
+    --floating-ips
+    --fixed-ips <fixed-ips>
+    --metadata-items <metadata-items>
+    --injected-files <injected-files>
+    --injected-file-content-bytes <injected-file-content-bytes>
+    --injected-file-path-bytes <injected-file-path-bytes>
+    --key-pairs <key-pairs>
+    --security-groups <security-groups>
+    --security-group-rules <security-group-rules>
+    --flavor_<flavor-id> <flavors>
+    --flavor <flavors>
+    default
+
+Performance Impact
+------------------
+
+The main impact on performance is when:
+
+* A flavor is deleted due to all the quotas with that flavor have to be
+  deleted.
+
+* Some quota operations will require to go to the db in order to check that
+  the flavor resource exists.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee: sergio-j-cazzolato
+
+
+Work Items
+----------
+
+Tasks to do as part of this bp:
+
+* Change the API to verify quotas when boot a new instance.
+* Create the DB migration script.
+* Update the db to update quotas when a flavor is deleted.
+* Update the client to support the new resources.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest tests are needed to validate:
+
+* The new API
+* The new instances cannot be created once the quota has been reached
+* The db migration
+
+Documentation Impact
+====================
+
+Documentation needed for:
+
+* Rest API
+* Client Interface
+* Operators Guide
+
+References
+==========
+
+Link to notes from a summit session:
+https://etherpad.openstack.org/p/operators-feedback-mar14 -> line 448
diff --git a/specs/juno/proposed/periodic-heartbeat.rst b/specs/juno/proposed/periodic-heartbeat.rst
new file mode 100644
index 0000000..9209c7b
--- /dev/null
+++ b/specs/juno/proposed/periodic-heartbeat.rst
@@ -0,0 +1,118 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================
+Periodic heartbeat
+==================
+
+https://blueprints.launchpad.net/nova/+spec/periodic-heartbeat
+
+At present it's difficult to verify that all Nova services are responsive,
+rather than merely running. Touch a heartbeat file, the freshness of which can
+easily be monitored by EG an Icinga check.
+
+Problem description
+===================
+
+As a service operator, I would like to ensure Nova services have not hung. It
+is naturally easy to check the responsiveness of the API service, but there is
+no simple way to check the other services.
+
+Proposed change
+===============
+
+This spec proposes an additional periodic task as part of nova/manager.py which
+will run as part of the Nova API, scheduler, conductor and compute services.
+This task will touch a file in a well-known location, the freshness of which
+can easily be monitored by EG an Icinga check.
+
+Alternatives
+------------
+
+We could open up a REST API on the other services. That seems a heavyweight
+solution to the simple requirements posed by this spec. If more extensive
+service status requirements are introduced, this alternative may be more
+proportionate.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+Very low to none.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  alexisl
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* Add periodic task
+
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+This is a simple change, so unit tests should be sufficient.
+
+Documentation Impact
+====================
+
+A note should be made about this behaviour so that operators may take advantage
+of it.
+
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/persist-scheduler-hints.rst b/specs/juno/proposed/persist-scheduler-hints.rst
new file mode 100644
index 0000000..03c6205
--- /dev/null
+++ b/specs/juno/proposed/persist-scheduler-hints.rst
@@ -0,0 +1,188 @@
+
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================
+Persist scheduler hints
+===========================================
+
+https://blueprints.launchpad.net/nova/+spec/persist-scheduler-hints
+
+The aim of this feature is to persist scheduler hints when creating
+VM instance so that the scheduler hints can be available for the whole
+VM life cycle.
+
+
+Problem description
+===================
+
+When creating VM instance with scheduler hints, scheduler hints will
+only take effect at deploying time. After the VM was deployed, scheduler
+hints will be lost.
+
+Later on when an admin tries to migrate the VM, this VM can be migrated to
+a host which might violated the original scheduler hints. The same problems
+also exist for resize, cold migration etc.
+
+The proposed solution is to store scheduler hints so that it can be available
+for the whole life cycle of the VM instance, this can make sure the VM can
+retrieve and evaluate the scheduler hints before doing some VM operations
+so as to make sure the VM will always obey its scheduler hints during its
+life cycle.
+
+It is up to the admin to decide if they want to obey scheduler hints when
+doing VM operations but the scheduler hints will always be there.
+
+For example, when admin creates a vm with different_host scheduler hints, then
+when migrate the VM, the VM will always migrate to different hosts specified
+by scheduler hints.
+
+Create a VM with different host filter: nova boot --image test --flavor 1
+--hint different_host=1-1-1-1-1 vm1
+
+Migrate the VM instance with scheduler validation: nova live-migration vm1
+--scheduler_validation, Nova scheduler will help select the best target host
+for live migration which can satisfy the different host scheduler hints.
+
+Migrate the VM instance without scheduler validation: nova live-migration vm1,
+if not specify scheduler_validation, the scheduler hints will be ignored by
+nova scheduler. This will be covered by bp validate-targethost-live-migration.
+
+This blueprint was mainly for persisting the scheduler hints to nova instance
+database and also enable "nova show" can list the scheduler hints.
+
+
+Proposed change
+===============
+
+* A new field named as scheduler_hints will be added to the table of
+  instances, its type is text.
+* A new field named as scheduler_hints will be added to instance object,
+  its type is DictOfNullableStringsField.
+* Update nova api to enable persisting scheduler hints when creating VM
+  instance.
+
+Alternatives
+------------
+None
+
+Data model impact
+-----------------
+
+* Add a new field scheduler_hints to the table of instances to persist
+  scheduler hints.
+* Add script for DB upgrade and downgrade.
+
+REST API impact
+---------------
+
+* Enable "nova show" can list scheduler hints for both V2 and V3 API.
+* Both admin and normal user can get scheduler hints.
+* For V2 API, need add an API extension to enable this feature.
+* Response body for V2 API::
+
+       server = {
+                     "id": uuid,
+                     "user_id": "fake_user",
+                     "tenant_id": "fake_project",
+                     "updated": "2010-11-11T11:00:00Z",
+                     "created": "2010-10-10T12:00:00Z",
+                     "progress": progress,
+                     "name": "server1",
+                     "status": status,
+                     "accessIPv4": "",
+                     "accessIPv6": "",
+                     "hostId": "",
+                     "OS-SCH-HNT:scheduler_hints": "{'foo': 'bar'}"
+       }
+
+* Response body for V3 API::
+
+       server = {
+                     "id": uuid,
+                     "user_id": "fake_user",
+                     "tenant_id": "fake_project",
+                     "updated": "2010-11-11T11:00:00Z",
+                     "created": "2010-10-10T12:00:00Z",
+                     "progress": progress,
+                     "name": "server1",
+                     "status": status,
+                     "os-access-ips:access_ip_v4": "",
+                     "os-access-ips:access_ip_v6": "",
+                     "host_id": "",
+                     "os-scheduler-hints:scheduler_hints": "{'foo': 'bar'}"
+       }
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    jay-lau-513 (liugya@cn.ibm.com)
+
+Other contributors:
+    Yassine Lamgarchal (yassine.lamgarchal@enovance.com)
+
+Work Items
+----------
+
+* Add a new table named as instance_scheduler_hints.
+* Add DB upgrade logic for the new table.
+* Update API v2/v3 for "nova show".
+
+
+Dependencies
+============
+None
+
+
+Testing
+=======
+
+* Add unit test for testing scheduler hints inclduing DB and resource
+  tracker.
+* Add unit test for API change related to "nova show".
+* Add tempest function tests.
+
+
+Documentation Impact
+====================
+
+* "nova show" will also list all scheduler hints
+* Update Cloud Admin Guide and Operations Guide to tell admin that
+  scheduler hints will be persisted when creating VMs.
+
+
+References
+==========
+None
diff --git a/specs/juno/proposed/policy-based-scheduing-engine.rst b/specs/juno/proposed/policy-based-scheduing-engine.rst
new file mode 100644
index 0000000..8d4663d
--- /dev/null
+++ b/specs/juno/proposed/policy-based-scheduing-engine.rst
@@ -0,0 +1,269 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+PBSM - A Scheduling engine using policy
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/policy-based-scheduler
+
+This blueprint proposes a new scheduler that allows admin to apply different
+policies to different parts of the infrastructures or different clients while
+performing scheduling. Admin can define scheduling rules for each client,
+each aggreggate, each availabiloity-zone or the whole infrastructure
+separatedly. Admin can also modify these rules in real time without restarting
+the scheduler.
+
+
+Problem description
+===================
+
+Current Filter_Scheduler uses Filters and Weighers to help choose the best
+suited host for any requested VM. However, FilterScheduler has several limits
+that make it unable to make the best use of its Filter and Weigher catalog and
+provide business-level services to clients. Among others:
+
+* Static policy: admin cannot change the placement policy in runtime without
+  restarting nova-scheduler. Most of Filters and Weighers use parameters from
+  configuration and thus also require restarting nova-scheduler. Thus adapting
+  scheduling policy to the runtime context is impossible.
+
+* Lack of client context: the same filters and weighers are applied with the
+  same parameters regardless of clients. In this situation, it is difficult to
+  provide different qualities of services to clients who sign different
+  contracts.
+
+* Global setup only: the same filters and weighers are applied to all hosts.
+  Even though Openstack defines aggregates for regrouping a set of hosts with
+  similar characteristics, it still does not allow admin to define different
+  policies for these aggregates (eventhough some Filters allow per-aggregate
+  parameters). For instance, admin cannot call different Filters/Weighers on
+  different aggregates).
+
+The following usecases are unfeasible with FilterScheduler:
+
+* Transparency to clients: A company signs a contract with gold service class.
+  With this contract its VMs will be hosted in the aggregate where all
+  high-quality hosts are regrouped, regardless of flavors its users choose.
+
+* Runtime modification: Admin can select any set of filters and weighers from
+  the catalog available in Openstack to execute without restarting nova-
+  scheduler.
+
+* Local policy vs Global policy: Admin wants to define a Consolidation policy
+  in one aggregate to regroup all VMs into a minimal number of hosts, and a
+  global Load Balancing policy to share the workload in the remaining
+  aggregates.
+
+
+Proposed change
+===============
+
+Our idea is to separate the scheduling logic ("how do you want to provision the
+VMs?") from the application target ("on which part of the resources that you
+want to apply the scheduling logic?"). This separation allows admin to specify
+how he wants to provision the resources in each and any context: per client,
+per aggregate, or per situation.
+
+Let's take the first scenario as example. With FilterScheduler, admin has to
+set up a dedicated flavor associated with this aggregate (via metadata) and let
+the client select this flavor. However, the client has to know about this
+flavor, and if he changes the contract, he has to modify all his applications
+to select the new flavor. The Policy-Based Scheduler will provide clients with
+transparency: the client just selects any flavor, and the system will
+automatically put his VMs into suitable aggregates as defined by the rules.
+Thus no special flavor is needed, and client does not need to modify his
+appplication at all.
+
+Another client will have another set of rules corresponding his context. He may
+process the same step as the previous one (selecting the same image, the same
+flavor, etc) but the deployment of his VMs will depend on his context alone.
+
+With the transparency provided by Policy-Based Scheduler, admin can use an
+analytic system to monitor and analyse the client's usage and behaviour and
+apply suitable rules for the client to better suit his need without reactions
+from the client's part.
+
+The architecture of this scheduler is illustrated as follows:
+::
+
+  +--------------------------------+
+  |                                |
+  |        Nova-scheduler          |
+  |                                |
+  |   +-----------------------+    |
+  |   |                       |    |
+  |   | Policy-Based Scheduler|    |
+  |   |                       |    |
+  |   +----------+------------+    |
+  |              |                 |
+  +--------------+-----------------+
+                 |
+                 |
+                 v
+      +-----------------------+        +----------------+
+      |                       |        |                |
+      |     Policy-Based      |        |     Policy     |
+      |   Scheduling Module   +------->|   Repository   |
+      |                       |        |                |
+      +----------+------------+        +----------------+
+                 |
+                 |
+                 |
+                 v
+         +----------------+
+         |                |
+         | Policy plugins |
+         |                |
+         +----------------+
+
+
+* All the policy rules will be stored in the Policy Repository. The Repository
+  can be a file, a database or a policy system. The format of the Rule is as
+  follows:
+  Target - Effect - Condition
+  ("Under this Condition, apply this Effect to this Target").
+  Condition can be time, overload situation, etcc.
+  Effect can be Load Balancing, Oversubsription, Service_class, etc.
+  Target can be a tenant, an aggregate, an availability-zone, the entire infra,
+  etc.
+
+* The Policy-Based Scheduling Module (PBSM) is the main engine of this
+  architecture. It gets the rules from the Policy Repository and applies the
+  rules via Policy plugins - the implementation of the rule effect. Plugins can
+  apply Filters or Weighers to the hosts.
+
+* Policy-Based Scheduler (PBS) is located inside Nova-scheduler and relays the
+  call from Nova-scheduler to PBSM.
+
+
+PBS will inherit from FilterScheduler with little modification to reduce the
+impact to the system and benefit from all the development in Nova (especially
+the work from Instance-group [2]). It also covers all the functionalities of
+FilterScheduler. If admin does not put any rule into Policy Repository, PBS
+will function exactly the same way as FilterScheduler. Admin can configure
+the filters and weighers as if with current FilterScheduler. Thus migration
+from FilterScheduler to PBS will be seamless and does not require any
+re-configuration from admin.
+
+A prototype of this architecture was presented in Demo Session in OpenStack
+Juno Summit [1].
+
+Alternatives
+------------
+
+An alternative is to build PBS from scratch. However, with the works in
+progress in nova (especially instance-group [2]), it is more convenient to keep
+it inherit from FilterScheduler.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+The PBS and its modules will not make changes to REST API. In the future, if
+a policy management system such as Congress [3] is used as Policy Repository's
+backend, we can use its REST API to add/modify/delete rules.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+The execution time (making scheduling decision) depends on number of compute
+nodes and number of policy rules. If admin does not make any rule, it will
+function exactly the same as FilterScheduler, thus having the same performance.
+
+Other deployer impact
+---------------------
+
+To fully exploit PBS, some configuration is needed:
+
+* Make Policy-Based Scheduler the scheduling engine in nova.conf
+
+* Precify the Policy Repository backend. The default is a file in /etc/nova/.
+
+* (Optional) Put rules in Policy Repository. If a policy management system such
+  as Congress is used as the Repository's backend, this can be done via the
+  system's REST API.
+
+At the very least, PBS does not require any other configuration than what admin
+does with FilterScheduler, except making it the scheduling engine in nova.conf.
+In this case (no rule is set), PBS will function exactly the same as
+FilterScheduler.
+
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  toan-tran
+
+
+Work Items
+----------
+
+The main part of the architecture will be implemented first:
+
+* Policy-based Scheduler
+* Polci-based Scheduling Module
+* Policy Repository: file backend
+
+The first version of the architecture will use simple file as a Policy
+Repository's backend. Others will be proposed in separated blueprints.
+
+The Policy plugins will be implemented in separated patches of the same
+blueprint.
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+None other than unittests.
+
+Documentation Impact
+====================
+
+Documents will be provided on how to write policy rules.
+
+References
+==========
+
+[1] http://openstacksummitmay2014atlanta.sched.org/event/
+    b4313b37de4645079e3d5506b1d725df
+
+[2] https://blueprints.launchpad.net/nova/+spec/instance-group-api-extension
+
+[3] https://wiki.openstack.org/wiki/Congress
diff --git a/specs/juno/proposed/projects-to-aggregate.rst b/specs/juno/proposed/projects-to-aggregate.rst
new file mode 100644
index 0000000..d004ff0
--- /dev/null
+++ b/specs/juno/proposed/projects-to-aggregate.rst
@@ -0,0 +1,117 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================================
+Add a new filter to implement project isolation feature
+=======================================================
+
+https://blueprints.launchpad.net/nova/+spec/projects-to-aggregate
+This blueprint proposes a new scheduler filter which
+routes instance to a pre-associated host aggregates
+
+
+Problem description
+===================
+At present, there is no scheduler filter to route all instances created by an
+user to pre-associated aggregates without having to specify zones as part of
+the nova boot command.
+
+Proposed change
+===============
+
+This filter aims to route tenant instances to pre-associated aggregate,
+
+* Administrator has to associate tenant projects to aggregate as metadata field
+  with  key as 'project_to_aggregate' and set of projectids as values
+* Hosts which are tagged under the aggregates are given high priority
+  while creating instances for those specified projects
+* One tenant can be mapped to more than one host aggregate
+
+This change does not restrict projects which are
+not attached to any specific aggregate.
+(i.e) If a project is not tagged to any aggregate,then its instances
+are free to boot on any of the filtered hosts irrespective of the aggregates.
+
+Alternatives
+------------
+In each boot request, user has to specify the Zone to which the
+instance has to reach.
+
+Data model impact
+-----------------
+None.
+
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+None.
+
+Notifications impact
+--------------------
+None.
+
+Other end user impact
+---------------------
+None.
+
+Performance Impact
+------------------
+As scheduler filters get called once per host for every instance being created,
+the database calls might impact the performance of the system in cases of
+huge multi node architectures.
+
+Other deployer impact
+---------------------
+Config Options includes addition of the filter class in
+nova.conf under the scheduler_available_filters attribute.
+
+Developer impact
+----------------
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Primary assignee:
+  Ram-nalluri
+
+Other contributors:
+  himapriya
+
+Work Items
+----------
+* Implement the filter and develop unit test cases.
+
+Dependencies
+============
+None.
+
+
+Testing
+=======
+Unit test cases are being developed.
+No special tempest tests are necessary to test the new filter.
+
+Documentation Impact
+====================
+We expect to have the following document changes:
+
+* The scheduler_available_filters flag should include
+  project-to-aggregate filter class.
+* Documentation for the project-to-aggregate filter should be added.
+
+
+References
+==========
+https://blueprints.launchpad.net/nova/+spec/projects-to-aggregate
diff --git a/specs/juno/proposed/pxe-boot-instance.rst b/specs/juno/proposed/pxe-boot-instance.rst
new file mode 100644
index 0000000..71b6f09
--- /dev/null
+++ b/specs/juno/proposed/pxe-boot-instance.rst
@@ -0,0 +1,147 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================
+PXE Boot Instance
+=================
+
+
+https://blueprints.launchpad.net/nova/+spec/pxe-boot-instance
+
+
+This specification proposes a way to grant a compute instance the chance to 
+boot from network.
+
+Problem description
+===================
+
+
+Currently, compute instances must be booted from images (or snapshots) 
+stored in Glance or volumes stored in Cinder. 
+
+In enterprise environments it is often desired to be able to use a Preboot 
+Execution Environment (PXE) to boot and install custom operating systems on 
+virtual machines. For this reason the aim of this specification is to give the 
+End Users the chance to select "network" as a possible booting source.
+
+It has to be underlined that the aim of this document is not to define a PXE 
+service within OpenStack: the PXE boot service should managed externally via 
+another machine within a tenant/provider network. Selecting the "network" as 
+boot device a compute instance will be created and it will broadcast a DHCP 
+request from which the PXE boot sequence will start, if a PXE Server is 
+configured to manage that request.
+
+Once PXE/Network booting is selected by the user as the instance boot 
+source, this will always be the default boot option for the instance's entire 
+lifecycle. i.e. it won't just PXE boot one-time and then revert back to 
+default.
+
+Proposed change
+===============
+
+Libvirt grants yet a way to trigger the boot from network of a virtual 
+machine,  specifing the Libvirt XML attribute <boot 
+dev='network'/> where required. We propose to 
+take advantage of this libvirt feature within OpenStack.
+
+A new option "--netboot" will be introduced in python-novaclient module to 
+specify the compute instance will boot from network and not from an image or 
+volume. Python-novaclient will associate to this option a fixed special 
+image token id, in this way no changes to REST API will be necessary.
+
+Nova module will recognize the special image token id, will bypass any 
+check on its existence and it will use the Libvirt Driver to generate an 
+appropriate Libvirt XML domain to boot the compute instance from network.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+  
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+[[Aside from the API, are there other ways a user will interact with this
+feature?
+
+* Does this change have an impact on python-novaclient? What does the user
+  interface there look like?]]
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+
+Developer impact
+----------------
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  matarazzoangelo
+
+Other contributors:
+  
+
+Work Items
+----------
+
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+
+
+Documentation Impact
+====================
+
+User guide is affected by these changes:
+--A new section "Lauch an instance from network" should be added
+--The Section nova commands should be changed
+    
+
+References
+==========
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-empty-vm-boot-pxe
+
+https://wiki.openstack.org/wiki/Nova/Blueprints/pxe-boot-instance
diff --git a/specs/juno/proposed/quota-state-management.rst b/specs/juno/proposed/quota-state-management.rst
new file mode 100644
index 0000000..ebde870
--- /dev/null
+++ b/specs/juno/proposed/quota-state-management.rst
@@ -0,0 +1,236 @@
+..
+   This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Quota state management
+==========================================
+
+Blueprint is here:
+https://blueprints.launchpad.net/nova/+spec/quota-state-management
+
+Enhance Nova quotas by adding a concept of state management to quotas. Making
+it easier to align existing business processes with OpenStack.
+
+Problem description
+===================
+Some private cloud operators depend on quota to not only control the rate at
+which their tenants consume their cloud, but also track how much of their
+cloud has been allocated. Unfortunately the operator has no visibility into
+how often a tenant has requested their quota be increased, or if there are
+any quota increase requests in the pipe. At the moment nova quota allocations
+cannot be tracked on a discrete basis. When one looks at the quota for a given
+tenant they will only see what is currently available to that tenant, without
+any history associated with that.
+
+When one of our private cloud tenants wants to increase their quota
+they’ll need to file a request. That request will be compared against
+projected cluster capacity as well as certain financial models. If the request
+falls within certain parameters it’ll need larger finance approval before the
+quota can be allocated. During this process there is no visibility to either
+operators,finance partners, or capacity planning teams who may need to view
+the overall capacity of a given cluster. In an enterprise each request could
+change hands many times. There will be race conditions when multiple tenants
+request capacity at the same time. We need a way to request, review, approve,
+and deny quota enhancement requests within OpenStack.
+
+Some quota allocations are granted on a temporary basis. Say during a traffic
+spike you want to increase quota for a given tenant, but take it away after a
+predetermined period of time. For that we need a concept of leasing quotas.
+
+
+Proposed change
+===============
+
+Implement state management and leasing in quotas. Tenant quota will be no
+longer be a single row per tenant, per resource, but instead per-tenant,
+per-resource, per-request. A tenants available quota will now be the sum of
+their active quotas.
+This means that there is now a concept of "soft" vs "hard" quota. Soft quota
+will be the sum of all quota requests from a given project excluding those
+that have been deleted. Hard quota will represent only the quota that has been
+marked as active.
+Additionally each quota can have an optional "TTL" applied to it. Once TTL has
+expired that quota should be marked as disabled in the DB.
+
+Alternatives
+------------
+Public and private providers could continue to manage this quota state outside
+of openstack. This would result in significant duplication of effort. Bringing
+this simple concept in to OpenStack would make it much easier to integrate into
+enterprise.
+
+Data model impact
+-----------------
+
+Add 4 elements to the quota model:
+
+* 'Active' (boolean) to indicate whether or not the row should be used to
+  calculate a Project's total available quota.
+* 'state' to track whether the quota is New, In-Review, Denied, or Expired.
+  This should be an INT with a foreign key constraint to the ID in a new
+  quota-state table.
+* 'external_ref' medium text field for a json blob to associate arbitrary
+  external reference key/value pairs.
+  eg: ``{
+  "original_request":"http://someurl.com/bug.cgi/ref=12345",
+  "capacity_approval":"http://some-site.com/blah.cgi?foo=12345"
+  }``
+* 'expires' (datetime, default=NULL) the date on which the given quota
+  should no longer be considered valid. A null value indicates the
+  quota will never expire.
+
+A new model will be created called "quota_state" with the following elements:
+
+* 'id' Auto increment,
+* 'state' Varchar(32), Unique, Not NULL. Contains the name of the state. "New,
+  In-Review, Denied, Approved, Expired" etc
+* 'created_at' datetime, Not NULL
+* 'modified_at' datetime, auto(now)
+
+The purpose behind this new model is to make the state concept as extensible
+as possible.
+
+
+REST API impact
+---------------
+
+* os-quota-sets
+
+  * enhance this method to return a list of all quota requests, as well as thei
+    list of soft vs hard quota.
+  * A description of what the method does suitable for use in user i
+    documentation
+  * GET
+  * Normal http response code(s)
+
+  * Expected error http response code(s)
+
+    * A description for each possible error code should be included
+      describing semantic errors which can cause it such as
+      inconsistent parameters supplied to the method, or when an
+      instance is not in an appropriate state for the request to
+      succeed. Errors caused by syntactic problems covered by the JSON
+      schema defintion do not need to be included.
+
+  * URL for the resource
+
+  * Parameters which can be passed via the url
+
+  * JSON schema definition for the body data if allowed
+
+  * JSON schema definition for the response data if any
+
+
+limits
+* This method should be modified to return a dict of the sum of the "soft"
+limits in the json blob as well as the "absolute" limits.
+
+absolute-limits
+* Add a flag to allow returning the sum of the json blob for all tenants,
+rather than just a specified tenant.
+
+Security impact
+---------------
+
+Displaying absolute limits for all tenants should be locked down to just the
+admin user, or another arbitrary role type (capacity_admin). Many companies
+would consider the size of their private cloud, allocated quota, and used
+capacity to be proprietery information and would not want that shared.
+Further concern, if you have a very large number of tenants, quotas, and quota
+requests this method could require significant database time to select, sum,
+and return those lines. An attacker could use this as a resource exhaustion
+DoS.
+
+Notifications impact
+--------------------
+
+Other end user impact
+---------------------
+
+python-novaclient will need to be extended to include the --all-tenants flag
+for the absolute-limits method.
+
+the json blob returned by absolute-limits will include "soft" limits, which
+will be a deviation from the way it returns now.
+
+when allocating quota the cloud admin will need to supply an additional flag to
+mark the state of quota as "approved".
+
+Performance Impact
+------------------
+
+Calculating quota by summing rows is going to require more compute resources on
+the DB server. This would only be significant in large or 'mega' scale
+environments. This can be mitigated in those environments by calling quota-get
+against read-only DB hosts.
+
+Other deployer impact
+---------------------
+By default this wont make any changes to how quotas are allocated. But there
+will be additional hooks to allow operators to take advantage of this. By
+default creating quotas will be 'hard' quota.
+
+Developer impact
+----------------
+
+* Any API changes will need to be aligned with this
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  penick@yahoo-inc.com
+
+Other contributors:
+ openstack-dev@yahoo-inc.com,
+ Anyone else who wants to help out.
+
+Work Items
+----------
+
+* Gain consensus on the blueprint
+* Write the code :)
+
+
+Dependencies
+============
+
+
+
+Testing
+=======
+
+
+
+Documentation Impact
+====================
+
+There will be moderate impact to documentation to track not only the CLI
+changes mentioned above, but also the best practices for integrating quota
+management with enterprise processes.
+
+
+References
+==========
+
+Please add any useful references here. You are not required to have any
+reference. Moreover, this specification should still make sense when your
+references are unavailable. Examples of what you could include are:
+
+* Links to mailing list or IRC discussions
+
+* Links to notes from a summit session
+
+* Links to relevant research, if appropriate
+
+* Related specifications as appropriate (e.g.  if it's an EC2 thing, link the
+  EC2 docs)
+
+* Anything else you feel it is worthwhile to refer to
diff --git a/specs/juno/proposed/ram-as-percentage.rst b/specs/juno/proposed/ram-as-percentage.rst
new file mode 100644
index 0000000..13bcf99
--- /dev/null
+++ b/specs/juno/proposed/ram-as-percentage.rst
@@ -0,0 +1,151 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================
+ Implement percentage-based RAM Weigher
+========================================
+
+https://blueprints.launchpad.net/nova/+spec/ram-as-percentage
+
+This blueprint is just a continuation of a blueprint already approved for
+Havana that missed feature freeze:
+https://blueprints.launchpad.net/nova/+spec/normalize-scheduler-weights
+
+Nova's scheduler used the raw values returned by each of the weighers to
+compute the final weight of a compute host or cell. This made difficult for
+operators to setup and use multipliers to establish the relative importance
+between weighers since they weren't able to know the maximim weight for an
+object in advance, as it was a variable value. Moreover, in order to make a
+weigher prevail among others in some cases it was needed to artificially
+inflate either the weigher's returned value or the weigher's multiplier.
+
+The blueprint that was already implemented introduced weight normalization.
+This mechanism maps all the values returned from a weigher between 0 and 1,
+thus they have an even relative influence in the final weight for a host.
+Therefore, an operator knows a-priori that a host with a weight of 1 is the
+winner of that weighing process, and a host with 0 is the loser, making easier
+to setup multipliers in order to establish the relative importance between
+weighers.
+
+Problem description
+===================
+
+The current RAM weigher weights objects using absolute values. This means that,
+for example, two nodes with 10GB of free RAM will get the same score, even if
+the first has 10GB free out of 20GB and the second 10GB out of 200GB. In some
+cases (for example in infrastructures with heterogeneous nodes) it may be
+desirable to select hosts based on the percentage of free RAM used, instead of
+using the absolute values.
+
+Without weight normalization it was difficult to use percentages for the RAM
+weigher, but with the normalization in place the RAM weigher can be easily
+improved to support that.
+
+Proposed change
+===============
+
+The 'nova.scheduler.weights.ram.RAMWeigher' will be adapted to use relative
+values (i.e. percentage based). This way operators can change the scheduling
+behaviour in their infrastructure being able to schedule based on the
+percentage of RAM that is used/available. Currently only absolute values are
+used, making scheduling difficult in heterogeneous infrastructures.
+
+A new flag will be introduced to select this behaviour (i.e. percentage based
+scheduling), thus the RAMWeigher will remain compatible with its current
+status (i.e. use absolute values).
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+A new configuration option will be included for the 'RAMWeigher':
+'ram_weight_percentage=False' so as to use the percentage of free RAM available
+instead of the usage of the absolute values.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  aloga
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+- Modify RamWeigher so that percentages can be used instead of absolute values
+  (new flag 'ram_weight_percentage').
+- Update both the developer documentation and the Cloud Admin Guide.
+
+Dependencies
+============
+
+This blueprint is a continuation of the following (already implemented):
+https://blueprints.launchpad.net/nova/+spec/normalize-scheduler-weights
+
+Testing
+=======
+
+There is no need for new tempest tests.
+
+Documentation Impact
+====================
+
+A new configuration option 'ram_weight_percentage' will be introduced, so
+that the RAM weighing will take into account the percentage of free ram
+instead of the absolute values.
+
+These configuration changes will be noted in the Release Notes and the Cloud
+Admin Guide will be updated along with this blueprint.
+
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/refactor-virt-capabilities.rst b/specs/juno/proposed/refactor-virt-capabilities.rst
new file mode 100644
index 0000000..00c0c6a
--- /dev/null
+++ b/specs/juno/proposed/refactor-virt-capabilities.rst
@@ -0,0 +1,145 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================
+Refactor Virt Driver Capabilities
+=================================
+
+https://blueprints.launchpad.net/nova/+spec/refactor-virt-capabilities
+
+Refactor the capabilites exposed by virt drivers to support basic
+inheritance.
+
+
+Problem description
+===================
+
+The virt driver provides a capabilities dict which allows the compute
+manager to determine the support for optional features such as
+"has_imagecache" and "supports_recreate".
+
+Currently this is a dict which each hypervisor driver either leaves
+intact of replaces completly, which means that whan a new capability
+is introduced, with a default value in the generic driver which maps
+to the existing functionality all drivers that have any specific
+capability settings need to be updated.    Since Ironic includes
+a hypervisor driver which sits outside Nova the problem is extended
+to one that spans projects.
+
+
+Proposed change
+===============
+
+The proposed solution is to refactor the code slightly so that capabilities
+only need to be defined in a hypervisor specific driver if they differ from
+the default.
+
+The change could be as simple as something like the following in the
+generic driver init method:
+
+.. code-block:: none
+
+     capabilites.update(driver_specific_capabilites)
+
+Alternatives
+------------
+
+The alternatives are to either continue redefining all capabilities in any
+driver that needs to change one or mor values, or to make sure that any code
+that reads a capaibility provides the required deafult value if the capability
+is not defined.
+
+Data model impact
+-----------------
+
+None, the change is restricted to the initialisation of the virt driver.
+
+REST API impact
+---------------
+
+None, the change is restricted to the initialisation of the virt driver.
+
+Security impact
+---------------
+
+None, the change is restricted to the initialisation of the virt driver.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None 
+
+Developer impact
+----------------
+
+Developers adding a new capability to the virt driver will only have
+to declare the capability in the generic driver and any driver that
+supports the non-default setting.
+
+The default setting of any new capabilty should always match the
+existing functionality.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+
+  philip-day
+
+
+Work Items
+----------
+
+* Refactor virt driver initialisation
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+The net effect of the change will be that each driver still exposes the 
+same set of capabilities.
+
+The basic mechanism will be covered by unit tests.
+
+Existing Hypervisor specific CI tests will highlight any functional changes
+introduced by accident.
+
+
+Documentation Impact
+====================
+
+None 
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/remove-fakelibvirt.rst b/specs/juno/proposed/remove-fakelibvirt.rst
new file mode 100644
index 0000000..a054410
--- /dev/null
+++ b/specs/juno/proposed/remove-fakelibvirt.rst
@@ -0,0 +1,153 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================
+Remove Fakelibvirt
+==================
+
+https://blueprints.launchpad.net/nova/+spec/remove-fakelibvirt
+
+Using `fakelibvirt` can lead to situations where tests pass locally but fail on
+Jenkins. This spec proposes to remove `fakelibvirt` and instead skip Libvirt
+tests if real `libvirt` isn't present.
+
+Problem description
+===================
+
+The Libvirt tests use the real `libvirt` Python module if it's installed or
+`fakelibvirt` if it's not.
+
+Since `fakelibvirt` can fall out-of-sync with real `libvirt` this leads to a
+situation where tests can pass locally (using `fakelibvirt`) but fail on
+Jenkins (which uses some version of real `libvirt`).
+
+The current solution to the divergence problem is to continually update
+`fakelibvirt` as `libvirt` changes, sometimes even involving a wholesale copy
+of code from `libvirt` into the `nova` tree [1]_.
+
+Proposed change
+===============
+
+We should remove the `fakelibvirt` module entirely.
+
+If a developer does not have real `libvirt` installed, this would cause the
+tests to fail. So, using the database migration tests as a model, we should
+skip the Libvirt tests if we detect that `libvirt` is not importable.
+
+
+Alternatives
+------------
+
+* NO CHANGE: We could do nothing, but this means we may have to continue to
+  sync `fakelibvirt` which is overhead; but more importantly, since the tests
+  are really integration tests to a large degree, mocking this out means that
+  we're not testing much of anything.
+
+* REQUIRE LIBVIRT: We could remove `fakelibvirt` but not skip tests. This
+  would require *all* Nova developers to have `libvirt` installed in order for
+  tests to pass. This would prevent Mac developers from developing locally as
+  `libvirt` doesn't install cleanly.
+
+* ONLY FAKELIBVIRT: We could remove the import check and instead have both
+  Jenkins and local developers use `fakelibvirt` exclusively. The benefit is
+  that it's consistent across environments. The drawback is we're going to
+  need to keep `fakelibvirt` in sync with real `libvirt` and we're likely to
+  have bugs in `fakelibvirt` which cause tests to pass both locally and
+  Jenkins but result in production bugs.
+
+* REWRITE TESTS: The suggestion here is rewrite all of our libvirt tests so
+  that we don't actually ever talk to a `libvirt` client at all. This supposes
+  that the Tempest tests are sufficient for integration tests.
+
+  The pro is that we'd have consistency across environments, and as
+  unit-tests, they'd be blazingly fast.
+
+  The downside are that:
+
+    * We'd have to rewrite all of our tests, a considerable effort
+
+    * Tempest tests are probably not sufficient because we probably want more
+      fine-grained, in-tree integration tests for Nova. Tempest is great for
+      testing integration between Neutron and Nova, but not as ideal for
+      testing integration between `nova/virt/libvirt/driver.py:spawn` and
+      `libvirt:createDomain`.
+
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+Developers will need to have `libvirt` installed to run the `libvirt` tests,
+otherwise they'll be skipped.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+rconradharris
+
+Work Items
+----------
+
+* Remove `fakelibvirt`
+* Add skip-test-if-libvirt-not-installed checks to libvirt tests (if tests do
+  in fact require `libvirt`)
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+Too meta. This is fixing the tests themselves.
+
+Documentation Impact
+====================
+
+None.
+
+References
+==========
+
+.. [1] https://review.openstack.org/#/c/86396/
diff --git a/specs/juno/proposed/restrict-image-types.rst b/specs/juno/proposed/restrict-image-types.rst
new file mode 100644
index 0000000..efc638d
--- /dev/null
+++ b/specs/juno/proposed/restrict-image-types.rst
@@ -0,0 +1,126 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================
+Restrict Image Types
+====================
+
+https://blueprints.launchpad.net/nova/+spec/restrict-image-types
+
+Add the ability to restrict certain image types from being built on a flavor.
+
+Problem description
+===================
+
+Today all image types are allowed to boot if an image of that type is present
+for a flavor in Glance.  In some cases, the operator may want to restrict a
+flavor from being able to boot a certain image type.
+
+Examples of this are:
+
+A diskless flavor offering might have limited space on the hypervisor.  In this
+case, you would want to avoid using space on the disk by either booting from
+volume or utilizing use a small iPXE boot ISO to boot the operating system over
+the network.  In this case, we'd restrict all image_types except for ISO.
+
+In another case, you might want to restrict QCOW images from being built on a
+XenServer environment.  You'd want to ensure that QCOW and any other types of
+images are restricted, while allowing VHDs.
+
+Proposed change
+===============
+
+A restrict_image_types configuration would be set as a property in
+instance_types_extra_specs that would restrict the specified image types.  If
+the option wasn't set, all image types would be allowed.  This way you could
+limit what types of images could be provisioned on a flavor.
+
+The current image types listed here would be the ones that could be specified
+as restricted:
+
+http://docs.openstack.org/developer/glance/formats.html
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  antonym
+
+Work Items
+----------
+
+Add a restrict_image_types option to instance_types_extra_specs that would
+restrict certain image types from being provisioned on an instance.  This
+would include glance image types as well as boot from volume.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Create appropriate tests to validate that the restrictions work properly.
+
+Documentation Impact
+====================
+
+Documentation would need to be written to detail how to set the image type
+restrictions.
+
+References
+==========
+
+* Image types: http://docs.openstack.org/developer/glance/formats.html
diff --git a/specs/juno/proposed/restrict-instance-migration.rst b/specs/juno/proposed/restrict-instance-migration.rst
new file mode 100644
index 0000000..813bc5a
--- /dev/null
+++ b/specs/juno/proposed/restrict-instance-migration.rst
@@ -0,0 +1,176 @@
+=====================================================
+Restrict instance migration when vm consoles are open
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/restrict-instance-migration
+
+Administrators don't want to bother users by performing live migration
+specially when they are connected to the vm console. When vm will be live
+migrated from source to the destination compute node, the vnc server running
+on the source compute node will abruptly terminate all of the opened tcp
+connections on the graphics port of the vm without intimating to the users.
+
+
+Problem description
+===================
+
+When vm is live migrated to another compute node, users connected to the vm
+consoles (vnc/spice) will be terminated without prior intimation. This might
+sound annoying to the users.
+
+
+Proposed change
+===============
+We are thinking of introducing a new 'force' parameter to the live migration
+api. If force parameter is set to false, then it will check whether users are
+connected to the vm console using vnc/spice before actually performing live
+migration. If users are connected to the vm console, then it will return 409
+error to the administrator.
+Then the administrator (operations team) will inform to the owner of the vm
+that they want to perform live migration within a specified time frame and
+refrain them from connecting to the vm consoles otherwise their connections
+will be terminated automatically. This way users will be informed when the vm
+consoles will be terminated after the live migration is finished.
+
+Administrator will run live migration api with force parameter set as 'true'
+within the given time frame communicated to the user earlier. when force
+parameter is set to True, then it won't check whether users are connected to
+the vm consoles or not and straight way perform live migration.
+
+force parameter default value will be set to 'True'.
+
+Libvirt didn't expose any api to check whether the vm console is open or not.
+So in order to detect that, first we need to get all of the opened tcp
+connections on the compute node where the vm is running using command
+
+sudo netstat -nt
+
+Then using output of netstat command we can filter out all of the tcp
+connections opened for a given vm's graphics port
+<vncserver_listen:graphics port of a vm>.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+* Live-migrate server
+
+  * Description: Live-migrates a server to a new host without rebooting
+  * Method: POST
+  * Normal response code(s): 202
+  * Expected error http response code(s): 409
+
+    * When instance migration is prformed and consoles are open
+      for given instance then 409 error will be returned to administrator.
+
+  * URL for the resource: /v2/{tenant_id}/servers/{server_id}/action
+  * Parameters which can be passed via the url
+    {tenant_id}, String, The ID for the tenant in a multi-tenancy cloud.
+    {server_id}, UUID, The UUID for the server.
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+One additional compute rpc call will be made if --force parameter is set
+to 'false'.
+
+Deployer impact
+---------------
+
+Added netstat command in compute.filters file
+
+Format for netstat command in compute.filters:
+
+netstat: CommandFilter, netstat, root
+
+
+Developer impact
+----------------
+
+We intend to add this check for the KVM but making provision so that other
+hypervisors can benefit too.
+A new abstract method "check_console_ports" will be added to the
+virt/driver.py
+This method will be implemented in the libvirt driver. Other hypervisors can
+implement this method based on their requirement, otherwise
+NotImplementedError will be raised with error message
+"Hypervisor driver doesn't support checking of opened consoles ports" which
+will be caught in the compute manager and no action would be taken. In short,
+force parameter will  be ignored if check_console_ports method is not
+implemented in the hypervisor.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  abhishek-kekane
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+- Add a new method 'check_console_ports' in libvirt driver to check console
+  ports are opened for a given instance.
+- Add a new 'check_console_ports' compute rpc api which will be called
+  (called synchronously using call method) from the compute/api.py to check
+  users are connected to the vm console when force parameter is set to
+  'false'.
+- Add a new force parameter to live migration API.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+- Add tempest tests to verify the newly added force parameter is working
+  properly.
+- Add unit tests to verify the newly added force parameter is working
+  properly.
+
+
+Documentation Impact
+====================
+
+Refer Deployer impact
+
+
+References
+==========
+
+https://bugs.launchpad.net/nova/+bug/1240584
diff --git a/specs/juno/proposed/rootwrap-daemon-mode.rst b/specs/juno/proposed/rootwrap-daemon-mode.rst
new file mode 100644
index 0000000..596efc5
--- /dev/null
+++ b/specs/juno/proposed/rootwrap-daemon-mode.rst
@@ -0,0 +1,147 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================
+rootwrap daemon mode
+====================
+
+https://blueprints.launchpad.net/nova/+spec/rootwrap-daemon-mode
+
+Nova is one of projects that heavily depends on executing actions on compute
+and network nodes that require root priviledges on Linux system. Currently this
+is achieved with oslo.rootwrap that has to be run with sudo. Both sudo and
+rootwrap produce significant performance overhead. This blueprint is one of the
+series of blueprints that would cover mitigating rootwrap part of the overhead
+using new mode of operations for rootwrap - daemon mode. These blueprints will
+be created in several projects starting with oslo.rootwrap [#rw_bp]_.
+
+Problem description
+===================
+
+As you can see in [#ne_ml]_ rootwrap presents big performance overhead for
+Neutron. Impact on Nova is not as signigicant since most of the work is done
+with libvirt's API but it is still there.
+Details of the overhead are covered in [#rw_bp]_.
+
+Proposed change
+===============
+
+This blueprint proposes adopting upcoming change in oslo.rootwrap that would
+allow to run rootwrap daemon. The daemon will work just as a usual rootwrap but
+will accept commands to be run over authenticated UNIX domain socket instead of
+command line and will run continuously in background.
+
+Note that this is not usual RPC over some message queue. It uses UNIX socket,
+so no remote connections are available. It also uses digest authentication with
+key shared over stdout (pipe) with parent process, so no other processes will
+have access to the daemon. Further details of rootwrap daemon are covered in
+[#rw_bp]_.
+
+``use_rootwrap_daemon`` configuration option should be added that will make
+``utils.execute`` use daemon instead of usual rootwrap.
+
+Alternatives
+------------
+
+Alternative approaches have been discussed for Neutron in [#ne_eth]_.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+This change requires additional endpoint to be available to run as root -
+``nova-rootwrap-daemon``. It should be added to the ``sudoers`` file.
+
+All security issues with using client+daemon instead of plain rootwrap are
+covered in [#rw_bp]_.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+This change introduces performance boost for disk and network operations that
+are required to be run with root priviledges in ``nova-compute`` and
+``nova-network``. Current state of rootwrap daemon shows over 10x speedup
+comparing to usual ``sudo rootwrap`` call. Total speedup for Nova will be less
+impressive but should be noticeable.
+
+Other deployer impact
+---------------------
+
+This change introduces new config variable ``use_rootwrap_daemon`` that
+switches on new behavior. Note that by default ``use_rootwrap_daemon`` will be
+turned off so to get the speedup one will have to turn it on. With it turned on
+``nova-rootwrap-daemon`` is used to run commands that require root priviledges.
+
+This change also introduces new binary ``nova-rootwrap-daemon`` that should
+be deployed beside ``nova-rootwrap`` and added to ``sudoers``.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  yorik-sar (Yuriy Taraday, YorikSar @ freenode)
+
+Work Items
+----------
+
+The only work item here is to implement new config variable and run rootwrap in
+daemon mode with it.
+
+Dependencies
+============
+
+* rootwrap-daemon-mode blueprint in oslo.rootwrap [#rw_bp]_.
+
+Testing
+=======
+
+This change doesn't change APIs so it doesn't require additional integration
+tests. If tempest is happy with ``use_rootwrap_daemon`` turned on, the feature
+works.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+.. [#rw_bp] oslo.rootwrap blueprint:
+   https://blueprints.launchpad.net/oslo/+spec/rootwrap-daemon-mode
+
+.. [#ne_ml] Original mailing list thread:
+   http://lists.openstack.org/pipermail/openstack-dev/2014-March/029017.html
+
+.. [#ne_eth] Original problem statement summarized here:
+   https://etherpad.openstack.org/p/neutron-agent-exec-performance
diff --git a/specs/juno/proposed/schedule-available-node-return.rst b/specs/juno/proposed/schedule-available-node-return.rst
new file mode 100644
index 0000000..edfeb32
--- /dev/null
+++ b/specs/juno/proposed/schedule-available-node-return.rst
@@ -0,0 +1,118 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================================
+Schedule the available nodes for nova-compute service
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/smart-available-node-return
+
+
+If nova compute service connects to multiple available nodes, the hypervisor
+driver needs to implement internal scheduling rules to return the most
+appropriate available node instead of always picking up the first one.
+
+
+Problem description
+===================
+
+In nova, nova-compute service can support multiple available nodes. When we
+build an instance, prep-resize for an instance, unshelve an instance, etc,
+the first(0th) available node is always returned in the manager. I suggest we
+should make the nova-compute able to select one available node in a certain
+rule.
+
+
+Proposed change
+===============
+
+* Each hypervisor driver can implement the internal rules to select one
+  available node. The rules can be different for different drivers.
+
+Alternatives
+------------
+
+Hypervisor drivers can expose multiple available nodes to nova-scheduler, but
+this will bring complexities to nova-scheduler.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  TBD
+
+
+Work Items
+----------
+
+* Internal scheduling rules should be implemented for each driver, which
+  supports multiple available nodes.
+
+
+Dependencies
+============
+
+Each hypervisor driver has to taken the internal scheduling rules into
+account.
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+TBD.
diff --git a/specs/juno/proposed/scheduler-host-az-caching.rst b/specs/juno/proposed/scheduler-host-az-caching.rst
new file mode 100644
index 0000000..5d4d8a1
--- /dev/null
+++ b/specs/juno/proposed/scheduler-host-az-caching.rst
@@ -0,0 +1,117 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================================
+Scheduler: add host az caching to AvailabilityZoneFilter
+========================================================
+
+https://blueprints.launchpad.net/nova/+spec/host-az-caching
+
+Add host az caching to the Availability Zone Filter to reduce db
+calls and therefore improve performance.
+
+Problem description
+===================
+
+Currently the AvailabilityZoneFilter does a lookup against the database to
+retrieve the availability zone for the host.
+
+The mapping between the host and az is usually static and so could be cached.
+
+Proposed change
+===============
+
+Implement a cache with a randomised timeout (default = 1hr) to prevent all
+hosts refreshing at the same time.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Scheduler filters get called once per host for every instance being created.
+Introducing host az caching will reduce time spent in the Availability Zone
+Filter.
+By caching the host az for an hour will reduce the calls to the database
+from once per host for every instance created to once per host per hour.
+
+Other deployer impact
+---------------------
+
+If users wish to modify from default of (60*60) 1 hour a config option of
+az_cache_timeout can be set to specify the time in seconds before a
+cached host-availability zone mapping is forgotten.
+
+Setting az_cache_timeout to -1 will disable host az caching.
+
+This change that takes immediate effect after its merged.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  moorryan
+
+Work Items
+----------
+
+Code posted - https://review.openstack.org/#/c/99127/
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit tests.
+
+Documentation Impact
+====================
+
+Add details of new configuration parameter 'az_cache_timeout'.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/separate-stats-from-periodic-task.rst b/specs/juno/proposed/separate-stats-from-periodic-task.rst
new file mode 100644
index 0000000..a87df56
--- /dev/null
+++ b/specs/juno/proposed/separate-stats-from-periodic-task.rst
@@ -0,0 +1,140 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================
+Separate the stats update with resource update
+==============================================
+
+https://blueprints.launchpad.net/nova/+spec/stats-out-update-resource
+
+Separate stats information update out of the update_available_resource()
+periodic task in compute resource tracker.
+
+Problem description
+===================
+
+Currently the update_available_resource() periodic task in compute resource
+tracker updates both the stats information and resource usage
+information, and persistent this information to DB.
+
+However, the resource usage changes and stats information is different. The
+resource usage may changes more frequently, while the stats information should
+be more static, especially if nova track everything correctly.
+
+So it will be better to have the stats update and resource update to happen
+with two different periodic intervals. Especially we can eliminate the
+instance/migration DB access for resource update using persistent claims.
+This will be great help for big scale cloud because the DB access will happen
+only in stats update, which is low frequent.
+
+Proposed change
+===============
+
+Resource tracker:
+
+* Separate the _update_usage_from_instance() to two function, one is for
+  resource availability/usage update, and one is for stats update.
+
+* Add a new function, update_stats() to update the stats information.
+
+Compute manager:
+
+* A new periodic task in compute manager to invoke resource tracker's
+  update_stats().
+
+* A new config option to decide the stats update frequency.
+
+Alternatives
+------------
+
+One alternative is to have the conductor to maintain the stats information
+since the conductor will know everything. However, how to keep the
+synchronization between multiple conductor instances will be tricky.
+
+Another alternative is don't keep the stats information at all, but calculated
+dynamically. But it will have bad performance.
+
+Data model impact
+-----------------
+
+No
+
+REST API impact
+---------------
+
+No
+
+Security impact
+---------------
+
+No
+
+Notifications impact
+--------------------
+
+No
+
+Other end user impact
+---------------------
+
+A new config option is added for the interval of the resource update and
+the stats update.
+
+Performance Impact
+------------------
+
+When combined with the persistent resource claim, it will reduce two DB
+access in the periodic task. Considering the DB accesses are per
+compute node, the performance benifit is big.
+
+Also this change reduce the periodic task a bit. Considering that task holds
+a lock, it will reduce the lock held time and help the performance.
+
+Other deployer impact
+---------------------
+
+Noe
+
+Developer impact
+----------------
+
+No
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  yunhong-jiang
+
+Work Items
+----------
+
+* Change the resource tracker code.
+* Change the compute manager code.
+
+
+Dependencies
+============
+
+This is related to the presistent resource claim at
+https://review.openstack.org/#/c/84906/
+
+Testing
+=======
+
+We need add some integration test to make sure the stats infomrmation is always
+up to date.
+
+Documentation Impact
+====================
+No
+
+References
+==========
+No
diff --git a/specs/juno/proposed/separated-policy-rule-v3-api.rst b/specs/juno/proposed/separated-policy-rule-v3-api.rst
new file mode 100644
index 0000000..bf662a9
--- /dev/null
+++ b/specs/juno/proposed/separated-policy-rule-v3-api.rst
@@ -0,0 +1,159 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Add separated policy rule for each v3 api
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/separated-policy-rule-v3-api
+
+There are different ways to add policy rule for an API in the v3 extension.
+In some extensions, there is only one policy rule for all the APIs in that
+extension. In other extensions, there is a policy rule for each API in that
+extension.
+
+So this BP want to add policy rule for each v3 API, then we can get finer
+granularity to permission control and consistent way to configure policy
+rule for API.
+
+Problem description
+===================
+
+1. It didn't have finer granularity permission control in v3 API, This
+problem is same with this bp https://blueprints.launchpad.net/nova/+spec/aggregate-api-policy
+that want to resolve.
+
+2. Hard to configure a policy rule for cloud operator. There isn't any
+document to mention that how to write an policy rule for each API. But
+there are some extensions only have one rule for all the API, and some
+extension is not. Cloud operator need check the source code to know How
+to write poliry rule for a specific API.
+
+Proposed change
+===============
+
+Add policy rule for each API in v3 API. Then it get finer granularity
+permission control. And Cloud Operator is easy to know how to write a
+policy rule. Because all the policy rule follow the pattern:
+compute_extension:v3:[extension_name]:[action_name]
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+It will make compute more secure since the one who is granted on one action
+may not be authorized to execute other actions in same extension.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Deployer get finer granularity permission control of API and consistent
+way to write policy rule.
+
+If the deployer have changed default rules for extensions that added
+separated in this BP, deployer need update their rules into those new
+separated rules.
+
+Developer impact
+----------------
+
+When developer add new extension, developer should provide policy rule
+for each API, not share an policy rule for the API in that extension.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Alex Xu <xuhj@linux.vnet.ibm.com>
+
+Other contributors:
+  Ji Chen <jcjichen@cn.ibm.com>
+
+Work Items
+----------
+
+admin_password
+agents
+attach_interface
+cells
+console_auth_token
+console_output
+consoles
+create_backup
+deferred_delete
+evacuate
+flavor_access
+flavor_manage
+hosts
+hypervisors
+ips
+multinic
+remote_consoles
+rescue
+server_actions
+server_diagnostics
+server_metadata
+server_password
+services
+
+Working list:
+https://etherpad.openstack.org/p/separated_policy_rule
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+For unittest, it will add to ensure the policy rule existed and works.
+For tempest, because of the difficulty of configuring policy in tempest, so it
+won't test in tempest.
+
+Documentation Impact
+====================
+
+For deployer, they can set policy rule for each API.
+
+References
+==========
+
+None
+
diff --git a/specs/juno/proposed/server-snapshot-support.rst b/specs/juno/proposed/server-snapshot-support.rst
new file mode 100644
index 0000000..6f4ab23
--- /dev/null
+++ b/specs/juno/proposed/server-snapshot-support.rst
@@ -0,0 +1,293 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================
+Add server snapshot support
+===========================
+
+https://blueprints.launchpad.net/nova/+spec/driver-specific-snapshot
+
+The server snapshot here means a snapshot/restore point used to save
+the status of an VM and restore this VM only, which is different from
+the nova snapshot. This proposal adds the server snapshot support for
+the VM, so that the VM can be fast restored from this server snapshot
+without connection to glance.
+
+Problem description
+===================
+
+There is already a snapshot concept in nova, which actually creates an
+image and uploads it to glance store for other VMs to spawn. This
+proposal bring in a server snapshot, which does not need to be uploaded
+to glance and is used for the fast recover of the master VM.
+
+It takes a very long time to upload and download a big image. With the
+server snapshot saved in the local repository of the hypervisor to keep
+the status of the VM, the VM can be restored from it directly without
+the image transfer.
+
+How to use the server snapshot?
+The VM allows multiple server snapshots to be created. The user can choose
+which server snapshot the VM will restore from. The server snapshots for
+one VM exist in a tree structure and they can save the root disk and the
+memory. Take VMware as an example, the snapshot is saved in the same folder as
+the VM. When we migrate the VM via the command, the server snapshots can be
+migrated together with the VM. Ephemeral storage and volumes are tentatively
+not in consideration. The server snapshot is used to restore the master VM
+only, not to spawn other new VMs.
+
+
+Proposed change
+===============
+
+Take VMware VCenter driver as the initial implementation: reference code
+can be found via https://review.openstack.org/#/c/85243/
+1. Add a new model for the server snapshot.
+2. Add an API extension to create server snapshot, revert/restore from
+the server snapshot, list the server snapshot for an instance and
+delete the server snapshot.
+3. Add quota management to the server snapshot. There will be a quota limit
+for the number of server snapshots one VM can create under one tenant.
+4. API extension to create server snapshot, revert/restore from
+the server snapshot, list the server snapshot for an instance and
+delete the server snapshot.
+
+Some hypervisors do not have to implement it, if it is not necessary.
+However, the others may implement it with their server snapshot.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+Add a data model for server snapshot.
+
+REST API impact
+---------------
+
+Use the nova api extension to implement it.
+
+* Create a server snapshot
+
+  * The user can created a server snapshot for a VM via this method.
+
+  * Method type: PUT
+
+  * Normal http response code(s): 201
+
+  * Resource URl: http://IP:port/instance/instance-ID
+
+  * The parameters contain server snapshot name, description and metadata.
+    Different hypervisors to have different key-value pairs for
+    configuration. For example, if we set memory=true for VMware, the memory
+    status will be saved.
+
+  * JSON schema definition for the body data:
+
+.. code-block:: none
+
+    {
+        "os-createServerSnapshot": {
+        "snapshot_name": "snapshot_name",
+        "description": "description",
+        "metadata": {
+            "memory": "false",
+            "quiesce": "true"
+        }
+        }
+    }
+
+  * JSON schema definition for the response data:
+
+.. code-block:: none
+
+    {
+        "server_snapshot": {
+            "instance_id": "instance_id",
+            "snapshot_name": "snapshot_name",
+            "snapshot_id": "snapshot_id",
+            "create_time": "create_time",
+            "description": "description",
+            "is_current_snapshot": "True",
+            "metadata": {
+                "vm_state": "vm_state",
+                "quiesced": "quiesced",
+                "replaySupported": "replaySupported"
+            }
+        }
+    }
+
+* List all the server snapshots for a VM
+
+  * The user can list all the available server snapshots for a given VM
+    via this method.
+
+  * Method type: GET
+
+  * Normal http response code(s): 202
+
+  * Resource URl: http://IP:port/instance/instance-ID
+
+  * The parameters contain VM ID.
+
+  * JSON schema definition for the body data:
+
+.. code-block:: none
+
+    {
+        "os-listServerSnapshot":
+            "instance_id": "instance_id"
+    }
+
+  * JSON schema definition for the response data:
+
+.. code-block:: none
+
+    {
+        "server_snapshots": [
+        {
+            "instance_id": "instance_id",
+            "snapshot_name": "snapshot_name",
+            "snapshot_id": "snapshot_id",
+            "create_time": "create_time",
+            "description": "description",
+            "is_current_snapshot": "True",
+            "metadata": {
+                 "vm_state": "vm_state",
+                 "quiesced": "quiesced",
+                 "replaySupported": "replaySupported"
+            }
+        }
+        ]
+    }
+
+* Restore the VM from one of its server snapshots
+
+  * The user can restore the VM to its server snapshot.
+
+  * Method type: POST
+
+  * Normal http response code(s): 203
+
+  * Resource URl: http://IP:port/instance/instance-ID
+
+  * The parameters contain server snapshot ID.
+
+  * JSON schema definition for the body data:
+
+.. code-block:: none
+
+    {
+        "os-restoreServerSnapshot": {
+            "snapshot_id": "snapshot_id"
+        }
+    }
+
+* Delete a server snapshot
+
+  * The user can delete the server snapshot via this method.
+
+  * Method type: DELETE
+
+  * Normal http response code(s): 204
+
+  * Resource URl: http://IP:port/server_snapshot/server_snapahot-ID
+
+  * The parameters contain server snapshot ID..
+
+  * JSON schema definition for the body data:
+
+.. code-block:: none
+
+    {
+        "os-listServerSnapshot": {
+            "snapshot_id": "snapshot_id"
+        }
+    }
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The users can create the nova snapshot and the server snapshot. If they
+do not need to save the snapshot in the glance store and just need fast
+recover for the master VM, they can choose to create the server snapshot.
+
+Performance Impact
+------------------
+
+It takes less time to recover the VM back to the server snapshot than
+rebuild the VM from the image.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  houshengbo(Vincent Hou)
+
+
+Work Items
+----------
+
+1. Add an API extension for the server snapshot.
+2. Add a data model for the server snapshot.
+3. Add the implementation of the server snapshot for the hypervisor,
+   which needs the server snapshot. The following interfaces will be
+   implemented: CreateServerSnapshot, RestoreFromServerSnapshot,
+   DeleteServerSnapshot and List/Get server Snapshot.
+
+An implementation for VMware drivers is going:
+https://review.openstack.org/#/c/85243/
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Add unit tests to test the actions of create, revert/restore, list
+and delete for the server snapshot.
+
+Documentation Impact
+====================
+
+Add explanations to the server snapshot and its difference from the
+nova snapshot.
+
+References
+==========
+
+https://blueprints.launchpad.net/nova/+spec/driver-specific-snapshot
+https://etherpad.openstack.org/p/live-snapshot
+A reference implementation for VMware: https://review.openstack.org/#/c/85243/
diff --git a/specs/juno/proposed/server_http_proxy.rst b/specs/juno/proposed/server_http_proxy.rst
new file mode 100644
index 0000000..221b9f0
--- /dev/null
+++ b/specs/juno/proposed/server_http_proxy.rst
@@ -0,0 +1,196 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================
+Nova Server HTTP Proxy
+======================
+
+https://blueprints.launchpad.net/nova/+spec/server-http-proxy
+
+An HTTP proxy on 169.254.169.254 to allow OpenStack API calls from nova
+servers launched within private neutron networks.
+
+
+Problem description
+===================
+
+It is becoming increasingly necessary for heat-provisioned nova servers
+to be able to perform requests against OpenStack endpoints. However since
+access to OpenStack endpoints is determined by the neutron network resources
+which the server is attached to, a trade-off is currently required between
+a server which can make the required API calls and a server which is in the
+desired network architecture.
+
+Examples of API calls which a server could legitimately make include:
+
+* Polling for heat resource metadata so that configuration changes can be
+  triggered when the metadata changes
+
+* Signalling to heat the output values of a completed configuration task
+
+* Pushing custom metrics to ceilometer to provide data for alarming or scaling
+  policies (eg, request latency)
+
+* Reading or writing to a swift object, using the object as a webhook or a
+  place to store data too large to transfer via other API calls
+
+* Push and poll to Marconi queues
+
+* Using credentials to generate a keystone token to use for all of the above
+
+There are some aspects of this problem which have been solved already,
+including the following:
+
+* Credentials are created by heat which are scoped to only that server, and
+  can also be limited by roles policy to only be able to make a limited set
+  of API calls
+
+* The neutron-metadata-agent allows the nova metadata API on 169.254.169.254
+  to be accessed from servers even when in private neutron networks
+
+Proposed change
+===============
+
+* An HTTP proxy is implemented in the nova metadata API, accessible via
+  169.254.169.254
+
+* OpenStack clients on the server are invoked with HTTP_PROXY pointing
+  at 169.254.169.254 with the provided credentials
+
+* The nova-api proxy is configured to only proxy requests to a white-list
+  of OpenStack endpoints
+
+Points for further discussion, in this spec review, or in a Design Summit
+session include whether:
+
+* the http-proxy endpoint should be in a sub-path of 169.254.169.254:80
+  or on a different port on 169.254.169.254
+
+* the implementation should occur in nova-api or neutron-metadata-agent
+
+* using paste.proxy meets the needs for implementing the proxy, or if a new
+  dependency or an in-tree proxy implementation is required
+
+* whether the white-list endpoints should be explicitly configured in nova.conf,
+  or if the white-list config only lists service names and a keystone lookup is
+  performed to confirm if the endpoint is valid, or if the white-list should
+  just be derived from all endpoints in the keystone catalog.
+
+Alternatives
+------------
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None, except for a new path which represents the proxy URL
+
+OpenStack client operations can be performed from nova provisioned servers
+if they are invoked with the following
+* HTTP_PROXY url
+* valid credentials
+* keystone endpoint
+
+Security impact
+---------------
+
+A private neutron network may have been chosen due to the sensitive workloads
+within it so the following security implications need to be considered:
+
+* Can a compromised server be used to get information out of the private network
+* Can a compromised user account perform actions on servers within the private
+  network which wouldn't otherwise be possible if the proxy was disabled
+
+The above 2 points needs to be considered for every API operation which is
+allowed to proxy via this mechanism. It could be that in the long term a user
+will want to be able to specify what proxying is allowed during nova boot.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+* Increased load on nova-api
+
+* Increased load on keystone
+
+* Increased load on neutron-metadata-agent
+
+Other deployer impact
+---------------------
+
+A deployer needs to decide whether they will enable this feature, and possibly
+what API operations they will allow via this proxy method.
+
+The current intent would be for the default configuration to enable proxying
+of an agreed-upon list of API operations
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Steve Baker (steve-stevebaker)
+
+Work Items
+----------
+
+* Implement proxy in nova-api
+* Implement nova configuration for proxy policy
+* Implement neutron-metadata-agent forwarding to proxy if port is other than 80
+
+
+Dependencies
+============
+
+Will evaluate paste.proxy as being suitable for this feature. Otherwise will
+investigate options for an in-tree proxy implementation or a new library
+dependency.
+
+
+Testing
+=======
+
+This can be tested in tempest by modifying the existing heat tests to launch
+nova servers in private networks, the assert that the existing server operations
+still work via the proxy.
+
+There are no known infrastructure barriers to testing this in the current
+gate environment.
+
+
+Documentation Impact
+====================
+
+* Document the nova.conf policy configuration
+* Document what the proxy URL will be and how to configure an OpenStack client
+  to use it.
+
+Impact on the docs team should be minimal.
+
+References
+==========
diff --git a/specs/juno/proposed/set-vm-swapfile-location.rst b/specs/juno/proposed/set-vm-swapfile-location.rst
new file mode 100644
index 0000000..aba4bcd
--- /dev/null
+++ b/specs/juno/proposed/set-vm-swapfile-location.rst
@@ -0,0 +1,127 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================================================
+add configuration item to set virtual machine swap file location
+=================================================================
+
+https://blueprints.launchpad.net/nova/+spec/set-vm-swapfile-location
+
+Add support to set the location of virtual machine swap files separately.
+with such a feature enabled, swapfiles can be placed onto a specified storage,
+e.g. a SSD, separately, that improve the performance of guest VM.
+
+Problem description
+====================
+
+Currently, disk.swap(the swap file of instance) is created in the
+<instances_path> (default is /var/lib/nova/instances/<vm-uuid>),
+the base file is in /var/lib/nova/instances/_base/swap_XXX.
+We cannot place the swap files onto a specified storage, e.g. a SSD,
+separately.
+
+This feature that I want to implement is targeted for
+libvirt-kvm.
+
+Proposed change
+================
+
+1.Add configuration item <swap_files_path> in nova.conf to
+  set the location of virtual machine swap file, such as:
+  swap_files_path=/var/lib/nova/swapfiles/
+
+2.Place the swap file and its base file onto <swap_files_path>.
+
+Alternatives
+-------------
+
+None
+
+Data model impact
+------------------
+
+None
+
+REST API impact
+----------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+---------------------
+
+None
+
+Other end user impact
+-----------------------
+
+None
+
+Performance Impact
+---------------------
+
+1.If the swap files are placed on a SSD, I think it will improve
+the performance of guest vm in some cases.
+
+2.There is no impact about the performance of nova.
+
+Other deployer impact
+----------------------
+
+To strive for better performance, deployers can configure <swap_files_path>
+to place the swap files of VM onto a specified storage, e.g. a SSD.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+assignee:Zhou Yu <vitas.yuzhou@huawei.com>
+
+
+Work Items
+----------
+
+Only implement this feature in libvirtDriver.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Unit tests will check if nova create swapfiles onto <swap_files_path>,
+the code changes about this bp are little, I think unit tests are sufficient.
+
+
+Documentation Impact
+====================
+
+Add a suggestion in configuration doc, such as:
+To strive for better performance, deployers can configure <swap_files_path>
+to place the swap files of VM onto a specified storage, e.g. a SSD.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/shelf-snapshot-selection.rst b/specs/juno/proposed/shelf-snapshot-selection.rst
new file mode 100644
index 0000000..9b64cb8
--- /dev/null
+++ b/specs/juno/proposed/shelf-snapshot-selection.rst
@@ -0,0 +1,179 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Shelf Snapshot Selection
+==========================================
+
+Nova BP:
+https://blueprints.launchpad.net/nova/+spec/shelf-snapshot-selection
+Novaclient BP:
+https://blueprints.launchpad.net/python-novaclient/+spec/shelf-snapshot-selection
+
+Add an option to the existing Shelving API to allow for specifying an existing
+snapshot when shelving a VM.
+
+
+Problem description
+===================
+
+The current implementation of the shelving API allows for instances to be
+offloaded while retaining its UUID and networking info. Each time an instance
+is shelved a snapshot of the running instance is created. Some instances such
+as web server pools contain local data that doesn’t change much from day to
+day, and are often created based on the same image. Users should be able shelve
+offload these types of instances without creating additional new snapshots
+provided a snapshot of these instances already exist. This would allow users to
+take an instance offline immediately without waiting for a new snapshot to be
+created. This will also save snapshot storage space.
+
+The following is an example of how this feature might work in the wild: Let's
+say there are 5 instances currently running that are built from the same image.
+After shelving 1 of the instances, a snapshot of the instance would be created.
+If an user wants to shelve the remaining instances, they should be able to
+select an snapshot to associate with the instance when booting up the shelved
+instance, and thus allow the instance to shelve offloaded without creating
+additional snapshots.
+
+Instances taking advantage of this feature would be assumed to be based off the
+same image. To enforce this, we will make use of the instance's image_ref when
+comparing the instance being shelved to the instance of the snapshot that was
+selected.
+
+
+Proposed change
+===============
+
+Allow operators to specify the name of the snapshot that would be created when
+shelving an instance. This is different from shelve snapshot selection.
+Currently, a default name is assigned to the snapshot that is created when an
+instance is shelved and uses the following naming convention "<instance display
+name>-shelved". The purpose of this is to override the default name of the
+snapshot created and help clarify snapshots that are associated with multiple
+shelved instances, where using the default naming convention would be
+undesirable.
+Example nova command CLI call: nova shelve --name <snapshot name> <instance>
+
+Allow operators to specify an existing snapshot's UUID when shelving an
+instance and then associates the instance's shelved image id with the specified
+snapshot. If the snapshot selected does not exist, shelving will fail and an
+appropriate error message will be provided. To minimize the risk that an
+operator uncarefully selects a wrong snapshot, the instance will be soft
+deleted and allow a reasonable interval to reclaim the instance.s
+Example nova command CLI call: nova shelve --image <image UUID> <instance>
+
+
+Alternatives
+------------
+
+None.
+
+
+Data model impact
+-----------------
+
+Currently, the instance metadata contains a field shelved_image_id that keeps
+track of the shelved image id for the given instance. No additional field will
+be needed in instance object.
+
+
+REST API impact
+---------------
+
+None. The blueprint perserves the existing behaviour of the current POST REST
+API for shelving, and no additional image ref will need to be added.
+Currently, the POST method takes in the following:
+
+* server:
+
+    * name
+    * imageRef
+    * flavorRef
+    * metadata
+    * personality
+
+This sample REST api is found in:
+nova/doc/api_samples/os-shelve/server-post-req.json
+
+The original shelve API creates the image prior to the POST request to shelve
+the instance.
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+None
+
+
+Performance Impact
+------------------
+
+None
+
+
+Other deployer impact
+---------------------
+
+None
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Gabriel Luong<gabriel.luong@gmail.com>
+
+
+Work Items
+----------
+
+* nova: Add shelve snapshot selection
+
+* novaclient: Add --image argument to the shelve command
+
+
+Dependencies
+============
+
+
+Testing
+=======
+
+Add unit tests for the shelve snapshot selection in the shelving and unshelving
+test files.
+
+
+Documentation Impact
+====================
+
+Add documentation regarding the new options available for the shelving API.
+
+
+References
+==========
+
+https://blueprints.launchpad.net/nova/+spec/shelve-instance
diff --git a/specs/juno/proposed/simultaneous-server-group.rst b/specs/juno/proposed/simultaneous-server-group.rst
new file mode 100644
index 0000000..3f3521a
--- /dev/null
+++ b/specs/juno/proposed/simultaneous-server-group.rst
@@ -0,0 +1,492 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=========================================
+Simultaneous Scheduling of a Server Group
+=========================================
+
+https://blueprints.launchpad.net/nova/+spec/simultaneous-server-group
+
+This blueprint is about extending server group functionality so that
+the scheduling decisions about all the members can be made
+simultaneously.  Currently scheduling decisions are made serially ---
+one member (or homogenous cohort) is scheduled at a time, taking into
+account the decisions that have been made previously.  The current
+approach can paint itself into a corner that would be avoided if the
+decisions were made simultaneously.  That is, the new implementation
+will transform the scheduling input into a multi-variable optimization
+problem about the whole group and solve it.  Following OpenStack
+tradition, this blueprint uses the term "scheduling" even though it is
+really focused only on placement.
+
+
+Problem description
+===================
+
+Consider the following use case.  A cloud user wants to create a
+3-tier web application with a preference for affinity among all the
+servers (that is, VM instances) in the application.  The three
+different servers have different images and flavors.  The cloud
+operator prefers to keep some of his hosts highly utilized and the
+rest powered off.  Consider a situation in which, for each tier of the
+new application, there is sufficient available capacity on some host
+but no powered-up host has capacity for all three tiers.  There is
+also a powered-down host that could accommodate all three tiers.  With
+the current scheduling approach, the application will be spread across
+those three distant hosts.  With the new scheduling approach the whole
+application could be co-located on a formerly powered-down host.
+
+Proposed change
+===============
+
+The proposed change is described in distinct sections below for API,
+Data Model, and Implementation.
+
+This builds on two or three other recent or current pieces of work.
+The server group functionality is still being completed and
+documented.  It is hoped that the work proposed in
+https://blueprints.launchpad.net/nova/+spec/solver-scheduler will be
+revived and utilized here.  Finally, as reservations are involved,
+this might somehow use Blazar (nee Climate); today Blazar does not
+seem to expose the needed functionality, which is reserving fractions
+of host capacity without creating virtual resources.
+
+There are several independent directions in which follow-on work can
+proceed.  One is increasing the expressiveness of the input: refining
+the existing policy types and adding additional policy types, and
+allowing nesting of groups.  Another direction is generalizing beyond
+Compute instances, which can be done in two steps: first allowing the
+Compute instances to be constrained by already-scheduled resources of
+other types (e.g., Cinder volumes), second by bringing the non-Compute
+resources into the simultaneous decision-making.
+
+API
+
+The current API for server groups can be organized into two phases:
+defining the group, and then creating the members of the group.  The
+proposed change adds two phases and moves the scheduling
+decision-making from one phase to an earlier one.  The proposed
+phases are as follows.
+
+#. Group Definition/Update
+#. Simultaneous Scheduling
+#. Member Creation/Update
+#. Final Confirmation
+
+The flow can be summarized in ASCII art as follows.
+
+| Group Definition/Update<----+
+|            +                |
+|            |                |
+|            |                |
+|            v                |
+| Simultaneous Scheduling+----+
+|            +                |
+|            |                |
+|            |                |
+|            v                |
+| Member Creation/Update+-----+
+|            +                |
+|            |                |
+|            |                |
+|            v                |
+| Final Confirmation+---------+
+
+In the Group Definition phase the group's identity is established, the
+client provides the group's scheduling policies, and the client
+describes each intended member of the group.  Each of those
+descriptions carries the information needed for scheduling --- which
+is less than the information needed later for creation/update.  In
+particular, a member description provided by the client in the Group
+Definition phase is the following subset of the information provided
+in the create/update operation: flavor, availability zone, scheduling
+hints (if any are needed beyond the group's policies).  The client
+also supplies an identifier for each member, with no two members in a
+group sharing an identifier.  Because the client supplies the ID, it
+is easy to make this operation idempotent.
+
+The client can return to the Group Definition/Update phase at any
+time, to modify the policies and member descriptions for an existing
+group.
+
+In the Simultaneous Scheduling phase, the client invokes one
+operation that causes the joint decision to be made.  This takes into
+account the policies and member descriptions that are currently in
+effect, and returns a token for the decision made.  The decision is
+internally remembered in a reservation, perhaps using Climate's
+reservation fucntionality.  The reservation is only leased.  The lease
+time is long, but the client must occasionally refresh the lease if
+the client wants to hold it for a long time.  The reservation needs to
+be held until the Final Confirmation phase is completed.  This is a
+matter of the time to do management operations, this is NOT the time
+for the created/updated application to complete its work.
+
+After requesting that a joint decision be made, the client can then
+issue the same request again (without changing the group's policies or
+its set of member descriptions); in this case the implementation
+considers the problem to be unchanged (even though other aspects of
+the problem could have actually changed), does not change its
+decision, and *does* return the same decision token.  In short, the
+joint decision making operation is idempotent.
+
+While a decision has been made but not yet reached Final Confirmation,
+the client can potentially return to Group Definition/Update to modify
+the policies and member descriptions and then request a new joint
+decision.  In this case the new decision overrides the old decision;
+the old decision's lease is terminated and a new lease is started.
+
+In the Member Creation/Update phase, the client proceeds (much like it
+does today) to create/update the individual members of the group.
+Each creation operation includes a reference to the group.  The
+proposed change here is that each creation operation will also include
+the corresponding member identifier (or identifiers, in the case of
+creation of a homogenous batch of servers).
+
+During the Member Creation/Update phase, in general some create
+operations will succeed, some will fail, and some will never even be
+tried (i.e., the client forgoes them because of earlier failures).
+For some of the failures, Nova will recognize that the reserved
+capacities are nonetheless being used; for others, Nova will know that
+the reserved capacities are not being used; and for the remainder Nova
+will not be sure.
+
+If a create operation requests different capacities than were in the
+corresponding member description...
+
+If a create operation includes a group ID but not a member ID then it
+is scheduled on its own at that time, using the group's policies, and
+is added to the group.  In other words, the current behavior applies.
+If a create operation does not include a group ID then the server is
+scheduled in isolation and is not added to a group, just like the
+current behavior.
+
+In the Final Confirmation phase, the client invokes one operation (the
+final confirmation operation) and gives it the corresponding decision
+token.  This allows the implementation to quickly free capacities that
+were reserved but Nova now knows will not be used.
+
+The implementation will build on the work on the solver scheduler.  It
+will introduce a new scheduling operation, which takes a whole group
+as input.  It also introduces operations for refreshing a lease and
+doing final confirmation.
+
+This is proposed to happen after the Gantt and no-db-scheduler work
+are done.
+
+
+Alternatives
+------------
+
+The API could be simplified by moving responsibility for creating the
+members from the client into the implementation.  That has been
+considered and rejected, because it removes desired modularity from
+the system.
+
+Rather than describing all the group members up front, one alternative
+would be to continue with the current style of API in which group
+members are introduced only as they are created but, as each member
+(or homogenous cohort) is introduced, make a new joint decision about
+all the group members revealed thus far (with a preference for leaving
+already-placed members in place).  If it is decided to move an
+already-placed member then live migration would be used to move that
+member to the newly-chosen place.  This has the disadvantage of
+increasing the asymptotic computational complexity of creating a group
+by a factor that is the number of create operations.  Also, live
+migration is not always possible.  And when it is possible, it has
+some costs: it uses some memory, CPU, disk, and network --- thus
+having an indirect performance on all servers using those underlying
+resources.  Live migration also has some direct impact on the server
+being migrated.
+
+Consider this modification of the previous alternative: when a new
+server (or homogenous cohort) is created, first try sequential
+scheduling (that is, see if it can be placed without allowing moves of
+already-placed servers) and, only if it cannot, do simultaneous
+scheduling of the whole group-thus-far and do live migration of
+existing members for which a new placement is chosen.  The performance
+of this alternative depends on the probability that the sequential
+scheduling fails.  That probability is difficult to estimate, but I
+expect it will be low in most situations.  The expected computational
+cost of this alternative will be lower than that of the previous, by
+an amount that depends on that difficult-to-estimate probability; the
+other disadvantages of the previous approach apply to this one too.
+Additionally, this alternative will produce a less good placement than
+the previous alternative would in cases where sequential scheduling
+finds some placement but simultaneous scheduling finds a better
+placement.  If all the placement policies are hard --- that is, must
+be satisifed --- then every placement is either allowed or disallowed
+and that last disadvantage does not apply; OTOH if some placement
+policies are soft --- that is, state a goal that can be met to some
+degree --- then some placements are better than others and the last
+disadvantage applies.
+
+
+Data model impact
+-----------------
+
+New Data Model
+^^^^^^^^^^^^^^
+
+The proposed change introduces a table, which might be called this
+instance_requests table, to hold the member descriptions provided in
+the Group Definition/Update phase.  This table will have a record for
+each member description, keyed by the combination of the group UUID
+and the client-supplied ID for the member description.  The dependent
+fields of the record will contain the description.
+
+The proposed change also introduces another table, which might be
+called the instance_decisions table, to hold the decisions made.  It
+will have a record for each member for which a decision has been made.
+The key of a record will be compounded from the group's UUID, the
+decision token, and the client-supplied member description ID.
+
+The proposed change also introduces a table, which might be called the
+decisions table.  It holds a record for each decision.
+
+The usual story about deleted records.
+
+Schema and data migration
+^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The new tables are added before the new operations are enabled.  No
+records need to be created as the new tables are added.  There are no
+changes to the schemas of existing tables.
+
+REST API impact
+---------------
+
+Each API method which is either added or changed should have the following
+
+* Specification for the method
+
+  * A description of what the method does suitable for use in
+    user documentation
+
+  * Method type (POST/PUT/GET/DELETE)
+
+  * Normal http response code(s)
+
+  * Expected error http response code(s)
+
+    * A description for each possible error code should be included
+      describing semantic errors which can cause it such as
+      inconsistent parameters supplied to the method, or when an
+      instance is not in an appropriate state for the request to
+      succeed. Errors caused by syntactic problems covered by the JSON
+      schema defintion do not need to be included.
+
+  * URL for the resource
+
+  * Parameters which can be passed via the url
+
+  * JSON schema definition for the body data if allowed
+
+  * JSON schema definition for the response data if any
+
+* Example use case including typical API samples for both data supplied
+  by the caller and the response
+
+* Discuss any policy changes, and discuss what things a deployer needs to
+  think about when defining their policy.
+
+Example JSON schema definitions can be found in the Nova tree
+http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute/schemas/v3
+
+Note that the schema should be defined as restrictively as
+possible. Parameters which are required should be marked as such and
+only under exceptional circumstances should additional parameters
+which are not defined in the schema be permitted (eg
+additionaProperties should be False).
+
+Reuse of existing predefined parameter types such as regexps for
+passwords and user defined names is highly encouraged.
+
+Idempotency impact
+^^^^^^^^^^^^^^^^^^
+
+The proposed change does not make things worse.  The new operations
+are idempotent, and the old ones remain as bad as they currently are.
+
+Security impact
+---------------
+
+Describe any potential security impact on the system.  Some of the items to
+consider include:
+
+* Does this change touch sensitive data such as tokens, keys, or user data?
+
+* Does this change alter the API in a way that may impact security, such as
+  a new way to access sensitive information or a new way to login?
+
+* Does this change involve cryptography or hashing?
+
+* Does this change require the use of sudo or any elevated privileges?
+
+* Does this change involve using or parsing user-provided data? This could
+  be directly at the API level or indirectly such as changes to a cache layer.
+
+* Can this change enable a resource exhaustion attack, such as allowing a
+  single API interaction to consume significant server resources? Some examples
+  of this include launching subprocesses for each connection, or entity
+  expansion attacks in XML.
+
+For more detailed guidance, please see the OpenStack Security Guidelines as
+a reference (https://wiki.openstack.org/wiki/Security/Guidelines).  These
+guidelines are a work in progress and are designed to help you identify
+security best practices.  For further information, feel free to reach out
+to the OpenStack Security Group at openstack-security@lists.openstack.org.
+
+Notifications impact
+--------------------
+
+Please specify any changes to notifications. Be that an extra notification,
+changes to an existing notification, or removing a notification.
+
+Other end user impact
+---------------------
+
+Aside from the API, are there other ways a user will interact with this feature?
+
+* Does this change have an impact on python-novaclient? What does the user
+  interface there look like?
+
+Performance Impact
+------------------
+
+Describe any potential performance impact on the system, for example
+how often will new code be called, and is there a major change to the calling
+pattern of existing code.
+
+Examples of things to consider here include:
+
+* A periodic task might look like a small addition but if it calls conductor or
+  another service the load is multiplied by the number of nodes in the system.
+
+* Scheduler filters get called once per host for every instance being created, so
+  any latency they introduce is linear with the size of the system.
+
+* A small change in a utility function or a commonly used decorator can have a
+  large impacts on performance.
+
+* Calls which result in a database queries (whether direct or via conductor) can
+  have a profound impact on performance when called in critical sections of the
+  code.
+
+* Will the change include any locking, and if so what considerations are there on
+  holding the lock?
+
+Other deployer impact
+---------------------
+
+Discuss things that will affect how you deploy and configure OpenStack
+that have not already been mentioned, such as:
+
+* What config options are being added? Should they be more generic than
+  proposed (for example a flag that other hypervisor drivers might want to
+  implement as well)? Are the default values ones which will work well in
+  real deployments?
+
+* Is this a change that takes immediate effect after its merged, or is it
+  something that has to be explicitly enabled?
+
+* If this change is a new binary, how would it be deployed?
+
+* Please state anything that those doing continuous deployment, or those
+  upgrading from the previous release, need to be aware of. Also describe
+  any plans to deprecate configuration values or features.  For example, if we
+  change the directory name that instances are stored in, how do we handle
+  instance directories created before the change landed?  Do we move them?  Do
+  we have a special case in the code? Do we assume that the operator will
+  recreate all the instances in their cloud?
+
+Developer impact
+----------------
+
+Discuss things that will affect other developers working on OpenStack,
+such as:
+
+* If the blueprint proposes a change to the driver API, discussion of how
+  other hypervisors would implement the feature is required.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Who is leading the writing of the code? Or is this a blueprint where you're
+throwing it out there to see who picks it up?
+
+If more than one person is working on the implementation, please designate the
+primary author and contact.
+
+Primary assignee:
+  <launchpad-id or None>
+
+Other contributors:
+  <launchpad-id or None>
+
+Work Items
+----------
+
+Work items or tasks -- break the feature up into the things that need to be
+done to implement it. Those parts might end up being done by different people,
+but we're mostly trying to understand the timeline for implementation.
+
+
+Dependencies
+============
+
+* Include specific references to specs and/or blueprints in nova, or in other
+  projects, that this one either depends on or is related to.
+
+* If this requires functionality of another project that is not currently used
+  by Nova (such as the glance v2 API when we previously only required v1),
+  document that fact.
+
+* Does this feature require any new library dependencies or code otherwise not
+  included in OpenStack? Or does it depend on a specific version of library?
+
+
+Testing
+=======
+
+Please discuss how the change will be tested. We especially want to know what
+tempest tests will be added. It is assumed that unit test coverage will be
+added so that doesn't need to be mentioned explicitly, but discussion of why
+you think unit tests are sufficient and we don't need to add more tempest
+tests would need to be included.
+
+Is this untestable in gate given current limitations (specific hardware /
+software configurations available)? If so, are there mitigation plans (3rd
+party testing, gate enhancements, etc).
+
+
+Documentation Impact
+====================
+
+What is the impact on the docs team of this change? Some changes might require
+donating resources to the docs team to have the documentation updated. Don't
+repeat details discussed above, but please reference them here.
+
+
+References
+==========
+
+Please add any useful references here. You are not required to have any
+reference. Moreover, this specification should still make sense when your
+references are unavailable. Examples of what you could include are:
+
+* Links to mailing list or IRC discussions
+
+* Links to notes from a summit session
+
+* Links to relevant research, if appropriate
+
+* Related specifications as appropriate (e.g.  if it's an EC2 thing, link the EC2 docs)
+
+* Anything else you feel it is worthwhile to refer to
diff --git a/specs/juno/proposed/slow-queries.rst b/specs/juno/proposed/slow-queries.rst
new file mode 100644
index 0000000..4245d7d
--- /dev/null
+++ b/specs/juno/proposed/slow-queries.rst
@@ -0,0 +1,260 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================
+Optimise slow queries
+=====================
+
+https://blueprints.launchpad.net/nova/+spec/nova-juno-slow-queries
+
+Problem description
+===================
+
+Analysis of the MySQL slow query log has revealed some queries which are not
+using indexes. This causes a scalability issue when combined with soft delete.
+
+Proposed change
+===============
+
+This specification proposes a number of changes to table indexes to alleviate
+these symptoms.
+
+The volume_usage_cache table is missing an index on (volume_id)::
+
+    Count: 9848  Time=3.71s (36526s)
+      SELECT ...
+      FROM volume_usage_cache
+      WHERE volume_usage_cache.volume_id = 'S'
+      LIMIT N
+
+The reservations table is missing an index on (deleted, expire)::
+
+    Count: 612  Time=7.51s (4596s)
+      UPDATE reservations SET updated_at=updated_at, deleted_at='S', deleted=id
+      WHERE reservations.deleted = N AND reservations.expire < 'S'
+
+    Count: 612  Time=7.22s (4420s)
+      SELECT ...
+      FROM reservations INNER JOIN quota_usages
+        ON quota_usages.id = reservations.usage_id
+      WHERE reservations.deleted = N AND reservations.expire < 'S'
+
+Table instances has several indexes which include 'deleted'::
+
+    host, deleted
+    uuid, deleted
+    host, node, deleted
+    host, deleted, cleaned
+
+As 'deleted' is often a very powerful way to reduce the result, it should go
+first. Further, the (host, deleted) index is unnecessary, as the more-specific
+indexes can provide that::
+
+    deleted, uuid
+    deleted, host, node
+    deleted, host, cleaned
+
+An additional index should be added to cover (deleted, project_id, user_id)::
+
+    Count: 1236  Time=2.70s (3342s)
+      SELECT count(i.id), sum(instances.vcpus), sum(i.memory_mb)
+      FROM instances i
+      WHERE i.deleted = N
+      AND i.project_id = 'S'
+      AND i.user_id = 'S'
+      LIMIT N
+
+Finally, an index should be added to cover (deleted, vm_state, created_at,
+project_id). These queries are somewhat complex but very similar, the
+interesting part of each is the subselect::
+
+    Count: 2417  Time=2.96s (7160s)
+      SELECT ...
+      FROM (SELECT ...
+            FROM instances
+            WHERE i.deleted = N
+            AND i.vm_state != 'S'
+            AND i.project_id = 'S'
+            ORDER BY i.created_at DESC, i.created_at DESC,
+                     i.created_at DESC, i.id DESC
+            LIMIT N) AS a
+      LEFT OUTER JOIN security_group_instance_association AS sgia
+        ON sgia.instance_uuid = a.instances_uuid
+        AND a.instances_deleted = N
+      LEFT OUTER JOIN security_groups AS sg
+        ON sg.id = sgia.security_group_id
+        AND sgia.deleted = N
+        AND sg.deleted = N
+      LEFT OUTER JOIN instance_info_caches AS iic
+        ON iic.instance_uuid = a.instances_uuid
+      ORDER BY a.instances_created_at DESC, a.instances_created_at DESC,
+               a.instances_created_at DESC, a.instances_id DESC
+
+    Count: 399  Time=3.02s (1206s)
+      SELECT ...
+      FROM (SELECT ...
+            FROM instances
+            WHERE i.deleted = N
+            AND i.vm_state != 'S'
+            AND i.project_id = 'S'
+            ORDER BY i.created_at DESC, i.created_at DESC,
+                     i.created_at DESC, i.id DESC
+            LIMIT N) AS a
+      LEFT OUTER JOIN instance_info_caches AS iic
+        ON iic.instance_uuid = a.instances_uuid
+      LEFT OUTER JOIN security_group_instance_association AS sgia
+        ON sgia.instance_uuid = a.instances_uuid
+        AND a.instances_deleted = N
+      LEFT OUTER JOIN security_groups AS sg
+        ON sg.id = sgia.security_group_id
+        AND sgia.deleted = N
+        AND sg.deleted = N
+      ORDER BY a.instances_created_at DESC, a.instances_created_at DESC,
+               a.instances_created_at DESC, a.instances_id DESC
+
+    Count: 52  Time=5.38s (279s)
+      SELECT ...
+      FROM (SELECT ...
+            FROM instances
+            WHERE i.deleted = N
+            AND i.vm_state != 'S'
+            AND (i.created_at < 'S' OR i.created_at = 'S'
+            AND i.created_at < 'S' OR i.created_at = 'S'
+            AND i.created_at = 'S'
+            AND i.id < N)
+            ORDER BY i.created_at DESC, i.created_at DESC,
+                     i.created_at DESC, i.id DESC LIMIT N) AS a
+      LEFT OUTER JOIN security_group_instance_association AS sgia
+        ON sgia.instance_uuid = a.instances_uuid
+        AND a.instances_deleted = N
+      LEFT OUTER JOIN security_groups AS sg
+        ON sg.id = sgia.security_group_id
+        AND sgia.deleted = N
+        AND sg.deleted = N
+      LEFT OUTER JOIN instance_info_caches AS iic
+        ON iic.instance_uuid = a.instances_uuid
+      ORDER BY a.instances_created_at DESC, a.instances_created_at DESC,
+               a.instances_created_at DESC, a.instances_id DESC
+
+    Count: 15  Time=3.05s (45s)
+      SELECT ...
+      FROM (SELECT ...
+            FROM instances
+            WHERE i.deleted = N
+            AND i.vm_state != 'S'
+            ORDER BY i.created_at DESC, i.created_at DESC,
+                     i.created_at DESC, i.id DESC
+            LIMIT N) AS a
+      LEFT OUTER JOIN security_group_instance_association AS sgia
+        ON sgia.instance_uuid = a.instances_uuid
+        AND a.instances_deleted = N
+      LEFT OUTER JOIN security_groups AS sg
+        ON sg.id = sgia.security_group_id
+        AND sgia.deleted = N
+        AND sg.deleted = N
+      LEFT OUTER JOIN instance_info_caches AS iic
+        ON iic.instance_uuid = a.instances_uuid
+      ORDER BY a.instances_created_at DESC, a.instances_created_at DESC,
+               a.instances_created_at DESC, a.instances_id DESC
+
+Alternatives
+------------
+
+The scalability issue can be mitigated through automatically clearing down
+soft-deleted rows. However performance will always be improved through correct
+use of indexes.
+
+Data model impact
+-----------------
+
+At least one migration is proposed by this change. It is open for discussion
+how many migrations the proposed changes are split over.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+Database performance should be improved. A modest amount of additional storage
+will be required to hold the additional indexes.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  alexisl
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+ * Add volume_usage_cache (volume_id)
+ * Add reservations (deleted, expire)
+ * Delete instances indexes::
+    host, deleted
+    uuid, deleted
+    host, node, deleted
+    host, deleted, cleaned
+ * Add instances indexes::
+    deleted, uuid
+    deleted, host, node
+    deleted, host, cleaned
+    deleted, project_id, user_id
+    deleted, vm_state, created_at, project_id
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+I'd expect this to be adequately tested by whatever performance testing is
+already in place.
+
+Documentation Impact
+====================
+
+None.
+
+References
+==========
+
+None.
diff --git a/specs/juno/proposed/soft-affinity-for-server-group.rst b/specs/juno/proposed/soft-affinity-for-server-group.rst
new file mode 100644
index 0000000..13a2454
--- /dev/null
+++ b/specs/juno/proposed/soft-affinity-for-server-group.rst
@@ -0,0 +1,163 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================
+Add soft affinity support for server group
+=============================================
+
+https://blueprints.launchpad.net/nova/+spec/soft-affinity-for-server-group
+
+As a tenant I would like to schedule instances on the same host if possible,
+so that I can achieve colocation. However if it is not possible to schedule
+some instance to the same host then I still want that the subsequent
+instances are scheduled together on another host. In this way I can express
+a good-to-have relationship between a group of instances.
+
+As a tenant I would like to schedule instances on different hosts if possible.
+However if it is not possible I still want my instances to be scheduled even
+if it means that some of them are placed on a host where another instances
+are running from the same group.
+
+
+Problem description
+===================
+
+End User might want to have a less strict affinity and anti-affinity
+rule than what is today available in server-group API extension.
+With the proposed good-to-have affinity rule the End User can request nova
+to schedule the instance to the same host if possible. However if it is not
+possible (e.g. due to resource limitations) then End User still wants to keep
+the instances on a small amount of different host.
+With the proposed good-to-have anti-affinity rule the End User can request
+nova to spread the instances in the same group as much as possible.
+
+
+Proposed change
+===============
+
+This change would extend the existing server-group API extension with two new
+policies soft-affinity and soft-anti-affinity.
+When a instance is booted into a group with soft-affinity policy the scheduler
+will use a new weight AffinityWeight to sort the available hosts according to
+the number of instances running on them from the same server-group in a
+descending order.
+When an instance is booted into a group with soft-anti-affinity policy the
+scheduler will use a new weight AntiAffinityWeight to sort the available hosts
+according to the number of instances running on them from the same
+server-group in a ascending order.
+
+The two new weights will get the necessary information about the number of
+instances per host through the weight_properties (filter_properties) in
+a similar way as the GroupAntiAffinityFilter gets the list of hosts used by
+a group via the filter_properties.
+
+These new soft-affinity and soft-anti-affinity policies are mutually exclusive
+with each other and with the other existing server-group policies.
+
+Alternatives
+------------
+
+Alternatively End User can use the server-group with affinity policy and if
+the instance cannot be scheduled because the host associated to the group is
+full then End User can create a new server-group for the subsequent instances.
+However with large amount of instances that occupy many hosts this manual
+process can become quite cumbersome.
+
+Data model impact
+-----------------
+
+No schema change is needed.
+
+There will be two new possible values soft-affinity and soft-anti-affinity for
+he policy column of the instance_group_policy table.
+
+REST API impact
+---------------
+
+POST: v2/{tenant-id}/os-server-groups
+  The value of the policy request parameter can be soft-affinity and
+  soft-anti-affinity as well.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  balazs-gibizer
+
+
+Work Items
+----------
+
+* Add two new weights to the filter scheduler. These weights will
+  sort the available hosts by the number of instances from the same
+  server-group.
+* Update FilterScheduler to use the proper weight for the instance scheduling
+  if soft-affinity or soft-anti-affinity is used as a policy of the
+  server-group the instance booted into.
+* Update the server-group API extension to allow soft-affinity and
+  soft-anti-affinty as the policy of a group.
+
+
+Dependencies
+============
+
+* This BP depends on the instance-group-api-extension
+
+
+Testing
+=======
+
+It is not testable in the gate with tempest as we would need at least two
+compute hosts to be able to write meaningful tests.
+
+Unit test coverage will be provided.
+
+Documentation Impact
+====================
+
+New weights need to be described in filter_scheduler.rst
+
+
+References
+==========
+
+* instance-group-api-extension BP
+  https://blueprints.launchpad.net/nova/+spec/instance-group-api-extension
+* Group API wiki
+  https://wiki.openstack.org/wiki/GroupApiExtension
diff --git a/specs/juno/proposed/solver-scheduler.rst b/specs/juno/proposed/solver-scheduler.rst
new file mode 100644
index 0000000..d593cf1
--- /dev/null
+++ b/specs/juno/proposed/solver-scheduler.rst
@@ -0,0 +1,233 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================================================
+Smart Scheduler (Solver Scheduler) - Constraint based resource placement
+========================================================================
+
+https://blueprints.launchpad.net/nova/+spec/solver-scheduler
+
+OpenStack scheduler currently supports nice options via pluggable filters that
+does resource selection based on simple constraints e.g. Don't put instances
+on a given host.  However for complex constraints, building a filter would be
+as complex as building a real constraint solver with complex constraints e.g.
+place VMs while minimizing average (VM-storage-bandwidth).  On the other hand,
+complex solvers are available in open source (PULP, CVXOPT, COIN_OR).  Hence a
+natural solution to the above problem is to design a pluggable scheduler that
+leverages existing solvers.  We believe that this will open new avenues for
+complex constraint (and objectives) based resource placement in large
+OpenStack deployments.
+
+
+Problem description
+===================
+The Smart Scheduler (Solver Scheduler) provides an extensible mechanism
+for making smarter, complex constraints optimization based resource
+scheduling in Nova.  The Nova compute resource placement can be described as a
+problem of placing a set of VMs on a set of physical hosts, where each VM has
+a set of resource requirements that have to be satisfied by a host with
+available resource capacity.  In addition to the constraints, we optimize the
+solution for some cost metrics, so that the net cost of placing all VMs to
+certain hosts is minimized.
+
+This driver supports pluggable Solvers, that can leverage
+existing complex constraint solving frameworks, available in open source
+such as PULP, CVXOPT, Google OR-TOOLS, etc. This Scheduler is currently
+supported to work with Compute Nodes in Nova.
+
+* See https://projects.coin-or.org/PuLP, http://cvxopt.org/
+  https://code.google.com/p/or-tools/
+
+This is an alternative to the existing Filter Scheduler in Nova and can be
+plugged in to address complex constraint scenarios.  Some of the use cases
+relevant to this new scheduling mechanism:
+
+* Placing a group of VMs with a policy that requires them to be very close to
+  storage blobs within a network distance of x
+
+* Placing a VM closest to a group of VMs but avoid a bunch of storage nodes
+  (because they are redhot nodes)
+
+* Placing a VM in a network that supports x bandwidth
+
+* Placing a bunch of VMs and a bunch of volumes that are close to each other.
+  (Each vm_i is close to its local volume vol_i)
+
+and so on.
+
+More often we need to be able to support a combination of constraints and
+optimize for a combination of cost metrics.  Enabling these complex scenarios
+and a combination of them using the existing Filter Scheduler with filters and
+weights support makes it as complex as building a complex constraint solver.
+Also the current design of the Filter Scheduler makes the filter code be run
+against each host iteratively to figure out the final placement. In terms of
+performance, this may not scale well.  In constrast the Solver Scheduler
+runs the optimization problem all at once using external fast solver
+implementations such as COIN-OR, CLP, CBC, GLPK, and so on.
+
+The Nova compute resource placement optimization problem when subject to a set
+of linear constraints, can be formulated and solved as a linear programming
+(LP) problem. A LP problem involves maximizing or minimizing a linear function
+subject to linear constraints.
+
+* See: http://en.wikipedia.org/wiki/COIN-OR
+  http://en.wikipedia.org/wiki/COIN-OR#CLP
+  http://en.wikipedia.org/wiki/COIN-OR#CBC
+  http://en.wikipedia.org/wiki/GNU_Linear_Programming_Kit
+  http://en.wikipedia.org/wiki/Linear_programming
+
+
+Proposed change
+===============
+A Solver Scheduler Driver will be implemented as a Scheduler Driver that
+extends the existing Filter Scheduler Driver to most extent, but provides
+the new logic for making placement decisions using a pluggable Solver.
+The pluggable Solver used by the Solver Scheduler driver models the Nova
+compute placement request as a constraint optimization problem using a set of
+constraints derived from the placement request specification and a net cost
+value to optimize.  This Solver should model the constraints and costs and
+feed it to a constraint problem specification, which is eventually solved by
+using an external solver.
+
+The list of changes we have planned:
+
+* Base Solver Scheduler driver implementation.  This provides the basic driver
+  code that enables plugging in Solvers to do the placements
+
+* A reference solver implementation that is provided to show how a resource
+  placement problem can be specified
+
+* A Solver implementation that supports using pluggable Constraints and Costs
+
+* Constraint and Cost implementations covering the various scenarios, first
+  to get feature parity with the Filters, and then new ones addressing the
+  cross-service and more complex scenarios
+
+This feature will easily integrate with Nova or the split-scheduler Gantt
+project, and it will be non-disruptive as it is a pluggable scheduler driver.
+
+
+Alternatives
+------------
+When we had shown working code and demos in the Icehouse time frame in HKG,
+we were asked why not implement this as a Filter.  But the idea behind this
+Solver Scheduler is different compared to the functionality of a
+FilterScheduler, and it cannot be implemented as a Filter.  We perform the
+entire placement decision calculation as part of one constraint optimization
+problem, rather than iterating the Filters over each of the Hosts.
+
+
+Data model impact
+-----------------
+
+None.
+
+
+REST API impact
+---------------
+
+None.
+
+
+Security impact
+---------------
+
+None.
+
+
+Notifications impact
+--------------------
+
+None.
+
+
+Other end user impact
+---------------------
+
+The end user will need to provide certain additional configuration options
+and provide additional hints for the various constraint scenarios.
+
+
+Performance Impact
+------------------
+
+We believe this new scheduler framework will remove the burden associated
+with FilterScheduler where a Filter is called multiple times for each Host.
+Also using external fast implementation of Solvers, and solving for the entire
+placement decision problem all at once, we believe this framework should
+perform well in scale.
+
+
+Other deployer impact
+---------------------
+
+Config options: Additional config options will be needed to specify what
+constraints and costs are currently being used.
+Please see the project github for additional configurations:
+https://github.com/CiscoSystems/nova-solver-scheduler#configurations
+
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+Yathiraj Udupi <yudupi>
+Email: yudupi AT cisco DOT com
+
+Other contributors:
+Xinyuan Huang <xyhuang>
+Debo Dutta <debo>
+
+Work Items
+----------
+
+This blueprint was approved as part of the Icehouse release cycle,
+and several patches were already submitted as part of it. However, it missed
+the deadline to get all the reviews. Hence being submitted now as a Juno
+nova-spec document.
+Please see the blueprint:
+https://blueprints.launchpad.net/nova/+spec/solver-schedule
+for all the associated work items.
+
+
+Dependencies
+============
+
+This requires a new library called coin-or.pulp for the constraints modeling
+and solvers.  This was included in the requirements project as part of the
+Icehouse time frame.
+
+
+Testing
+=======
+
+Tests are added similar to the tests being conducted for Filter Scheduler.
+
+
+Documentation Impact
+====================
+
+Additional configurations needed will need documentation updates.
+
+
+References
+==========
+
+
+* Detailed Specification document: http://goo.gl/p7FCC9
+
+* Working project code: https://github.com/CiscoSystems/nova-solver-scheduler
+
+* A reference solver implementation documentation: http://goo.gl/vuHcz2
diff --git a/specs/juno/proposed/specify-number-of-cores-per-socket.rst b/specs/juno/proposed/specify-number-of-cores-per-socket.rst
new file mode 100644
index 0000000..ba3f081
--- /dev/null
+++ b/specs/juno/proposed/specify-number-of-cores-per-socket.rst
@@ -0,0 +1,121 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================
+Specify Number of Cores Per Socket
+==================================
+
+https://blueprints.launchpad.net/nova/+spec/specify-number-of-cores-per-socket
+
+The proposal is to add an image property that specifies cores_per_socket on an
+image which would allow you to override the default of one core per socket and
+specify the number you want on the instance.
+
+Problem description
+===================
+
+Typically an instance is assigned a single core per socket ( instance with 4
+vcpus would have 4 sockets ). In some cases of licensed images like Microsoft
+SQL, the licensing is done by socket. In this case it's advantageous to stack
+as many cores as possible on a socket. XenServer 6.2 provides this
+functionality for free now (used to be part of the licensed version).
+
+This could be something useful to other hypervisors as well if they support
+this feature.
+
+Proposed change
+===============
+
+Add support for an image property to specify cores_per_socket.  If this is set,
+the instance would receive a cores-per-socket parameter to allow the number of
+cores per socket:
+
+xe vm-param-set platform:cores-per-socket=’X’ uuid=<VM UUID>
+
+If not set, the default configuration of a single core per socket would apply.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+A user requiring a specific layout of cores based on their instance type would
+need to specify the cores_per_socket image property in order to take advantage
+of it.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  antonym
+
+Work Items
+----------
+
+Add image property for cores_per_socket and make the appropriate changes to
+vm-params on instance creation.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+Create documentation about the new cores_per_socket value and how it works.
+
+References
+==========
+
+* Information on cores-per-socket in XenServer:
+  https://support.citrix.com/article/CTX126524
diff --git a/specs/juno/proposed/spot-instances.rst b/specs/juno/proposed/spot-instances.rst
new file mode 100644
index 0000000..7510f12
--- /dev/null
+++ b/specs/juno/proposed/spot-instances.rst
@@ -0,0 +1,164 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Spot Instances Support
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/spot-instances
+
+Cloud computing may give users the illusion of infinite capacity, but it is not
+always like that. When the resources are full, and a request may not be
+satisfied, giving an error to the users. In some cases, there are tasks that can
+be interrupted (i.e. they are fault tolerant) to leave room to other tasks.
+Users may benefit from this (paying a lower price for more computing power) and
+operators may get their infrastructure usage increased.
+
+Problem description
+===================
+
+A spot instance is a special kind of instance that can be stopped or suspended
+if there are no resources available to satisfy another request with higher
+priority. This way a user may launch spot instances (probably at a fraction
+of the normal price) to fulfil fault-tolerant tasks (such as batch processing,
+web crawling, etc.), taking into account that they may be terminated (or
+suspended) at a given point without further advise.
+
+In some commercial providers, spot instances are bounded to a price bid
+that the user is able to pay for its instances. This way, a spot instance may be
+stopped if the price bid goes above the paid price or if there is no room for a
+non-spot instance.
+
+With the current OpenStack scheduling mechanism, instances are only spawned
+if there are enough available resources to satisfy the request. There is no
+way to prioritise a request against another, and there is no way a request
+could stop (or preempt) a running instance. When the computing capacity is
+tight, this situation may lead to an underutilization of the infrastructure.
+
+Consider the following use case: A scientific cloud computing infrastructure,
+with two kind of users: users from group-A requiring interactive tasks that
+need to be spawned in a short period of time and group-B that require a large
+amounts of computing power (for example, tasks that are traditionally executed
+on a batch system).
+
+This use case will require that the operators set a tight quota for group-B,
+since their requests may fill up the complete infrastructure for long periods
+of time. If the requests from group-B saturate the infrastructure, requests
+from group-A could not be satisfied. With the spot instances in place, request
+from group-B would run as spot instances and requests from group-A would stop
+some of the running instances if there is no room for them.
+
+Proposed change
+===============
+
+The aim of this blueprint is to introduce initial support for spot instances
+into nova. It will add the ability to tag an instance as a spot instance (via
+flavors) and the ability to stop running spot instances to free enough space
+for a non-spot instance. As a first implementation of spot instances it will not
+focus con the more complex bidding system.
+
+In order to tag an instance as an spot instance, the extra_specs attribute of
+a flavor will be used. This way, any instance spawned using that flavor, will
+become a spot instance. This makes possible that an operator can define
+precisely what instance types can be requested as a spot instance.
+
+Whenever a non-spot request cannot be satisfied, the scheduler will look into
+the running spot instances (if any) and will check if stopping some of them
+will release the needed resources for the new request. If the request can be
+satisfied, the scheduler will then stop that instances and the new request
+will be eventually scheduled into that slot.
+
+Alternatives
+------------
+
+To be done.
+
+Data model impact
+-----------------
+
+There should be no changes to the data model, since the extra_specs field of
+the flavors will be used to tag an instance as a spot one.
+
+REST API impact
+---------------
+
+This BP does not affect REST APIs.
+
+Security impact
+---------------
+
+There is a possible denial of service for spot instances, as pointed by Daniel
+Berrange: Somebody could request a large number of non-spot instances, causing
+many spot instances to be destroyed, then they could just immediately shut down
+their non-spot instances again. They wouldnt have incurred any real expense for
+the short time their non-spot instances run, but they will have killed off many
+other people's spot instances.
+
+Notifications impact
+--------------------
+
+Extra notifications will be raised when a machine is terminated.
+
+Other end user impact
+---------------------
+
+The end users will have a new type of instance type to take into
+consideration.
+
+They should be aware that any spot instance can be stopped at a given point,
+without further advise.
+
+Performance Impact
+------------------
+
+To be done.
+
+Other deployer impact
+---------------------
+
+To be done.
+
+Developer impact
+----------------
+
+To be done.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  aloga
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+To be done.
+
+Dependencies
+============
+
+To be done.
+
+Testing
+=======
+
+To be done.
+
+Documentation Impact
+====================
+
+To be done.
+
+References
+==========
+
+To be done.
diff --git a/specs/juno/proposed/standardize-client-params.rst b/specs/juno/proposed/standardize-client-params.rst
new file mode 100644
index 0000000..77a6525
--- /dev/null
+++ b/specs/juno/proposed/standardize-client-params.rst
@@ -0,0 +1,332 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================
+Standardize Client Parameters
+=============================
+
+https://blueprints.launchpad.net/nova/+spec/standardize-client-params
+
+Every time nova (or any other service) wishes to talk to another service we end
+up in the position of having to support that client's every option in the
+config file. This is a burden on developers and can lead to missing security
+options as they are added or differences in parameter handling.
+
+Problem description
+===================
+
+The lack of consistency in the clients has led to the config options describing
+server to server communications being handled on an as needed basis. This has
+led to inconsistencies such as being able to set CA certificates for most
+services, but not for glance. It also means that valuable deployment options
+such as HTTP timeouts and client certificates cannot be configured from nova.
+
+Further as we move to Keystone V3 authentication and other additional
+authentication mechanisms the number of possible authentication options
+available for every client becomes endless.
+
+There needs to be one standard way of loading security and authentication
+options regardless of the server such that Nova (and others) are not having to
+manually keep track of these configuration options.
+
+Current the Cinder options look like::
+
+    [DEFAULT]
+
+    # Info to match when looking for cinder in the service
+    # catalog. Format is: separated values of the form:
+    # <service_type>:<service_name>:<endpoint_type> (string value)
+    #cinder_catalog_info=volume:cinder:publicURL
+
+    # Override service catalog lookup with template for cinder
+    # endpoint e.g. http://localhost:8776/v1/%(project_id)s
+    # (string value)
+    #cinder_endpoint_template=<None>
+
+    # Region name of this node (string value)
+    #os_region_name=<None>
+
+    # Location of ca certificates file to use for cinder client
+    # requests. (string value)
+    #cinder_ca_certificates_file=<None>
+
+    # Number of cinderclient retries on failed http calls (integer
+    # value)
+    #cinder_http_retries=3
+
+    # Allow to perform insecure SSL requests to cinder (boolean
+    # value)
+    #cinder_api_insecure=false
+
+    # Allow attach between instance and volume in different
+    # availability zones. (boolean value)
+    #cinder_cross_az_attach=true
+
+Neutron::
+
+    [DEFAULT]
+
+    # URL for connecting to neutron (string value)
+    #neutron_url=http://127.0.0.1:9696
+
+    # Timeout value for connecting to neutron in seconds (integer
+    # value)
+    #neutron_url_timeout=30
+
+    # Username for connecting to neutron in admin context (string
+    # value)
+    #neutron_admin_username=<None>
+
+    # Password for connecting to neutron in admin context (string
+    # value)
+    #neutron_admin_password=<None>
+
+    # Tenant id for connecting to neutron in admin context (string
+    # value)
+    #neutron_admin_tenant_id=<None>
+
+    # Tenant name for connecting to neutron in admin context. This
+    # option is mutually exclusive with neutron_admin_tenant_id.
+    # Note that with Keystone V3 tenant names are only unique
+    # within a domain. (string value)
+    #neutron_admin_tenant_name=<None>
+
+    # Region name for connecting to neutron in admin context
+    # (string value)
+    #neutron_region_name=<None>
+
+    # Authorization URL for connecting to neutron in admin context
+    # (string value)
+    #neutron_admin_auth_url=http://localhost:5000/v2.0
+
+    # If set, ignore any SSL validation issues (boolean value)
+    #neutron_api_insecure=false
+
+    # Authorization strategy for connecting to neutron in admin
+    # context (string value)
+    #neutron_auth_strategy=keystone
+
+    # Name of Integration Bridge used by Open vSwitch (string
+    # value)
+    #neutron_ovs_bridge=br-int
+
+    # Number of seconds before querying neutron for extensions
+    # (integer value)
+    #neutron_extension_sync_interval=600
+
+    # Location of CA certificates file to use for neutron client
+    # requests. (string value)
+    #neutron_ca_certificates_file=<None>
+
+and Glance::
+
+    [DEFAULT]
+
+    # Default glance hostname or IP address (string value)
+    #glance_host=$my_ip
+
+    # Default glance port (integer value)
+    #glance_port=9292
+
+    # Default protocol to use when connecting to glance. Set to
+    # https for SSL. (string value)
+    #glance_protocol=http
+
+    # A list of the glance api servers available to nova. Prefix
+    # with https:// for ssl-based glance api servers.
+    # ([hostname|ip]:port) (list value)
+    #glance_api_servers=$glance_host:$glance_port
+
+    # Allow to perform insecure SSL (https) requests to glance
+    # (boolean value)
+    #glance_api_insecure=false
+
+    # Number of retries when downloading an image from glance
+    # (integer value)
+    #glance_num_retries=0
+
+Many of these options such as `neutron_extension_sync_interval` or
+`cinder_catalog_info` are specific to the clients and will remain. However some
+such as `*_ca_certificates` (Note: Glance doesn't have one) `*_api_insecure`
+are common and will be loaded by the helper functions.
+
+Proposed change
+===============
+
+The clients are in the process of having the security and authorization
+components standardized. To use a client you:
+
+1. Construct an authentication plugin which has a username and password or any
+   other form of authentication (e.g. oauth, kerberos) options.
+
+2. Construct a session object containing options such as certificates and
+   connection parameters (e.g. http timeout), and the authentication plugin.
+
+3. Construct a client with the session object.
+
+The first two steps above will become standard for all clients and as such
+keystoneclient is defining a standard set of helper mechanisms to allow them to
+be loaded from any OSLO config file.
+
+To facilitate this the proposal is to group related client functions into their
+own ini section rather than prefix them in the DEFAULT section. An example
+would be::
+
+    [neutron]
+
+    certfile = '/path/to/client.crt'
+    keyfile = '/path/to/key.key'
+    cafile = '/path/to/ca.crt'
+    insecure = False
+    timeout = 300
+    auth_name = v2password
+    auth_url = 'http://keystone:5000/v2.0/
+    user_name = 'user'
+    password = 'pass'
+    tenant_name = 'tenn'
+
+Note: the use of `[neutron]` is purely an example, what the sections are called
+is left undefined for now.
+
+Using these options then looks something like::
+
+    from keystoneclient.auth import conf
+    from keystoneclient import session
+    from neutronclient.v2_0 import client
+
+    option_group = 'neutron'
+
+    sess = session.Session.load_from_conf_options(CONF, option_group)
+    sess.auth = conf.plugin_from_conf(CONF, option_group)
+    client = client.Client(session=sess)
+
+In the case where authentication is not needed (using the user's
+authentication) the auth plugin would be provided by auth_token middleware or
+constructible from those headers.
+
+For simplicity it would be good to move the other non-standard options into
+this same ini group, however that is left open for now.
+
+Alternatives
+------------
+
+Essentially the alternative is to go on manually supporting the options for the
+individual clients. This leads to shortcomings such as the current inability to
+use client certificates to do inter-service communication and the inability to
+set glance CA certificates from configuration.
+
+With the standardization happening in the clients already Nova could maintain
+it's own config loading scheme however for consistency across the projects it
+would be better to just have one defined scheme. It also means that as new
+options are added Nova would automatically gain the ability to use these
+options if set in config.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+This proposal will give deployers more control over the security settings of
+the clients and allow continuing improvements in security without each option
+being additionally supported by nova.
+
+It will go most of the way to allowing SSL everywhere deployments and allow a
+new range of authentication mechanisms to be used for server interaction.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Configuration changes are well defined above. The exact names of these are
+currently still in review in keystoneclient.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jamielennox
+
+Other contributors:
+
+Work Items
+----------
+
+The majority of work to implement this will not be done in nova.
+
+* Finalize the config and authentication loading in keystoneclient
+* Convert cinderclient, neturonclient and glanceclient to use the session
+  object.
+* Change nova to instantiate client objects with the session.
+
+Dependencies
+============
+
+This proposal will requires ongoing work across at least 3 different client
+projects. The intention of raising this spec now is to make sure that the end
+goal is agreed upon by the servers rather than going and implementing the
+client side only to find that Nova and others are unwilling to change.
+
+There is still time to affect things like option names in keystoneclient.
+
+Testing
+=======
+
+As applicable changes to configuration should be mirrored in devstack and gate
+testing as well as updating any affected unit tests.
+
+As we are changing the default configuration options the continual passing of
+gate tests will be good indication that the changes are working.
+
+Documentation Impact
+====================
+
+The configuration options for the client related options are going to need to
+be changed and the old ones deprecated.
+
+References
+==========
+
+* Session objects:
+  http://www.jamielennox.net/blog/2014/02/24/client-session-objects/
+
+* cinderclient use session object:
+  https://review.openstack.org/#/c/95986/
+
+* Create a session object from a config file:
+  https://review.openstack.org/#/c/95015/
+
+* Create an auth plugin from a config file:
+  https://review.openstack.org/#/c/79542/
diff --git a/specs/juno/proposed/storage-optimization-for-multi-datastore-clusters.rst b/specs/juno/proposed/storage-optimization-for-multi-datastore-clusters.rst
new file mode 100644
index 0000000..27928b2
--- /dev/null
+++ b/specs/juno/proposed/storage-optimization-for-multi-datastore-clusters.rst
@@ -0,0 +1,211 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================================================
+VMware nova driver storage optimization for clusters with multiple datastores
+=============================================================================
+
+https://blueprints.launchpad.net/nova/+spec/storage-optimization-for-multi-datastore-clusters
+
+This blueprint storage-optimization-for-multi-datastore-clusters allows images
+to launch faster and saves disk space by creating linked clones of instances
+using the base disk already present in the image cache in another datastore of
+the ESX cluster.
+VMware ESX cluster can have multiple datastores that are shared across all the
+hosts of the cluster. It is much more efficient to have a single copy of the
+image in only one datastore of the cluster and create linked clones using
+this image.
+
+Problem description
+===================
+
+ESX clusters can have multiple shared datastores configured.
+A datastore is a logical container that stores the virtual machine files.
+This includes both the disk (vmdk), configuration (vmx) files and
+snapshot files.
+
+A datastore can be created from different types of physical storage
+like local direct attached storage, iSCSI, FC SAN and NFS.
+The physical storage is presented to all host in the cluster and therefore
+the datastore is accessible from all the hosts of the cluster.
+Each cluster will typically have its own set of datastores and this is
+the recommended configuration. This configuration enable the live migration
+of VMs across all hosts of the cluster.
+
+When using the VMware vCenter nova driver for such clusters with
+multiple shared datastores the image can get cached in each datastore.
+Additionally a disk of size equal to the flavor of the instance is also
+created in the cache. Instances are created as linked clones to this flavor
+sized disks.
+This approach has the following problems
+
+* Every time a new datastore is selected by the driver for deployment, a new
+  cache is created on the datastore and the image is copied.
+* A copy of the image expanded to the size of the flavor is created for each
+  instance deployed with a different flavor
+
+This approach leads to a reduction of the actual storage space available for
+creating instances.
+
+In the existing design, when an instance creation is done, the following
+occurs
+
+* Step 1. The driver determines the best datastore to place the instance. It
+  does this by selecting the datastore with maximum free space.
+* Step 2. The existence of the image is checked in the cache
+  (directory named _vmware_base) on the clusters datastore
+* Step 3. If the image is not available in the cache, then
+  (i) the image is downloaded from glance into nova-compute
+  (the VM where the compute service runs)
+  (ii) Then the image is transferred from the nova-compute to the datastore
+  by vCenter
+* Step 4.Instance is spawned using the cached image by first creating a disk
+  of the size of the specified in the flavor (if it doesnt exist). Then
+  creating a linked clone using the flavor sized disk as the base disk.
+
+The problems exist in Step 1 and Step 2 since the the driver is not utilizing
+the cache in other datastores.
+
+Proposed change
+===============
+
+* 1. The drivers datastore selection can be modified to select the datastore
+  that has the image already cached and if has enough space for the new
+  instance (as per Step 4 above)
+* 2. If the datastore where the image is cached does not have enough space for
+  the new instance, then create a linked clone in a different datastore but
+  still using the cached image as the base disk.
+  This is based on the configuration option set in nova.conf. The
+  configuration parameter is span_datastores.
+  This is configurable because in vCenter a datastore can be placed in
+  maintenance mode. A datastore can be placed in maintenance mode when there
+  are no VMs accessing the datastore. In the case where the instances are
+  referencing base disk in different datastores it would be difficult for the
+  admin to identify the children of a specific base disk and move the disks.
+  By making it configurable, admins who do not use the datastore maintenance
+  feature or admins who can identify dependent disks using scripts can set
+  this option.
+
+  Default would be not to create linked that are across datastores of the
+  cluster (Existing implementation). However the download from glance will be
+  avoided if the image is present in the cache of other datastores. This is
+  the additional change introduced.
+
+  When this option is set, then the linked clone creation is modified so
+  that base disk in another datastore is used.
+
+  When this option is not set, then the following occur:
+  a. The lock on the image is aquired. The lock is the same as the one
+  aquired by the periodic task during image cache cleanup.
+  b. The image is copied from the datastore to the cache of the datastore
+  that is selected for the instance.
+  c. The lock is released
+  d. The existing spawn implementation continues
+  If the lock cannot be aquired since the image was deleted by the image
+  cache cleanup periodic task, then the download from glance will occur.
+
+Alternatives
+------------
+
+Another alternative is to schedule instances to the clusters that have the
+image already cached. This will work fine until the cluster is at capacity.
+To use this alternative, the following changes will be required
+(1) the VMware nova driver will have to publish the
+list of images in its cache.
+(2) A new scheduler filter will then select the nova-compute (cluster) that
+already has the image in its cache. If there are no clusters that have the
+image then it would return all clusters (existing behaviour). This can be
+implemented using weights as well. This will be addressed a separate
+blueprint (will be submitting this).
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Given that in a private cloud there will be different set of standard
+images, and the number of datastores per cluster is generaly more than 1,
+enhancing the datastore selection logic and creating linked clones across
+datastores will significantly improve user experience, reduce instance
+creation times since multiple caches are not created and improve storage
+utilization.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+* The similar approach can be used by other nova drivers that support nodes
+  with multiple logical storage where cache is maintained per logical storage.
+* This change only impacts the VMware nova driver. Other drivers will not be
+  impacted due to this change.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  aiswarya-sundaran
+
+Other contributors:
+  kiran-kumar-vaddi
+
+Work Items
+----------
+
+* Modify the code the selects the datastore to spawn an instance to also use
+  the cache as a criteria
+* Modify the instance creation code to be able to point to a different
+  datastore for the instance base disk
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+The unit tests will be modified to test the branches introduced by the above
+work items
+
+Documentation Impact
+====================
+
+A new config option span_datastores is added.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/support-keystone-v3-api.rst b/specs/juno/proposed/support-keystone-v3-api.rst
new file mode 100644
index 0000000..193ab5a
--- /dev/null
+++ b/specs/juno/proposed/support-keystone-v3-api.rst
@@ -0,0 +1,218 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================
+Support Identity v3 API
+=======================
+
+Launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/support-keystone-v3-api
+
+The Keystone server has supported v3 of the Identity API for some time, and the
+goal is to eventually deprecate the v2 API. In order to do this Nova and other
+applications using the Identity API need to be able to be configured to use the
+v3 API rather than the v2 API.
+
+Note that there have been attempts at getting Nova to support the Identity V3
+API before, but those attempts were rejected with the reason that a
+comprehensive solution needed to be presented. As such, this document attempts
+to present a comprehensive solution. In some cases it describes work that
+doesn't need to be done, because someplace that somebody thinks might be
+affected isn't.
+
+If you're wondering about the novaclient library, that work was already done
+with https://review.openstack.org/#/c/85920/ .
+
+Problem description
+===================
+
+Nova can't be configured to use the Identity v3 API. Nova doesn't use the
+Identity v3 API directly, but the Neutron client library that Nova uses does.
+
+Identity v3 authentication requires the application to specify the user and
+project's domain ID or domain name if the user or project name is given (the
+user ID and project ID are unique, but the names are only unique within a
+domain). New configuration options are required to be able to specify the
+identity API version to use and the domain for the user or project when
+authentication is done.
+
+For Neutron access, Nova uses python-neutronclient. Once python-neutronclient
+supports Identity v3 authentication (typically provided by adding support for
+keystoneclient's session), then Nova can be changed to set up the client to use
+Identity v3 authentication.
+
+If you're wondering about changes to how Nova accesses Cinder and Glance, note
+that rather than Nova getting a new token using Identity v3 API, an existing
+token is passed to the respective client. Because a token is available those
+clients don't need to use the Identity API at all to get a token and therefore
+nothing will be changed in Nova for Cinder and Glance access for this
+blueprint.
+
+If you're wondering about the auth_token middleware, yes, Nova uses it, and it
+doesn't at this point support V3 authentication. Once the auth_token middleware
+supports V3 authentication then Nova will pick that up automatically, so
+there's no work to do in Nova for auth_token support.
+
+Proposed change
+===============
+
+Once python-neutronclient supports using v3 for authentication, Nova will be
+changed to use it. The support is provided via neutronclient's support for
+keystoneclient's "session". When using the session, rather than passing the
+username, password, etc. to neutronclient, you instead pass a Session object.
+The Session object is initialized with the authentication plugin to use, so in
+the case of authenticating for Neutron access, Nova will use the v3 password
+plugin. Here's some docs on using sessions:
+http://docs.openstack.org/developer/python-keystoneclient/using-sessions.html
+
+In order to do v3 authentication, more information is needed. The identity API
+version to use and the user and project domains are required. These will be new
+configuration options.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+There will be new configuration options for Neutron access.
+
+Here are the current relevant options for Neutron authentication::
+
+  [neutron]
+
+  #admin_username=<None>
+  #admin_tenant_id=<None>
+  #admin_tenant_name=<None>
+  #admin_auth_url=http://localhost:5000/v2.0
+
+In order to use v3 auth, ``admin_auth_url`` will need to be set to a v3
+endpoint or an endpoint that can be converted to a v3 endpoint. This config
+option will support being set to an unversioned endpoint, like
+``http://localhost:5000/``. Nova will use keystoneclient's version discovery
+support to find if the identity server supports v3 or is v2 only. If the server
+reports that it supports v3 then v3 authentication will be used, otherwise Nova
+will fall back to v2 authentication. This will also support ``admin_auth_url``
+being set to a v2.0 endpoint or a v3 endpoint. The help text for
+``admin_auth_url`` will be updated.
+
+V3 authentication requires that a domain is provided for a user name or project
+name (otherwise it's not unique). So there will be new options
+``admin_user_domain_name`` and ``admin_project_domain_name``. Keystone creates
+a default domain named ``Default``, so that will be the default.
+
+The ``admin_tenant_name`` option will be renamed to ``admin_project_name`` and
+``admin_tenant_id`` to ``admin_project_id`` (the old names will still work but
+will be deprecated).
+
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  blk-u <Brant Knudson>
+
+Other contributors:
+  - mriedem <Matt Riedemann>
+
+Work Items
+----------
+
+* Rename ``admin_tenant_name`` option to ``admin_project_name`` and
+  ``admin_tenant_id`` to ``admin_project_id``, leaving the old option names a
+  deprecated names.
+
+* Once neutronclient supports keystoneclient's sessions
+  (`keystone-api-v3-support in Neutron`_)
+
+  * Update requirements.txt to the version of python-neutronclient that has
+    support for sessions.
+  * Change Nova to use a session for existing v2 auth for Neutron access.
+  * Change Nova to also support using v3 auth for Neutron access
+    * Add new config options for user and project domain.
+    * Use the V3 password plugin if configured for v3.
+  * Change Nova to default to v3 auth rather than v2 auth.
+  * Change Nova to also support using unversioned endpoint for Neutron access.
+  * Change Nova to default to unversioned endpoint and do discovery.
+  * Change devstack to allow configuring Nova for v2, v3, or to use discovery
+    for auth.
+  * Provide a Tempest daily configuration that sets nova to use v2 for Neutron
+    access.
+
+.. _`keystone-api-v3-support in Neutron`: https://blueprints.launchpad.net/python-neutronclient/+spec/keystone-api-v3-support
+
+Dependencies
+============
+
+* python-neutronclient needs to be updated to support v3 auth. Nova will depend
+  on the version of the python-neutronclient package that has support for
+  v3. This work is in progress: keystone-api-v3-support_
+
+.. _keystone-api-v3-support: https://blueprints.launchpad.net/python-neutronclient/+spec/keystone-api-v3-support
+
+Testing
+=======
+
+Nova will support using both v3 auth and v2 auth, with v3 being the
+default. Use of v3 auth will thus be tested on every commit using the normal
+Tempest testing. We should ensure that using v2 auth still works as long as
+it's supported, and that will be done by having a nightly job that configures
+Nova for v2 auth. (We'll need a way to configure the system for this using
+devstack, so support will be added to devstack to configure Nova for v2 auth.)
+
+
+Documentation Impact
+====================
+
+There's new config options as described above.
+
+References
+==========
+
+* `Identity v3 auth`_
+
+.. _`Identity v3 auth`: https://github.com/openstack/identity-api/blob/master/v3/src/markdown/identity-api-v3.md#authenticate-post-authtokens
diff --git a/specs/juno/proposed/synchronous-read-support.rst b/specs/juno/proposed/synchronous-read-support.rst
new file mode 100644
index 0000000..931e4b6
--- /dev/null
+++ b/specs/juno/proposed/synchronous-read-support.rst
@@ -0,0 +1,162 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================================
+Provide a connection for a synchronous slave
+============================================
+
+
+https://blueprints.launchpad.net/nova/+spec/synchronous-read-support
+
+We want to be able to scale the DB layer of Nova further and to do this
+we want to let deployers isolate their workloads to different classes of
+hardware. By seperating out writes, reads affected by causality and reads
+not affected by causality we can optimize for those workloads.
+
+Problem description
+===================
+
+Support for offloading some reads to replication slaves was added in the
+Icehouse release. This was nice, but was unusable from the perspective of
+workflows that are sensitive to a non-causal replication slave. For example,
+if you call an API that writes to the DB then call a different API that
+should take into consideration the data that was just written, there is a
+chance that the asynchronous slaves hasn't written that out to it's version
+of the database. This has the potential to break the workflow.
+
+Proposed change
+===============
+
+I want to add an additional db connection that is intended for a synchronous
+replication slave or slave cluster. If a deployer chooses to specify this
+connection in Nova's configuration all reads will go here unless explicitly
+sent to the asynchronous connection.
+
+Alternatives
+------------
+
+There is a sharding model in Nova, cells, which was designed specifically to
+deal with issues of scale. Because of it's bolt-on nature it is sometimes
+unattractive to deployers. It could be argued that we need to handle the DB
+scaling problem in Nova itself rather than scaling the DB as a seperate
+problem.
+
+It could be argued that time would be better spent working on cells or
+another form of handling application level sharding of the data problem.
+
+Another alternative would be to provide a all_reads_from_slave boolean
+to let deployers take care of the sync vs async problems themselves and
+not worry about another handle from the application level.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Adding any synchronous db slaves will affect performance in the following
+ways:
+
+The Round Trip Time of any write is a function of the slowest node that
+is part of the relationship. The takeway is that you must make sure your
+hardware that hosts these is equal to the performance that you expect
+across the board.
+
+Other deployer impact
+---------------------
+
+We will add a new configuration parameter:
+
+sync_slave_connection
+
+also will begin the deprecation process for
+
+slave_connection
+
+in favor of
+
+async_slave_connection
+
+If a deployment provides a valid sync_slave_connection all reads that are not
+explicitly sent to the async_slave_connection will be sent there. Deployments
+should understand the implications of adding a synchronous db slave into
+their infrastructure. It would also be important to tune all three sql
+connections for their particular workload so that the benefits may be fully
+enjoyed.
+
+Also, it should be noted that a synchronous cluster is going to multiply your
+number of required writes for each transaction. Specifically take writes and
+times that by the number of synchronous nodes in the cluster.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <geekinutah>
+
+Other contributors:
+  <launchpad-id or None>
+
+Work Items
+----------
+
+-Add another connection to the db session code
+-Add logic to determine if a query is a write or a read and conditionally
+send a read down to the synchronous session.
+-Start the process of deprecating the slave_conection configuration option
+in favor of async_slave_conection.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+We will want to add an additional database instance into some testing
+environments to make sure interactions are smooth.
+
+Documentation Impact
+====================
+
+We will need to upgrade the operations guide to reflect the ability to send
+reads to a seperate database cluster.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/tenant-aggregate-exclusive-filter.rst b/specs/juno/proposed/tenant-aggregate-exclusive-filter.rst
new file mode 100644
index 0000000..30c7ba5
--- /dev/null
+++ b/specs/juno/proposed/tenant-aggregate-exclusive-filter.rst
@@ -0,0 +1,181 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================
+Tenants to Aggregate Filter
+============================
+
+https://blueprints.launchpad.net/nova/+spec/multi-tenancy-isolation-only\
+-aggregates
+
+Extend the "AggregateMultiTenancyIsolation" scheduler filter in order to
+optionally exclude instances from being schedule for deployment in hosts that
+don't belong to the defined aggregate(s).
+
+Problem description
+===================
+
+The "AggregateMultiTenancyIsolation" scheduler filter allows the creation of
+instances from defined tenants in specific aggregates.
+If an aggregate has the metadata key "filter_tenant_id" defined with tenant
+ids, (ex: filter_tenant_id=tenant_id1,tenant_id2) only instances from those
+tenants will be scheduled for deployment in the aggregate.
+
+However, it doesn't exclude those instances from being scheduled for
+deployment in other hosts that don't belong to the aggregate. Is not possible
+to exclusively dedicate an aggregate to a set of tenants.
+
+
+Proposed change
+===============
+
+We propose the introduction of the
+"aggregate_multitenancy_isolation_tenant_exclusive" configuration option for
+the "AggregateMultiTenancyIsolation" scheduler filter. The default value of
+this new configuration option is "false" which preserves the current filter
+behavior. If it's "true" instances from the tenant defined in the aggregate
+metadata "filter_tenant_id" will only be scheduled for deployment to the
+aggregate.
+This will allow the exclusive dedication of an aggregate to a tenant.
+
+Example:
+Aggregate_A => filter_tenant_id = project_a
+Aggregate_B => filter_tenant_id = project_b
+Aggregate_C => filter_tenant_id = project_a
+Aggregate_D
+
+If aggregate_multitenancy_isolation_tenant_exclusive=false (current behavior)
+
+ * Instance_1 from project_a can run in:
+   Aggregate_A, Aggregate_C, Aggregate_D
+ * Instance_2 from project_b can run in:
+   Aggregate_B, Aggregate_D
+ * Instance_3 from project_x can run in:
+   Aggregate_D
+
+If aggregate_multitenancy_isolation_tenant_exclusive=true
+
+ * Instance_1 from project_a can run in:
+   Aggregate_A, Aggregate_C
+ * Instance_2 from project_b can run in:
+   Aggregate_B
+ * Instance_3 from project_x can run in:
+   Aggregate_D
+
+
+Alternatives
+------------
+
+Implement a new filter with this functionality.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Current filter behavior is preserved and remains the default.
+New functionality needs to be explicitly enabled by adding the configuration
+option "aggregate_multitenancy_isolation_tenant_exclusive=true".
+
+Performance Impact
+------------------
+
+The filter will use the existing DB query
+"aggregate_host_get_by_metadata_key()" if the configuration option
+"aggregate_multitenancy_isolation_tenant_exclusive=true" in order to evalute
+if "filter_tenant_id" is defined in other hosts.
+
+Depending in the deployment size it could have a performance impact.
+Documentation will reflect this.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Work Items
+----------
+
+ * Modify "AggregateMultiTenancyIsolation" to dedicate an aggregate to a
+   tenant if the configuration option
+   "aggregate_multitenancy_isolation_tenant_exclusive" is defined and is
+   "true".
+ * Change the scheduler filter documentation reflecting the new functionality.
+
+Assignee(s)
+-----------
+
+Primary assignee:
+moreira-belmiro-email-lists
+
+Other contributors:
+jesse-pretorius
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Additional unit tests:
+ * Set configuration option
+   aggregate_multitenancy_isolation_tenant_exclusive=true.
+   Validate that if the tenant is not in the filter_tenant_id list then the
+   host fails.
+ * Set configuration option
+   aggregate_multitenancy_isolation_tenant_exclusive=true.
+   Validate that if the tenant is in the filter_tenant_id list then the host
+   passes.
+ * Set configuration option
+   aggregate_multitenancy_isolation_tenant_exclusive=true.
+   Validate that if the tenant is in the filter_tenant_id list then all hosts
+   that are not in the aggregate fail.
+ * Set configuration option
+   aggregate_multitenancy_isolation_tenant_exclusive=false.
+   Validate that the host passes for any tenant if it doesn't belong to an
+   aggregate with filter_tenant_id defined.
+
+Documentation Impact
+====================
+
+"AggregateMultiTenancyIsolation" documentation will be updated with the new
+feature.
+
+References
+==========
+
+None
+
+
diff --git a/specs/juno/proposed/thunderboost-proposal-1.rst b/specs/juno/proposed/thunderboost-proposal-1.rst
new file mode 100644
index 0000000..2351a82
--- /dev/null
+++ b/specs/juno/proposed/thunderboost-proposal-1.rst
@@ -0,0 +1,189 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=========================================================================
+A Lightweight Proposal for Fast Booting Many Homogeneous Virtual Machines
+=========================================================================
+
+https://blueprints.launchpad.net/nova/+spec/thunderboost
+
+
+Nova supports to boot virtual machines (VMs) atop the Cinder volumes. However,
+in the current implementation (version i), booting up a large number of
+homogeneous VMs is time-consuming. To overcome this drawback, we propose a
+lightweight patch for Nova, which adopts a third-party library, called
+VMThunder, for fast booting homogeneous VMs. VMThunder accelerates the booting
+process through on-demand data transfer in a P2P manner.
+
+Problem description
+===================
+
+Currently, Openstack provides two categories of methods for booting a virtual
+machine: (i) booting from a local image or (ii) booting from a remote Cinder
+volume.
+
+The first category needs to copy the entire image to a compute node, making it
+suffer from a long transfer delay for large images. The second category
+remotely attaches a volume to a VM and transfers only the necessary data from
+the volume, thus having better performance. However, this approach only allows
+booting a single VM from a volume at a time. Moreover, preparing a volume for
+each VM requires a long time. As a result, it is currently inevitable to take a
+long time for booting a large number of homogeneous VMs in Openstack.
+
+Proposed change
+===============
+
+We propose to add a new method, named "Boot from VMThunder", for fast booting
+multiple homogeneous VMs. This method uses a third-party library (VMThunder) to
+support simultaneous booting of a large number of VMs.
+
+VMThunder configures each VM with two volumes (the figure can be found here
+http://www.kylinx.com/vmthunder/vmthunder.png): a (read-only) template volume
+exactly the same as the pre-created original volume and a (writable) snapshot
+volume storing each VM's difference to the template. The original volume is the
+root of a template volume relay tree, and each VM fetches only the necessary
+data from its parent over the multi-path iSCSI protocol. In addition, VMThunder
+makes use of a compute node's local storage as a cache to accelerate the image
+transferring process and avoid a repetitive data transfer. The P2P-style,
+on-demand data transfer dramatically accelerates VMs' booting process.
+
+Our modification to Nova is light-weighted (about 80 lines of insertions and
+deletions). Two major functions, i.e., the creation and deletion of the
+template and snapshot volumes, are implemented as following: (i) creation: We
+add a volume-driver class (about 50 lines, depends on VMThunder's API) in file
+"nova/virt/block_device.py" to prepare the template and snapshot volumes.
+(ii) deletion: We add a delete method (about 20 lines, depends on VMThunder's
+API) in file "nova/compute/manager.py' to destroy the unused template and
+snapshot volumes.
+
+More details of the implementation can be found in the following links:
+Paper, http://www.computer.org/csdl/trans/td/preprint/06719385.pdf
+Modification diff file, http://www.kylinx.com/vmthunder/diff2.txt
+VMThunder demo videos,
+http://www.kylinx.com/vmthunder/boot_vmthunder_win7_success-V2.mp4
+Image booting demo videos,
+http://www.kylinx.com/vmthunder/boot_image_test_win7_success-V2.mp4
+
+Alternatives
+------------
+
+(1)Image cache:
+Nova's image-caching facility reduces the start-up time for creating
+homogeneous virtual machines on one nova-compute node. However, it helps
+neither the first-time provisioning nor the Cinder-based booting process.
+
+(2)P2P transferring:
+The P2P protocol can increase the speed of the file distribution. For example,
+the glance-bittorrent-delivery proposal transfers the image templates from the
+glance storage to Nova-compute servers. This approach, however, needs to
+transfer the entire image to all peers.
+
+(3)Backend storage optimization:
+The distributed storages like NFS, cluster FS, distributed FS or SAN can
+decrease the size of transferred volumes. However, the I/O pressure on the
+storage servers increases dramatically when powering on a large number of
+homogeneous VMs, since there may not be enough replicas on the storage servers
+for offloading the I/O demands.
+
+(4)Multi-attach volume:
+(https://wiki.openstack.org/wiki/Cinder/blueprints/multi-attach-volume)
+This approach allows a volume to be attached to more than one instance
+simultaneously. As a result, volumes can be shared among multiple guests when
+the instances are already available. Besides, these volumes can also be used
+for booting a number of VMs by enforcing the multi-attach volumes as read-only
+image disks. Unfortunately, this approach does not scale well because of the
+star-structured topology in the data dissemination process.
+
+(5)Direct image access:
+(https://blueprints.launchpad.net/nova/+spec/nova-image-zero-copy).
+This approach uses the direct_url of the Glance v2 API, such that the number of
+needed hops to transfer an image to a Nova-compute node is decreased. When
+images are stored at multiple backend locations, the Nova-compute servers can
+select a proper image storage for speeding up the downloading process.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+We will significantly decrease the delay of booting up large numbers of
+Cinder-volume-based VMs.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee: vmThunderGroup (vmthunder)
+
+Work Items
+----------
+
+We only add one lightweight patch for Nova.
+
+Dependencies
+============
+
+The patch depends on the VMThunder project
+(https://launchpad.net/VMThunder).
+
+Testing
+=======
+
+We will add necessary tests into nova's test framework, in order to show the
+effectiveness of VMThunder. The unit tests and integrated tests will be added
+to the component.
+
+Documentation Impact
+====================
+
+We need to document how to create many homogeneous virtual machines though our
+new option.
+
+References
+==========
+
+VMThunder: http://www.VMThunder.org/
+
+Mailing list:
+http://lists.openstack.org/pipermail/openstack-dev/2014-April/032883.html
+
+VMThunder Publication:http://www.VMThunder.org/blog/2014/03/02/publication/
\ No newline at end of file
diff --git a/specs/juno/proposed/thunderboost-proposal-2.rst b/specs/juno/proposed/thunderboost-proposal-2.rst
new file mode 100644
index 0000000..d043868
--- /dev/null
+++ b/specs/juno/proposed/thunderboost-proposal-2.rst
@@ -0,0 +1,189 @@
+This work is licensed under a Creative Commons Attribution 3.0 License.
+
+http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================================================
+A Lightweight Proposal for Fast Booting Many Homogeneous Virtual Machines  
+===============================================================================
+  
+https://blueprints.launchpad.net/nova/+spec/thunderboost
+
+Nova supports to boot virtual machines (VMs) atop the Cinder volumes. However, 
+in the current implementation (version i), booting up a large number of 
+homogeneous VMs is time-consuming. To overcome this drawback, we propose a 
+lightweight patch for Nova, which adopts a third-party library, called 
+VMThunder, for fast booting homogeneous VMs. VMThunder accelerates the booting 
+process through on-demand data transfer in a P2P manner.
+
+Problem description
+===================
+
+Currently, Openstack provides two categories of methods for booting a virtual 
+machine:  (i) booting from a local image or (ii) booting from a remote Cinder 
+volume.
+
+The first category needs to copy the entire image to a compute node, making it 
+suffer from a long transfer delay for large images. The second category remotely 
+attaches a volume to a VM and transfers only the necessary data from the volume, 
+thus having better performance. However, this approach only allows booting a 
+single VM from a volume at a time. Moreover, preparing a volume for each VM 
+requires a long time. As a result, it is currently inevitable to take a long 
+time for booting a large number of homogeneous VMs in Openstack.
+
+Proposed change
+===============
+
+We propose to add a new method, named "Boot from VMThunder", for fast booting 
+multiple homogeneous VMs. This method uses a third-party library (VMThunder) to 
+support simultaneous booting of a large number of VMs. 
+
+VMThunder configures each VM with two volumes (the figure can be found here 
+http://www.kylinx.com/vmthunder/vmthunder.png): a (read-only) template volume 
+exactly the same as the pre-created original volume and a (writable) snapshot 
+volume storing each VM's difference to the template. The original volume is the 
+root of a template volume relay tree, and each VM fetches only the necessary 
+data from its parent over the multi-path iSCSI protocol. In addition, VMThunder 
+makes use of a compute node's local storage as a cache to accelerate the image 
+transferring process and avoid a repetitive data transfer. The P2P-style, 
+on-demand data transfer dramatically accelerates VMs' booting process.
+
+Our modification to Nova is light-weighted (about 80 lines of 
+insertions/deletions). Two major functions, i.e., the creation and deletion of 
+the template and snapshot volumes, are implemented as following:
+(i) creation: We add a volume-driver class (about 50 lines, depends on 
+VMThunder's API) in file "nova/virt/block_device.py" to prepare the template and 
+snapshot volumes.
+(ii) deletion: We add a delete method (about 20 lines, depends on VMThunder's 
+API) in file "nova/compute/manager.py' to destroy the unused template and snapshot volumes. 
+
+More details of the implementation can be found in the following links:
+Paper, http://www.computer.org/csdl/trans/td/preprint/06719385.pdf
+Modification diff file, http://www.kylinx.com/vmthunder/diff2.txt
+VMThunder demo videos,
+http://www.kylinx.com/vmthunder/boot_vmthunder_win7_success-V2.mp4
+Image booting demo videos,
+http://www.kylinx.com/vmthunder/boot_image_test_win7_success-V2.mp4
+
+Alternatives
+------------
+(1)Image cache: 
+Nova's image-caching facility reduces the start-up time for creating homogeneous 
+virtual machines on one nova-compute node. However, it helps neither the 
+first-time provisioning nor the Cinder-based booting process.
+
+(2)P2P transferring:
+The P2P protocol can increase the speed of the file distribution. For example, 
+the glance-bittorrent-delivery proposal transfers the image templates from the 
+glance storage to Nova-compute servers. This approach, however, needs to 
+transfer the entire image to all peers.
+
+(3)Backend storage optimization:  
+The distributed storages like NFS, cluster FS, distributed FS or SAN can 
+decrease the size of transferred volumes. However, the I/O pressure on the 
+storage servers increases dramatically when powering on a large number of 
+homogeneous VMs, since there may not be enough replicas on the storage servers 
+for offloading the I/O demands. 
+
+(4)Multi-attach volume:
+(https://wiki.openstack.org/wiki/Cinder/blueprints/multi-attach-volume)
+This approach allows a volume to be attached to more than one instance 
+simultaneously. As a result, volumes can be shared among multiple guests when 
+the instances are already available. Besides, these volumes can also be used for 
+booting a number of VMs by enforcing the multi-attach volumes as read-only image 
+disks. Unfortunately, this approach does not scale well because of the 
+star-structured topology in the data dissemination process.
+
+(5)Direct image access:
+(https://blueprints.launchpad.net/nova/+spec/nova-image-zero-copy). 
+This approach uses the direct_url of the Glance v2 API, such that the number of 
+needed hops to transfer an image to a Nova-compute node is decreased. When 
+images are stored at multiple backend locations, the Nova-compute servers can 
+select a proper image storage for speeding up the downloading process. 
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+We will significantly decrease the delay of booting up large numbers of 
+Cinder-volume-based VMs.
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+
+None
+
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee: vmThunderGroup (vmthunder)
+
+Work Items
+----------
+
+We only add one lightweight patch for Nova.
+
+Dependencies
+============
+
+The patch depends on the VMThunder project 
+(https://launchpad.net/VMThunder). 
+
+Testing
+=======
+
+We will add necessary tests into nova's test framework, in order to show the 
+effectiveness of VMThunder. The unit tests and integrated tests will be added to 
+the component. 
+
+
+Documentation Impact
+====================
+
+We need to document how to create many homogeneous virtual machines though our 
+new option.
+
+
+References
+==========
+
+VMThunder: http://www.VMThunder.org/
+
+Mailing list: 
+http://lists.openstack.org/pipermail/openstack-dev/2014-April/032883.html
+
+VMThunder Publication:http://www.VMThunder.org/blog/2014/03/02/publication/
+
diff --git a/specs/juno/proposed/thunderboost-proposal-3.rst b/specs/juno/proposed/thunderboost-proposal-3.rst
new file mode 100644
index 0000000..2351a82
--- /dev/null
+++ b/specs/juno/proposed/thunderboost-proposal-3.rst
@@ -0,0 +1,189 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=========================================================================
+A Lightweight Proposal for Fast Booting Many Homogeneous Virtual Machines
+=========================================================================
+
+https://blueprints.launchpad.net/nova/+spec/thunderboost
+
+
+Nova supports to boot virtual machines (VMs) atop the Cinder volumes. However,
+in the current implementation (version i), booting up a large number of
+homogeneous VMs is time-consuming. To overcome this drawback, we propose a
+lightweight patch for Nova, which adopts a third-party library, called
+VMThunder, for fast booting homogeneous VMs. VMThunder accelerates the booting
+process through on-demand data transfer in a P2P manner.
+
+Problem description
+===================
+
+Currently, Openstack provides two categories of methods for booting a virtual
+machine: (i) booting from a local image or (ii) booting from a remote Cinder
+volume.
+
+The first category needs to copy the entire image to a compute node, making it
+suffer from a long transfer delay for large images. The second category
+remotely attaches a volume to a VM and transfers only the necessary data from
+the volume, thus having better performance. However, this approach only allows
+booting a single VM from a volume at a time. Moreover, preparing a volume for
+each VM requires a long time. As a result, it is currently inevitable to take a
+long time for booting a large number of homogeneous VMs in Openstack.
+
+Proposed change
+===============
+
+We propose to add a new method, named "Boot from VMThunder", for fast booting
+multiple homogeneous VMs. This method uses a third-party library (VMThunder) to
+support simultaneous booting of a large number of VMs.
+
+VMThunder configures each VM with two volumes (the figure can be found here
+http://www.kylinx.com/vmthunder/vmthunder.png): a (read-only) template volume
+exactly the same as the pre-created original volume and a (writable) snapshot
+volume storing each VM's difference to the template. The original volume is the
+root of a template volume relay tree, and each VM fetches only the necessary
+data from its parent over the multi-path iSCSI protocol. In addition, VMThunder
+makes use of a compute node's local storage as a cache to accelerate the image
+transferring process and avoid a repetitive data transfer. The P2P-style,
+on-demand data transfer dramatically accelerates VMs' booting process.
+
+Our modification to Nova is light-weighted (about 80 lines of insertions and
+deletions). Two major functions, i.e., the creation and deletion of the
+template and snapshot volumes, are implemented as following: (i) creation: We
+add a volume-driver class (about 50 lines, depends on VMThunder's API) in file
+"nova/virt/block_device.py" to prepare the template and snapshot volumes.
+(ii) deletion: We add a delete method (about 20 lines, depends on VMThunder's
+API) in file "nova/compute/manager.py' to destroy the unused template and
+snapshot volumes.
+
+More details of the implementation can be found in the following links:
+Paper, http://www.computer.org/csdl/trans/td/preprint/06719385.pdf
+Modification diff file, http://www.kylinx.com/vmthunder/diff2.txt
+VMThunder demo videos,
+http://www.kylinx.com/vmthunder/boot_vmthunder_win7_success-V2.mp4
+Image booting demo videos,
+http://www.kylinx.com/vmthunder/boot_image_test_win7_success-V2.mp4
+
+Alternatives
+------------
+
+(1)Image cache:
+Nova's image-caching facility reduces the start-up time for creating
+homogeneous virtual machines on one nova-compute node. However, it helps
+neither the first-time provisioning nor the Cinder-based booting process.
+
+(2)P2P transferring:
+The P2P protocol can increase the speed of the file distribution. For example,
+the glance-bittorrent-delivery proposal transfers the image templates from the
+glance storage to Nova-compute servers. This approach, however, needs to
+transfer the entire image to all peers.
+
+(3)Backend storage optimization:
+The distributed storages like NFS, cluster FS, distributed FS or SAN can
+decrease the size of transferred volumes. However, the I/O pressure on the
+storage servers increases dramatically when powering on a large number of
+homogeneous VMs, since there may not be enough replicas on the storage servers
+for offloading the I/O demands.
+
+(4)Multi-attach volume:
+(https://wiki.openstack.org/wiki/Cinder/blueprints/multi-attach-volume)
+This approach allows a volume to be attached to more than one instance
+simultaneously. As a result, volumes can be shared among multiple guests when
+the instances are already available. Besides, these volumes can also be used
+for booting a number of VMs by enforcing the multi-attach volumes as read-only
+image disks. Unfortunately, this approach does not scale well because of the
+star-structured topology in the data dissemination process.
+
+(5)Direct image access:
+(https://blueprints.launchpad.net/nova/+spec/nova-image-zero-copy).
+This approach uses the direct_url of the Glance v2 API, such that the number of
+needed hops to transfer an image to a Nova-compute node is decreased. When
+images are stored at multiple backend locations, the Nova-compute servers can
+select a proper image storage for speeding up the downloading process.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+We will significantly decrease the delay of booting up large numbers of
+Cinder-volume-based VMs.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee: vmThunderGroup (vmthunder)
+
+Work Items
+----------
+
+We only add one lightweight patch for Nova.
+
+Dependencies
+============
+
+The patch depends on the VMThunder project
+(https://launchpad.net/VMThunder).
+
+Testing
+=======
+
+We will add necessary tests into nova's test framework, in order to show the
+effectiveness of VMThunder. The unit tests and integrated tests will be added
+to the component.
+
+Documentation Impact
+====================
+
+We need to document how to create many homogeneous virtual machines though our
+new option.
+
+References
+==========
+
+VMThunder: http://www.VMThunder.org/
+
+Mailing list:
+http://lists.openstack.org/pipermail/openstack-dev/2014-April/032883.html
+
+VMThunder Publication:http://www.VMThunder.org/blog/2014/03/02/publication/
\ No newline at end of file
diff --git a/specs/juno/proposed/transfer-instance-ownership.rst b/specs/juno/proposed/transfer-instance-ownership.rst
new file mode 100644
index 0000000..5fac707
--- /dev/null
+++ b/specs/juno/proposed/transfer-instance-ownership.rst
@@ -0,0 +1,138 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================
+Transfer instance ownership
+===========================
+
+https://blueprints.launchpad.net/nova/+spec/transfer-instance-ownership
+
+Support to transfer an existing instance to another tenant or owner
+
+Problem description
+===================
+
+We need provide a way to update the tenant id and user id of an existing
+instance to allow transferring ownership of the instance to another
+tenant/project or owner.
+
+
+Proposed change
+===============
+
+* Update the tenant id or user id of an existing instance
+
+  * If the target user id does not belong to the specified tenant, need throw
+    exception.
+  * If the network doesn't belong to the target tenant this transfer should
+    be denied.
+  * If the based image doesn't belong to the target tenant this transfer
+    should be denied.
+  * If the related flavor doesn't belong to the target tenant this transfer
+    should be denied.
+  * If the attached volume will be transfered as well.
+
+* To support this function, we need new nova cli named 'update'
+
+  * The user id is an required attribute.
+  * The tenant id is an optional attribute.
+  * The existing rename command will be merged into this function, and suggest
+    to deprecate the rename command.
+
+Alternatives
+------------
+
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+New provide Nova API extension for the v2 API to support update tenant_id,
+user_id and name of the instance.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+
+Performance Impact
+------------------
+
+When update the tenant id or user id, we need invoke the keystone client
+api to validate it.
+Need call cinder api to update the ownership of the attached volume.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  genggjh
+
+Other contributors:
+
+Work Items
+----------
+
+* Update the tenant id or user id in nova api
+
+* Update novaclient to support this
+
+* Add new nova cli: 'nova update', to support update the tenant id or
+  user id and instance name.
+
+* Add new test cases in tempest
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+New new test cases in tempest to cover this end to end function.
+
+
+Documentation Impact
+====================
+
+Need provide new nova cli 'nova update' to support this function.
+
+
+References
+==========
+
+* https://blueprints.launchpad.net/nova/+spec/transfer-instance-ownership
\ No newline at end of file
diff --git a/specs/juno/proposed/usb-hot-plug.rst b/specs/juno/proposed/usb-hot-plug.rst
new file mode 100644
index 0000000..0a2c6f2
--- /dev/null
+++ b/specs/juno/proposed/usb-hot-plug.rst
@@ -0,0 +1,173 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================
+Support hot-plug USB device
+===========================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/usb-hot-plug
+
+Users have requirements of using USB device, the detailed information can
+refer to BP
+https://blueprints.launchpad.net/nova/+spec/usb-passthrough.
+
+There maybe some VMs which need to use USB device, so I think it is
+necessary to provide function of hot-plug USB device.
+
+
+Problem description
+===================
+
+Use cases:
+
+A user wants to install ERP software which needs USB-KEY for authentication
+in a running VM. he proposes request to system administrator and they
+perform the following steps:
+
+1. The administrator goes to the machine room and insert USB-KEY to host
+where the VM is running in.
+
+2. The administrator adds some configuration(like white-list with USB-KEY's
+product and vendor information) to nova-compute, restart nova-compute to
+trigger it to discovery new devices and save them to DB.
+
+3. The end user queries USB-KEY information from API or portal.
+
+4. The end user attaches USB-KEY to VM.
+
+
+Proposed change
+===============
+
+We should add the following functions to nova:
+
+1. Add API to query USB device information of a hypervisor.
+
+2. Add API to attach USB device to a VM.
+
+3. Add API to detach USB device from a VM.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+It can refer to USB-passthrough in
+https://review.openstack.org/#/c/86118/
+
+
+REST API impact
+---------------
+
+* API for attach USB device to VM
+
+V2 API specification:
+
+Post  v2/​{tenant_id}​/servers/​{server_id}/action
+
+{
+
+  "attach_usb_devices": {"usb_devices_requests":[{"id": 1}]}
+
+}
+
+V3 API specification:
+
+Post  v3​/servers/​{server_id}/action
+
+
+* API for detach USB device from VM
+
+V2 API specification:
+
+Post  v2/​{tenant_id}​/servers/​{server_id}/action
+
+{
+
+  "detach_usb_devices": {"usb_devices_requests":[{"id": 1}]}
+
+}
+
+V3 API specification:
+
+Post  v3​/servers/​{server_id}/action
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+It can refer to USB-passthrough in
+https://review.openstack.org/#/c/86118/
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+It can refer to USB-passthrough in
+https://review.openstack.org/#/c/86118/
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <Jing Yuan>
+
+
+Work Items
+----------
+
+Step 1: Implement function of attach USB device to VM.
+
+Step 2: Implement function of detach USB device from VM.
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+It is necessary to add tempest for this new function.
+
+Documentation Impact
+====================
+
+It is necessary to add doc for how to use this new function.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/usb-passthrough-proposal-1.rst b/specs/juno/proposed/usb-passthrough-proposal-1.rst
new file mode 100644
index 0000000..78b5982
--- /dev/null
+++ b/specs/juno/proposed/usb-passthrough-proposal-1.rst
@@ -0,0 +1,352 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================
+Support passthrough of USB device
+=================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/usb-passthrough
+
+We provide VDI(Virtual Desktop) and server virtualization solutions for
+customers, our customers have strong requirements for using USB devices.
+
+The typical use cases and our solutions are described as below:
+
+1.In VDI solution, customers want to use local USB printers or USB scanners
+with TC(Thin-Client), because remote desktop protocol like ICA have already
+supported USB-redirection, so customers only need to attach USB device to
+TC,  the protocol can map USB device to VM.
+
+2. In virtualization solution, when starting or restarting some
+business-critical applications, a connected USB-KEY is needed for
+authentication, some applications even need a daily authentication by USB-KEY
+. we suggest the following solutions:
+
+(1) Using physical 'USB-HUB' box and technology of USB-redirection over
+TCP/IP. Customers need to buy USB-HUB and install software in guest os,
+the software helps redirecting USB device to VM.
+
+(2) Using USB-Passthrough functions provided by our virtualization software.
+The end users(normally application or system administrators) insert USB
+devices to host that containing the VM, then can see USB device list in
+portal and choose USB device to attach.
+
+This solution has advantages that
+
+1. It doesn't need additional physical devices.
+
+2. It doesn't need a special server to run spice client for USB-Redirection.
+
+3. Business-critical applications commonly need stable and long-standing
+USB-KEY to attach, USB-Passthrough maybe more stable than USB-Redirection
+over TCP/IP or remote desktop protocol.
+
+As described above, I think USB-Passthrough is valuable in Openstack.
+
+This BP is focus on how to provide USB-Passthrough function in Openstack
+
+
+Problem description
+===================
+
+Use cases:
+
+In private cloud, a end user wants to create a VM to run ERP software which
+needs a USB-KEY for authentication, he proposes request to system
+administrator, they perform the following steps:
+
+1. The administrator goes to the machine room and chooses a host to insert
+USB-KEY to.
+
+2. The system automatically discovery the device and store device information
+to DB.
+
+3. The administrator queries USB-KEY information from API or portal.
+
+4. The administrator creates a flavor with USB-KEY information.
+
+5. The end user creates a VM with above flavor, the system chooses the host
+which contains the USB-KEY to run VM and attach USB-KEY to it.
+
+
+Proposed change
+===============
+
+The idea of how to implement this function can refer to pci-passthrough.
+
+We should add the following functions to nova:
+
+1. Nova should support to auto-discover usb devices and store them in DB.
+
+2. Nova should support to create a flavor which contains usb device
+information.
+
+3. Nova should support to create VM with a flavor which contains usb device
+information.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+The idea of how to implement this function can reference to pci-passthrough.
+
+1. In libvirt driver layer, add function to discover USB devices.
+
+There maybe some USB devices in a host, like USB controller, USB keyboard,
+USB disk, but user only want to use USB disk. we can do the following
+steps to find proper device:
+
+(1) Libvirt driver use self._conn.listDevices('usb_device', 0) to get
+USB device information.
+
+(2) Use black-list function to filter out USB controller and
+USB keyboard, only retain USB disk.
+
+For example: usb_passthrough_blacklist =
+'[{"vendor_id": "1d6b", "product_id": "0002"}]', which is the
+'Linux ehci_hcd'
+
+2. In DB layer, (1) add a new table 'usb_devices' to store USB device
+information. (2) add a key-value pair to instance_system_metadata table to
+store USB device information which will be assigned to the VM.
+
+usb_devices:
+
++-----------------+--------------+------+-----+---------+----------------+
+| Field           | Type         | Null | Key | Default | Extra          |
++-----------------+--------------+------+-----+---------+----------------+
+| created_at      | datetime     | YES  |     | NULL    |                |
+| updated_at      | datetime     | YES  |     | NULL    |                |
+| deleted_at      | datetime     | YES  |     | NULL    |                |
+| deleted         | int(11)      | NO   |     | NULL    |                |
+| id              | int(11)      | NO   | PRI | NULL    | auto_increment |
+| compute_node_id | int(11)      | NO   | MUL | NULL    |                |
+| address         | varchar(12)  | NO   |     | NULL    |                |
+| product_id      | varchar(4)   | YES  |     | NULL    |                |
+| vendor_id       | varchar(4)   | YES  |     | NULL    |                |
+| bus_id          | int(10)      | YES  |     | NULL    |                |
+| device_id       | int(10)      | YES  |     | NULL    |                |
+| port_id         | int(10)      | YES  |     | NULL    |                |
+| dev_id          | varchar(255) | YES  |     | NULL    |                |
+| status          | varchar(36)  | NO   |     | NULL    |                |
+| extra_info      | text         | YES  |     | NULL    |                |
+| instance_uuid   | varchar(36)  | YES  | MUL | NULL    |                |
++-----------------+--------------+------+-----+---------+----------------+
+
+3. In scheduler layer, add a filter to find proper host which contains the
+usb devices and create VM in this host.
+
+4. The schema of flavor which contains USB device information may like this:
+{"usb_devices":[{"id": 5}]}
+
+5. How to distinguish USB device is important because user wants to specify
+which USB device to which VM.
+
+I consider it as follows:
+
+1. The identifier of a USB device which are supported by libvirt are
+vendor id, product id, bus and device. Bellow is a example:
+
+<hostdev mode='subsystem' type='usb'>
+<source>
+<vendor id='0x136b'/>
+<product id='0x0003'/>
+<address bus='2' device='3'/>
+</source>
+</hostdev>
+
+Different USB devices may have the same vendor id and product id, so they are
+not appropriate to distinguish devices.
+
+The bus+device maybe a good choice to distinguish devices, but I have made some
+tests and found that the device id will change every time reboot host or
+detach/reattach USB device from/to host.
+
+I have tested vsphere and saw some documents, they in fact pass-through
+physical port of a host to VM, if user insert a USB device to this port,
+guest os will automatic discovery the new devices.
+
+I think libvirt doesn't support this function by now.
+
+So I think bus+device maybe the best choice.
+
+6. Use which usb controller is also an important thing to consider. I have
+made some tests with libvirt-1.2.0-1.el6.x86_64 and qemu-kvm-1.6.2-00001.x86_64
+in centos 6.5. The test results are:
+
+guest           xp(sp3)		    win7		    centos6.5
+usb controller  uhci	ehci	uhci	ehci	uhci	ehci
+create vm	    OK	    NOK	    NOK	    OK	    OK	    OK
+detach usb	    OK		                OK	    OK	    OK
+atach usb	    OK		                OK	    OK	    OK
+start vm	    OK		                OK	    OK	    OK
+restart	        OK		                OK	    OK	    OK
+suspend/resume 	OK		                OK	    OK	    OK
+
+xp(sp3)+ehci: Device is in abnormal status. After shutdown VM the device will
+not be normally released to hypervisor. Is the guest os problem?
+
+win7+uhci: Device is in abnormal status. After shutdown VM the device will be
+normally released to hypervisor. A related bug about it in
+https://bugs.launchpad.net/qemu/+bug/685096
+
+So I think the default uhci controller can not meet the requirements. I
+suggest two options:
+
+Option 1: Create a default ehci USB controller in libvirt driver, all usb
+devices use this default controller. Guest os should support ehci controller
+by installing driver or they will not been supported by this function.
+
+Option 2: User can specify USB controller type when specify USB device, then
+system create USB controller(ehci, xhci) for USB device.
+
+I suggest option 2 because although it is more complex to implement but it
+is more flexible for future extension.
+
+7. Restriction on max supported USB devices: I think a VM with 2 USB devices
+will cover 99% use scenarios. So it is not necessary to consider USB hub
+or some other mechanisms to extend more ports for more USB device. Every
+kind of USB controller has only one.
+
+REST API impact
+---------------
+
+* API for query USB device information of a hypervisor
+
+V2 API specification:
+
+GET v2/{tenant_id}/os-hypervisors/{hypervisor_hostname}
+
+V3 API specification:
+
+GET v3/os-hypervisors/{hypervisor_hostname}
+
+The response JSON contains the variable "usb_device_stats"
+{"hypervisor":{"usb_device_stats":[{"id": 1,"compute_node_id": 1,
+"address": "usb_2_3", "product_id": "1520","vendor_id": "8086",
+"bus_id": "2", "device_id": "3","port_id": "", "status":"available",
+"extra_info": """instance_uuid":""}]}}
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+I have implemented a demo and made some tests:
+
+1. If migrate(live-migration, with shared storage or not) a VM with USB device
+from host A to host B, because the destination host doesn't have USB device
+the migration may be failed.
+
+2. Currently I want to use <address bus='2' device='3'/> to exclusively
+identify a physical USB device of a host. But I found that every time reboot
+host or detach/re-attach USB device from/to host the device id will change.
+
+For this problem I suggest to refer to pci-passthrough:
+
+(1) System change the 'invalid' device's status to 'deleted', and create a new
+device record in DB.
+
+(2) If user starts VM with 'invalid' device, the libvirt driver will
+automatically ignore the 'invalid' device when construct xml. So VM will start
+normally but user can not see USB device in guest os.
+
+(3) If user wants to use USB device again, him can create a new flavor with
+this 'new' USB device and resize VM with the new flavor.
+
+3. System has restriction on max supported USB device number because a single
+USB controller has restricted port numbers.For example a uhci controller has
+2 ports so only supports two USB devices.
+
+4. Different guest os may have restriction on supporting USB device.
+
+5. Currently I only plan to provide this function in KVM so not consider too
+much for xen.
+
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+I have implemented a demo and made some tests:
+
+1. This new function doesn't affect other server actions including
+start, stop, reboot, pause/unpause, suspend/resume, rebuild.
+
+2. We need to add some processes of USB devices in resize function,
+like resize VM from flavor A with USB device A to flavor B with USB device B.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <Jing Yuan>
+
+
+Work Items
+----------
+
+Step 1: Implement function of discover usb device in libvirt driver.
+
+Step 2: Implement function of periodically update USB device information
+from nova-compute to DB.
+
+Step 3: Implement function of query USB devices of hypervisor.
+
+Step 4: Implement function of create VM with USB device.
+
+Step 5: Modify other functions which have been affected by this new
+function.
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+It is necessary to add tempest for this new function.
+
+Documentation Impact
+====================
+
+It is necessary to add doc for how to use this new function.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/usb-passthrough-proposal-2.rst b/specs/juno/proposed/usb-passthrough-proposal-2.rst
new file mode 100644
index 0000000..b3733ae
--- /dev/null
+++ b/specs/juno/proposed/usb-passthrough-proposal-2.rst
@@ -0,0 +1,215 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================
+Support passthrough of USB device and USB controller
+====================================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/usb-passthrough
+
+Some telecom and enterprise customers have requirement of usb-passthrough
+especially in private cloud. So I think maybe it's a good choice to provide this
+feature in openstack.
+
+
+Problem description
+===================
+
+Use cases:
+
+* There are some ERP software deployed in VM that need usb-key for
+  authentication, the end user proposes the request to administrator,
+  the administrator helps the end user to insert a usb device to a host,
+  and makes some configuration so that system can discovery the device.
+
+* The end user creates a VM with specified usb device.
+
+* The end user attaches a usb device to a specified VM.
+
+* The end user specifies a usb controller for usb device.
+
+Proposed change
+===============
+
+Most of the implementation can reference to the idea of pci-passthrough.
+
+The function is only considered for libvirt/qemu-kvm environment.
+
+For usb controller, I suggest the following implementations:
+
+* Nova supports creating a default piix3-usb-uhci controller and a default
+  ehci controller when creating VM. xhci controller is not supported well by
+  now.
+  The piix3-usb-uhci controller has no usb device number restriction because
+  libvirt/qemu will create more usb hub if usb ports are not enough. The ehci
+  controller has a maximum of 6 usb ports limitation so that only supports a
+  maximum of 6 usb device which we think are enough for business uses.
+
+For usb device, I suggest the following implementations:
+
+* Nova supports auto-discovering usb devices and saving them to DB.
+
+* Nova supports creating a flavor containing usb device information.
+
+* Nova supports creating VM with a flavor which contains usb device
+  information. System schedulers the request to appropriate host which contains
+  the specified usb device and attaches usb device to VM.
+
+* Nova supports hot-plug/cold-plug usb devices to an exist VM.
+
+* Nova supports optionally specifying usb controller type(piix3-usb-uhci or
+  ehci) when using usb device. If not specify then use the default
+  piix3-usb-uhci.System not record which usb device to which usb controller,
+  only randomly and dynamically specify the relationship.
+
+Alternatives
+------------
+
+For usb controller, there are other two options:
+
+Option 2:
+
+* Nova supports new API for usb controller, the end user can create usb
+  controller when creating VM or call API to create new usb controller if usb
+  ports are not enough. The key parameters of usb controller are number and
+  type.
+
+Shortcoming:
+
+* It may make the operation of end user too complex.
+
+* It is complex to implement this function, system should make decision
+  about which device to which controller and record the relationships.
+  In other words, there are no business use case that too many usb devices
+  with one VM.
+
+Option 3:
+
+* Nova not creates default usb controllers when creating VM,
+  only creates ones when attaching usb device to VM.
+
+Shortcoming:
+
+* It is also complex to implement this function like Option 2.
+
+Option 4:
+
+* Don't specify usb-controller for usb device, so libvirt/qemu-kvm will attach
+  usb device to default piix3-usb-uhci controller.
+
+Shortcoming:
+
+* The speed of usb device may mismatch with usb controller.
+
+Data model impact
+-----------------
+
+
+
+For usb controller:
+
+* The libvirt driver should add usb controller data object.
+
+For usb device:
+
+* In hypervisor layer, add function to discover usb devices, and add a
+  white-list function to filter invalid information like physical usb
+  controller.
+
+* In DB layer, add a new data table usb_devices to record usb devices
+  information.
+
+* In DB layer, add a key-value pair to instance_system_metadata table to
+  record usb device information which have been attached to a VM.
+
+* In scheduler layer, add a filter to filter appropriate host which
+  contains the usb devices.
+  
+* In API layer, add a key-value to extra_specs property of flavor API to
+  specify usb devices. The schema may like:{"usb_devices":[{"id": 5,
+  "usbcontroller_type":"ehci"}]}
+
+* In API layer, add hot-plugin/cold-plugin API for attaching/detaching usb
+  devices to/from a running/stopped VM.
+
+* Some function like suspend/resume a VM, make snapshot of a VM
+  may be affected by this new function. For example it may be necessary to
+  detach usb devices before suspend a VM.
+
+
+REST API impact
+---------------
+
+NULL
+
+Security impact
+---------------
+
+NULL
+
+Notifications impact
+--------------------
+
+NULL
+
+Other end user impact
+---------------------
+
+NULL
+
+Performance Impact
+------------------
+
+NULL
+
+Other deployer impact
+---------------------
+
+NULL
+
+Developer impact
+----------------
+
+NULL
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <Jing Yuan>
+
+
+Work Items
+----------
+NULL
+
+Dependencies
+============
+
+NULL
+
+
+Testing
+=======
+
+NULL
+
+Documentation Impact
+====================
+
+NULL
+
+
+References
+==========
+
+NULL
diff --git a/specs/juno/proposed/usb-passthrough-with-usb-controller.rst b/specs/juno/proposed/usb-passthrough-with-usb-controller.rst
new file mode 100644
index 0000000..7607a70
--- /dev/null
+++ b/specs/juno/proposed/usb-passthrough-with-usb-controller.rst
@@ -0,0 +1,149 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+Support specify USB controller for USB-passthrough
+==================================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/usb-passthrough-with-usb-controller
+
+Users have requirement of using USB devices, the detailed information can
+reference the bp of
+https://blueprints.launchpad.net/nova/+spec/usb-passthrough.
+
+If not specify appropriate type of USB controller for USB device, USB device
+will use the default piix3-usb-uhci, the default USB device's speed may
+mismatch with USB device. This result in the following problems.
+
+1. The low speed of USB device.
+
+2. If use spice client to redirect USB device to VM, the mismatched speed may
+prevent the connection.
+
+As described above, I think specify USB controller for USB-passthrough is
+valuable in Openstack.
+
+
+Problem description
+===================
+
+Use cases:
+
+1. The administrator creates a VM with flavor which contains USB device
+information and USB controller information.
+
+2. The system creates VM with USB controller specified in flavor above, it
+also attach USB device to the created USB controller but not the default
+piix3-usb-uhci.
+
+Proposed change
+===============
+
+We should add the following functions to nova:
+
+1. When create flavor with USB device, specify the USB controller type for USB
+device.
+
+2. Nova should support to attach USB device to specified USB controller when
+create VM.
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+1. The schema of flavor which contains USB device information and USB
+controller information may like this:
+{"usb_devices":[{"id": 5, "usb_controller_type": "ehci"}]}
+
+2. In libvirt driver layer, add function to specify USB controller for
+USB device. System should select distinguish port for USB device to attach.
+
+3. In DB layer, add USB controller type parameter to USB device information
+which have been assigned to the VM.
+
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <Jing Yuan>
+
+
+Work Items
+----------
+
+Step 1: Add USB controller type to USB device data model in libvirt driver.
+
+Step 2: Implement function of choose USB controller port for USB device.
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+It is necessary to add tempest for this new function.
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+https://blueprints.launchpad.net/nova/+spec/usb-passthrough
+
+https://blueprints.launchpad.net/nova/+spec/add-usb-controller
diff --git a/specs/juno/proposed/usb-redirection.rst b/specs/juno/proposed/usb-redirection.rst
new file mode 100644
index 0000000..4dbda19
--- /dev/null
+++ b/specs/juno/proposed/usb-redirection.rst
@@ -0,0 +1,209 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================
+Support function of USB-redirection
+===================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/usb-redirection
+
+We provide VDI(Virtual Desktop)solution for customers, our customers have
+strong requirements for using USB devices.
+
+The typical use cases and our solutions are described as below:
+
+Customers want to use local USB printers or USB scanners with
+TC(Thin-Client), because remote desktop protocol like ICA have already
+supported USB-redirection, so customers only need to attach USB device to TC,
+the protocol can map USB device to VM.
+
+
+Problem description
+===================
+
+Use cases 1:
+
+A end user wants to print some documents in cloud environments, him perform
+the following steps:
+
+1. Create a VM with virtual USB-redirection devices.
+
+2. Use TC with spice client to connect to VM, insert USB printer to TC,
+select printer to redirect to VM in GUI of spice client.
+
+3. User can see printer in VM and use printer to print documents.
+
+Use cases 2:
+
+A end user wants to take a video chat with friends in cloud environments,
+him perform the following steps:
+
+1. Create a VM with virtual USB-redirection devices.
+
+2. Use TC with spice client to connect to VM, insert USB camera to TC,
+select camera to redirect to VM in GUI of spice client.
+
+3. User can see camera in VM and take video chat with friends.
+
+Use cases 3:
+
+A end user wants a on-line shopping, which needs a USB-key for more secure
+payment.
+
+1. Create a VM with virtual USB-redirection devices.
+
+2. Use TC with spice client to connect to VM, insert USB-key to TC, select
+USB-key to redirect to VM in GUI of spice client.
+
+3. User can see USB-key in VM, the security check and payment plugin in
+Browser like IE can use USB-key for authentication.
+
+
+Proposed change
+===============
+
+1. Add USB-redirection device class in libvirt driver.
+
+Details:
+
+Add 'LibvirtConfigGuestDeviceRedirector' class to libvirt driver. The
+related xml may like:
+
+<redirdev bus='usb' type='spicevmc'>
+
+<address type='usb' bus='1' port='2'/>
+
+</redirdev>
+
+The values of 'bus' and 'type' should be specified by user. The values of
+'address' use constants which user not need to specify.
+
+BTW: BP about echi controller can refer to
+https://review.openstack.org/#/c/88334/
+
+2. If a user wants to use this function, him should firstly set property
+of 'usb_redirector' to image which him uses to create VM, the command
+may like:
+
+glance image-update img1 --property usb_redirector="{'bus': 'usb',
+'type': 'spicevmc'}"
+
+Once libvirt driver finds this property, it creates a
+'LibvirtConfigGuestDeviceRedirector' object, then to_xml function will use
+this object to construct xml of VM, so the xml of VM wil include 'redirdev'.
+
+Alternatives
+------------
+
+Option2 : Libvirt driver creates a default USB redirdev object for every VM.
+
+Option3 : Compared to option2, uses a nova configuration to control if create
+a default USB redirdev for every VM.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <Jing Yuan>
+
+
+Work Items
+----------
+
+Step 1: Add object of USB-redirection device in libvirt driver.
+
+Step 2: Create VM with USB-redirection device.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+About how to create USB redirdev:
+
+1. If choose option 1 as proposed in 'Proposed change', user should set
+property usb_redirector into image and choose this image to create VM.
+
+2. If choose option 2, user needs to do nothing.
+
+3. If choose option 3, user should change nova configuration to control if
+create a default USB redirdev.
+
+About how to use spice client to do the USB redirection, I have tested in
+CentOS 6.4 with virt-viewer.
+
+1. The system should support x11 and run in x11 mode.
+
+2. Use 'remote-viewer spice://<host>:<port_number>' to connect to VM which
+has USB redirdev.
+
+3. In remote-viewer interface, choose 'File'->'USB device selection' to open
+'Select USB device for redirection' interface, then tick off USB device which
+you want to use. Finally the USB device will appears in your VM.
+
+User may want to install spice-vdagent in guest OS for other functions like
+copy text between spice client and VM. We not go into it here.
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/use-configdrive-with-ironic.rst b/specs/juno/proposed/use-configdrive-with-ironic.rst
new file mode 100644
index 0000000..bfad93d
--- /dev/null
+++ b/specs/juno/proposed/use-configdrive-with-ironic.rst
@@ -0,0 +1,178 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================
+Use configdrive with Ironic
+===========================
+
+https://blueprints.launchpad.net/nova/+spec/use-configdrive-with-ironic
+
+This blueprint adds support for configdrive for the Ironic virt driver, to
+enable Nova to pass a configdrive to Ironic, to be used when deploying a
+bare metal instance.
+
+Problem description
+===================
+
+Instances deployed by Ironic should be able to use cloud-init (or similar
+software) to put an end user's data on an instance. This is possible today with
+Ironic by including cloud-init with the image, and pointing it at a Nova
+metadata service.
+
+There are two issues with this approach:
+
+* Some deployers do not run a metadata service in their environment.
+
+* If a deployer provisions Ironic machines using static IP address assignment,
+  the instance will not have network access until cloud-init puts the network
+  configuration into place. If the metadata service is the only way to get
+  the network configuration, the instance is deadlocked on getting network
+  access.
+
+To solve these problems, a configdrive image can take the place of the metadata
+service. In the VM world, this is typically handled by the hypervisor exposing
+a configdrive image to the VM as a volume.
+
+In Ironic's case, there is no hypervisor, so this image needs to be exposed to
+the instance in some other fashion. This could be accomplished by writing the
+image to a partition on the node, exposing the image via the out-of-band
+mechanism (e.g. a virtual floppy in HP's iLO), or configuring the node to mount
+the image from a SAN. In any case, this needs to be handled by Ironic, rather
+than Nova. However, Nova has the data that belongs in the configdrive, as well
+as the code to generate the image. So, it makes sense for Nova to generate an
+image and pass it to Ironic.
+
+
+Proposed change
+===============
+
+Nova should generate the configdrive image and pass it to Ironic, if needed.
+This should use the existing code (nova.virt.configdrive:required_by) to
+determine if a configdrive should be generated.
+
+This will consist of these steps:
+
+* The Ironic virt driver decides if a configdrive should be generated for this
+  instance. If so:
+
+* The virt driver generates the configdrive and gzips it.
+
+* The virt driver puts the gzipped configdrive image in Glance.
+
+* If present, Ironic will pass this Glance URL to its DeployDriver, which will
+  in turn provide it to the instance during provisioning, by means appropriate
+  to that driver.
+
+* Ironic uses this Glance URL to write a configdrive partition on the
+  instance during provisioning.
+
+Alternatives
+------------
+
+The only alternative to solving the problems described above, is to write
+network configuration and user data directly to the image to be deployed, on
+the fly. I think it is suffice to say that Ironic should not be in the business
+of injecting files directly into images, nor should we force end users to
+use custom images with this data already injected.
+
+As mentioned before, Ironic's drivers may provide various mechanisms for
+exposing this image to the instance. However, no matter the mechanism used,
+the interaction will be the same from Nova's perspective.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+This proposal involves storing end user data in Glance. This may be a security
+concern, as this data is not encrypted at rest.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+Generating the configdrive image and sending it to another service will cause
+Nova to spend more time in Ironic's virt driver, although the additional time
+spent should be relatively small.
+
+Other deployer impact
+---------------------
+
+The force_config_drive option defaults to False, as does instance.config_drive,
+so deploying this code should have no impact on deployers.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jroll
+
+Work Items
+----------
+
+* Implement the code and unit tests. This will involve changing the deploy()
+  function in Ironic's virt driver to generate the configdrive, upload
+  it to Glance, and pass the Glance URL for the configdrive in the PATCH
+  request to Ironic. The tear_down() method of Ironic's virt driver will
+  be changed to remove the configdrive key from instance_info. Unit tests will
+  need to be updated accordingly.
+
+* Write Tempest tests - see testing plan below.
+
+
+Dependencies
+============
+
+This change depends on the Ironic virt driver landing in Nova, as well as
+Ironic support for writing the configdrive to the instance.
+
+
+Testing
+=======
+
+TODO: need to find out how configdrive things are tested in Tempest today and
+do something similar.
+
+
+Documentation Impact
+====================
+
+Documentation may need to be updated to indicate that a configdrive may
+be used with bare metal instances.
+
+
+References
+==========
+
+* Ironic virt driver blueprint: https://blueprints.launchpad.net/nova/+spec/add-ironic-driver
+
+* Ironic virt driver spec: https://review.openstack.org/#/c/95024
diff --git a/specs/juno/proposed/use-glance-v2-api.rst b/specs/juno/proposed/use-glance-v2-api.rst
new file mode 100644
index 0000000..8ace162
--- /dev/null
+++ b/specs/juno/proposed/use-glance-v2-api.rst
@@ -0,0 +1,167 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================
+Allow Nova to use Glance V1 or V2 API
+=====================================
+
+https://blueprints.launchpad.net/nova/+spec/use-glance-v2-api
+
+This blueprint adds support for Glance V2 API, while keeping support for the V1
+API. While this adds no extra functionality, it will allow the deprecation of
+Glance V1.
+
+Problem description
+===================
+
+Due to the fact that Nova is currently using Glance V1 API, Nova cannot take
+advantages of features available in Glance V2 API: for example, to get direct
+access to the image locations regarding backend storage.
+Also, Glance is planning to deprecate V1 API: so it becomes critical to make
+Nova working with Glance V2.
+This blueprint will make Nova configurable via a configuration parameter to use
+either Glance V1 or V2 API. Due to differences in the workflow exposed by the
+API, the image service layer will need to be able to present a consistent
+interface to Nova while performing necessary translations to interact with the
+configured Glance API version.
+
+Proposed change
+===============
+
+This approach proposed here is to have a configuration option giving the
+ability to select if Nova uses Glance V1 or V2 API.
+This ability involves some refactoring to have glance drivers for each version
+of the API in Nova.
+If V1 is selected, then the Glance driver V1 is used (similarly for V2).
+
+Alternatives
+------------
+
+- Have a discovery mechanism of the version of the endpoints available:
+  the main problem with that is that the operator might want to keep using a
+  previous version of the API that is trusted and slowly move to the new
+  version of the API. Also, the behavior of features has changed in V2 and is
+  not completely compatible with current one: image properties in V2 can have
+  schema restrictions placed by the deployer but V1 ignore these restrictions.
+  Auto-negotiating could potentially result in undesired changes.
+- Have some part of Glance (client, stores) becoming a transfer service that
+  has the capability of providing to Nova what is needed to get the disk image
+  on the hypervisor and being version agnostic.
+  This would require several key architectural changes both in Glance and Nova:
+  both are not ready for that at this point.
+
+Data model impact
+-----------------
+
+From the Nova perspective, there should not be any impact on Nova's
+functionality. All the differences in the Glance API should be encapsulated
+within the Glance V1 and V2 drivers.
+All the metadata created using V1 API could also be created using V2 API.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+A configuration section will need to be added in the configuration [glance]
+with a config parameter 'api_version' in it. Its default value is 1 meaning
+that by default Nova will be using Glance API V1. The upgrade impact should
+warn about the need for Glance V2: the deployer either needs to deploy a new
+glance-api node and enable V2 API or needs to configure Nova to use Glance V2
+API. The default version will be V1 for Juno and we plan to use V2 as default
+in the K release.
+It might be risky to expose an OpenStack cloud mixing Nova compute nodes using
+V1 and V2 Glance API at the same time: especially due to the fact that V1 and
+V2 are not fully compatible, ex: schema restrictions in V2 but not in V1.
+The existing features required to communicate with Glance will still work: the
+workflow changes will be totally transparent for the deployer.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    Eddie Sheffield (esheffield)
+
+Other contributors:
+    Zhi Yan Liu (zhiyan)
+    Arnaud Legendre (arnaud)
+    Vincent Hou
+
+Work Items
+----------
+
+- Refactor the image layer to support Glance V2 support
+- Add Glance driver for V1
+- Add Glance driver for V2
+- Enable V1 and V2 driver for Nova
+- Update drivers using Glance directly (such as XenServer)
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+Tempest tests should be added to make sure that V2 API features are fully
+workable with Nova exercised.
+We could make one of the devstack gate job use Glance V1 and one Glance V2.
+
+Documentation Impact
+====================
+
+The new configuration variable 'api_version' in the [glance] section needs to
+be mentioned in the reference documentation. Also, the Cloud Admin Guide needs
+to be updated to let people know how to configure Nova to use one version or
+the other.
+Also, it will important to stress in the release notes, docs, etc. that the
+behavior of Glance features have changed between V1 and V2 and consequently
+are not wholly compatible: for example image properties in V2 can have schema
+restrictions placed by the deployer but V1 ignore these restrictions.
+
+References
+==========
+
+Mailing list discussion:
+http://www.mail-archive.com/openstack-dev%40lists.openstack.org/msg06385.html
+
+Discussed at the Glance Mini-summit in Washington DC (see Takeaways):
+https://etherpad.openstack.org/p/glance-client-common-api
+
+Icehouse summit images V1 API design session:
+https://etherpad.openstack.org/p/icehouse-summit-images-v1-api
diff --git a/specs/juno/proposed/use-physical-cdrom.rst b/specs/juno/proposed/use-physical-cdrom.rst
new file mode 100644
index 0000000..10a89aa
--- /dev/null
+++ b/specs/juno/proposed/use-physical-cdrom.rst
@@ -0,0 +1,125 @@
+
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+Add support for physical CD-ROM when VM is running
+==================================================
+
+https://blueprints.launchpad.net/nova/+spec/use-physical-cdrom
+
+Sometimes we need to attach/detach PC's CD-ROM device to a running instance
+when we want to access content directly without upload to glance.
+
+Base on this feature, user needn't to convert CD-ROM disk to ISO file or copy
+the content of CD-ROM disk into the instance.
+
+The feature makes administrator's maintenance more convenient.
+
+Problem description
+===================
+
+Currently the user access the ISO's content has the following methods:
+
+* Downloading the ISO (or the files on it) into the instance and access it by
+  network.
+* Uploading the ISO file to glance build it to image, and then use the image
+  to create disk, and then attach disk to VMs to access content.
+
+The above is base on ISO, if the user only has the CD/DVD, he/she must use
+some software convert it to ISO or copy content base on VM's network first.
+
+With this feature, the user can access the PC's CD-ROM device directly, so it
+easy to operate.
+
+
+Proposed change
+===============
+
+Add CD-ROM service in compute node, add plug in dashboard as CD-ROM client.
+Thus the CD-ROM of the PC can be use as a host's device.
+
+
+Alternatives
+------------
+None
+
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+
+Modify the existing "attach_block_device" action.
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+   <hs.chen@huawei.com>
+
+
+
+Work Items
+----------
+
+
+* Add server in compute node.
+* Add client plug in dashboard.
+* Modify attach_block_device API.
+* Add testcase.
+
+
+Dependencies
+============
+
+The compute node must be able to connect to the PC which visits dashborad.
+
+
+Testing
+=======
+
+Unit tests and tempest tests will added.
+
+Documentation Impact
+====================
+
+Add the use case to the Virtual Machine Image Guide.
+
+
+References
+==========
+None
\ No newline at end of file
diff --git a/specs/juno/proposed/user-project-metadata.rst b/specs/juno/proposed/user-project-metadata.rst
new file mode 100644
index 0000000..152320e
--- /dev/null
+++ b/specs/juno/proposed/user-project-metadata.rst
@@ -0,0 +1,171 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================================
+User,project,domain exposed via the openstack metadata services
+===============================================================
+
+**BP:** https://blueprints.launchpad.net/nova/+spec/user-project-metadata
+
+The proposal here is to add a few select fields to the metadata services that
+nova provides compute resource access to. These fields would be related to the
+individual who has created the compute resource; allowing for various on-boot
+operations to work on that users behalf (for example).
+
+Problem description
+===================
+
+It is useful to expose the following tuple (user uuid, user name, project name,
+project uuid) of information to the instance to allow for the instance upon
+being booted to configure itself for the user or project that created the
+instance.
+
+A few use-cases that require this:
+
+* Installing system tools on-boot that should only be on available for project
+  uuid X and not for another project uuid Y. Supplying these tuples in
+  metadata allows for cloud-init (or other metadata/userdata consuming scripts)
+  to perform different actions *automatically* on-behalf of the user or
+  project.
+* Connecting into a system registration and management system, like
+  `landscape`_. For example, every system launched in the *WebApps* project
+  gets automatically tagged (in such system) so that the *WebApps* sysadmins
+  can manage them.
+
+.. _landscape: https://landscape.canonical.com/
+
+Proposed change
+===============
+
+When populating the extra metadata that is provided to the config drive and
+to the openstack metadata service provide the user_id, project_id,
+user_name, project_name as extra data that will be provided for the instance
+to consume as instance metadata.
+
+New fields will be added to the openstack `metadata`_ and to
+the openstack `config drive`_ (a new metadata version should be created
+to match this new data fields)::
+
+    {
+        "uuid":"d8e02d56-2648-49a3-bf97-6be8f1204f38",
+        ...
+        "owner":{
+            "project_id": "4bdce62d-18b0-470e-9f10-f6d37d5b0714",
+            "project_name": "harlowja",
+            "user_id": "eb4b83cd-35f3-4896-b35d-88cc67d08c31",
+            "user_name": "harlowja",
+        },
+        ...
+    }
+
+.. _metadata: http://docs.openstack.org/admin-guide-cloud/content/section_metadata-service.html
+.. _config drive: http://docs.openstack.org/user-guide/content/config-drive.html
+
+Alternatives
+------------
+
+1. User has to submit same data manually by providing it as user-data (and
+   cloud-init then has to know to look at user-data locations for this
+   information).
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+* The existing openstack metadata version (which also appears on the config
+  drive as a folder representation) should be incremented and the new
+  data will be added to this API version, the current version being
+  ``2013-10-17.``
+
+Security impact
+---------------
+
+* Provides instance owner their own project uuid/user uuid/domain uuid and
+  associated names making it easier for an attacker to provision VMs from
+  inside an instance by calling out to nova. Of course the instance would not
+  have access to the users password or keystone credentials so the risk should
+  be minimial (although the risk is more than it was when this information was
+  not available in the first place).
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None, existing query for data from nova-compute to populate config-drive or
+openstack metadata will be expanded to include new fields (which should
+exist in the available ``context`` anyway).
+
+Other deployer impact
+---------------------
+
+* No new config options
+* Will take effect on config-drive and metadata service/s immediately.
+* The change will be backwards compatible (since the change is additive) so
+  continuous deployments should see no negative affects.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  harlowja
+
+Work Items
+----------
+
+* Add extraction of needed data at location where metadata and config drive
+  extra metadata is initialized.
+* Provide new information & increment openstack metadata version.
+* Incorporate change in config-drive as new folder/version where this data is
+  provided and adjust openstack code so that ``latest`` points to this new
+  version (instead of the prior version).
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Seeing that there appears to be zero metadata tests (?) in tempest it would be
+great if this change could start adding at least a few tests to tempest that
+post-boot call into the metadata service (with tempest calling on behalf of
+the vm) and verifying that the metadata returned contains the newly added
+information. The unit tests inside nova related to the metadata service would
+need to be adjusted to ensure that this feature locally works (and it's a great
+addition to tempest to verify it externally).
+
+Documentation Impact
+====================
+
+* Adjustment of documentation about config-drive provided files.
+* Adjustment of documentation about openstack metadata provided apis.
+
+References
+==========
+
+* https://review.openstack.org/#/c/72018/
diff --git a/specs/juno/proposed/username-in-nova-list-for-admin-purpose.rst b/specs/juno/proposed/username-in-nova-list-for-admin-purpose.rst
new file mode 100644
index 0000000..447531b
--- /dev/null
+++ b/specs/juno/proposed/username-in-nova-list-for-admin-purpose.rst
@@ -0,0 +1,166 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================
+Add username in nova list response
+==================================
+
+https://blueprints.launchpad.net/nova/+spec/add-username-in-nova-list-response
+
+Currently nova list command does not show which user the particular
+VM belong too. In enterprise deployments, knowing which user booted
+the VM could be very useful for the cluster administrator.
+This will save admin a lot of manual steps before knowing the owner
+of the VM. In this blueprint, we plan to add an additional column
+'username' to 'nova list' output which will facilitate cluster
+maintenance for the administrator
+
+Problem description
+===================
+
+There are various scenarios in an enterprise cluster when an
+administrator needs to contact the owner/user of the VM,
+e.g. tracking down the owner of a noisy VM to ask him about
+the unexpected amount of traffic, or finding all VMs in
+SHUTOFF/ERROR state and asking the owners for permission to delete
+the VMs, etc. In the current implementation, nova list command does
+not show any information about the user that booted the VM.
+The administrator should have an easy way to get that information
+without manually looking at the database. Hence, it will be nice to
+have that in the output of 'nova list' command. In order to populate
+user information, novaclient needs to query keystone to get the user list
+each time it receives the request. Since it might be an expensive
+operation, it makes sense to keep it 'admin' only.
+
+Proposed change
+===============
+
+Following modifications to novaclient are expected
+
+  * When a user runs a command 'nova list --owner', novaclient creates
+    an object of keystoneclient with current auth_token and endpoint
+    as the auth_url.
+  * Using this keystoneclient object, novaclient queries users list
+    for the requested tenant from keystone and creates a users dict.
+  * While printing the response for 'nova list --owner', novaclient
+    checks whether this information is available. If it is, novaclient
+    shell displays it.
+  * If the context is non-admin, keystoneclient returns Forbidden
+    exception. Novaclient handles this exception and returns an
+    empty users dict.
+  * Seeing that users dict is empty, novaclient shell skips displaying
+    Username column.
+
+In order to achieve this, keystoneclient needs to be imported inside
+novaclient, which means a dependency on keystoneclient would be added
+in novaclient.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+* Additional call to keystone from novaclient
+
+    keystoneclient.users.list(tenant)
+
+    GET /v2.0/users
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+End user will see an additional column in the output of
+'nova list --owner' command, titled 'Username'
+
+Performance Impact
+------------------
+
+Minor performance impact will be expected from keystoneclient query.
+Everytime the admin runs this command, a new request will be sent to
+keystoneclient. This might add minor performance impact depending on
+the size of the output
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+   shraddha-pandhe
+Other contributors:
+   None
+
+Work Items
+----------
+
+* Import keystoneclient in novaclient
+* If --owner is present in the args, create a keystoneclient object
+  using the auth_key and the endpoint url
+* call keystoneclient.users.list
+* The call will fail if the user is non-admin. The exception will be
+  handled and empty dictionary will be returned to the caller
+* The call will succeed if the user is admin and a dictionary of
+  usernames and user-id's will be returned to the caller
+* For every user-id, novaclient will then fetch the username and display
+  it in the output
+
+Dependencies
+============
+
+* Keystoneclient would be a dependency for novaclient
+
+None
+
+Testing
+=======
+
+* The username information should be displayed in 'nova list --owner'
+  output it the user is admin
+* Make sure the username is returned only for admin role
+  and not for any other role.
+
+Documentation Impact
+====================
+
+Changes to be made to novaclient documentation to include the
+additional argument to 'nova list' command and parameter
+'username' in the response.
+
+References
+==========
+
+None
+
+
diff --git a/specs/juno/proposed/v2-api-detailed-quotas.rst b/specs/juno/proposed/v2-api-detailed-quotas.rst
new file mode 100644
index 0000000..787762c
--- /dev/null
+++ b/specs/juno/proposed/v2-api-detailed-quotas.rst
@@ -0,0 +1,208 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================================
+ Nova v2 API should be able to return detailed quotas
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/v2-api-detailed-quotas
+
+Compute V2 API does not return detailed information about quotas.
+There is no direct call to retrieve this information and there is no other
+way to get this information.
+
+Basically this is a documented part of the os-quota-sets API extension
+in V2 but isn't implemented in the V2 API. It's implemented in the V3
+API.
+This change is adding the detail support for the V2 API so
+this works:
+/v2/?{tenant_id}?/os-quota-sets/?{tenant_id}?/detail/?{user_id}?
+
+Problem description
+===================
+
+Right now v2 API documentation shows that it is able to return detailed
+quota information. This feature is only implemented in v3 API and there
+is a need to keep consistency with the documentation.
+
+Proposed change
+===============
+
+The idea is to implement this feature as described by the API
+
+Alternatives
+------------
+None
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+* There will be created a new extension os-quota-sets-detail that will
+  allow the call of the detail method in os-quota-sets. When loaded, this
+  extension will allow the detail method to be called and when not loaded
+  detail will not be allowed to be called.
+
+* Specification for the method in os-quota-sets
+
+  * The method returns detailed quota information
+
+  * GET method
+
+  * Normal http response code(s)
+
+    * 200
+
+  * Expected error http response code(s)
+
+    * 404 is the expected error code. This can be due to wrong url.
+
+  * /v2/?{tenant_id}?/os-quota-sets/?{tenant_id}?/detail/?{user_id}?
+
+  * Parameters which can be passed via the url
+
+    * user_id={user_id}
+
+  * Expected reponse json::
+
+        {
+          "quota_set": {
+            "cores": {
+              "in_use": 0,
+              "limit": 20,
+              "reserved": 0
+            },
+            "fixed_ips": {
+              "in_use": 0,
+              "limit": -1,
+              "reserved": 0
+            },
+            "floating_ips": {
+              "in_use": 0,
+              "limit": 10,
+              "reserved": 0
+            },
+            "injected_files": {
+              "in_use": 0,
+              "limit": 5,
+              "reserved": 0
+            },
+            "instances": {
+              "in_use": 0,
+              "limit": 10,
+              "reserved": 0
+            },
+            "key_pairs": {
+              "in_use": 0,
+              "limit": 100,
+              "reserved": 0
+            },
+            "metadata_items": {
+              "in_use": 0,
+              "limit": 128,
+              "reserved": 0
+            },
+            "ram": {
+              "in_use": 0,
+              "limit": 51200,
+              "reserved": 0
+            },
+            "security_groups": {
+              "in_use": 0,
+              "limit": 10,
+              "reserved": 0
+            },
+            "injected_file_content_bytes": {
+              "in_use": 0,
+              "limit": 10240,
+              "reserved": 0
+            },
+            "injected_file_path_bytes": {
+              "in_use": 0,
+              "limit": 255,
+              "reserved": 0
+            },
+            "security_group_rules": {
+              "in_use": 0,
+              "limit": 20,
+              "reserved": 0
+            }
+          }
+        }
+
+* The policy change needed is to include the call
+  "compute_extension:quotas:detail": "rule:admin_api" to the policy.json
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  tellesmvn
+
+Work Items
+----------
+
+* Implement the detail method that works just like show
+  but returns detailed quota.
+
+Dependencies
+============
+None
+
+Testing
+=======
+
+* Unit tests will be implemented under
+  nova/tests/api/openstack/compute/contrib/test_quotas.py based on the show
+  method.
+
+* Integration tests will be implemented under
+  nova/tests/integrated/test_api_samples.py based on the show method.
+
+* Integration tests will be added using the tempest test for show method
+  as a basis for this one under tempest/api/compute/test_quotas.py.
+
+Documentation Impact
+====================
+None since it's already documented, just not implemented.
+
+References
+==========
+
+Documentation:
+http://docs.openstack.org/api/openstack-compute/2/content/GET_os-quota-sets-v2_showQuotaDetailUser__v2__tenant_id__os-quota-sets__tenant_id__detail__user_id__ext-os-quota-sets.html
+
+Existing change:
+https://review.openstack.org/#/c/99443/
diff --git a/specs/juno/proposed/v3-api-neutron-network-support.rst b/specs/juno/proposed/v3-api-neutron-network-support.rst
new file mode 100644
index 0000000..b0df90a
--- /dev/null
+++ b/specs/juno/proposed/v3-api-neutron-network-support.rst
@@ -0,0 +1,220 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Neutron network support in V3 API
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/v3-api-remove-nova-network
+
+V3 API didn't want to proxy any API call for other openstack service. It's
+include the proxy of openstack network service - Neutron. Currently nova
+REST API accept network-id as parameter to create new instance. Then nova
+will proxy Neutron API call to create port for new instance. V3 API is
+expected to only accept port-id as parameter to create new instance. The
+port should be created by user through Neutron REST API directly.
+
+Problem description
+===================
+
+Currently some V3 API still proxy Neutron API call.
+
+* Currently accept network-id, fixed-address when associate instance with
+  network (include creating new instance with network, attach/detach port with
+  instance). Accepting those parameters means user expect to Nova proxy
+  neutron call to create port for instance.
+
+* The response of showing detail of instances include the detail information
+  of ports. Nova will proxy the neutron call to get the port detail
+  information.
+
+Proposed change
+===============
+
+* Add os-networks extension for neutron network support. In this extension,
+  it will provide server create extension point, server's response extending,
+  and attach/detach port.
+
+    * Only accept port-id when associate instance with network. User should be
+      responsible for creating port from neutron api.
+
+    * Only show the list of port-id when show the detail of server. If user
+      want to know the port detail info, user should call neutron api.
+
+* Disable create/delete port automaticlly when create/delete instance.
+  create/delete the port.
+
+This proposing only provide a REST API without any proxy information. Because
+v2(.1) won't be depreciated for a while, the neutron proxy code can't be
+removed for now. So propose to pass a parameter down to the code to
+distinguish different behavior.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+* Create new server with the list of port-ids
+
+  POST '/v3/servers'
+
+  * Example::
+
+      {"server": {
+          ...
+          "os-networks:ports": [{"id": "354e2be8-2591-4337-86e3-1a6066c90b82"},
+                                {"id": "b9c07cd8-e9c6-4179-8ff1-dfd9130f2741"}]
+          }
+      }
+
+  * JSONSchema::
+
+      'os-networks:ports': {
+          'type': 'array',
+          'items': {
+              'type': 'object',
+              'properties': {
+                  'id': {'type': 'string', 'format': 'uuid'},
+              },
+              'additionalProperties': False,
+          }
+      }
+
+
+* Attach/detach port with server
+
+  POST 'v3/servers/[server id]/action'
+
+  * Example::
+
+      {'attach_port': {'id': '354e2be8-2591-4337-86e3-1a6066c90b82'}}
+      {'detach_port': {'id': '354e2be8-2591-4337-86e3-1a6066c90b82'}}
+
+  * JSONSchema::
+
+      {
+          'type': 'object',
+          'properties': {
+              'attach_port': {
+                  'type': 'object',
+                  'properties': {
+                      'id': {'type': 'string', 'format': 'uuid'}
+                  },
+                  'required': ['id'],
+                  'additionalProperties': False,
+              }
+          },
+          'required': ['attach_port'],
+          'additionalProperties': False,
+      }
+
+     {
+         'type': 'object',
+         'properties': {
+             'detach_port': {
+                 'type': 'object',
+                 'properties': {
+                     'id': {'type': 'string', 'format': 'uuid'}
+                 },
+                 'required': ['id'],
+                 'additionalProperties': False,
+             }
+         },
+         'required': ['detach_port'],
+         'additionalProperties': False,
+     }
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+After this change, user should create port from neutron directly, not
+expect nova create port from specified network. When delete server,
+the port that assocaited this server won't be deleted by nova anymore.
+
+This is also effect python-novaclient. It need update python-novaclient
+to support this.
+
+Performance Impact
+------------------
+
+Nova didn't proxy neutron call to create port for servers. This will
+speed up the server creation.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+If nova didn't proxy any neutron, nova also need disable create/delete port
+automaticlly, the port should be created/deleted by user through neutron api.
+But for now nova also need support nova-network. So for distinguish different
+behavior new v3 neutron support behavior and nova-network support, it will
+pass a parameter 'legacy_network' down to the code. When
+'legacy_network=True', the nova code will try to create/delete port
+automaticlly that follow nova-network behavior. When 'legacy_network=False',
+the nova code will skip the code for create/delete port.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Alex Xu <xuhj@linux.vnet.ibm.com>
+
+Work Items
+----------
+
+* Disable auto allocate port when create server
+* Disable auto deallocate port when delete server
+* Create server with port_id list
+* Enable pass port_id for multiple create
+* Create server with port_id list
+* Extend server response with port_id list
+* Attach/detach port with port_id
+* Delete old network stuff from v3 API
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Add unittest and tempest test for new neutron network support
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/validate-targethost-live-migration.rst b/specs/juno/proposed/validate-targethost-live-migration.rst
new file mode 100644
index 0000000..2f53cad
--- /dev/null
+++ b/specs/juno/proposed/validate-targethost-live-migration.rst
@@ -0,0 +1,157 @@
+
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================
+Validate target host for live migration
+===========================================
+
+https://blueprints.launchpad.net/nova/+spec/validate-targethost-live-migration
+
+The aim of this feature is to let nova scheduler evaluate the scheduler hints
+when live migrate a VM with target host.
+
+
+Problem description
+===================
+
+At the moment, for live migration with target host, the current implementation
+is that once target host was specified for live migration, the conductor only
+validates the host by nova compute.
+
+For some cases, there is also a need to validate the host in nova scheduler,
+as there might be some requirement that admin wants to validate the target host
+for live migration with scheduler hints and filters.
+
+In the current implementation the user can specify scheduler hints related to
+the placement of the instance. Those scheduler hints are used when the
+instance run for the first time and they are ignored afterwards. In order to
+achieve the validation of the target host we need to persist scheduler hints
+into the database which makes it possible to verify that the target host
+passes scheduler filters/hints during live migration.
+
+There are some cases when this verification during live migration will lead
+the system locked and the user will not be able to migrate an instance, so a
+new option should be added in order to force the migration without verifying
+the target host by nova-scheduler.
+
+
+Proposed change
+===============
+
+Modify the current live migration work flow to let the scheduler verify the
+scheduler hints and filters when live migration with a target host.
+
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+None
+
+
+REST API impact
+---------------
+
+* Enable ignores scheduler validation for both V2/V3 API.
+    * Request body::
+
+        {
+            "os-migrateLive": {
+                "host": "0443e9a1254044d8b99f35eace132080",
+                "block_migration": false,
+                "disk_over_commit": false,
+                "scheduler_validation": true
+            }
+        }
+
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+
+python-novaclient will be modified to have scheduler_validation option when
+live migrate a VM instance with target host.
+
+The user can enable scheduler validation by:
+nova live-migration server --scheduler_validation
+
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    jay-lau-513 (liugya@cn.ibm.com)
+
+Other contributors:
+    Yassine Lamgarchal (yassine.lamgarchal@enovance.com)
+
+
+Work Items
+----------
+
+* Add validation logic in live migration task.
+* Add validation logic in nova scheduler for scheduler hints/filters.
+* Add logic to make validation optional for both V2/V3 API.
+
+
+Dependencies
+============
+
+For a complete use-case the following bp will be required
+https://blueprints.launchpad.net/nova/+spec/persist-scheduler-hints,
+since we can retrieve the original scheduler hints from that a particular
+instance and let the scheduler validate the scheduler hints for live
+migration with target host.
+
+
+Testing
+=======
+
+* Add unit test for validation logic related to live migration task.
+* Add unit test for validation logic related to nova scheduler.
+* Add unit test for validation logic related to V2/V3 API.
+
+
+Documentation Impact
+====================
+
+* Api Docs to reflect the new scheduler_validation option for live migration.
+  If not present in the body the new feature will be ignored.
+* Client docs ( due to the new added live migration option).
+* Admin User Guide on live migration topic.
+
+
+References
+==========
+None
diff --git a/specs/juno/proposed/validate-tenant-user-with-keystone.rst b/specs/juno/proposed/validate-tenant-user-with-keystone.rst
new file mode 100644
index 0000000..35a7312
--- /dev/null
+++ b/specs/juno/proposed/validate-tenant-user-with-keystone.rst
@@ -0,0 +1,215 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Validate tenant and user with Keystone
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/validate-tenant-user-with-keystone
+
+Today there is no functionality to validate the tenant or user that is
+consumed by nova.  One reason for the lack of such functionality is
+performance, where validating to external services can cause poor performance.
+However, such functionality is needed in cases where the user passes in the
+tenant/user ID or name (i.e. quota management), so that the correct quota is
+set.  Such functionality is also needed where the user wants to grant a
+tenant access to a flavor.
+
+This blueprint is only intended for cases where the API calls are done on a
+very infrequent basis.  More specifically, it is only meant to be used for
+validating tenant/user ID in quota management (i.e. quota-defaults,
+quota-delete, quota-show, quota-update) and in flavor management (i.e.
+flavor-access-add, flavor-access-list, flavor-access-remove).  A separate
+blueprint is required for any functionality that is outside of quota and
+flavor management.
+
+
+Problem description
+===================
+
+The quota management feature of nova requires a tenant and user ID to be
+specified as part of the CLI.  This affects nova quota-show, quota-update, and
+quota-delete.  When a tenant or user is specified in one of the quota actions
+above, they are not checked against keystone to validate their IDs.  A user
+could specify a tenant or user name instead of the tenant or user ID,
+e.g. nova quota-update --instances 9 demo.  Since no checks are done, an entry
+is created in nova's project_user_quotas table where the project_id or
+user_id is set to the tenant or user name.  This causes invalid quotas to be
+set and returned if the tenant or user ID does not match what is in the
+project_user_quotas table.  It also affects flavor management.  More
+specifically, nova flavor-access-add, flavor-access-list, and
+flavor-access-remove.  When a tenant is specified in one of the flavor actions
+above, they are not checked against keystone to validate their IDs.
+
+
+Proposed change
+===============
+
+The proposed solution is to expose a keystone client in nova, similar to the
+cinder client that exists today in nova/volume/cinder.py.  Methods to get the
+tenant or user by their ID would be implemented.  When a tenant/user ID is
+be specified, it would be queried against the keystone client and validated.
+
+Alternatives
+------------
+
+The existing behavior, where the tenant/user ID is not validated, could be
+left as-is.  It would be up to the user to figure out the appropriate
+tenant/user ID or name from "keystone tenant-list" or "keystone user-list".
+However, this alternative presents user errors, where the user could
+mistakenly specify the tenant/user name instead of the tenant/user ID for
+nova quota-show, quota-update, or quota-delete.  If this occurs, the quota
+would not be set correctly.
+
+Data model impact
+-----------------
+It may be possible that there are entries in nova's project_user_quotas table,
+where the project_id or user_id is set to an invalid ID.  However, these
+entries do not hold any significant value, since no actual tenant is tied to
+them.  In such case, a --force option will be introduced to the quota-delete
+command.  This will allow invalid entries to be deleted by skipping over the
+keystone validation of the specified tenant/user ID.
+
+
+REST API impact
+---------------
+Previously, a POST and GET request using an invalid tenant or user ID would
+create an entry in nova's project_user_quotas table or return the quota
+value (if any) for the tenant or user.  With this proposal, if a keystone
+service account exists to validate the tenant/user ID, appropriate error
+messages will be returned from the POST and GET requests for an invalid tenant
+or user.  If a keystone service account does not exist, the functionality
+would continue to function as before and only a warning message would be logged
+(not returned via the POST or GET request).
+
+In the case where an invalid entry may exist in the database and the user wants
+to delete it, a new force option will be introduced to the DELETE request.
+By default, the tenant/user ID will be validated, unless the force option is
+specified.
+
+Security impact
+---------------
+To properly validate a tenant/user ID, a keystone service account with enough
+privileges to list users and projects needs to be created.  The account
+should have limited access to keystone and not any other services.  This can
+be done by creating the appropriate role-based rules in
+/etc/keystone/policy.json and granting the ability to list projects and
+users to a role.
+
+For example, an admin can create a user (e.g. validator) and add a role to it
+(e.g. validation).  Under /etc/keystone/policy.json, the admin would add these
+rules::
+
+  "identity:list_projects": "rule:admin_required or role:validation",
+  "identity:list_users": "rule:admin_required or role:validation",
+
+Under /etc/nova/nova.conf, the admin would add these credentials::
+
+  keystone_service_tenant_name = service
+  keystone_service_user = validator
+  keystone_service_password = keystone
+
+A keystone client would be instantiated using the credentials above, if they
+exists.  If a keystone service account does not exist, the operation should
+not be blocked.  Instead, a warning message should be logged stating that the
+keystone service account does not exist.
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+If a keystone service account exists to validate the tenant/user ID,
+appropriate error messages will be returned from the POST and GET requests for
+an invalid tenant or user.  These messages would be:
+
+* The specified tenant ID is not valid.
+
+* The specified user ID is not valid.
+
+Performance Impact
+------------------
+
+There will be minor impact to performance.  A connection to keystone is
+required to validate the tenant/user ID.  However, it would be a
+low-frequency operation because quotas/flavor access are not often changed.
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+To properly validate a tenant/user ID, a keystone service account needs to be
+created.  If a keystone service account does not exist, the quota and flavor
+operation should not be blocked.  Instead, a warning message should be
+logged stating that the keystone service account does not exist.  This will
+allow existing deployments to continue to work.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  thang-pham
+
+Work Items
+----------
+
+* Create a method to instantiate the keystone client.
+
+* Implement methods to get the tenant and user by a given ID.
+
+* Modify QuotaTemplate class in nova/api/openstack/compute/contrib/quotas.py
+  to validate the tenant and user ID, if any.
+
+* Modify FlavorActionController class in
+  nova/api/openstack/compute/contrib/flavor_access.py to validate the tenant
+  ID, if any.
+
+* Modify devstack to create the keystone service account, saving its
+  credentials in /etc/nova/nova.conf, and limiting its access in
+  /etc/keystone/policy.json.
+
+* Create tempest test cases and nova unit test cases to verify functionality.
+
+Dependencies
+============
+None
+
+
+Testing
+=======
+
+Tempest test cases, as well as nova unit test cases, will be created to verify
+this feature.  The following commands should be tested: nova quota-show,
+quota-update, and quota-delete.  More specifically, the --tenant and --user
+options need to be specified with the proper ID for positive test cases, and
+invalid IDs for negative test cases.  The following commands should also be
+tested: flavor-access-add, flavor-access-list, and flavor-access-remove.  The
+tenant_id needs to be specified with the proper ID for positive
+test cases, and invalid IDs for negative test cases.
+
+
+Documentation Impact
+====================
+None
+
+
+References
+==========
+
+* Proposed code change: https://review.openstack.org/#/c/91866/
+* Reported bugs:
+  https://bugs.launchpad.net/nova/+bug/1313935
+  https://bugs.launchpad.net/nova/+bug/1317515
+  https://bugs.launchpad.net/nova/+bug/1118066
+* Customizing authorization:
+  http://docs.openstack.org/trunk/openstack-ops/content/projects_users.html
diff --git a/specs/juno/proposed/vcpus-in-api.rst b/specs/juno/proposed/vcpus-in-api.rst
new file mode 100644
index 0000000..0f984c0
--- /dev/null
+++ b/specs/juno/proposed/vcpus-in-api.rst
@@ -0,0 +1,142 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================================
+correction vcpus in 'nova hypervisor-stats' and horizon
+=======================================================
+
+https://blueprints.launchpad.net/nova/+spec/vcpus-in-api
+
+vcpus returned from API should be reflect total vCPUs
+provided from hypervisor, but it returns number of physical
+CPUs that confuses users.
+
+Problem description
+===================
+
+Currently, 'vcpus' field in 'nova hypervisor-stats' and vcpus
+in horizon shows physical CPU number instead of vCPUs available
+in the system that should do reflect 'cpu_allocation_ratio'.
+
+$ grep "cpu_allocation_ratio" /etc/nova/nova.conf
+cpu_allocation_ratio=16.0
+
+$ cat /proc/cpuinfo | grep processor | wc -l
+8
+
+$ nova hypervisor-stats | grep "vcpus "
+| vcpus | 8 |
+
+It shows 8 for vcpus even though cpu_allocation_ratio is 16.0.
+It should be 128 in above case.
+
+It causes of confusion to the user who especially wants to know
+how many vCPUs available to use.  In Horizon, the graph becomes
+weird when vcpus_used becomes bigger than vcpus as vcpus returns
+physical CPU numbers, not physical cpu # * cpu_allocation_ratio.
+
+Proposed change
+===============
+
+We should correct API implementation to make it return vcpus
+based on the below calculation.
+
+(physical CPU number) X (cpu_allocation_ratio in the current filter)
+
+If there's no CoreFilter such as CoreFilter or AggregateCoreFilter,
+it will return the same number as physical CPU number.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+'nova hypervisor-stats' will return different value for 'vcpus' based
+on 'cpu_allocation_ratio' in the filter in hypervisor.
+
+If there's any custom script customer is using based on above, that will
+impact in the number, but as there's no way to get 'cpu_allocation_ratio'
+via REST API, custom script wouldn't get higher value than actually
+available vCPUs.
+
+Performance Impact
+------------------
+
+Minimal as only traverse filter list to get cpu_allocation_ratio
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <sungju-kwon>
+
+Other contributors:
+  <None>
+
+Work Items
+----------
+
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+Unit test should reflect vcpus, not physical cpus which is proposed
+in patch in references session.
+
+
+Documentation Impact
+====================
+
+None as detailed output is not described in doc for 'nova hypervisor-stats'.
+
+References
+==========
+
+* Patch : https://review.openstack.org/#/c/93168/
+* Bug 1: https://bugs.launchpad.net/nova/+bug/1202965
+* Bug 2: https://bugs.launchpad.net/nova/+bug/1326147
diff --git a/specs/juno/proposed/virt-image-transfer-layer.rst b/specs/juno/proposed/virt-image-transfer-layer.rst
new file mode 100644
index 0000000..e86f2a9
--- /dev/null
+++ b/specs/juno/proposed/virt-image-transfer-layer.rst
@@ -0,0 +1,218 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+Adding generic image transfer layer to nova.virt
+==================================================
+
+https://blueprints.launchpad.net/nova/+spec/virt-image-transfer-layer
+
+Take advantage of the Glance image multiple location feature, adding a layer
+to transparently dispatch image consuming request, e.g. download, upload, to
+relevant handler plug-ins which working for different storage backend with
+appropriate approach.
+
+Problem description
+===================
+
+Currently Nova driver is always consuming image from single
+backend storage which Glance API internal selected or exposed by direct-url
+mechanism. For XenAPI driver, it could use bittorrent approach additionally.
+This approach is imperfect:
+
+1. If one image backend storage failure, Nova doesn't know to consume image
+   from another locations.
+
+2. Nova is consuming Glance APIs to fetch and push images while spawning and
+   snapshotting virtual machines, it requires Nova to invoke the upload and
+   download Glance operations which means that the bits will go through the
+   network with HTTP request. But for different image backend storage, it will
+   be great if Nova be able to evaluate the backend model where images location
+   is communicated and using storage particular technology to handle images,
+   this would significantly reduce the data transfer (up-bound and down-bound)
+   overhead and increase Glance efficiency.
+
+3. Nova doesn't allow to appoint image backend storage selection order.
+   For example deployer might like try to consuming image from Ceph first then
+   to VMware storage for particular rack, compute nodes, availability zone.
+
+Proposed change
+===============
+
+Adding a generic image handler framework, it lives in the virt folder and can
+be consumed by every driver. The framework triggers a handler based on the
+location of a Glance image or not for generic handlers that do not require a
+location to be triggered. Each driver implements its own flavour of the
+interface (fetch, push, move, delete) with optimized storage accessing
+technology particularly which required by image handler framework.
+
+Each hypervisor driver can have several image handlers (see References), the
+framework will load relevant handlers according to configuration for different
+hypervisor, and those non-applicable handlers will be skipped automatically as
+well.
+
+The common code will be put into nova.virt.image, and any driver-specific code
+goes into the drivers' namespace.
+
+Fundamental implementation steps:
+
+1. To change Glance image service/api to expose get_locations()
+   as a public interface.
+
+2. Adding nova.virt.images.ImageHandler base class to the Nova common image
+   layer which has fetch, push, move and delete interfaces.
+
+3. Implementing nova.virt.images.DefaultImageHandler class which inherits
+   above base class. It uses download and upload approach (HTTP GET and POST)
+   to fetch and push image bits as Nova current default handling behaviour.
+
+4. To implement load_image_handlers() entry function, it will be called when
+   particular hypervisor driver initialization and loads deployer configured
+   image handlers plug-ins for different hypervisor separately.
+
+5. Implementing nova.virt.images.handle_image() function, evaluate the schema
+   where images location is communicated and using applicable handler to handle
+   that image.
+
+6. To change nova.virt.images.fetch_to_raw() function to leverage image
+   handler framework.
+
+7. Changing like nova.virt.libvirt.ImageCacheManager to leverage image handler
+   framework as well.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+There is no any data model impact for this idea/change. But I'd like to share
+my thoughts about the priority of image locations selection when Nova consumes
+it. Currently Glance already support this selection function of image location
+(see References), it allows deployer provided a prioritized location "type"
+list with preferred order (each of them is a store name), which equals the
+preference of backend storage. Now, Glance team are separating glance.store out
+from codebase as a dedicated sub-project, which will contains this fucntion.
+So in future patch, I'd like to leverage it in image handler framework level
+(be transparent for under layer handlers), and as result the location can be
+prioritized by operator as particular deployment needed. This can be used to
+support the case of #3 in "Problem description" section.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+The change of this blueprint will have no any impact, it doesn't need any input
+from end user, but the follow up image handler probably will do some.
+(see References)
+
+Performance Impact
+------------------
+
+There's no any impact but the performance optimization is the main value of
+follow up blueprints which based on this image handler framework.
+(see References)
+
+Other deployer impact
+---------------------
+
+Deployer has no any impact, but follow steps will be needed for follow
+up handler:
+
+- Enabling image handlers by Nova settings as deployer needed order.
+  This change will add:
+
+  1. A new configure option called 'image_handlers'.
+  2. A download handler which is the default one for Nova named 'download'.
+     It can be used as a fall-back for follow up handler (see References).
+
+  So deployer could configure Nova option as "image_handlers=download", and
+  when Nova has more available image handlers in future, deployer could add
+  them as needed to this option with required order.
+
+- Glance should be configured to have 'show_multiple_locations' and/or
+  'show_image_direct_url' set to True. If Glance isn't configured properly(s)
+  nova just fall-back to the way it works today which could be covered by
+  'download handler' default one.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Zhi Yan Liu (lzy-dev)
+
+Work Items
+----------
+
+The implementation is not complicated so one patch is enough (see References).
+
+Dependencies
+============
+
+- This blueprint requires use-glance-v2-api changes to be merged.
+  blueprint: https://blueprints.launchpad.net/nova/+spec/use-glance-v2-api
+
+- This blueprint needs standardize-nova-image changes to be merged
+  blueprint: https://blueprints.launchpad.net/nova/+spec/standardize-nova-image
+
+Testing
+=======
+
+The necessary tempest testing for the case is covered by use-glance-v2-api
+blueprint as a dependency which will be used by this feature like using Glance
+v2 api to obtain image locations, and "download" handler, the only image
+handler this blueprint implemented, will just keep current Nova existing
+behavior (using Glance v1 api or v2), so this BP doesn't need additional
+tempest case. The testing for other handers will be covered by other
+dedicated blueprint (see References).
+
+Documentation Impact
+====================
+
+- The new configuration variable 'image_handlers' in the 'DEFAULT' section
+  needs to be documented.
+
+- To document how to implement an image handler and plug it in Nova.
+
+References
+==========
+
+- The idea for Ceph storage:
+  https://blueprints.launchpad.net/nova/+spec/rbd-clone-image-handler
+
+- The idea for VMware storage:
+  https://blueprints.launchpad.net/nova/+spec/vmware-clone-image-handler
+
+- The change of this blueprint is reverted due to problems with glance v2 API:
+  Change-Id: Idce8d21ae37bfdbb28a2567120a83d1061061904
+  http://lists.openstack.org/pipermail/openstack-dev/2014-March/029831.html
+
+- The idea of image location selection Glance implemented:
+  https://blueprints.launchpad.net/glance/+spec/image-location-selection-strategy
\ No newline at end of file
diff --git a/specs/juno/proposed/virt-properties-object.rst b/specs/juno/proposed/virt-properties-object.rst
new file mode 100644
index 0000000..5e4dda8
--- /dev/null
+++ b/specs/juno/proposed/virt-properties-object.rst
@@ -0,0 +1,125 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================
+Virt Properties Objects Support
+===========================================
+
+https://blueprints.launchpad.net/nova/+spec/convert-image-meta-into-nova-object
+
+In an effort to standardize which image properties are available to the nova
+drivers, a NovaObject needs to be created. This object will take a dictionary
+of values provided by Glance and verify that the keys are one of the allowed
+ones listed here: https://wiki.openstack.org/wiki/VirtDriverImageProperties
+
+The new VirtProperties object will be consumed by the nova drivers. It will
+not be backed by a database.
+
+Problem description
+===================
+
+Each nova driver expects slightly different property names from glance image
+meta data.
+
+Proposed change
+===============
+
+Create a NovaObject that will contain a standard list of properties. The object
+will be contructed from glance image meta properties.  Each driver will be
+upgraded to use the new object instead of the properties dictionary.
+
+Alternatives
+------------
+
+This is the accepted direction of the project to solve this
+problem. However, alternatives would be:
+
+1. Don't solve the problem and continue using unversioned and completely
+   arbitrary dictionaries of properties.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+Moving to objects enhances the ability for deployers to incrementally
+roll out new code. It is, however, largely transparent for them.
+
+Developer impact
+----------------
+
+This is normal refactoring, so the impact is minimal. In general,
+objects-based code is easier to work with, so long-term it is a win
+for the developers.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  dkliban
+
+Work Items
+----------
+
+* Create VirtProperties object
+* Upgrade libvirt driver
+* Upgrade xenapi driver
+* Upgrade vmwareapi driver
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+In general, unit tests require minimal change when this happens,
+depending on how the tests are structured. Ideally, they are already
+mocking out database calls, which means the change to objects is a
+transparent one. In reality, this usually means minor tweaking to the
+tests to return whole data models, etc.
+
+Documentation Impact
+====================
+
+Need to publish the new list of standard virt property names.
+
+References
+==========
+
+* https://blueprints.launchpad.net/nova/+spec/convert-image-meta-into-nova-object
diff --git a/specs/juno/proposed/virtio-scsi-settings.rst b/specs/juno/proposed/virtio-scsi-settings.rst
new file mode 100644
index 0000000..51a00a7
--- /dev/null
+++ b/specs/juno/proposed/virtio-scsi-settings.rst
@@ -0,0 +1,195 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Add virtio-scsi options
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/virtio-scsi-settings
+
+This proposal is for providing additional options with Virtio-SCSI via the
+LibvirtConfigGuestController so Nova can have more fine tunable settings for
+Virtio-SCSI.
+
+* `queues` controls the number of virtio queues that are available to an
+  individual virtio-scsi PCI controller instance.  The usage scenario for
+  queues is increased parallelism for adding + removing virtio vring
+  elements, that can result in higher performance when the underlying storage
+  is not limited by how quickly a single virtio queue can process requests.
+  When the backend storage is faster than what a single virtio queues can
+  process, then using a queues > 1 makes sense.
+
+* `cmd_per_lun` is exposed through the Virtio-SCSI configuration registers to
+  the guest, which determines the number of outstanding I/Os per LUN that the
+  guest SCSI subsystem is allowed to keep in flight at a single time. The usage
+  scenario for cmd_per_lun is to allow an individual virtio-scsi PCI controller
+  instance to limit the maximum number of I/Os in flight at a given time. By
+  default, virtio-scsi uses cmd_per_lun = 1024. Reducing cmd_per_lun can be
+  useful when an individual controller (or guest) needs to be limited in the
+  number of outstanding I/O it can generate for QoS purposes.
+
+* `max_sectors` is exposed through the Virtio-SCSI configuration registers to
+  the guest, which determines the number of sectors per I/O (eg: total size per
+  I/O) that the guest SCSI subsystem is allowed to send for a single I/O
+  request. The usage scenario for max_sectors is to allow an individual
+  virtio-scsi PCI controller instance to limit the maximum size of individual
+  I/O requests being generated by the guest. By default, virtio-scsi uses
+  max_sectors = 0xFFFF. Reducing max_sectors can be useful when dealing with
+  storage devices that have difficulty keeping up with large I/O requests, but
+  breaking them up into smaller max_sectors * sector_size chunks.
+
+* Building on top of Virtio-SCSI with vHost-SCSI for better performance with
+  attached devices to guests. vHost can also take advantage of the options
+  listed above and is supported in both the Linux kernel and qemu. vHost is
+  a target to a a LUN. The block device that is under the LUN is exported
+  locally on the compute node. Since vHost is a target created locally,
+  migration must be handled outside of QEMU with any number of solutions that
+  allow you expose a block device to another host (e.g. ISCSI), and create
+  a new vHost endpoint at the new host.
+
+  Along with the performance benefits, vhost-scsi is the only way to expose
+  guest T10 protection information (DIF) to the host. This is because there is
+  currently no user space interface to attach T10 PI to a read/write i/o
+  request.  This means that any future virtio-scsi data plane effort will not
+  be able to support end-to-end data protection until such a user space
+  interface is merged into the upstream kernel. vhost-scsi already supports T10
+  PI starting with the 3.16 kernel. [1][2][3]
+
+* `wwpn` is the vHost endpoint. When vHost is used, wwpn will be a required
+  attribute to be set in the LibvirtConfigGuestController. An example of an
+  endpoint looks like `naa.60014050a13df4f2`.
+
+
+Problem description
+===================
+
+Virtio-SCSI comes with additional configuration options that would allow users
+to fine tune how a block device is used by a virtual machine. The options
+listed above: queues, cmd_per_lun and max_sectors are features that are
+available in Virtio-SCSI today. The ability to configure guest controllers was
+added in the Juno release [4]. The additional settings available in QEMU and
+exposed by libvirt [5].
+
+In addition to Virtio-SCSI, vHost-SCSI takes advantage of the same options, but
+provides even greater performance [6]. The vHost driver was added in the 3.6
+Linux kernel. It provides virtually bare-metal local storage performance for
+KVM guests. The vHost driver is not a self-contained virtio device, as it
+depends on userspace to handle the control plane while the data plane is done
+in kernel. This means the data plane does not go through emulations, which can
+slow down I/O performance.
+
+
+Proposed change
+===============
+
+These settings will be set by the administrator only via Cinder's volume types.
+Cinder's vHost connector [7] will make attempts of looking for these settings
+specified to send in addition to the target_wwpn to Nova.
+
+Since virtio-scsi only looks for these settings from the SCSI host [8], each
+volume attach Nova does will be creating a new controller with these settings
+on the instance to attach the device with.
+
+
+Alternatives
+------------
+
+n/a
+
+Data model impact
+-----------------
+
+n/a
+
+REST API impact
+---------------
+
+n/a
+
+Security impact
+---------------
+
+n/a
+
+Notifications impact
+--------------------
+
+n/a
+
+Other end user impact
+---------------------
+
+n/a
+
+Performance Impact
+------------------
+
+vhost-scsi virtually provides bare-metal local storage performance for KVM
+guests [6]. The vHost driver is not a self-contained virtio device, as it
+depends on userspace to handle the control plane while the data plane is done
+in kernel. This means the data plane does not go through emulations, which can
+slow down I/O performance.
+
+Other deployer impact
+---------------------
+
+These features are completely optional for the administrator to enable. If the
+administrator chooses to enable these features for certain storage backends,
+they can be set in volume types that are mapped to certain Cinder volume
+drivers that are making use of the vHost connector provided by Cinder. Each
+volume that Nova requests Cinder for connection data, Cinder will look to see
+if these settings are set by the administrator, and provide them with the
+connection data to Nova to set in the guest controller.
+
+Developer impact
+----------------
+
+n/a
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    thingee
+
+Work Items
+----------
+
+* Libvirt patch merged to expose `queues`, `cmd_per_lun` and `max_sectors`.
+* vHost connector in Cinder.
+* New libvirt volume class to accept connection data from Cinder.
+
+Dependencies
+============
+
+n/a
+
+Testing
+=======
+
+Appropriate unit tests will be provided. Gate will likely not have the
+appropriate libvirt/qemu version to do testing in tempest.
+
+Documentation Impact
+====================
+
+Provide similar information on these settings in either Nova, Cinder
+documentation. It'll likely make more sense to add it in the operator's guide.
+
+References
+==========
+
+[1] - https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/drivers/scsi/virtio_scsi.c?id=e6dc783a38ec0f2a5a91edda3f76195dffb17a16
+[2] - https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/drivers/vhost/scsi.c?id=e31885dd901e80d5bd528c1cbedde07ebbf051b2
+[3] - https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/drivers/vhost/scsi.c?id=95e7c4341b8e28dae5204378087c1e2a115abc82
+[4] - https://review.openstack.org/#/c/84494/
+[5] - http://libvirt.org/git/?p=libvirt.git;a=commit;h=d950494129513558a303387e26a2bab057012c5e
+[6] - http://linux-iscsi.org/wiki/VHost#Linux_performance
+[7] - https://github.com/openstack/cinder-specs/blob/master/specs/juno/vhost-support.rst
+[8] - https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/drivers/scsi/virtio_scsi.c#n884
diff --git a/specs/juno/proposed/vm-cpu-pinning-support.rst b/specs/juno/proposed/vm-cpu-pinning-support.rst
new file mode 100644
index 0000000..b0a6f46
--- /dev/null
+++ b/specs/juno/proposed/vm-cpu-pinning-support.rst
@@ -0,0 +1,164 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================
+Dynamically pin vCPUs to some pCPUs per VM
+==============================================
+
+https://blueprints.launchpad.net/nova/+spec/vm-cpu-pinning-support
+
+Sometimes for some critical instances, guaranteeing their quality of service
+(QoS) is required. When they are running on a host, there might exist noisy
+neighbors. When we detect that occurs by monitoring, we hope to adopt some
+policies to make sure those critical instances still can run well without or
+with impact as little as possible. One of the options is to dynamically pin
+vCPUs of those critical instances to those pCPUs on the same host which are
+reserved at the beginning.
+
+Therefore, we hope to add the capability of CPU pinning support per VM.
+
+Problem description
+===================
+
+For critical instances which critical missions are running on and more money
+is paid for, the quality of service (QoS) becomes important. It is required
+that cloud providers should treat them differently and guarantee their
+quality.
+
+However, when they are running on a host, there might exist noisy neighbors
+from time to time. As long as the noisy neighbors are detected by monitoring,
+it is expected to adopt some policies to make sure those critical instances
+still can run very well without or with impact as little as possible, such as
+resizing to add more vCPUs to the instances (i.e. vcpu hotplug), migrating the
+instances to some other hosts with more resources, or migrating the noisy
+neighbors to some other hosts, and so on.
+
+However, there is also another alternative to reserve some of the resources
+on the host (say pCPUs) into the resource pool in advance and allow to pin
+vCPUs of the instances to those pCPUs.
+
+Later on, if things get better, the feature also allows to release those
+pCPUs into the resource pool by pinning vCPUs of the instances back to those
+which are specified in CONF.vcpu_pin_set.
+
+Proposed change
+===============
+
+The resource reservation is beyond the scope of the blueprint. We suppose some
+pCPUs have been reserved in other ways, for example, to specify vcpu_pin_set
+in the config so those pCPUs which are not in vcpu_pin_set are reserved, or
+to reserve resource by Climate.
+
+A function pin_guest_vcpu() is going to be implemented to change the runtime
+configuration of an active domain in nova/virt/libvirt/driver.py.
+Its parameters include domain, and cpuset to indicate that all vCPUs on the
+domain need to be pinned to pCPUs specified by the new cpuset. In most cases,
+the new cpuset is the cpu set reserved.
+
+If cpuset is None, the function will pin the vCPUs of the guest back to those
+pCPUs specified in CONF.vcpu_pin_set.
+
+Again, we will add a Nova API to expose the capability and change novaclient.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+A new API for server admin action is expected to add:
+POST /v3/servers/{server_id}/action/{server_id}/action
+
+The JSON request looks like::
+
+    {
+        "pin_guest_vcpu": "0,1",
+    }
+
+where pCPU #0 and #1 have been reserved already.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+python-novaclient will also implement a sub command to allow clients to pin
+vCPUs on a domain to those reserved pCPUs.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+For deployers, they can take advantage of CONF.vcpu_pin_set to reserve pCPUs
+if they want. And this change proposed in this blueprint won't take immediate
+effect after it's merged.
+
+Developer impact
+----------------
+
+The proposed change is to modify libvirt driver, for other drivers, they need
+to call corresponding APIs to ask hypervisors to pin vCPUs. By default, the
+drivers would keep it as an abstract function until it is implemented,
+in order not to cause any exception in upper level Nova API and novaclient.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  yongli-he
+
+Other contributors:
+  shane-wang
+
+Work Items
+----------
+
+The virtual driver can be broken into:
+1) virt driver change
+2) Nova API change
+3) novaclient change
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+No specific tests except unit tests
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/vmware-clone-image-handler.rst b/specs/juno/proposed/vmware-clone-image-handler.rst
new file mode 100644
index 0000000..41f8822
--- /dev/null
+++ b/specs/juno/proposed/vmware-clone-image-handler.rst
@@ -0,0 +1,231 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+Storage: Cloning for VMware Datastore backed disks
+==================================================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-clone-image-handler
+
+Take advantage of the Glance VMware storage backend merged in Icehouse
+to avoid fetching and pushing the image through the network with the Nova
+VMware driver.
+
+Problem description
+===================
+
+Currently, the VMware driver is consuming Glance APIs to fetch and push images
+while spawning and snapshotting virtual machines.
+It requires Nova to invoke the upload and download Glance operations which means
+that the bits will go through the network unless they are already cached.
+
+Proposed change
+===============
+
+In Icehouse, the VMware backend storage was introduced in Glance: it gives
+the ability to store images in the VMware datastores.
+
+For a spawn operation, the Nova VMware driver should now be able to copy
+directly the image in the datastore using the vCenter primitive operation
+CopyDatastoreFile.
+
+For a snapshot operation, it should be able to copy the snapshot to the Glance
+folder, update the location and make the image ready.
+
+This blueprint relies on the blueprint image-multiple-location (see References)
+which provides the image handling logic. The image handler framework is generic:
+it lives in the virt folder and can be consumed by every driver.
+The framework triggers a handler based on the location of a Glance image or not
+for generic handlers that do not require a location scheme to be triggered.
+
+Each driver implements its own flavor of the interface
+(fetch, push, move, delete) provided in the image handler framework.
+Each driver can have several image handlers: in the case of VMware,
+we will have one generic "download" which provides the same functionality as it
+is today (download/upload the image to Glance) and one "copy" that is using the
+location provided by the Glance V2 API and will use this location to
+locate the image on the datastore and use it.
+
+To summarize, this blueprint gives the ability to copy the image in the
+datastore directly instead of upload/downloading the bits by leveraging
+the image handler framework which itself leverages the Glance image location.
+
+Alternatives
+------------
+
+- Alternative for RBD:
+  https://blueprints.launchpad.net/nova/+spec/rbd-clone-image-handler
+- Upload an image to Glance backed by Cinder (this requires a change to
+  the Glance Cinder driver to enable upload operation, which requires
+  multi-attach of volumes in Cinder and make it available through
+  the Brick library). This approach is currently discussed by several people but
+  it doesn't exactly fit the use case of this blueprint: here we want to
+  consume images stored in the Glance datastore backend without using Cinder
+  (and provide the optimization if possible).
+
+The approach adopted in this blueprint doesn't break any existing feature.
+
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+Spawn:
+If Glance datastore backend is used and Glance API V2 is consumed by Nova,
+the optimization will happen if possible without any input from the user.
+
+Snapshot:
+Nova needs to know where to put the snaphot which means that it needs
+to know where Glance expects the images to be located. If the Nova driver fails
+to put the image to the folder for some reason, the snapshot will be uploaded
+to Glance (and consequently any kind of storage: swift, s3, etc. can be used)
+
+In both cases (spawn and snapshot):
+To get the copy optimization, the datastore containing the image should be
+mounted to one of the ESX hosts of the cluster where the VM is
+spawned/snapshotted.
+If no single ESX host in the cluster has concurrent access to the snapshotted
+image and the glance datastore, vCenter will trigger a host-to-host network
+copy which will still be considerably more efficient the regular way involving
+a data path through glance service, vCenter and an ESX host.
+In case of a deployment contains several regions: as long as Glance is
+accessible from both regions, the image can be downloaded/uploaded the regular
+way.
+
+Performance Impact
+------------------
+
+The performance optimization is the main value of this blueprint.
+Spawn:
+- With the download image handler (same speed as it is today):
+first spawn has to fetch the image, next spawn for the same image should use
+the cache if the image is cached.
+- With the copy image handler (much faster than what is proposed today):
+first spawn copies the bits to the cache, next spawn for the same image
+uses the cache if the image is cached.
+
+Snapshot:
+- With the download image handler (same speed as it is today):
+upload the snapshot to Glance
+- With the copy image handler (much faster than what is proposed today):
+snapshot copied to the Glance for a new image
+
+Other deployer impact
+---------------------
+
+- The Glance folder needs to be configured: 'store_image_dir' in the [vmware]
+  section which contains the images in Glance. The value of this setting in
+  Nova should match the value set in Glance 'vmware_store_image_dir':
+  https://github.com/openstack/glance/blob/master/etc/glance-api.conf#L512
+- Glance should be configured to have 'show_image_direct_url' set to True.
+- The image handler framework relies on the Glance location scheme of the image.
+  If the location is different than vsphere://, the copy handler
+  is not going to be triggered. Consequently, existing images with scheme
+  file://, swift:// etc. will go through the download image handler.
+- We provide "copy, download" as default order for the image handler.
+  This means that if the location is vsphere://, the copy will be preferred but
+  download can be used as a fallback.
+
+This optimization is optional. It will apply only if the deployment is using
+the Glance datastore backend and the admin wants to take advantage of it.
+The general idea/behavior is download by default or copy if possible (possible
+means location is vpshere:// and the folder in the config is specified).
+
+Everything will work correctly even if the configuration provided is incorrect
+or if nothing is provided: no optimization (that's it).
+
+Also, the Glance datastore containing the images (preferably NFS) should be
+mounted to one of the ESX host of the cluster otherwise the copy might
+not be as efficient as it should be.
+The choice for NFS is due to the fact that VMFS has some limitation in the
+number of hosts that can attach it (NFS doesn't have this limitation).
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    Arnaud Legendre (arnaud)
+
+Work Items
+----------
+
+This blueprint is divided in three parts:
+
+1. Integrate with the image handler framework. This means
+   that we should provide a default download image handler.
+
+2. Provide a copy image handler which provides the performance
+   optimization for spawn discussed above.
+
+3. Add the required code to keep the snapshot in the array.
+
+Dependencies
+============
+
+This change requires the Image Handler framework to be merged
+(review: https://review.openstack.org/#/c/33409/ bp:
+https://blueprints.launchpad.net/nova/+spec/image-multiple-location).
+
+Testing
+=======
+
+Current tests should work. If the copy fails, the image handler will
+fall back to the download.
+Spawning a VM using the copy both for spawn and snapshot should work
+but faster.
+Minesweeper will be able to run in this mode. We actually plan to make it
+work in both modes (Juno roadmap).
+
+Documentation Impact
+====================
+
+The new configuration variable 'store_image_dir' in the [vmware] section
+needs to be documentated. This variable should be the same as
+'vmware_store_image_dir' in the Glance configuration.
+
+Also, the Glance datastore containing the images (preferably NFS) should be
+mounted to each ESX host otherwise the copy might not be as efficient as it
+should be: see Other Deployer section for more details.
+
+References
+==========
+
+Same idea for RBD:
+https://blueprints.launchpad.net/nova/+spec/rbd-clone-image-handler
+
+Initial Blueprint containing everything (image handler framework + consumers):
+https://blueprints.launchpad.net/nova/+spec/image-multiple-location
+
+Glance Datastore:
+http://git.openstack.org/cgit/openstack/glance/tree/glance/store/\
+vmware_datastore.py
diff --git a/specs/juno/proposed/vmware-encrypt-vcenter-passwords-proposal-1.rst b/specs/juno/proposed/vmware-encrypt-vcenter-passwords-proposal-1.rst
new file mode 100644
index 0000000..f304cc4
--- /dev/null
+++ b/specs/juno/proposed/vmware-encrypt-vcenter-passwords-proposal-1.rst
@@ -0,0 +1,179 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================================
+VMware: avoid vCenter plain text passwords in nova.conf
+=======================================================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-encrypt-vcenter-passwords
+
+This blueprint proposes an alternative way of storing vCenter user names and 
+passwords for OpenStack administrators that are leery of keeping them within
+nova.conf as plain text.
+
+
+Problem description
+===================
+
+One of the primary concerns to customers using OpenStack is the use of plain
+text passwords in conf files. This blueprint's goal is scoped only to the
+concern of the VMware vCenter/ESX passwords found in nova.conf. When using the
+vCenter driver, a usernames and passwords are stored in plain text inside the
+nova.conf file in order to establish a connection to the remote hypervisor.
+
+The goal is to protect access to such passwords. This can be done in a number
+of ways, but the below solution outlines a technique using existing OpenStack
+infrastructure. 
+
+
+Proposed change
+===============
+
+The proposed solution is to give a user the capability to put the vCenter user
+and password in the keystone credential store instead of nova.conf. As of
+version 3, keystone includes a credential store.  The credential store database
+access is secured through use of the nova user and password.
+
+The vmware driver would first check the 'vmware' section of the nova.conf for
+the host_ip, host_username, and host_password. Only in the case where
+host_username and/or host_password are not found in nova.conf, does the driver
+contact keystone to search for an appropriate credential.
+
+The credential store in keystone does not have a strict schema. The most common
+type of credential used there is 'ec2'. This blueprint introduces a new type of
+string 'vmware'. The credential blob is encoded with three fields host_ip,
+host_username, and host_password, matching those found in nova.conf. The
+host_ip is required in order to match the specific host being used by this
+nova compute instance.
+
+
+Alternatives
+------------
+
+One alternative to this blueprint is to encrypt the password string contained
+within nova.conf.  When reviewing this proposal with others within the
+community, the consensus was that this was no more secure than plain text.
+That's because any user who is able to get access to the nova.conf, also would
+have the ability to brute force decrypt the password stored within it.
+
+Another alternative suggested is to use Cloudkeep (a.k.a Barbican).  Cloudkeep
+would be a more general solution that could apply to any private credential
+storage.  But the drawback to a customer might be the introduction of another
+service that needs to be maintained.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+This solution does not introduce a security impact.  Instead it's meant to
+alleviate the perception of one to a customer.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+A potential performance impact is present since the proposed solution would
+involve issuing requests to the keystone service.  Therefore it's important
+to utilize caching of the credentials that are returned so as to minimize
+network traffic.
+
+Other deployer impact
+---------------------
+
+In order to take advantage of this feature, the credential must be prepopulated
+in the keystone credential store prior to starting the nova compute instance.
+In order to do this, a user would need to use curl, REST client, or some other
+script to create the credential in keystone. The keystone-client does not
+support creation of credentials (mostly because of the lack of v3 support).
+The future openstack-client does allow creation of credentials, but only of
+type 'ec2' and 'cert'. In the future, hopefully the openstack-client could be
+extended to allow an type as the API does.
+
+Usage
+
+* Create credential in keystone with type='vmware', host_ip matching that in
+  nova.conf, and user_id of the nova user.
+* Edit /etc/nova/nova.conf and remove host_username and host_password from the
+  vmware section.
+* Restart that nova compute instance. Check for errors.
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  ericwb
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* Create a set of convenience functions for looking up a vCenter credential
+  based on a given host name or IP address.
+* Modify VMware driver to use convenience functions.
+* Modify openstack-client to allow creation of vmware type credentials (only
+  ec2 supported today). 
+
+
+Dependencies
+============
+
+* Currently Nova is already dependent on the use of the keystone-client for
+  middleware.  This proposal would also add a dependency on the use of keystone
+  v3 in order to access its credential store.  To date, Nova has not yet had
+  any keystone v3 dependency.  
+
+
+Testing
+=======
+
+Addition of tempest tests is probably not necessary.  Tempest is currently
+driver agnostic and this change is VMware specific.
+
+
+Documentation Impact
+====================
+
+The documentation of the use of the Nova VMware driver would need to be updated
+to state that use of the host_username and host_password could be optional
+instead of required in nova.conf.  The docs would need to describe how someone
+can populate the keystone credential store with this user and password to take
+advantage of this feature.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/proposed/vmware-encrypt-vcenter-passwords-proposal-2.rst b/specs/juno/proposed/vmware-encrypt-vcenter-passwords-proposal-2.rst
new file mode 100644
index 0000000..da578ed
--- /dev/null
+++ b/specs/juno/proposed/vmware-encrypt-vcenter-passwords-proposal-2.rst
@@ -0,0 +1,209 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================================
+VMware: avoid vCenter plain text passwords in nova.conf
+=======================================================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-encrypt-vcenter-passwords
+
+This blueprint proposes an alternative way of storing vCenter user names and
+passwords for OpenStack administrators that are leery of keeping them within
+nova.conf as plain text.
+
+
+Problem description
+===================
+
+One of the primary concerns to customers using OpenStack is the use of plain
+text passwords in conf files. This blueprint's goal is scoped only to the
+concern of the VMware vCenter/ESX passwords found in nova.conf. When using the
+vCenter driver, usernames and passwords are stored in plain text inside the
+nova.conf file in order to establish a connection to the remote hypervisor.
+
+The goal is to protect access to such passwords. This can be done in a number
+of ways, but the below solution outlines a technique using existing OpenStack
+infrastructure.
+
+
+Proposed change
+===============
+
+The proposed solution is to give a user the capability to put the vCenter user
+and password in the keystone credential store instead of nova.conf. As of
+version 3, keystone includes a credential store.  The credential store database
+access is secured through use of the nova user and password.
+
+The vmware driver would first check the 'vmware' section of the nova.conf for
+the host_ip, host_username, and host_password. Only in the case where
+host_username and/or host_password are not found in nova.conf, does the driver
+contact keystone to search for an appropriate credential.
+
+The credential store in keystone does not have a strict schema. The most common
+type of credential used there is 'ec2'. This blueprint introduces a new type of
+string 'nova'. The credential blob is encoded with three fields host_ip,
+host_username, and host_password, matching those found in nova.conf. The
+host_ip is required in order to match the specific host being used by a
+particular nova compute instance.
+
+Here is an example of the metadata for this credential type::
+
+    Id: 3cb25d0da8084f7d889548f8dd77df3b
+    User: nova (e29969d944314e15a5d02556b2298745)
+    Type: nova.127.0.0.1
+    Blob: {"vmware": {"host_ip": "10.0.0.5",
+                      "host_username": "root",
+                      "host_password": "password"}}
+
+Or a future use for XenServer might be as follows::
+
+    Id: 3cb25d0da8084f7d889548f8dd77df3b
+    User: nova (e29969d944314e15a5d02556b2298745)
+    Type: nova.127.0.0.1
+    Blob: {"xenserver": {"connection_url": "10.0.0.5",
+                         "connection_username": "root",
+                         "connection_password": "password"}}
+
+where the 'type' could be either 'nova' in cases where someone would wish to
+share the credential accross all nodes, or 'nova.<host>' which would be a
+specific credential for a given nova host.
+
+This scheme might be applicable to other services such as cinder which also
+has options to specify the host/ip, user, password.
+
+As of today, the only way to get a credential not of type 'ec2' into the
+keystone store is to write a script or use a REST client to insert it.  As
+part of this work item, the hope is to also get a change to the
+openstack-client to support non 'ec2' credential creation.
+
+Alternatives
+------------
+
+One alternative to this blueprint is to encrypt the password string contained
+within nova.conf.  When reviewing this proposal with others within the
+community, the consensus was that this was no more secure than plain text.
+That's because any user who is able to get access to the nova.conf, also would
+have the ability to brute force decrypt the password stored within it.
+
+Another alternative suggested is to use Cloudkeep (a.k.a Barbican).  Cloudkeep
+would be a more general solution that could apply to any private credential
+storage.  But the drawback to a customer might be the introduction of another
+service that needs to be maintained.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+This solution does not introduce a security impact.  Instead it's meant to
+alleviate the perception of one to a customer.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+A potential performance impact is present since the proposed solution would
+involve issuing requests to the keystone service.  Therefore it's important
+to utilize caching of the credentials that are returned so as to minimize
+network traffic.
+
+Other deployer impact
+---------------------
+
+In order to take advantage of this feature, the credential must be prepopulated
+in the keystone credential store prior to starting the nova compute instance.
+In order to do this, a user would need to use curl, REST client, or some other
+script to create the credential in keystone. The keystone-client does not
+support creation of credentials (mostly because of the lack of v3 support).
+The future openstack-client does allow creation of credentials, but only of
+type 'ec2' and 'cert'. In the future, hopefully the openstack-client could be
+extended to allow an type as the API does.
+
+Usage
+
+* Create credential in keystone with type='vmware', host_ip matching that in
+  nova.conf, and user_id of the nova user.
+* Edit /etc/nova/nova.conf and remove host_username and host_password from the
+  vmware section.
+* Restart that nova compute instance. Check for errors.
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  ericwb
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* Create a set of convenience functions for looking up a vCenter credential
+  based on a given host name or IP address.
+* Modify VMware driver to use convenience functions.
+* Modify openstack-client to allow creation of vmware type credentials (only
+  ec2 supported today).
+
+
+Dependencies
+============
+
+* Currently Nova is already dependent on the use of the keystone-client for
+  middleware.  This proposal would also add a dependency on the use of keystone
+  v3 in order to access its credential store.  To date, Nova has not yet had
+  any keystone v3 dependency.
+
+
+Testing
+=======
+
+Addition of tempest tests is probably not necessary.  Tempest is currently
+driver agnostic and this change is VMware specific.  The VMware CI could
+utilize this configuration, if preferred.
+
+
+Documentation Impact
+====================
+
+The documentation of the use of the Nova VMware driver would need to be updated
+to state that use of the host_username and host_password could be optional
+instead of required in nova.conf.  The docs would need to describe how someone
+can populate the keystone credential store with this user and password to take
+advantage of this feature.
+
+
+References
+==========
+
+http://api.openstack.org/api-ref-identity.html#Credential_Calls
diff --git a/specs/juno/proposed/vmware-resource-pool-enablement.rst b/specs/juno/proposed/vmware-resource-pool-enablement.rst
new file mode 100644
index 0000000..d377d00
--- /dev/null
+++ b/specs/juno/proposed/vmware-resource-pool-enablement.rst
@@ -0,0 +1,165 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================================
+VMware resource pool enablement for VCenter
+============================================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-resource-pool-enablement
+
+To enable the resource pool in VCenter as a target, where the VM can
+be deployed.
+
+
+Problem description
+===================
+
+Right now the nova compute node can deploy the VM on a specified cluster in
+VCenter. Resource pool, as another important concept to group the compute
+resource, has not been implemented as a target for a VM to deploy on.
+This proposal will bring this resource pool into VCenter driver, so that
+the nova compute node can deploy the VM to a specified resource pool under a
+cluster or under a host.
+In this proposal, the cluster is directly under the data center of the VCenter,
+so is the host.
+
+Proposed change
+===============
+
+The idea will introduce the following changes:
+
+ * Add a new flag resource_pool in nova.conf for VMware. This flag is used
+   together with cluster_name as a combination to specify the resource
+   pool, because the resource pool can be either under a cluster or a host.
+
+1) For the resource pool under a cluster: the cluster_name is used to set the
+cluster name and the resource_pool is used to set the cluster and the resource
+pool name with a colon to split them For example, a resource pool named "pool"
+is under a cluster named "cluster1". We will set cluster_name=cluster1 and
+resource_pool=cluster1:pool. If the name before the colon(:) exists in
+cluster_name as well, we know this name is a cluster name and this resource pool
+is under a cluster. Besides the resource pool, the other clusters specified by
+cluster_name will be taken as the available node as well for the VMs to deploy.
+
+2)For the resource pool directly under a host directly in the VCenter data
+center: use resource_pool to specify the host and the resource pool name.
+For example, a resource pool named "pool" is under a host named "host1".
+We can set resource_pool=host1:pool, and DO NOT set "host1" in cluster_name,
+so we know the name before the colon(:) is the host name. This is the mechanism
+to determine whether the resource pool is under a cluster or under a host.
+Besides the resource pool, the other clusters specified by cluster_name will be
+taken as the available node as well for the VMs to deploy.
+
+ * In the method of spawning a VM, the resource pool specified in nova.conf
+   will be returned as the place to launch the VM.
+
+Examples of configurations:
+1. cluster_name=cluster1,cluster_name=cluster2,resource_pool=cluster1:rp1,
+resource_pool=cluster1:rp2
+With this configuration, we specify rp1 under cluster1, rp2 under cluster1 and
+cluster2 as the targets to deploy the VMs. Since cluster1 is used together
+with a resource pool name in resource_pool, this cluster will not be taken
+as the target for VMs to deploy, but the resource pools will be.
+
+2. cluster_name=cluster1,cluster_name=cluster2,resource_pool=host1:rp1
+With this configuration, we specify rp1 under host1, cluster1 and cluster2 as
+the targets to deploy the VMs. Neither cluster1 nor cluster2 is used in
+resource_pool, so the resource pool will be a pool under a host. The resource
+pool and two of the specified clusters will be the target for VMs to deploy.
+
+Alternatives
+------------
+Use the current cluster_name in nova.conf to specify the cluster or the resource
+pool. However, it makes cluster_name complicated to parse, since a lot of
+cases need to be taken into account. For example, we to differentiate the
+resource pool under a cluster from the resource pool under a host, because it is
+different for each of them to retrieve the resource pool. This alternative makes
+it hard to do so.
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+
+In nova.conf, the deployer can configure the resource pool(s) as the target,
+where the VMs are going to deploy.
+
+Developer impact
+----------------
+
+In VMware VCenter driver, a general method of returning the MOR of an
+resource pool by giving the name will be added. Other developers can use
+it to retrieve a resource pool reference.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Vincent Hou
+
+Work Items
+----------
+
+ * Add a new flag resource_pool for VMware in nova.conf.
+
+ * Implement the deployment a VM into a resource pool under a cluster.
+
+ * Implement the deployment a VM into a resource pool under a host directly in
+   the data center of VCenter.
+
+ * Extend the flag resource_pool into Multi-string option, so that multiple
+   resource pools can be set as the targets for one nova compute node.
+
+
+Dependencies
+============
+https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
+This proposal will be based on the vmware-spawn-refactor work.
+
+
+Testing
+=======
+New test cases need to be added for deploying the VM into a resource pool.
+
+Documentation Impact
+====================
+The resource pool configurations need to be added for the VMwareVCDriver.
+
+References
+==========
+https://blueprints.launchpad.net/nova/+spec/vmware-resource-pool-enablement
+
+https://blueprints.launchpad.net/nova/+spec/vmware-resource-pool-refactor
+
+https://blueprints.launchpad.net/nova/+spec/esx-resource-pools-as-compute-nodes
+
+Currently there are three blueprints related to this vmware resource pool issue.
+We can combine them together.
diff --git a/specs/juno/proposed/vmware-vm-ref-refactor.rst b/specs/juno/proposed/vmware-vm-ref-refactor.rst
new file mode 100644
index 0000000..9b3ed1a
--- /dev/null
+++ b/specs/juno/proposed/vmware-vm-ref-refactor.rst
@@ -0,0 +1,147 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================
+VMware vm_ref refactor
+======================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-vm-ref-refactor
+
+Improper use of virtual machine references (vm_ref) in vm_util.py is the
+source of many critical and high priority bugs. Refactoring this utility
+so that there is one-right-way to get a virtual machine reference will
+simplify the process of adding features to the driver and eliminate bugs.
+
+
+Problem description
+===================
+
+High/Critical priority problems caused by vm_util.py :
+
+* https://bugs.launchpad.net/nova/+bug/1258179
+
+* https://bugs.launchpad.net/nova/+bug/1290807
+
+* https://bugs.launchpad.net/nova/+bug/1295381
+
+The structure and use of the vm_util.py has become convoluted. It is time
+to reexamine the code around this utility.
+
+
+Proposed change
+===============
+
+* Create a data object to hold virtual machine properties such as vm_ref, name,
+  and other frequently accessed yet nonvolatile attributes of a virtual machine
+
+* Re-write and refactor common virtual machine search and retrieval utilities to
+  better conserve network bandwidth.
+
+* Re-configure the caching utilities to store virtual machine data objects by
+  faster to use indexes
+
+* Introduce a cache aging scheme.
+
+
+Alternatives
+------------
+
+* https://review.openstack.org/#/c/79833/ - began a general exploration of the
+  use of virtual machine references and its cache. This work is incomplete and
+  does not cover the cases needed to address other critical issues.
+
+Data model impact
+-----------------
+
+* Introduces transient vSphere virtual machine data objects that will be easier
+  to use with Nova. No data should be persisted to the Nova database.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+Not applicable.
+
+Other end user impact
+---------------------
+
+None applicable.
+
+Performance Impact
+------------------
+
+This will improve total aggregate performance of the VMware driver through more
+efficient use of networking bandwidth as the refactor will aggregate multiple
+separate network fetches into a single network fetch.
+
+Other deployer impact
+---------------------
+
+None applicable.
+
+Developer impact
+----------------
+
+* single point of authority for how to associate an instance with a virtual
+  machine reduces code-noise and improves capability to review
+
+* smarter use of vSphere API establishes better practice when dealing with
+  vSphere and components under remote management.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  hartsock
+
+Other contributors:
+  TBD
+
+Work Items
+----------
+
+* Identify all uses of virtual machine references in the driver
+
+* Introduce new virtual machine data model object
+
+* Introduce single source of authority for finding virtual machine references
+
+* route all virtual machine reference uses through new utility methods and objects
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+* Standard Minesweeper tests
+
+* extended unit tests
+
+Documentation Impact
+====================
+
+Internal driver developer documentation changes only.
+
+References
+==========
+
+For design discussions see:
+* https://etherpad.openstack.org/p/vmware-vm_util-refactor
diff --git a/specs/juno/proposed/vnc-configurable-share-policy.rst b/specs/juno/proposed/vnc-configurable-share-policy.rst
new file mode 100644
index 0000000..036aabf
--- /dev/null
+++ b/specs/juno/proposed/vnc-configurable-share-policy.rst
@@ -0,0 +1,168 @@
+=====================================================
+Add sharePolicy attribute to graphics element for vnc
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/vnc-configurable-share-policy
+
+From libvirt 1.0.6 version onwards share policy feature is supported to control 
+the way consoles are accessed by the user.
+
+Presently it is possible to configure share policy for vnc in 3 different
+ways:-
+1. allow-exclusive, allows clients to ask for exclusive access by dropping
+other connections 
+2. force-share, This is the default value, It allows multiple clients to
+connect to the console in parallel sharing the same session
+3. ignore, welcomes every connection unconditionally
+
+We can make use of this share policy which will allow service provider to
+restrict only single authorized user to connect to the console dropping
+previously connected users automatically if "allow-exclusive" share policy
+is configured.
+
+Problem description
+===================
+
+We have reported one bug related to the DoS style attack on noVNC server
+Reference to https://bugs.launchpad.net/nova/+bug/1227575
+
+The above problem can be solved to certain extent if provider uses share
+policy as allow-exclusive as oppose to "force-shared" which is the default
+one as of today.
+
+Proposed change
+===============
+
+Add a new 'vnc_sharepolicy' parameter to nova.conf, possible options available
+to the administrator are 'allow-exclusive | force-shared | ignore'. By default
+'vnc_sharepolicy' will be set to 'force-shared'.
+
+During loading of compute driver (libvirt), it will check whether the vnc is
+enabled or not. If yes, then it will validate the vnc_sharepolicy parameter
+only when the installed libvirt version is 1.0.6 or above. If the provider has
+configured incorrect value other than the above 3 possible options, then it
+will raise InvalidVNCSharedPolicyOption exception which will lead nova-compute
+service to fail to start.
+
+Implement logic to add 'sharePolicy' attribute to the graphics element for
+type vnc only when the installed libvirt version is 1.0.6 or above.
+
+Examples
+
+<graphics type="vnc" autoport="yes" keymap="en-us" listen="127.0.0.1"
+ sharePolicy="force-shared"/>
+<graphics type="vnc" autoport="yes" keymap="en-us" listen="127.0.0.1"
+ sharePolicy="allow-exclusive"/>
+<graphics type="vnc" autoport="yes" keymap="en-us" listen="127.0.0.1"
+ sharePolicy="ignore"/>
+
+Note: This share policy is only used by vnc and not spice.
+
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+None
+
+
+REST API impact
+---------------
+
+None
+
+
+Security Impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other End user impact
+---------------------
+
+None
+
+
+Performance Impact
+------------------
+
+None
+
+
+Other deployer impact
+---------------------
+
+Add vnc_sharepolicy parameter in nova.conf
+
+Format for vnc_sharepolicy in nova.conf:
+----------------------------------------
+vnc_sharepolicy = 'allow-exclusive | force-shared | ignore'
+
+Example:
+vnc_sharepolicy = allow-exclusive
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  abhishek-kekane
+
+Other contributors:
+  None
+
+
+Work Items
+----------
+
+- Add a new vnc_sharepolicy new parameter in the nova.conf
+- Depending on the vnc_enabled, validate vnc_sharepolicy parameter and
+  implement logic to add sharePolicy attribute to the graphics element
+  in libvirt driver
+- Add unitests for coverage
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+As this is internal change no need to add tempest tests for the same.
+Only unitests for coverage will do.
+
+
+Documentation Impact
+====================
+
+Refer Deployer impact
+
+
+References
+==========
+http://libvirt.org/formatdomain.html
+https://bugs.launchpad.net/nova/+bug/1227575
diff --git a/tests/test_titles.py b/tests/test_titles.py
index 46f86b9..afc9b2d 100644
--- a/tests/test_titles.py
+++ b/tests/test_titles.py
@@ -95,8 +95,13 @@ class TestTitles(testtools.TestCase):
 
             files = glob.glob("specs/%s/*/*" % release)
             for filename in files:
-                self.assertTrue(filename.endswith(".rst"),
-                                "spec's file must uses 'rst' extension.")
+                if filename.find('specs/%s/proposed/' % release) != -1:
+                    # Do not test proposed specs
+                    continue
+
+                self.assertTrue(
+                    filename.endswith(".rst"),
+                    "spec's file must uses 'rst' extension -- %s." % filename)
                 with open(filename) as f:
                     data = f.read()
 
-- 
1.9.1

