From f0b820407297ebb7e073fd43a90363d86bb0176f Mon Sep 17 00:00:00 2001
From: Michael Still <mikal@stillhq.com>
Date: Wed, 17 Sep 2014 17:01:52 +1000
Subject: [PATCH] Re-organize juno specs

As discussed at our nova meetings, reorganize the juno specs into
three directories:

 - proposed: things proposed which weren't approved
 - approved: things we approved but didn't implement
 - implemented: things approved and implemented

The first I suspect is the most controversial. I've done this
because I worry about the case where a future developer wants to
pick up something dropped by a previous developer, but has trouble
finding previous proposed specifications on the topic. Note that
the actual proposed specs for Juno are adding in a later commit.

Change-Id: Idcf55ca37a83d7098dcb7c2971240c4e8fd23dc8
---
 README.rst                                         |   9 +
 doc/source/conf.py                                 |   5 +-
 doc/source/index.rst                               |  12 +-
 doc/source/redirect.py                             |  50 +++
 .../add-all-in-list-operator-to-extra-spec-ops.rst | 152 --------
 .../juno/add-differencing-vhdx-resize-support.rst  | 132 -------
 specs/juno/add-ironic-driver.rst                   | 285 --------------
 specs/juno/add-virtio-scsi-bus-for-bdm.rst         | 139 -------
 .../juno/allocation-ratio-to-resource-tracker.rst  | 163 --------
 .../allow-image-to-be-specified-during-rescue.rst  | 204 ----------
 .../add-all-in-list-operator-to-extra-spec-ops.rst | 152 ++++++++
 .../juno/approved/add-virtio-scsi-bus-for-bdm.rst  | 139 +++++++
 .../allocation-ratio-to-resource-tracker.rst       | 163 ++++++++
 specs/juno/approved/clean-logs.rst                 | 149 ++++++++
 specs/juno/approved/cold-migration-with-target.rst | 145 ++++++++
 specs/juno/approved/db2-database.rst               | 267 +++++++++++++
 .../juno/approved/ec2-volume-and-snapshot-tags.rst | 161 ++++++++
 specs/juno/approved/encryption-with-barbican.rst   | 162 ++++++++
 .../enforce-unique-instance-uuid-in-db.rst         | 164 ++++++++
 .../input-output-based-numa-scheduling.rst         | 183 +++++++++
 specs/juno/approved/io-ops-weight.rst              | 132 +++++++
 .../approved/libvirt-driver-class-refactor.rst     | 222 +++++++++++
 .../approved/libvirt-sheepdog-backed-instances.rst | 181 +++++++++
 .../libvirt-start-lxc-from-block-devices.rst       | 128 +++++++
 specs/juno/approved/log-request-id-mappings.rst    | 167 +++++++++
 .../approved/lvm-ephemeral-storage-encryption.rst  | 204 ++++++++++
 .../approved/make-resource-tracker-use-objects.rst | 183 +++++++++
 specs/juno/approved/migrate-libvirt-volumes.rst    | 204 ++++++++++
 specs/juno/approved/nova-pagination.rst            | 333 +++++++++++++++++
 specs/juno/approved/persistent-resource-claim.rst  | 201 ++++++++++
 ...esced-image-snapshots-with-qemu-guest-agent.rst | 131 +++++++
 .../restrict-image-isolation-with-defined-keys.rst | 177 +++++++++
 .../return-all-servers-during-multiple-create.rst  | 225 +++++++++++
 .../approved/selecting-subnet-when-creating-vm.rst | 214 +++++++++++
 specs/juno/approved/server-count-api.rst           | 315 ++++++++++++++++
 specs/juno/approved/standardize-nova-image.rst     | 125 +++++++
 specs/juno/approved/string-field-max-length.rst    | 174 +++++++++
 .../approved/support-console-log-migration.rst     | 140 +++++++
 specs/juno/approved/tag-instances.rst              | 318 ++++++++++++++++
 specs/juno/approved/use-libvirt-storage-pools.rst  | 291 +++++++++++++++
 specs/juno/approved/vif-vhostuser.rst              | 163 ++++++++
 specs/juno/approved/virt-driver-cpu-pinning.rst    | 224 +++++++++++
 specs/juno/approved/virt-driver-large-pages.rst    | 340 +++++++++++++++++
 specs/juno/approved/vmware-driver-ova-support.rst  | 200 ++++++++++
 .../approved/vmware-ephemeral-disk-support.rst     | 117 ++++++
 specs/juno/approved/vmware-spbm-support.rst        | 213 +++++++++++
 specs/juno/approved/vmware-vsan-support.rst        | 193 ++++++++++
 .../approved/websocket-proxy-to-host-security.rst  | 215 +++++++++++
 .../xenapi-set-ipxe-url-as-img-metadata.rst        | 117 ++++++
 specs/juno/approved/xenapi-vcpu-topology.rst       | 152 ++++++++
 specs/juno/backportable-db-migrations-juno.rst     | 135 -------
 .../juno/better-support-for-multiple-networks.rst  | 207 -----------
 specs/juno/clean-logs.rst                          | 149 --------
 specs/juno/cold-migration-with-target.rst          | 145 --------
 specs/juno/compute-manager-objects-juno.rst        | 160 --------
 specs/juno/config-drive-image-property.rst         | 155 --------
 specs/juno/convert_ec2_api_to_use_nova_objects.rst | 143 -------
 specs/juno/cross-service-request-id.rst            | 147 --------
 specs/juno/db2-database.rst                        | 267 -------------
 specs/juno/ec2-volume-and-snapshot-tags.rst        | 161 --------
 specs/juno/enabled-qemu-memballoon-stats.rst       | 164 --------
 specs/juno/encryption-with-barbican.rst            | 162 --------
 specs/juno/enforce-unique-instance-uuid-in-db.rst  | 164 --------
 specs/juno/extensible-resource-tracking.rst        | 284 --------------
 specs/juno/find-host-and-evacuate-instance.rst     | 182 ---------
 specs/juno/hyper-v-console-log.rst                 | 127 -------
 specs/juno/hyper-v-soft-reboot.rst                 | 116 ------
 specs/juno/i18n-enablement.rst                     | 185 ---------
 .../add-differencing-vhdx-resize-support.rst       | 132 +++++++
 specs/juno/implemented/add-ironic-driver.rst       | 285 ++++++++++++++
 .../allow-image-to-be-specified-during-rescue.rst  | 204 ++++++++++
 .../backportable-db-migrations-juno.rst            | 135 +++++++
 .../better-support-for-multiple-networks.rst       | 207 +++++++++++
 .../implemented/compute-manager-objects-juno.rst   | 160 ++++++++
 .../implemented/config-drive-image-property.rst    | 155 ++++++++
 .../convert_ec2_api_to_use_nova_objects.rst        | 143 +++++++
 .../juno/implemented/cross-service-request-id.rst  | 147 ++++++++
 .../implemented/enabled-qemu-memballoon-stats.rst  | 164 ++++++++
 .../implemented/extensible-resource-tracking.rst   | 284 ++++++++++++++
 .../find-host-and-evacuate-instance.rst            | 182 +++++++++
 specs/juno/implemented/hyper-v-console-log.rst     | 127 +++++++
 specs/juno/implemented/hyper-v-soft-reboot.rst     | 116 ++++++
 specs/juno/implemented/i18n-enablement.rst         | 185 +++++++++
 .../implemented/instance-network-info-hook.rst     | 126 +++++++
 specs/juno/implemented/juno-slaveification.rst     | 157 ++++++++
 .../implemented/libvirt-disk-discard-option.rst    | 147 ++++++++
 .../implemented/libvirt-domain-listing-speedup.rst | 139 +++++++
 .../implemented/libvirt-driver-domain-metadata.rst | 158 ++++++++
 .../implemented/libvirt-lxc-user-namespaces.rst    | 184 +++++++++
 .../libvirt-volume-snap-network-disk.rst           | 160 ++++++++
 .../implemented/move-prep-resize-to-conductor.rst  | 125 +++++++
 specs/juno/implemented/nfv-multiple-if-1-net.rst   | 184 +++++++++
 specs/juno/implemented/object-subclassing.rst      | 146 ++++++++
 .../juno/implemented/on-demand-compute-update.rst  | 184 +++++++++
 specs/juno/implemented/pci-passthrough-sriov.rst   | 414 +++++++++++++++++++++
 specs/juno/implemented/per-aggregate-filters.rst   | 145 ++++++++
 specs/juno/implemented/rbd-clone-image-handler.rst | 222 +++++++++++
 specs/juno/implemented/refactor-network-api.rst    | 109 ++++++
 .../remove-cast-to-schedule-run-instance.rst       | 153 ++++++++
 specs/juno/implemented/rescue-attach-all-disks.rst | 135 +++++++
 .../return-status-for-hypervisor-node.rst          | 191 ++++++++++
 specs/juno/implemented/scheduler-lib.rst           | 221 +++++++++++
 specs/juno/implemented/serial-ports.rst            | 258 +++++++++++++
 specs/juno/implemented/server-group-quotas.rst     | 330 ++++++++++++++++
 .../servers-list-support-multi-status.rst          | 133 +++++++
 specs/juno/implemented/support-cinderclient-v2.rst | 155 ++++++++
 specs/juno/implemented/use-oslo-vmware.rst         | 152 ++++++++
 specs/juno/implemented/user-defined-shutdown.rst   | 259 +++++++++++++
 specs/juno/implemented/v2-on-v3-api.rst            | 262 +++++++++++++
 specs/juno/implemented/v3-api-schema.rst           | 223 +++++++++++
 specs/juno/implemented/v3-diagnostics.rst          | 339 +++++++++++++++++
 .../implemented/virt-driver-numa-placement.rst     | 361 ++++++++++++++++++
 .../juno/implemented/virt-driver-vcpu-topology.rst | 273 ++++++++++++++
 specs/juno/implemented/virt-objects-juno.rst       | 174 +++++++++
 specs/juno/implemented/vmware-hot-plug.rst         | 111 ++++++
 specs/juno/implemented/vmware-spawn-refactor.rst   | 175 +++++++++
 specs/juno/input-output-based-numa-scheduling.rst  | 183 ---------
 specs/juno/instance-network-info-hook.rst          | 126 -------
 specs/juno/io-ops-weight.rst                       | 132 -------
 specs/juno/juno-slaveification.rst                 | 157 --------
 specs/juno/libvirt-disk-discard-option.rst         | 147 --------
 specs/juno/libvirt-domain-listing-speedup.rst      | 139 -------
 specs/juno/libvirt-driver-class-refactor.rst       | 222 -----------
 specs/juno/libvirt-driver-domain-metadata.rst      | 158 --------
 specs/juno/libvirt-lxc-user-namespaces.rst         | 184 ---------
 specs/juno/libvirt-sheepdog-backed-instances.rst   | 181 ---------
 .../juno/libvirt-start-lxc-from-block-devices.rst  | 128 -------
 specs/juno/libvirt-volume-snap-network-disk.rst    | 160 --------
 specs/juno/log-request-id-mappings.rst             | 167 ---------
 specs/juno/lvm-ephemeral-storage-encryption.rst    | 204 ----------
 specs/juno/make-resource-tracker-use-objects.rst   | 183 ---------
 specs/juno/migrate-libvirt-volumes.rst             | 204 ----------
 specs/juno/move-prep-resize-to-conductor.rst       | 125 -------
 specs/juno/nfv-multiple-if-1-net.rst               | 184 ---------
 specs/juno/nova-pagination.rst                     | 333 -----------------
 specs/juno/object-subclassing.rst                  | 146 --------
 specs/juno/on-demand-compute-update.rst            | 184 ---------
 specs/juno/pci-passthrough-sriov.rst               | 414 ---------------------
 specs/juno/per-aggregate-filters.rst               | 145 --------
 specs/juno/persistent-resource-claim.rst           | 201 ----------
 ...esced-image-snapshots-with-qemu-guest-agent.rst | 131 -------
 specs/juno/rbd-clone-image-handler.rst             | 222 -----------
 specs/juno/redirects                               |  88 +++++
 specs/juno/refactor-network-api.rst                | 109 ------
 .../juno/remove-cast-to-schedule-run-instance.rst  | 153 --------
 specs/juno/rescue-attach-all-disks.rst             | 135 -------
 .../restrict-image-isolation-with-defined-keys.rst | 177 ---------
 .../return-all-servers-during-multiple-create.rst  | 225 -----------
 specs/juno/return-status-for-hypervisor-node.rst   | 191 ----------
 specs/juno/scheduler-lib.rst                       | 221 -----------
 specs/juno/selecting-subnet-when-creating-vm.rst   | 214 -----------
 specs/juno/serial-ports.rst                        | 258 -------------
 specs/juno/server-count-api.rst                    | 315 ----------------
 specs/juno/server-group-quotas.rst                 | 330 ----------------
 specs/juno/servers-list-support-multi-status.rst   | 133 -------
 specs/juno/standardize-nova-image.rst              | 125 -------
 specs/juno/string-field-max-length.rst             | 174 ---------
 specs/juno/support-cinderclient-v2.rst             | 155 --------
 specs/juno/support-console-log-migration.rst       | 140 -------
 specs/juno/tag-instances.rst                       | 318 ----------------
 specs/juno/use-libvirt-storage-pools.rst           | 291 ---------------
 specs/juno/use-oslo-vmware.rst                     | 152 --------
 specs/juno/user-defined-shutdown.rst               | 259 -------------
 specs/juno/v2-on-v3-api.rst                        | 262 -------------
 specs/juno/v3-api-schema.rst                       | 223 -----------
 specs/juno/v3-diagnostics.rst                      | 339 -----------------
 specs/juno/vif-vhostuser.rst                       | 163 --------
 specs/juno/virt-driver-cpu-pinning.rst             | 224 -----------
 specs/juno/virt-driver-large-pages.rst             | 340 -----------------
 specs/juno/virt-driver-numa-placement.rst          | 361 ------------------
 specs/juno/virt-driver-vcpu-topology.rst           | 273 --------------
 specs/juno/virt-objects-juno.rst                   | 174 ---------
 specs/juno/vmware-driver-ova-support.rst           | 200 ----------
 specs/juno/vmware-ephemeral-disk-support.rst       | 117 ------
 specs/juno/vmware-hot-plug.rst                     | 111 ------
 specs/juno/vmware-spawn-refactor.rst               | 175 ---------
 specs/juno/vmware-spbm-support.rst                 | 213 -----------
 specs/juno/vmware-vsan-support.rst                 | 193 ----------
 specs/juno/websocket-proxy-to-host-security.rst    | 215 -----------
 specs/juno/xenapi-set-ipxe-url-as-img-metadata.rst | 117 ------
 specs/juno/xenapi-vcpu-topology.rst                | 152 --------
 tests/test_titles.py                               |   2 +-
 182 files changed, 16976 insertions(+), 16820 deletions(-)
 create mode 100644 doc/source/redirect.py
 delete mode 100644 specs/juno/add-all-in-list-operator-to-extra-spec-ops.rst
 delete mode 100644 specs/juno/add-differencing-vhdx-resize-support.rst
 delete mode 100644 specs/juno/add-ironic-driver.rst
 delete mode 100644 specs/juno/add-virtio-scsi-bus-for-bdm.rst
 delete mode 100644 specs/juno/allocation-ratio-to-resource-tracker.rst
 delete mode 100644 specs/juno/allow-image-to-be-specified-during-rescue.rst
 create mode 100644 specs/juno/approved/add-all-in-list-operator-to-extra-spec-ops.rst
 create mode 100644 specs/juno/approved/add-virtio-scsi-bus-for-bdm.rst
 create mode 100644 specs/juno/approved/allocation-ratio-to-resource-tracker.rst
 create mode 100644 specs/juno/approved/clean-logs.rst
 create mode 100644 specs/juno/approved/cold-migration-with-target.rst
 create mode 100644 specs/juno/approved/db2-database.rst
 create mode 100644 specs/juno/approved/ec2-volume-and-snapshot-tags.rst
 create mode 100644 specs/juno/approved/encryption-with-barbican.rst
 create mode 100644 specs/juno/approved/enforce-unique-instance-uuid-in-db.rst
 create mode 100644 specs/juno/approved/input-output-based-numa-scheduling.rst
 create mode 100644 specs/juno/approved/io-ops-weight.rst
 create mode 100644 specs/juno/approved/libvirt-driver-class-refactor.rst
 create mode 100644 specs/juno/approved/libvirt-sheepdog-backed-instances.rst
 create mode 100644 specs/juno/approved/libvirt-start-lxc-from-block-devices.rst
 create mode 100644 specs/juno/approved/log-request-id-mappings.rst
 create mode 100644 specs/juno/approved/lvm-ephemeral-storage-encryption.rst
 create mode 100644 specs/juno/approved/make-resource-tracker-use-objects.rst
 create mode 100644 specs/juno/approved/migrate-libvirt-volumes.rst
 create mode 100644 specs/juno/approved/nova-pagination.rst
 create mode 100644 specs/juno/approved/persistent-resource-claim.rst
 create mode 100644 specs/juno/approved/quiesced-image-snapshots-with-qemu-guest-agent.rst
 create mode 100644 specs/juno/approved/restrict-image-isolation-with-defined-keys.rst
 create mode 100644 specs/juno/approved/return-all-servers-during-multiple-create.rst
 create mode 100644 specs/juno/approved/selecting-subnet-when-creating-vm.rst
 create mode 100644 specs/juno/approved/server-count-api.rst
 create mode 100644 specs/juno/approved/standardize-nova-image.rst
 create mode 100644 specs/juno/approved/string-field-max-length.rst
 create mode 100644 specs/juno/approved/support-console-log-migration.rst
 create mode 100644 specs/juno/approved/tag-instances.rst
 create mode 100644 specs/juno/approved/use-libvirt-storage-pools.rst
 create mode 100644 specs/juno/approved/vif-vhostuser.rst
 create mode 100644 specs/juno/approved/virt-driver-cpu-pinning.rst
 create mode 100644 specs/juno/approved/virt-driver-large-pages.rst
 create mode 100644 specs/juno/approved/vmware-driver-ova-support.rst
 create mode 100644 specs/juno/approved/vmware-ephemeral-disk-support.rst
 create mode 100644 specs/juno/approved/vmware-spbm-support.rst
 create mode 100644 specs/juno/approved/vmware-vsan-support.rst
 create mode 100644 specs/juno/approved/websocket-proxy-to-host-security.rst
 create mode 100644 specs/juno/approved/xenapi-set-ipxe-url-as-img-metadata.rst
 create mode 100644 specs/juno/approved/xenapi-vcpu-topology.rst
 delete mode 100644 specs/juno/backportable-db-migrations-juno.rst
 delete mode 100644 specs/juno/better-support-for-multiple-networks.rst
 delete mode 100644 specs/juno/clean-logs.rst
 delete mode 100644 specs/juno/cold-migration-with-target.rst
 delete mode 100644 specs/juno/compute-manager-objects-juno.rst
 delete mode 100644 specs/juno/config-drive-image-property.rst
 delete mode 100644 specs/juno/convert_ec2_api_to_use_nova_objects.rst
 delete mode 100644 specs/juno/cross-service-request-id.rst
 delete mode 100644 specs/juno/db2-database.rst
 delete mode 100644 specs/juno/ec2-volume-and-snapshot-tags.rst
 delete mode 100644 specs/juno/enabled-qemu-memballoon-stats.rst
 delete mode 100644 specs/juno/encryption-with-barbican.rst
 delete mode 100644 specs/juno/enforce-unique-instance-uuid-in-db.rst
 delete mode 100644 specs/juno/extensible-resource-tracking.rst
 delete mode 100644 specs/juno/find-host-and-evacuate-instance.rst
 delete mode 100644 specs/juno/hyper-v-console-log.rst
 delete mode 100644 specs/juno/hyper-v-soft-reboot.rst
 delete mode 100644 specs/juno/i18n-enablement.rst
 create mode 100644 specs/juno/implemented/add-differencing-vhdx-resize-support.rst
 create mode 100644 specs/juno/implemented/add-ironic-driver.rst
 create mode 100644 specs/juno/implemented/allow-image-to-be-specified-during-rescue.rst
 create mode 100644 specs/juno/implemented/backportable-db-migrations-juno.rst
 create mode 100644 specs/juno/implemented/better-support-for-multiple-networks.rst
 create mode 100644 specs/juno/implemented/compute-manager-objects-juno.rst
 create mode 100644 specs/juno/implemented/config-drive-image-property.rst
 create mode 100644 specs/juno/implemented/convert_ec2_api_to_use_nova_objects.rst
 create mode 100644 specs/juno/implemented/cross-service-request-id.rst
 create mode 100644 specs/juno/implemented/enabled-qemu-memballoon-stats.rst
 create mode 100644 specs/juno/implemented/extensible-resource-tracking.rst
 create mode 100644 specs/juno/implemented/find-host-and-evacuate-instance.rst
 create mode 100644 specs/juno/implemented/hyper-v-console-log.rst
 create mode 100644 specs/juno/implemented/hyper-v-soft-reboot.rst
 create mode 100644 specs/juno/implemented/i18n-enablement.rst
 create mode 100644 specs/juno/implemented/instance-network-info-hook.rst
 create mode 100644 specs/juno/implemented/juno-slaveification.rst
 create mode 100644 specs/juno/implemented/libvirt-disk-discard-option.rst
 create mode 100644 specs/juno/implemented/libvirt-domain-listing-speedup.rst
 create mode 100644 specs/juno/implemented/libvirt-driver-domain-metadata.rst
 create mode 100644 specs/juno/implemented/libvirt-lxc-user-namespaces.rst
 create mode 100644 specs/juno/implemented/libvirt-volume-snap-network-disk.rst
 create mode 100644 specs/juno/implemented/move-prep-resize-to-conductor.rst
 create mode 100644 specs/juno/implemented/nfv-multiple-if-1-net.rst
 create mode 100644 specs/juno/implemented/object-subclassing.rst
 create mode 100644 specs/juno/implemented/on-demand-compute-update.rst
 create mode 100644 specs/juno/implemented/pci-passthrough-sriov.rst
 create mode 100644 specs/juno/implemented/per-aggregate-filters.rst
 create mode 100644 specs/juno/implemented/rbd-clone-image-handler.rst
 create mode 100644 specs/juno/implemented/refactor-network-api.rst
 create mode 100644 specs/juno/implemented/remove-cast-to-schedule-run-instance.rst
 create mode 100644 specs/juno/implemented/rescue-attach-all-disks.rst
 create mode 100644 specs/juno/implemented/return-status-for-hypervisor-node.rst
 create mode 100644 specs/juno/implemented/scheduler-lib.rst
 create mode 100644 specs/juno/implemented/serial-ports.rst
 create mode 100644 specs/juno/implemented/server-group-quotas.rst
 create mode 100644 specs/juno/implemented/servers-list-support-multi-status.rst
 create mode 100644 specs/juno/implemented/support-cinderclient-v2.rst
 create mode 100644 specs/juno/implemented/use-oslo-vmware.rst
 create mode 100644 specs/juno/implemented/user-defined-shutdown.rst
 create mode 100644 specs/juno/implemented/v2-on-v3-api.rst
 create mode 100644 specs/juno/implemented/v3-api-schema.rst
 create mode 100644 specs/juno/implemented/v3-diagnostics.rst
 create mode 100644 specs/juno/implemented/virt-driver-numa-placement.rst
 create mode 100644 specs/juno/implemented/virt-driver-vcpu-topology.rst
 create mode 100644 specs/juno/implemented/virt-objects-juno.rst
 create mode 100644 specs/juno/implemented/vmware-hot-plug.rst
 create mode 100644 specs/juno/implemented/vmware-spawn-refactor.rst
 delete mode 100644 specs/juno/input-output-based-numa-scheduling.rst
 delete mode 100644 specs/juno/instance-network-info-hook.rst
 delete mode 100644 specs/juno/io-ops-weight.rst
 delete mode 100644 specs/juno/juno-slaveification.rst
 delete mode 100644 specs/juno/libvirt-disk-discard-option.rst
 delete mode 100644 specs/juno/libvirt-domain-listing-speedup.rst
 delete mode 100644 specs/juno/libvirt-driver-class-refactor.rst
 delete mode 100644 specs/juno/libvirt-driver-domain-metadata.rst
 delete mode 100644 specs/juno/libvirt-lxc-user-namespaces.rst
 delete mode 100644 specs/juno/libvirt-sheepdog-backed-instances.rst
 delete mode 100644 specs/juno/libvirt-start-lxc-from-block-devices.rst
 delete mode 100644 specs/juno/libvirt-volume-snap-network-disk.rst
 delete mode 100644 specs/juno/log-request-id-mappings.rst
 delete mode 100644 specs/juno/lvm-ephemeral-storage-encryption.rst
 delete mode 100644 specs/juno/make-resource-tracker-use-objects.rst
 delete mode 100644 specs/juno/migrate-libvirt-volumes.rst
 delete mode 100644 specs/juno/move-prep-resize-to-conductor.rst
 delete mode 100644 specs/juno/nfv-multiple-if-1-net.rst
 delete mode 100644 specs/juno/nova-pagination.rst
 delete mode 100644 specs/juno/object-subclassing.rst
 delete mode 100644 specs/juno/on-demand-compute-update.rst
 delete mode 100644 specs/juno/pci-passthrough-sriov.rst
 delete mode 100644 specs/juno/per-aggregate-filters.rst
 delete mode 100644 specs/juno/persistent-resource-claim.rst
 delete mode 100644 specs/juno/quiesced-image-snapshots-with-qemu-guest-agent.rst
 delete mode 100644 specs/juno/rbd-clone-image-handler.rst
 create mode 100644 specs/juno/redirects
 delete mode 100644 specs/juno/refactor-network-api.rst
 delete mode 100644 specs/juno/remove-cast-to-schedule-run-instance.rst
 delete mode 100644 specs/juno/rescue-attach-all-disks.rst
 delete mode 100644 specs/juno/restrict-image-isolation-with-defined-keys.rst
 delete mode 100644 specs/juno/return-all-servers-during-multiple-create.rst
 delete mode 100644 specs/juno/return-status-for-hypervisor-node.rst
 delete mode 100644 specs/juno/scheduler-lib.rst
 delete mode 100644 specs/juno/selecting-subnet-when-creating-vm.rst
 delete mode 100644 specs/juno/serial-ports.rst
 delete mode 100644 specs/juno/server-count-api.rst
 delete mode 100644 specs/juno/server-group-quotas.rst
 delete mode 100644 specs/juno/servers-list-support-multi-status.rst
 delete mode 100644 specs/juno/standardize-nova-image.rst
 delete mode 100644 specs/juno/string-field-max-length.rst
 delete mode 100644 specs/juno/support-cinderclient-v2.rst
 delete mode 100644 specs/juno/support-console-log-migration.rst
 delete mode 100644 specs/juno/tag-instances.rst
 delete mode 100644 specs/juno/use-libvirt-storage-pools.rst
 delete mode 100644 specs/juno/use-oslo-vmware.rst
 delete mode 100644 specs/juno/user-defined-shutdown.rst
 delete mode 100644 specs/juno/v2-on-v3-api.rst
 delete mode 100644 specs/juno/v3-api-schema.rst
 delete mode 100644 specs/juno/v3-diagnostics.rst
 delete mode 100644 specs/juno/vif-vhostuser.rst
 delete mode 100644 specs/juno/virt-driver-cpu-pinning.rst
 delete mode 100644 specs/juno/virt-driver-large-pages.rst
 delete mode 100644 specs/juno/virt-driver-numa-placement.rst
 delete mode 100644 specs/juno/virt-driver-vcpu-topology.rst
 delete mode 100644 specs/juno/virt-objects-juno.rst
 delete mode 100644 specs/juno/vmware-driver-ova-support.rst
 delete mode 100644 specs/juno/vmware-ephemeral-disk-support.rst
 delete mode 100644 specs/juno/vmware-hot-plug.rst
 delete mode 100644 specs/juno/vmware-spawn-refactor.rst
 delete mode 100644 specs/juno/vmware-spbm-support.rst
 delete mode 100644 specs/juno/vmware-vsan-support.rst
 delete mode 100644 specs/juno/websocket-proxy-to-host-security.rst
 delete mode 100644 specs/juno/xenapi-set-ipxe-url-as-img-metadata.rst
 delete mode 100644 specs/juno/xenapi-vcpu-topology.rst

diff --git a/README.rst b/README.rst
index fd0738f..df985fb 100644
--- a/README.rst
+++ b/README.rst
@@ -14,6 +14,15 @@ The layout of this repository is::
 
   specs/<release>/
 
+Where there are two sub-directories:
+
+  specs/<release>/approved: specifications approved but not yet implemented
+  specs/<release>/implemented: implemented specifications
+
+This directory structure allows you to see what we thought about doing,
+decided to do, and actually got done. Users interested in functionality in a
+given release should only refer to the ``implemented`` directory.
+
 You can find an example spec in ``specs/template.rst``.
 
 Specifications are proposed for a given release by adding them to the
diff --git a/doc/source/conf.py b/doc/source/conf.py
index 93be7bc..c85a38e 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -17,7 +17,7 @@ import os
 # If extensions (or modules to document with autodoc) are in another directory,
 # add these directories to sys.path here. If the directory is relative to the
 # documentation root, use os.path.abspath to make it absolute, like shown here.
-#sys.path.insert(0, os.path.abspath('.'))
+sys.path.insert(0, os.path.abspath('.'))
 
 # -- General configuration -----------------------------------------------------
 
@@ -26,7 +26,8 @@ import os
 
 # Add any Sphinx extension module names here, as strings. They can be extensions
 # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
-extensions = ['sphinx.ext.autodoc',
+extensions = ['redirect',
+              'sphinx.ext.autodoc',
               'sphinx.ext.intersphinx',
               'sphinx.ext.todo',
               'sphinx.ext.viewcode',
diff --git a/doc/source/index.rst b/doc/source/index.rst
index 5943626..69ca3d3 100644
--- a/doc/source/index.rst
+++ b/doc/source/index.rst
@@ -24,13 +24,21 @@ Contents:
    readme
    specs/*
 
-Juno approved specs:
+June implemented specs:
 
 .. toctree::
    :glob:
    :maxdepth: 1
 
-   specs/juno/*
+   specs/juno/implemented/*
+
+Juno approved (but not implemented) specs:
+
+.. toctree::
+   :glob:
+   :maxdepth: 1
+
+   specs/juno/approved/*
 
 ==================
 Indices and tables
diff --git a/doc/source/redirect.py b/doc/source/redirect.py
new file mode 100644
index 0000000..099159f
--- /dev/null
+++ b/doc/source/redirect.py
@@ -0,0 +1,50 @@
+# A simple sphinx plugin which creates HTML redirections from old names
+# to new names. It does this by looking for files named "redirect" in
+# the documentation source and using the contents to create simple HTML
+# redirection pages for changed filenames.
+
+import os.path
+
+from sphinx.application import ENV_PICKLE_FILENAME
+from sphinx.util.console import bold
+
+
+def setup(app):
+    from sphinx.application import Sphinx
+    if not isinstance(app, Sphinx):
+        return
+    app.connect('build-finished', emit_redirects)
+
+
+def process_redirect_file(app, path, ent):
+    parent_path = path.replace(app.builder.srcdir, app.builder.outdir)
+    with open(os.path.join(path, ent)) as redirects:
+        for line in redirects.readlines():
+            from_path, to_path = line.rstrip().split(' ')
+            from_path = from_path.replace('.rst', '.html')
+            to_path = to_path.replace('.rst', '.html')
+
+            redirected_filename = os.path.join(parent_path, from_path)
+            redirected_directory = os.path.dirname(redirected_filename)
+            if not os.path.exists(redirected_directory):
+                os.makedirs(redirected_directory)
+            with open(redirected_filename, 'w') as f:
+                f.write('<html><head><meta http-equiv="refresh" content="0; '
+                        'url=%s" /></head></html>'
+                        % to_path)
+
+
+def emit_redirects(app, exc):
+    app.builder.info(bold('scanning %s for redirects...') % app.builder.srcdir)
+
+    def process_directory(path):
+        for ent in os.listdir(path):
+            p = os.path.join(path, ent)
+            if os.path.isdir(p):
+                process_directory(p)
+            elif ent == 'redirects':
+                app.builder.info('   found redirects at %s' % p)
+                process_redirect_file(app, path, ent)
+
+    process_directory(app.builder.srcdir)
+    app.builder.info('...done')
diff --git a/specs/juno/add-all-in-list-operator-to-extra-spec-ops.rst b/specs/juno/add-all-in-list-operator-to-extra-spec-ops.rst
deleted file mode 100644
index 040c5ec..0000000
--- a/specs/juno/add-all-in-list-operator-to-extra-spec-ops.rst
+++ /dev/null
@@ -1,152 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-======================================
-Add ALL-IN operator to extra spec ops
-======================================
-
-https://blueprints.launchpad.net/nova/+spec/add-all-in-list-
-operator-to-extra-spec-ops
-
-Allow extra spec to match all values in a list by adding the ALL-IN operator.
-
-
-Problem description
-===================
-
-This blueprint aims to allow querying if ALL of the given values are present
-in a list.
-Currently there's support for an IN operator that returns True if a given
-element is present in a list. There is also an OR operator that
-only works for single values.
-
-Example:
-
-Suppose a flavor needs to be placed on a host that has the cpu flags 'aes'
-and 'vmx'. As it is today is not possible since the only posibility is to
-use the <in> operator. But, as the extra specs is a dict, the flavor
-extra-spec key would be the same:
-
-capabilities:cpu_info:features : <in> aes
-capabilities:cpu_info:features : <in> vmx
-
-Just one of them will be saved.
-
-something like this is needed:
-
-capabilities:cpu_info:features : <all-in> aes vmx
-
-Proposed change
-===============
-
-We need to add the new <all-in> operator and its lambda function to
-_op_methods dict in extra_specs_ops.py.
-
-...
-'<all-in>': lambda x, y: all(val in x for val in y),
-...
-
-Then add a call to this function with a list, instead of with a
-string if there are more than one element in the query.
-
-
-Alternatives
-------------
-
-Instead of add the '<all-in>' operator extend/overload the '<in>' operator to
-work with a list.
-
-capabilities:cpu_info:features : <in> aes vmx
-
-Seems to be easy to understand but could generate confusion because <in>
-operator as it is today, aims to be used to match a substring.
-
-Another possibility is add both <any> and <all> operators. By doing this, we
-are using <in> and <or> for single values and the new set of operators for
-collections values. But something is missing with this approach,
-<all> or <any> what? All elements in a list, all elements are True, or all
-elements are equal to a given value.
-
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Add a new lamda function to
-nova/scheduler/filter/extra_specs_ops.py _ops_method dict:
-
-'<all-in>': lambda x, y: all(val in x for val in y)
-
-Assignee(s)
------------
-
-Primary assignee:
-  facundo-n-maldonado
-
-Other contributors:
-  None
-
-Work Items
-----------
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Unit tests should be added for the new operator.
-
-Documentation Impact
-====================
-
-Filter scheduler documentation should be updated with the new operator.
-
-References
-==========
-
-None
diff --git a/specs/juno/add-differencing-vhdx-resize-support.rst b/specs/juno/add-differencing-vhdx-resize-support.rst
deleted file mode 100644
index f3bdffd..0000000
--- a/specs/juno/add-differencing-vhdx-resize-support.rst
+++ /dev/null
@@ -1,132 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-======================================================
-Add differencing vhdx resize support in Hyper-V Driver
-======================================================
-
-https://blueprints.launchpad.net/nova/+spec/add-differencing-vhdx-resize-support
-
-Differencing VHDX images can be resized, unlike differencing VHD images. Even
-so, the Nova Hyper-V driver currently does not support this.
-
-This feature is required for resizing existing instances which use CoW VHDX
-images and also in order to resize the root disk image when spawning a new
-instance.
-
-Problem description
-===================
-
-Currently, when using the Hyper-V Nova Driver and differencing (CoW) VHDX
-images for the instances, the differencing image will not get resized according
-to the flavor size. Instead, the VM root image will keep having the same size
-as the base image used when spawning a new instance.
-
-Also, when trying to resize such an instance, not only that the disk image will
-not get resized, but this will actually raise an exception as currently the
-method which gets the internal maximum size of a vhd/vhdx does not support
-differencing images.
-
-
-Proposed change
-===============
-
-The solution is simply passing the desired size when creating a new
-differencing vhdx image. Not passing it will result in the new disk having the
-same size as the base image.
-
-Also, it is required that the method which gets the internal maximum size of
-a vhdx to lookup the parent disk and return the according size instead of
-raising an exception.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  <lpetrut@cloudbasesolutions.com>
-
-Work Items
-----------
-
-Add a "size" argument to the create_differencing_vhd method.
-
-Adapt the vmops module to specify the new size only if resize is required
-when booting a new instance using CoW vhdx images.
-
-Lookup for the parent image and get the according size when getting the
-maximum internal size of a vhdx.
-
-Adapt the vhdutils according methods in order to have the same method
-signatures and keep consistency.
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Testing this feature will be covered by the Hyper-V CI.
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-Official VHDX format specs:
-http://www.microsoft.com/en-us/download/details.aspx?id=34750
diff --git a/specs/juno/add-ironic-driver.rst b/specs/juno/add-ironic-driver.rst
deleted file mode 100644
index c234799..0000000
--- a/specs/juno/add-ironic-driver.rst
+++ /dev/null
@@ -1,285 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-============================
-Add a virt driver for Ironic
-============================
-
-https://blueprints.launchpad.net/nova/+spec/add-ironic-driver
-
-This specification proposes to add a virt driver to enable Nova to deploy
-images to bare metal resources by using the OpenStack Bare Metal Provisioning
-Service ("Ironic").
-
-Problem description
-===================
-
-The community has split out the functionality of provisioning bare metal
-servers into a separate program, which includes the ironic and
-python-ironicclient projects. The original intent of the
-nova.virt.baremetal driver was two-fold:
-
-- to provide physical machines more suitable to HPC-style workloads,
-  eg where virtualization overhead is too high;
-- to be an experimental proof-of-concept for enabling the TripleO project.
-
-In order to address scalability and architectural concerns affecting both
-use-cases, this driver was split out into a separate OpenStack Program,
-and developed over the last year as such.
-
-Proposed change
-===============
-
-This proposal aims to enable Nova to use Ironic to perform the same functions
-which it is currently able to perform via the nova.virt.baremetal driver.
-This abstracts the details of physical hardware within Ironic, such that the
-user interacts with Nova in the same way when deploying instances to virtual or
-physical machines. The hardware-specific details are only exposed to the cloud
-operators.
-
-Specifically, this will:
-
-* add the nova.virt.ironic driver, which will use the python-ironicclient
-  library to interact with Ironic's REST API for the purpose of provisioning
-  physical machines.
-
-* add new IronicHostManager class, similar to BaremetalHostManager, which
-  fills the same purpose but is specific to Ironic. Namely, this provides
-  several customizations to Nova's HostManager, tailoring it to consuming
-  discrete and non-subdivisible physical resources.
-
-* add exact-match scheduler filters, to facilitate users who wish to match
-  nova flavor to hardware specifications exactly. The best matching possible
-  today is greater-than-or-equal, which is often undesirable (eg. because
-  a machine with 128GB of RAM could be selected to fulfil a request for an
-  instance with 16GB of RAM).
-
-This driver will initially implement a subset of the Nova virt driver API
-sufficient to support the same functionality that the nova.virt.baremetal
-driver supported. Over time, additional functionality will be added, as
-appropriate and possible for physical hardware. It is expected that some
-operations may never be added to this driver, eg when the operation is not
-possible where there is no local hypervisor.
-
-This driver will expose the complete resources of the ironic service it is
-connected to. Therefor, running multiple nova-compute processes within a
-single cell or region will not be possible, and HA for the nova-compute
-service must be achieved externally, eg. via pacemaker+corosync. Scale-out
-may be achieved by running multiple ironic clusters, with a single n-cpu
-connected to each ironic end-point. This is not optimal, and is a result
-of a current limitation within Nova. See the Alternatives section below
-for a summary of the discussion which has occurred around this limitation.
-
-Alternatives
-------------
-
-One alternative would be for users to directly interact with Ironic's API,
-circumventing Nova when deploying instances to bare metal. This would require
-Ironic to duplicate a significant amount of functionality present in Nova, and
-violate the abstraction layer. Note that giving end-users direct access to
-Ironic's API may present security concerns for some operators; see the
-Security Impact section below for a discussion of this.
-
-Instead of creating a new IronicHostManager class, the existing
-BaremetalHostManager class could be refactored to support both drivers.
-
-Instead of adding exact-match scheduler filters, we could create a new
-scheduler that is specifically geared towards non-divisible resources.
-However, this approach is sufficient for many use cases, and does not prevent
-the later creation of another scheduler.
-
-An alternative was proposed which would allow multiple nova-compute processes
-to proxy for the same Ironic service end-point at the same time. This could be
-done by setting the same 'host' property on each nova-compute service, such
-that they expose the same set of resources.  Therefor, certain operations would
-need to be skipped when starting the nova-compute process (eg, so it doesn't
-trample over an ongoing operation on another compute host). This would be
-accomplished by creating a new ClusteredComputeManager class (subclassed from
-ComputeManager) which would override init_host() to avoid the call to
-InstanceList.get_by_host(), self._destroy_evacuated_instances() and
-self._init_instance(). This proposal was denied due to architectural concerns
-within Nova, specifically around @utils.synchronize(instance['uuid']) calls,
-and event callbacks that could be routed to a host other than the one waiting
-for the callback.
-
-Instead of overriding ComputeManager.init_host(), a significant rewrite of
-Nova's internal resource model could be undertaken -- eg, to remove the (host,
-hypervisor_hostname) tuple from all places within the code and make the
-nova-conductor process handle resource locking for clustered hypervisors, such
-as Ironic.  This would be a significant undertaking, and while merited, it was
-agreed that this work would not block the Ironic driver.
-
-
-Data model impact
------------------
-
-Adding the nova.virt.ironic driver will not impact the db model.
-
-Some additional extra_specs may be leveraged in faciliating better scheduling
-in the future. There is precedent in the way that the nova.virt.baremetal
-driver leverages extra_specs:baremetal:cpu_arch. Ironic may extend this to
-support additional hardware metadata in the future.
-
-REST API impact
----------------
-
-The nova.virt.ironic driver will not add any REST API extensions or require
-changes to any of Nova's APIs.
-
-Security impact
----------------
-
-Allowing Nova to provision physical hardware has significant security
-implications. The nova.virt.baremetal driver required direct access to the OOB
-management (IPMI) network of the hardware it managed. A compromise of
-nova-compute would expose that hardware's management interface.
-
-In a properly-secured OpenStack deployment, security will be improved by moving
-this functionality out of nova-compute and into ironic-conductor, because there
-is a strict API between the two services.
-
-The ironic-api should not be reachable or discoverable by end-users, and only
-the ironic-conductor service should have access to the hardware management
-interface.  The user of Nova who requests an instance on bare metal will thus
-have no direct access to the services managing that bare metal host. Should
-nova-compute be compromised, the malicious user would still need to gain access
-to the ironic-conductor host before having any access to the hardware
-management interface.
-
-Considering how often IPMI is not properly secured, and that in many cases it
-can not be secured, the OOB management network should be as isolated from users
-as possible.
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-No impact on Nova itself.
-
-The performance profile of the nova.virt.ironic driver will be different than
-other virt drivers due to the nature of managing physical machines. For
-example, power cycling bare metal often takes more than five minutes as the
-hardware must complete a POST cycle. Thus, a deploy may be expected to take a
-minimum of ten minutes, though depending on the hardware, it may be more or
-less.
-
-Other deployer impact
----------------------
-
-Deploying Nova with the nova.virt.ironic driver will be considerably different
-to deploying Nova with other virt drivers, and also different from the
-nova.virt.baremetal driver. Main areas of difference are:
-
-* different system libraries will be required. No local hypervisor needs be
-  installed, and none of the system libraries to enable baremetal need to be
-  installed on the compute host itself.
-
-* the OpenStack Ironic services must be properly set up and discoverable
-  via Keystone in order for the nova.virt.ironic driver to function properly.
-
-* Nova must be supplied with admin credentials capable of interacting
-  with Ironic.
-
-An upgrade path from the nova.virt.baremetal driver to the nova.virt.ironic
-driver will be provided. The details of that are proposed in another document:
-
-  https://blueprints.launchpad.net/nova/+spec/deprecate-baremetal-driver
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-
-Primary assignee:
-  devananda
-
-Other contributors:
-  lucasagomes
-  nobodycam
-
-Work Items
-----------
-
-* Merge auxiliary components: HostManager and exact-match scheduler filters
-
-* Delete auxiliary components from Ironic's tree
-
-* Split the nova.virt.ironic driver into a series of patches, the sum of
-  which will pass unit and functional tests.
-
-* Delete driver from Ironic's tree after it has merged in Nova.
-
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-There is already tempest testing being done upstream against changes in
-ironic, nova, devstack, and tempest. However, it is non-voting today.
-The following paragraph describes how it works.
-
-Devstack creates a "mock" bare metal node, enrolls it with Ironic, and
-configures Nova appropriately to use the nova.virt.ironic driver. A tempest
-scenario test is then run against that devstack instance, which allows tempest
-to test functionality appropriate for this driver. Certain tests may be
-excluded when the functionality does not apply to bare metal (eg,
-live migrate). The current test is fairly simple: validate the boot process,
-network connectivity of the instance, and validate destroy. Additional tests
-have been proposed for more coverage, eg. "rebuild --preserve-ephemeral".
-
-Testing of functionality not exposed via the nova virt driver interface is done
-directly in Tempest via the Ironic API (eg, management operations) and is
-mentioned here only for completeness.
-
-Documentation Impact
-====================
-
-Documentation should be added to Nova stating the existence of the new driver,
-and should include links to the Ironic project's developer and deployer
-documentation.
-
-References
-==========
-
-Current code, in Ironic's git tree::
-  http://git.openstack.org/cgit/openstack/ironic/tree/ironic/nova
-
-Devstack support for testing this driver::
-  http://git.openstack.org/cgit/openstack-dev/devstack/tree/lib/ironic
-  http://git.openstack.org/cgit/openstack-dev/devstack/tree/tools/ironic
-
-Tempest test which deploys using the nova.virt.ironic driver::
-  http://git.openstack.org/cgit/openstack/tempest/tree/tempest/scenario/test_baremetal_basic_ops.py
-
-Juno summit etherpad discussing this::
-  https://etherpad.openstack.org/p/juno-nova-deprecating-baremetal
-
-Some best practices for IPMI sanity::
-  http://fish2.com/ipmi/bp.pdf
-
-Discussions of IPMI vulnerabilities::
-  http://fish2.com/ipmi/itrain.pdf
-  http://fish2.com/ipmi/river.pdf
diff --git a/specs/juno/add-virtio-scsi-bus-for-bdm.rst b/specs/juno/add-virtio-scsi-bus-for-bdm.rst
deleted file mode 100644
index 37058dc..0000000
--- a/specs/juno/add-virtio-scsi-bus-for-bdm.rst
+++ /dev/null
@@ -1,139 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=====================================================
-Add virtio-scsi bus support for block device mapping
-=====================================================
-
-https://blueprints.launchpad.net/nova/+spec/add-virtio-scsi-bus-for-bdm
-
-
-VirtIO SCSI is a new para-virtualized SCSI controller device for KVM instances.
-It has been designed to replace virtio-blk, increase it's performance and
-improve scalability. Currently, using virtio-scsi bus is not supported when
-booting from volume.
-
-
-
-Problem description
-===================
-
-VirtIO SCSI is a new para-virtualized SCSI controller device for KVM instances.
-It has been designed to replace virtio-blk, increase it's performance and
-improve scalability. The interface is capable of handling multiple block
-devices per virtual SCSI adapter, keeps the standard scsi device naming
-in the guests (e.x /dev/sda) and support SCSI devices passthrough.
-
-Currently, virtio-scsi bus has been supported when booting from glance image,
-which is implemented by BP ([1]) aimed to Icehouse.
-
-However, when we create a cinder volume from this image, and then booting
-from this volume and specify the bus_type as "scsi", the guest will still
-use lsi controller instead of virtio-scsi controller.
-
-The aim of this BP as follows:
-
-When booting from volume with "scsi" bus type, use virtio-scsi controller
-for volume which was created from glance image, which is set with
-"virtio-scsi" hw_scsi_model property.
-
-The main use case is to improve performance in I/O-intensive applications.
-
-
-Proposed change
-===============
-
-* Nova retrieve "hw_scsi_model" property from volume's glance_image_metadata
-  when booting from cinder volume
-
-* Libvirt driver will create the "virtio-scsi" controller if "hw_scsi_model"
-  property is "virtio-scsi" and the bus_type specified for volume is "scsi"
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-Will improve guest's performance.
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  <zhangleiqiang@huawei.com>
-
-
-Work Items
-----------
-
-* Nova retrieve "hw_scsi_model" property from volume's glance_image_metadata
-  when booting from cinder volume
-
-* Libvirt driver will create the "virtio-scsi" controller if "hw_scsi_model"
-  property is "virtio-scsi" and the bus_type specified for volume is "scsi"
-
-
-Dependencies
-============
-
-* Depend on the BP [1], which will provide the virtio-scsi-controller object
-
-Testing
-=======
-
-None
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-* [1] https://blueprints.launchpad.net/nova/+spec/libvirt-virtio-scsi-driver
diff --git a/specs/juno/allocation-ratio-to-resource-tracker.rst b/specs/juno/allocation-ratio-to-resource-tracker.rst
deleted file mode 100644
index 2b13ca6..0000000
--- a/specs/juno/allocation-ratio-to-resource-tracker.rst
+++ /dev/null
@@ -1,163 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Move allocation ratios to resource tracker
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/allocation-ratio-to-resource-tracker
-
-Move the definition and calculation of allocation ratios out of the scheduler
-and into the resource tracker.
-
-Problem description
-===================
-
-Allocation ratios are currently improperly defined in the scheduler. This leads
-to efficiency problems due to the scheduler interfacing with the database
-when it does not need to as well as recalculating adjusted resource usage
-numbers when it does not need to.
-
-The memory and CPU allocation ratios are currently controlled on a global or
-per-aggregate basis, with the global configuration settings determined in the
-core_filter and ram_filter filter modules in the scheduler, and the
-per-aggregate allocation ratio overrides are stored in the aggregates table in
-the database, with the core_filter scheduler filter performing repeated lookups
-to the aggregates table to determine the allocation ratio to use when host
-aggregates are in use in the deployment.
-
-Allocation ratios are NOT scheduler policy, and should neither be defined
-nor queried in the scheduler at all. Allocation ratios are simply a way for a
-compute node to advertise that it has the ability to service more resources
-than it physically has available: an overcommit ratio. Therefore, not only does
-the total advertised amount of resources on a compute node NOT need to be
-recalculated on each run of the scheduler to find a compute node for an
-instance, but the resource tracker is the most appropriate place to set the
-available resource amounts.
-
-Proposed change
-===============
-
-We propose to move the definition of CPU and memory allocation ratios out of
-the scheduler filters where they are currently defined (core_filter and
-ram_filter) and into the resource tracker (nova.compute.resource_tracker).
-
-Further, we propose to remove all calculation of the compute node's overcommit
-ratio for both CPU and RAM out of core_filter.py and ram_filter.py.
-
-This calculation will initially be moved into the host_manager.HostManager
-class, which will store the real and adjusted available resource amounts for
-each compute node in its collection of HostState structs. Because the current
-resource tracker in Nova only accounts for a single compute node, we must,
-for now, use the scheduler's internal resource tracker (HostManager) to track
-all compute nodes' allocation ratios.
-
-When constructing and refreshing its collection of these HostState structs, the
-HostManager calls nova.objects.compute_node.ComputeNodeList.get_all(). We will
-amend this particular call to include the host aggregate information for each
-compute node. If the compute node belongs to any host aggregates, then the
-overcommit ratio for CPU and memory shall be either the lowest ratio set for
-any of the host aggregates OR the default global configuration ratio value.
-
-The long-term goal is to enable each compute node in the system to have its
-own settable allocation ratios and remove the need for this particular check
-or calculation in the resource tracker or the scheduler itself. My personal
-end goal is to align the scheduler's internal resource tracker with the code
-in nova.compute.resource_tracker, but this particular blueprint is scoped
-only to the relatively minor changes described above.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-Even with the relatively minor changes introduced here, there should be a
-performance increase for a single scheduler request due to fewer
-calculations being made in each scheduler request. For deployments that
-use host aggregates, performance improvements will be much greater, as
-the number of DB queries per scheduler request will be reduced.
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Work Items
-----------
-
- * Modify nova.objects.compute_node.ComputeNode and ComputeNodeList() to be
-   able to include host aggregate information for the compute node
- * Move the definition of the allocation ratios out of the filters and into
-   nova.compute.resource_tracker, and then import_opt() in
-   nova.scheduler.host_manager to bring in those allocation ratio definitions
- * Change the behavior in HostManager.get_all_host_states() to calculate
-   resources available from either the host aggregate's min ratio or the
-   global conf definition ratio
-
-Assignee(s)
------------
-
-Primary assignee:
-  jaypipes
-
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Some minor adjustments to the existing unit tests would need to be performed.
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-Mailing list discussion:
-
-http://lists.openstack.org/pipermail/openstack-dev/2014-June/036602.html
diff --git a/specs/juno/allow-image-to-be-specified-during-rescue.rst b/specs/juno/allow-image-to-be-specified-during-rescue.rst
deleted file mode 100644
index dab5410..0000000
--- a/specs/juno/allow-image-to-be-specified-during-rescue.rst
+++ /dev/null
@@ -1,204 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Allow image to be specified during rescue
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/allow-image-to-be-specified-during-rescue
-
-In this blueprint we aim to add an additional optional parameter to the
-instance rescue API. This parameter will be used to specify the image to be
-used while rescuing the instance. If the parameter is not specified, the
-instance will be rescued using the base image.
-
-
-Problem description
-===================
-
-The custom image used during rescue might be corrupt, leading to errors,
-or too large, leading to timeouts.
-Also, if the base image is deleted, the image ref on the
-instance_system_metadata will be invalid, leading to the rescue operation
-failing.
-This feature can also be used in the case where the customer wants to rescue
-the instance with a specific image, rather the default one. This would provide
-more flexibility to the feature.
-
-
-Proposed change
-===============
-
-In order to implement this I propose that we allow the user to specify which
-image is to be used for rescue. (could be a default base image or a custom
-image)
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-API for specifying image to be used to rescue an instance:
-
-Scenarios:
-Case 1: If image_ref is specified as part of the rescue request, that image
-will be used.
-Case 2: If image_ref is not specified as part of the rescue request,
-image_base_image_ref on the system_metadata of the instance will be used.
-(Default behavior)
-
-V2 API specification:
-POST: v2/{tenant_id}/servers/{server_id}/action
-
-V3 API specification:
-POST: v3/servers/{server_id}/action
-
-Request parameters:
-* tenant_id: The ID for the tenant or account in a multi-tenancy cloud.
-* server_id: The UUID for the server of interest to you.
-* rescue: Specify the rescue action in the request body.
-* adminPass(Optional): Use this password for the rescued instance.
-Generate a new password if none is provided.
-* rescue_image_ref(Optional): Use this image_ref for rescue.
-
-JSON request:
-{"rescue": {"adminPass": "MySecretPass",
-"rescue_image_ref": "848b39fb-6904-46d6-af3c-baa3eefedffc"}}
-
-JSON response:
-{"adminPass": "MySecretPass"}
-
-Sample v2 request:
-POST: /v2/d1b123/servers/7d14f8123/action -d '{"rescue":
-{"rescue_image_ref": "848b39fb-6904-46d6-af3c-baa3eefedffc"}}'
-
-Sample v3 request:
-POST: /v3/servers/7d14f8123/action -d '{"rescue":
-{"rescue_image_ref": "848b39fb-6904-46d6-af3c-baa3eefedffc"}}'
-
-This would use image with ref "848b39fb-6904-46d6-af3c-baa3eefedffc" to
-rescue instance with uuid "7d14f8123"
-
-JSON schema definition::
-
-    rescue = {
-        'type': 'object',
-        'properties': {
-            'rescue': {
-                'type': ['object', 'null'],
-                'properties': {
-                    'admin_password': parameter_types.admin_password,
-                    'rescue_image_ref': parameter_types.image_ref,
-                },
-                'additionalProperties': False,
-            },
-        },
-        'required': ['rescue'],
-        'additionalProperties': False,
-    }
-
-HTTP response codes:
-v2:
-Normal HTTP Response Code: 200 on success
-v3:
-Normal HTTP Response Code: 202 on success
-(Will check whether these can be made consistent in v2 and v3 during
-implementation.)
-
-Validation:
-'rescue_image_ref' must be of a uuid-str format.
-Failure Response Code: HTTPBadRequest with "Invalid image ref format" message.
-
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-The rescue call in python-novaclient will have to include the additional
-optional parameter
-
-Optional argument:
---rescue_image_ref <image_ref> ID of image to be used for rescue
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-The parameter will be optional, so no other code needs to be changed.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-    aditirav
-
-Work Items
-----------
-
-* Changes to be made to the compute manager rescue method to use the
-  image ref passed in, during the rescue of the instance.
-* Add an extension to the V2 API to make rescue take in the optional parameter
-  'rescue_image_ref
-* Changes to the V3 API to take in the optional parameter 'rescue_image_ref'
-* Include tests in tempest to check the behavior of rescue instance with
-  the image ref passed in through the API call.
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-Tempest tests to be added to check if rescue of the instance uses the image
-specified in the API call.
-
-
-Documentation Impact
-====================
-
-Changes to be made to the rescue API documentation to include the additional
-parameter 'rescue_image_ref' that can be passed in.
-
-
-References
-==========
-
-None
-
diff --git a/specs/juno/approved/add-all-in-list-operator-to-extra-spec-ops.rst b/specs/juno/approved/add-all-in-list-operator-to-extra-spec-ops.rst
new file mode 100644
index 0000000..040c5ec
--- /dev/null
+++ b/specs/juno/approved/add-all-in-list-operator-to-extra-spec-ops.rst
@@ -0,0 +1,152 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================
+Add ALL-IN operator to extra spec ops
+======================================
+
+https://blueprints.launchpad.net/nova/+spec/add-all-in-list-
+operator-to-extra-spec-ops
+
+Allow extra spec to match all values in a list by adding the ALL-IN operator.
+
+
+Problem description
+===================
+
+This blueprint aims to allow querying if ALL of the given values are present
+in a list.
+Currently there's support for an IN operator that returns True if a given
+element is present in a list. There is also an OR operator that
+only works for single values.
+
+Example:
+
+Suppose a flavor needs to be placed on a host that has the cpu flags 'aes'
+and 'vmx'. As it is today is not possible since the only posibility is to
+use the <in> operator. But, as the extra specs is a dict, the flavor
+extra-spec key would be the same:
+
+capabilities:cpu_info:features : <in> aes
+capabilities:cpu_info:features : <in> vmx
+
+Just one of them will be saved.
+
+something like this is needed:
+
+capabilities:cpu_info:features : <all-in> aes vmx
+
+Proposed change
+===============
+
+We need to add the new <all-in> operator and its lambda function to
+_op_methods dict in extra_specs_ops.py.
+
+...
+'<all-in>': lambda x, y: all(val in x for val in y),
+...
+
+Then add a call to this function with a list, instead of with a
+string if there are more than one element in the query.
+
+
+Alternatives
+------------
+
+Instead of add the '<all-in>' operator extend/overload the '<in>' operator to
+work with a list.
+
+capabilities:cpu_info:features : <in> aes vmx
+
+Seems to be easy to understand but could generate confusion because <in>
+operator as it is today, aims to be used to match a substring.
+
+Another possibility is add both <any> and <all> operators. By doing this, we
+are using <in> and <or> for single values and the new set of operators for
+collections values. But something is missing with this approach,
+<all> or <any> what? All elements in a list, all elements are True, or all
+elements are equal to a given value.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Add a new lamda function to
+nova/scheduler/filter/extra_specs_ops.py _ops_method dict:
+
+'<all-in>': lambda x, y: all(val in x for val in y)
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  facundo-n-maldonado
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit tests should be added for the new operator.
+
+Documentation Impact
+====================
+
+Filter scheduler documentation should be updated with the new operator.
+
+References
+==========
+
+None
diff --git a/specs/juno/approved/add-virtio-scsi-bus-for-bdm.rst b/specs/juno/approved/add-virtio-scsi-bus-for-bdm.rst
new file mode 100644
index 0000000..37058dc
--- /dev/null
+++ b/specs/juno/approved/add-virtio-scsi-bus-for-bdm.rst
@@ -0,0 +1,139 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================================
+Add virtio-scsi bus support for block device mapping
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/add-virtio-scsi-bus-for-bdm
+
+
+VirtIO SCSI is a new para-virtualized SCSI controller device for KVM instances.
+It has been designed to replace virtio-blk, increase it's performance and
+improve scalability. Currently, using virtio-scsi bus is not supported when
+booting from volume.
+
+
+
+Problem description
+===================
+
+VirtIO SCSI is a new para-virtualized SCSI controller device for KVM instances.
+It has been designed to replace virtio-blk, increase it's performance and
+improve scalability. The interface is capable of handling multiple block
+devices per virtual SCSI adapter, keeps the standard scsi device naming
+in the guests (e.x /dev/sda) and support SCSI devices passthrough.
+
+Currently, virtio-scsi bus has been supported when booting from glance image,
+which is implemented by BP ([1]) aimed to Icehouse.
+
+However, when we create a cinder volume from this image, and then booting
+from this volume and specify the bus_type as "scsi", the guest will still
+use lsi controller instead of virtio-scsi controller.
+
+The aim of this BP as follows:
+
+When booting from volume with "scsi" bus type, use virtio-scsi controller
+for volume which was created from glance image, which is set with
+"virtio-scsi" hw_scsi_model property.
+
+The main use case is to improve performance in I/O-intensive applications.
+
+
+Proposed change
+===============
+
+* Nova retrieve "hw_scsi_model" property from volume's glance_image_metadata
+  when booting from cinder volume
+
+* Libvirt driver will create the "virtio-scsi" controller if "hw_scsi_model"
+  property is "virtio-scsi" and the bus_type specified for volume is "scsi"
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Will improve guest's performance.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <zhangleiqiang@huawei.com>
+
+
+Work Items
+----------
+
+* Nova retrieve "hw_scsi_model" property from volume's glance_image_metadata
+  when booting from cinder volume
+
+* Libvirt driver will create the "virtio-scsi" controller if "hw_scsi_model"
+  property is "virtio-scsi" and the bus_type specified for volume is "scsi"
+
+
+Dependencies
+============
+
+* Depend on the BP [1], which will provide the virtio-scsi-controller object
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+* [1] https://blueprints.launchpad.net/nova/+spec/libvirt-virtio-scsi-driver
diff --git a/specs/juno/approved/allocation-ratio-to-resource-tracker.rst b/specs/juno/approved/allocation-ratio-to-resource-tracker.rst
new file mode 100644
index 0000000..2b13ca6
--- /dev/null
+++ b/specs/juno/approved/allocation-ratio-to-resource-tracker.rst
@@ -0,0 +1,163 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Move allocation ratios to resource tracker
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/allocation-ratio-to-resource-tracker
+
+Move the definition and calculation of allocation ratios out of the scheduler
+and into the resource tracker.
+
+Problem description
+===================
+
+Allocation ratios are currently improperly defined in the scheduler. This leads
+to efficiency problems due to the scheduler interfacing with the database
+when it does not need to as well as recalculating adjusted resource usage
+numbers when it does not need to.
+
+The memory and CPU allocation ratios are currently controlled on a global or
+per-aggregate basis, with the global configuration settings determined in the
+core_filter and ram_filter filter modules in the scheduler, and the
+per-aggregate allocation ratio overrides are stored in the aggregates table in
+the database, with the core_filter scheduler filter performing repeated lookups
+to the aggregates table to determine the allocation ratio to use when host
+aggregates are in use in the deployment.
+
+Allocation ratios are NOT scheduler policy, and should neither be defined
+nor queried in the scheduler at all. Allocation ratios are simply a way for a
+compute node to advertise that it has the ability to service more resources
+than it physically has available: an overcommit ratio. Therefore, not only does
+the total advertised amount of resources on a compute node NOT need to be
+recalculated on each run of the scheduler to find a compute node for an
+instance, but the resource tracker is the most appropriate place to set the
+available resource amounts.
+
+Proposed change
+===============
+
+We propose to move the definition of CPU and memory allocation ratios out of
+the scheduler filters where they are currently defined (core_filter and
+ram_filter) and into the resource tracker (nova.compute.resource_tracker).
+
+Further, we propose to remove all calculation of the compute node's overcommit
+ratio for both CPU and RAM out of core_filter.py and ram_filter.py.
+
+This calculation will initially be moved into the host_manager.HostManager
+class, which will store the real and adjusted available resource amounts for
+each compute node in its collection of HostState structs. Because the current
+resource tracker in Nova only accounts for a single compute node, we must,
+for now, use the scheduler's internal resource tracker (HostManager) to track
+all compute nodes' allocation ratios.
+
+When constructing and refreshing its collection of these HostState structs, the
+HostManager calls nova.objects.compute_node.ComputeNodeList.get_all(). We will
+amend this particular call to include the host aggregate information for each
+compute node. If the compute node belongs to any host aggregates, then the
+overcommit ratio for CPU and memory shall be either the lowest ratio set for
+any of the host aggregates OR the default global configuration ratio value.
+
+The long-term goal is to enable each compute node in the system to have its
+own settable allocation ratios and remove the need for this particular check
+or calculation in the resource tracker or the scheduler itself. My personal
+end goal is to align the scheduler's internal resource tracker with the code
+in nova.compute.resource_tracker, but this particular blueprint is scoped
+only to the relatively minor changes described above.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Even with the relatively minor changes introduced here, there should be a
+performance increase for a single scheduler request due to fewer
+calculations being made in each scheduler request. For deployments that
+use host aggregates, performance improvements will be much greater, as
+the number of DB queries per scheduler request will be reduced.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Work Items
+----------
+
+ * Modify nova.objects.compute_node.ComputeNode and ComputeNodeList() to be
+   able to include host aggregate information for the compute node
+ * Move the definition of the allocation ratios out of the filters and into
+   nova.compute.resource_tracker, and then import_opt() in
+   nova.scheduler.host_manager to bring in those allocation ratio definitions
+ * Change the behavior in HostManager.get_all_host_states() to calculate
+   resources available from either the host aggregate's min ratio or the
+   global conf definition ratio
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jaypipes
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Some minor adjustments to the existing unit tests would need to be performed.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+Mailing list discussion:
+
+http://lists.openstack.org/pipermail/openstack-dev/2014-June/036602.html
diff --git a/specs/juno/approved/clean-logs.rst b/specs/juno/approved/clean-logs.rst
new file mode 100644
index 0000000..2469029
--- /dev/null
+++ b/specs/juno/approved/clean-logs.rst
@@ -0,0 +1,149 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================================
+Nova logs shouldn't have ERRORs or TRACEs in them
+==================================================
+
+https://blueprints.launchpad.net/nova/+spec/clean-logs
+
+Nova logs should be readable, sensible, and contain only errors and
+traces in exceptional situations.
+
+Problem description
+===================
+
+During a normal, successful, run of Tempest in the OpenStack gate we
+get a large number of ERRORs and stack traces in the logs. This is for
+passing results, which means the cloud should have been operating
+normally.
+
+Stack traces and errors in the logs under normal conditions make it
+very difficult for operators to actually determine when real issues
+are happening with their OpenStack cloud. We've seen this even as part
+of normal development where people will be tricked in debugging
+OpenStack issues by the ERRORs, when the real issue is masked.
+
+Proposed change
+===============
+We should clean up all the instances of Stack Traces and Errors
+happening under a normal Tempest run. This means addressing the bugs
+that this currently exposes, as well as changing some logging levels
+where we are logging Exceptions at log.exception level that are
+actually expected (and thus should be a log.debug or deleted
+entirely).
+
+See Testing section below for completion criteria.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+This will change some log messages for clarity. Users that built
+filters around the old error messages will have to adjust their
+filters. As they were probably filtering those messages out, this
+should be minimal.
+
+Performance Impact
+------------------
+
+None. (Possibly miniscule, and largely undetectable, boost because of
+not dumping stack traces so much.)
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+Developers will have to be more careful about doing arbitrary
+log.exception calls inside Nova code once this is enforcing, and will
+need to be more careful on catching appropriate exceptions for
+expected conditions.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  sdague
+
+Work Items
+----------
+
+The services should be tackled in this order (consider cleaning of
+each one a work item):
+
+ * n-sched
+ * n-net
+ * n-api
+ * n-cpu
+
+n-sched and n-net are currently the most critical to clean up as they
+are services that surface testing don't hit directly (only indirectly
+through n-api calls). Ensuring that they don't have unexpect behavior
+in dumping stack traces will provide extra verification that those
+services are working as expected.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Testing will be accomplished by the tempest check_logs.py script
+currently running in the gate. Once we are confident that we have
+cleaned up a service, we remove that service from the allowed_dirty
+list
+https://github.com/openstack/tempest/blob/master/tools/check_logs.py#L33. After
+that any change which causes there to be a stack trace or error in the
+logs for that service will cause the tempest tests to fail, thus
+blocking the change from merging.
+
+Documentation Impact
+====================
+
+There will be a related effort in overall logging standards (to be
+presented as a Juno cross project session) that will need to be
+fleshed out in conjunction with this.
+
+References
+==========
+
+ * Initial thread on Log Harmonization -
+   http://lists.openstack.org/pipermail/openstack-dev/2013-October/017300.html
diff --git a/specs/juno/approved/cold-migration-with-target.rst b/specs/juno/approved/cold-migration-with-target.rst
new file mode 100644
index 0000000..ca40a6f
--- /dev/null
+++ b/specs/juno/approved/cold-migration-with-target.rst
@@ -0,0 +1,145 @@
+
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================
+Enable cold migration with target host
+===========================================
+
+https://blueprints.launchpad.net/nova/+spec/cold-migration-with-target
+
+The aim of this feature is to let operators cold migrate instances with
+target host manually.
+
+
+Problem description
+===================
+
+I have a customized HA plugin which automatically performs migrations under
+certain conditions, and the HA plugin is able to intelligently pick up
+destinations, but only live migrations support specifying destinations.
+
+At the moment cold migration do not support migrate a VM instance with target
+host, this blueprint want to add this feature to nova so that above scenario
+can be satisified.
+
+It also make cold migration consistent with live-migrate operations as live
+migration support migration with and w/o target host.
+
+
+Proposed change
+===============
+
+Modify the current resize_instance flow to let the api can specify the target
+host for cold migration.
+
+
+Alternatives
+------------
+None
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+
+* For V2 API, a new extension will be added as:
+  alias: os-extended-admin-actions
+  name: ExtendedAdminActions
+  namespace:
+  http://docs.openstack.org/compute/ext/extended_admin_actions/api/v1.1
+
+  When the new extension "os-extended-admin-actions" is loaded, the api of
+  _migrate() wil support cold migration with target host.
+
+* For a later microversion of v2.1 API, no new extension needed, the
+  existing cold migration API will be updated to support this.
+
+* URL: existed admin actions extension as:
+       * /v2/{tenant_id}/servers/actions:
+       * /v2.1/servers/actions:
+
+  JSON request body::
+
+    {
+        "migrate":
+        {
+            "host": "fake_host"
+        }
+    }
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+
+python-novaclient will be modified to have target_host argument as
+optional.
+
+The user can trigger this feature by:
+nova migrate my_server target_host
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jay-lau-513
+
+Work Items
+----------
+
+* Add logic to select target host for cold migration
+* Add API v2/v2.1
+* Set target host optional on nova-client
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Add unit test in nova to cover the case of cold migration with target host,
+also we probably need to think about adding functionnal tests in tempest.
+
+
+Documentation Impact
+====================
+
+* Api Docs to reflect that target host field is optional.
+* Client docs ( due to optional arg)
+* Admin User Guide on cold migration topic.
+
+
+References
+==========
+None
diff --git a/specs/juno/approved/db2-database.rst b/specs/juno/approved/db2-database.rst
new file mode 100644
index 0000000..e5014ad
--- /dev/null
+++ b/specs/juno/approved/db2-database.rst
@@ -0,0 +1,267 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================
+Add Support for DB2 (v10.5+)
+=====================================
+
+https://blueprints.launchpad.net/nova/+spec/db2-database
+
+The community currently supports MySQL and PostgreSQL production databases.
+Several other core projects already support DB2 (Keystone, Glance, Neutron,
+Ceilometer, Heat). This blueprint adds support to Nova for DB2 as a production
+database.
+
+
+Problem description
+===================
+
+* Currently there is no support in the community for a deployer to run Nova
+  against a DB2 backend database.
+
+* For anyone running applications against an existing DB2 database that wants
+  to move to OpenStack, they'd have to use a different database engine to
+  run Nova in OpenStack.
+
+* There is currently an inconsistent support matrix across the core projects
+  since the majority of core projects support DB2 but Nova does not yet.
+
+
+Proposed change
+===============
+
+Add code to support migrating the Nova database against a DB2 backend. This
+would require a fresh deployment of Nova since there are no plans to migrate
+an existing Nova database from another engine, e.g. MySQL, to DB2.
+
+Unit test code would also be updated to support running tests against a DB2
+backend with the ibm_db_sa driver and all Nova patches will be tested against a
+Tempest full run with 3rd party CI running DB2 that IBM will maintain.
+
+There is already some code in Oslo's db.api layer to support common function
+with DB2 like duplicate entry error handling and connection trace, so that is
+not part of this spec.
+
+Alternatives
+------------
+
+Deployers can use other supported database backends like MySQL or PostgreSQL,
+but this may not be an ideal option for customers already running applications
+with DB2 that want to integrate with OpenStack. In addition, you could run
+other core projects with multiple schemas in a single DB2 OpenStack database,
+but you'd have to run Nova separately which is a maintenance/configuration
+problem.
+
+Data model impact
+-----------------
+
+#. The 216 migration will be updated to handle conditions with DB2 like index
+   and foreign key creation. The main issue here is that DB2 does not support
+   unique constraints over nullable columns, it will instead create a unique
+   index that excludes null keys. Most unique constraints created in Nova are
+   on non-nullable columns, but the instances.uuid column is nullable and the
+   216 migration creates a unique index on it, but this will not allow any
+   foreign keys on the instances.uuid column to be created with DB2 since the
+   reference column has to be a unique or primary key constraint.
+#. In order to support creating the same foreign keys that reference the
+   instances.uuid column as other database engines, the instances.uuid column
+   must be made non-nullable and a unique constraint must be created on it.
+   The dependent blueprint "Enforce unique instance uuid in data model" is
+   used to handle this change.
+#. Finally, add another migration script which creates the previously excluded
+   foreign keys from the 216 migration script for DB2.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+The only performance impact on existing deployments is in the migration
+script changes which would be tested with turbo-hipster.
+
+Other deployer impact
+---------------------
+
+The new database migration which creates the missing foreign keys since the
+control node needs to be down when running the migration. However, the new
+migration only creates foreign keys if the backend is DB2, which would be a new
+installation as noted in the "Proposed change" section so the impact should be
+minimal.
+
+Developer impact
+----------------
+
+The only impact on developers is if they are adding DB API code or migrations
+that do not work with DB2 they will have to adjust those appropriately, just
+like we do today with MySQL and PostgreSQL. IBM active technical contributors
+would provide support/guidance on issues like this which require specific
+conditions for DB2, although for the most part the DB2 InfoCenter provides
+adequate detail on how to work with the engine and provides details on error
+codes.
+
+* DB2 SQL error message explanations can be found here:
+  http://pic.dhe.ibm.com/infocenter/db2luw/v10r5/index.jsp?topic=%2Fcom.ibm.db2.luw.messages.sql.doc%2Fdoc%2Frsqlmsg.html
+
+* Information on developing with DB2 using python can be found here:
+  http://pic.dhe.ibm.com/infocenter/db2luw/v10r5/index.jsp?topic=%2Fcom.ibm.swg.im.dbclient.python.doc%2Fdoc%2Fc0054366.html
+
+* Main contacts for DB2 questions in OpenStack:
+
+   * Matt Riedemann (mriedem@us.ibm.com) - Nova core member
+   * Brant Knudson (bknudson@us.ibm.com) - Keystone core member
+   * Jay Bryant (jsbryant@us.ibm.com) - Cinder core member
+   * Rahul Priyadarshi (rahul.priyadarshi@in.ibm.com) - ibm_db_sa maintainer
+
+* The DB2 CI wiki page also provides contact information for issues with third
+  party testing failures:
+  https://wiki.openstack.org/wiki/IBM/DB2-TEST
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  mriedem@us.ibm.com
+
+Work Items
+----------
+
+#. Change the 216 migration to work with DB2.
+#. Add a new migration to create the excluded foreign keys from the 216 script
+   for DB2.
+#. Make the test_migrations.py module work with a configured DB2 backend for
+   running unit tests.
+
+See the WIP patch for details: https://review.openstack.org/#/c/69047/
+
+
+Dependencies
+============
+
+* Blueprint "Enforce unique instance uuid in data model":
+  https://blueprints.launchpad.net/nova/+spec/enforce-unique-instance-uuid-in-db
+
+* DB2 10.5 support was added to sqlalchemy-migrate 0.9 during Icehouse:
+  https://blueprints.launchpad.net/sqlalchemy-migrate/+spec/add-db2-support
+
+* There are no requirements changes in Nova for the unit tests to work. The
+  runtime requirements are the ibm-db-sa and ibm_db modules, which are both
+  available from pypi. sqlalchemy-migrate optionally imports ibm-db-sa. The
+  ibm-db-sa module requires a natively compiled ibm_db which has the c binding
+  that talks to the DB2 ODBC/CLI driver.
+
+* Note that only DB2 10.5+ is supported since that's what added unique index
+  support over nullable columns which is how sqlalchemy-migrate handles unique
+  constraints over nullable columns.
+
+
+Testing
+=======
+
+There are three types of testing requirements, Tempest, unit test and
+turbo-hipster performance/scale tests. Each have different timelines for when
+they are proposed to be implemented.
+
+* IBM is already running 3rd party CI for DB2 on the existing Nova WIP patch
+  that adds DB2 support. The same 3rd party CI is running against all
+  sqlalchemy-migrate changes with DB2 on py26/py27 and runs Tempest against
+  Keystone/Glance/Cinder/Heat patches with a DB2 backend. Once the DB2 support
+  is merged the DB2 3rd party CI would run against all Nova patches with a full
+  Tempest run. This is considered required testing for this blueprint to merge
+  in the Juno release.
+
+* While code will be added to make the Nova unit tests work against a DB2
+  backend, running Nova unit tests against DB2 with third party CI is not
+  considered in the scope of this blueprint for Juno, but long-term this is
+  something IBM wants to get running for additional QA coverage for DB2 in
+  Nova. This is something that would be worked on after getting Tempest
+  running. The plan for delivering third party unit test coverage is in the
+  K release.
+
+* Running 3rd party turbo-hipster CI against DB2 is not in plan for this
+  blueprint in Juno but like running unit tests against DB2 in 3rd party CI,
+  running turbo-hipster against DB2 in 3rd party CI would be a long-term goal
+  for QA and the IBM team will work on that after Tempest is running and after
+  unit test CI is worked on. The plan for delivering third party turbo-hipster
+  performance test coverage is in the K release.
+
+* The proposed penalty for failing to deliver third party unit test and/or
+  turbo-hipster performance test coverage in the K release is that the Nova
+  team will turn off voting/reporting of DB2 third party CI and not allow DB2
+  fixes to Nova until the third party CI is available.
+
+* More discussion in the mailing list here:
+  http://lists.openstack.org/pipermail/openstack-dev/2014-May/035009.html
+
+
+Documentation Impact
+====================
+
+* The install guides in the community do not go into specifics about setting up
+  the database.  The RHEL/Fedora install guide says to use the openstack-db
+  script provided by openstack-utils in RDO which uses MySQL.  The other
+  install guides just say that SQLite3, MySQL and PostgreSQL are widely used
+  databases. So for the install guides, those generic statements about
+  supported databases would be updated to add DB2 to the list. Similar generic
+  statements are also made in the following places which would be updated as
+  well:
+
+   * http://docs.openstack.org/training-guides/content/developer-getting-started.html
+   * http://docs.openstack.org/admin-guide-cloud/content/compute-service.html
+   * http://docs.openstack.org/trunk/openstack-ops/content/cloud_controller_design.html
+
+* There are database topics in the security guide, chapters 32-34, so there
+  would be DB2 considerations there as well, specifically:
+
+   * http://docs.openstack.org/security-guide/content/ch041_database-backend-considerations.html
+   * http://docs.openstack.org/security-guide/content/ch042_database-overview.html
+   * http://docs.openstack.org/security-guide/content/ch043_database-transport-security.html
+
+
+References
+==========
+
+* Work in progress nova patch: https://review.openstack.org/#/c/69047/
+
+* "Enforce unique instance uuid in data model" blueprint spec review:
+  https://review.openstack.org/#/c/97300/
+
+* There are Chef cookbooks on stackforge which support configuring OpenStack
+  to run with an existing DB2 installation:
+  http://git.openstack.org/cgit/stackforge/cookbook-openstack-common/
+
+* Mailing list thread on third party testing:
+  http://lists.openstack.org/pipermail/openstack-dev/2014-May/035009.html
+
+* DB2 10.5 InfoCenter: http://pic.dhe.ibm.com/infocenter/db2luw/v10r5/index.jsp
+
+* Some older manual setup instructions for DB2 with OpenStack:
+  http://www.ibm.com/developerworks/cloud/library/cl-openstackdb2/index.html
+
+* ibm-db-sa: https://code.google.com/p/ibm-db/source/clones?repo=ibm-db-sa
+
+* DB2 Third Party CI Wiki: https://wiki.openstack.org/wiki/IBM/DB2-TEST
diff --git a/specs/juno/approved/ec2-volume-and-snapshot-tags.rst b/specs/juno/approved/ec2-volume-and-snapshot-tags.rst
new file mode 100644
index 0000000..dc4e0ff
--- /dev/null
+++ b/specs/juno/approved/ec2-volume-and-snapshot-tags.rst
@@ -0,0 +1,161 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================================
+Tags support in EC2 API for volumes and volume snapshots
+========================================================
+
+https://blueprints.launchpad.net/nova/+spec/ec2-volume-and-snapshot-tags
+
+Expose volume and volume snapshot metadata as EC2 tags in the EC2 API.
+
+Problem description
+===================
+
+OpenStack's EC2 API has little support for 'tags' (resource metadata).
+Only instance metadata are exposed in the EC2 API, so a user
+can create, delete and list only instance metadata. OpenStack Cinder API
+has support for metadata as well, for both volumes and volume snapshots,
+and we just need to expose it into the EC2 API. This blueprint aims to
+do just that.
+
+
+Proposed change
+===============
+
+* EC2 API's 'CreateTags' method only used to work when one is creating
+  tags for an instance resource. After this patch, one will be also
+  able to create tags for volume and volume snapshot resources.
+
+* A user will be able to call the 'DeleteTags' API to delete any tag
+  associated with a volume or a volume snapshot.
+
+* While calling the 'DescribeTags' API, tags of volumes and volume
+  snapshots will be listed along with instance tags (provided tags
+  for these resources are present, obviously).
+
+* Support for specifying volume and volume snapshot IDs, and 'volume'
+  and 'snapshot' as resources as parameters while calling 'DescribeTags'
+  is added.
+
+* As this is the first time the supported resources for tags are becoming
+  plural in number, the code is made more generic so as to allow addition
+  of further resources easier.
+
+* Implementation detail: In the DescribeInstances API, user can specify
+  both resource ID and resource type as filters. If the query says filter
+  by resource IDs (vol-00000001 and ami-00000001) and also filter by
+  resource type (instances and volumes), the current implementation takes
+  the intersection of the resources (volumes in this case) and then checks
+  if those resources are implemented.
+
+Alternatives
+------------
+
+Alternative is: EC2 tags be different from volume tags by using scoped keys.
+So a user creating a tag stack=beta in EC2 API will, in the Cinder API, see
+it as EC2:stack=beta. This way, a user using the Cinder APIs will be able
+to clearly see which metadata entries are created using EC2 API and which
+are created by the OpenStack API.
+
+I think it makes sense to keep the EC2 API layer as transparent as possible.
+This means not going with the alternative proposed above. This also falls in
+line with what we have presently for instance metadata.
+
+Regarding the implementation detail specified above, an alternative is:
+Do not allow resource IDs and resource type to be specified in the same
+query.
+
+There is no doc of AWS which says such an API call is not allowed (atleast I
+can't find it). This implementation is easier, but IMO the way in which
+it is implemented right now gives a better user experience. Probably logging
+would help if we are dropping out a resource ID or resource type from the
+query.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+Only EC2 API will be affected. Affected API calls are: CreateTags, DeleteTags
+DescribeTags.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Insignificant. Note that in the case of DescribeTags, as we keep on adding
+resources, an API call will be made to all of them (e.g. Glance, Cinder, etc)
+when a DescribeTags call is made without specifying a resource type.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  rushiagr (Rushi Agrawal)
+
+Work Items
+----------
+
+* Implement support for volume and snapshot tags.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Comprehensive unit tests to test the functionality will be written.
+
+Documentation Impact
+====================
+
+EC2 API document should be updated to reflect the changes done to the EC2 API
+under this blueprint.
+
+
+References
+==========
+
+EC2 API reference:
+* CreateTags http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-CreateTags.html
+* DeleteTags http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-DeleteTags.html
+* DescribeTags http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-DescribeTags.html
diff --git a/specs/juno/approved/encryption-with-barbican.rst b/specs/juno/approved/encryption-with-barbican.rst
new file mode 100644
index 0000000..bd3be5a
--- /dev/null
+++ b/specs/juno/approved/encryption-with-barbican.rst
@@ -0,0 +1,162 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================
+Make key manager interface interoperable with Barbican
+======================================================
+
+URL to Launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/encryption-with-barbican
+
+The volume encryption feature added in the Havana release currently can only
+operate with a single key that is hardcoded in.  A much more flexible and
+secure solution would be to generate and store keys in Barbican, a cohesive and
+secure Linux-based key management system
+https://github.com/cloudkeep/barbican/wiki, which is now in the OpenStack
+incubation process.
+
+
+Problem description
+===================
+
+Problem 1: The OpenStack Volume Encryption feature currently cannot provide its
+designed level of security due to the absence of a key management service.
+Only a placeholder is available now, which isn't sufficient for the volume
+encryption feature to be used in an enterprise environment.  Keys cannot be
+stored, and only one hard-coded key is presented for all volumes. The proposed
+outcome would provide the ability to create and safely store dedicated keys for
+individual users or tenants.
+
+Problem 2: An ephemeral disk encryption feature supporting LVM was not accepted
+into the Icehouse release due to the lack of a key manager. For security
+reasons, since the disk is in close proximity to the virtual host, ephemeral
+disk encryption must use a key that's safely stored outside of the virtual host
+environment.
+
+An enterprise-grade key manager is needed for both cases, and Barbican
+(approved for incubation on 3/10/14) is becoming the default key manager that
+is slated to support OpenStack volume encryption, ephemeral disk storage
+encryption, and other potential security features.
+https://wiki.openstack.org/wiki/Barbican/Incubation. In order for Barbican to
+support these two storage encryption features, an interface between the
+existing key manager interface (nova/keymgr/key_mgr.py) used for volume
+encryption and the Barbican key manager needs to be developed.
+
+
+Proposed change
+===============
+
+Create an interface that will call python-barbicanclient, allowing Barbican to
+securely generate, store, and present encryption keys to Nova in support of the
+volume encryption feature.  The adapter will be a modification of the present
+key management abstraction layer in the volume encryption feature supporting
+block storage encryption on Cinder and ephemeral disk encryption.
+
+Alternatives
+------------
+
+Instead of implementing the existing key manager interface,
+python-barbicanclient could be invoked directly, but the additional indirection
+allows more extensibility if a different key manager needs to be integrated
+later.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+Use of a bonafide key manager greatly improves the security posture of the
+volume encryption and upcoming ephemeral disk encryption features.  When each
+user or tenant use a unique key instead of a common key, and when it is stored
+in a separate server, it will be much more difficult for an attacker to access
+stored encrypted data owned by a user or group of collective users within a
+tenant.
+
+Though the wrapper will be handling encryption keys, the security risk is
+considered minimal since the host must be trusted, and the wrapper is only
+holding the key temporarily.
+
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+The additional storage write and read time to initially query Barbican for the
+encryption key should be negligible.
+
+Other deployer impact
+---------------------
+
+Assuming that Barbican is the default key manager, then no impact.  If it's not
+the default, then a configuration flag in Nova will need to be added.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  hadi-esiely-barrera
+
+Other contributors:
+  brianna-poulos
+  bruce-benjamin
+
+Work Items
+----------
+
+Develop simple translation of existing key manager interface methods (e.g.,
+get_key) into the corresponding python-barbicanclient calls.
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest testing should be performed to ensure that the wrapper works correctly.
+
+
+Documentation Impact
+====================
+
+The use of Barbican as the default key manager for the storage encryption will
+need to be documented.
+
+
+References
+==========
+
+None
+
diff --git a/specs/juno/approved/enforce-unique-instance-uuid-in-db.rst b/specs/juno/approved/enforce-unique-instance-uuid-in-db.rst
new file mode 100644
index 0000000..458e7c2
--- /dev/null
+++ b/specs/juno/approved/enforce-unique-instance-uuid-in-db.rst
@@ -0,0 +1,164 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Enforce unique instance uuid in data model
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/enforce-unique-instance-uuid-in-db
+
+The instances.uuid column in the data model is not unique but by definition a
+UUID should be unique, and given how it's used within nova and across other
+openstack services like glance, neutron, ceilometer, etc, it should be unique.
+
+Furthermore, there are Foreign Keys created against instances.uuid so it should
+be unique.
+
+
+Problem description
+===================
+
+* Uniqueness for instances.uuid is not enforced in the data model.
+
+* There are foreign keys created on the instances.uuid column so it should be
+  unique.
+
+
+Proposed change
+===============
+
+Add a database migration that checks for existing records where the
+instances.uuid or related instance_uuid column is NULL and if found, fails the
+migration until those are deleted.
+
+A tool will be provided to scan the database for these records and list them,
+then prompt the user to delete them.  A --force option could also be provided
+in the tool to ignore any prompts and just delete the records if found.
+
+The new migration would be blocked until the records are deleted.  Once there
+are no records left, the migration will make those columns non-nullable via
+SQLAlchemy and create a UniqueConstraint on the instances.uuid column.
+
+Note that the fixed_ips table is the exception here since it can, by design,
+contain NULL instance_uuid records to indicate deallocated and disassociated
+fixed IPs.
+
+Alternatives
+------------
+
+Do nothing and leave the nova objects layer to enforce unique instance uuid
+entries, but this does not help with the foreign key issue in the data model.
+
+Data model impact
+-----------------
+
+#. NULL instances.uuid/instance_uuid records must be deleted, except in table
+   fixed_ips as described above.
+#. The instances.uuid column will be made non-nullable.
+#. A UniqueConstraint will be created on the instances.uuid column.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+The only performance impact on existing deployments is in the migration
+script changes which would be tested with turbo-hipster.
+
+Other deployer impact
+---------------------
+
+The main impacts to deployers are:
+
+#. The biggest impact is the new migration. Migrations are potentially slow and
+   require the controller service to be down when run.
+#. The hope is that existing deployments are not carrying records where
+   instances.uuid or instance_uuid are None so the NULL queries in the new
+   migration script would not yield large result sets. However, the impact to
+   the deployer here is that they would be forced to manually prune those
+   records before the migration can continue. Note that it's expected that
+   those cases are exceptional and they are only the result of an inconsistent
+   database. So finding these records is not expected, but if it is a problem
+   the migration will fail clearly with instructions on how to fix the problem.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  mriedem@us.ibm.com
+
+Work Items
+----------
+
+#. Add a new migration to make instances.uuid non-nullable and put a unique
+   constraint on that column.
+#. Write a tool to check for null instance_uuid records within the database
+   for operators to use before the actual migration.
+
+See the WIP patch for details: https://review.openstack.org/#/c/97946/
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+* Unit tests for the new database migration will be added to stub a database
+  with NULL instance_uuid records to make sure the migration fails when those
+  records are found and then test that when they are removed, the migration
+  completes successfully and the unique constraint is created. Similarly the
+  downgrade path will be unit tested.
+* Unit tests will also be written for the scan tool used to run outside of the
+  actual database migrations. This will mock out the backend database but will
+  be used to test the CLI and logic.
+* It is expected that turbo-hipster will cover scale testing the new migration
+  for MySQL.
+
+
+Documentation Impact
+====================
+
+None.
+
+
+References
+==========
+
+* Work in progress nova patch: https://review.openstack.org/#/c/97946/
+
+* Mailing list thread on making instances.uuid non-nullable:
+  http://lists.openstack.org/pipermail/openstack-dev/2014-March/029467.html
diff --git a/specs/juno/approved/input-output-based-numa-scheduling.rst b/specs/juno/approved/input-output-based-numa-scheduling.rst
new file mode 100644
index 0000000..6c9ce77
--- /dev/null
+++ b/specs/juno/approved/input-output-based-numa-scheduling.rst
@@ -0,0 +1,183 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================
+I/O (PCIe) based NUMA scheduling
+================================
+
+https://blueprints.launchpad.net/nova/+spec/input-output-based-numa-scheduling
+
+I/O based NUMA scheduling will add support for intelligent NUMA node placement
+for guests that have been assigned a host PCI device, avoiding unnecessary
+memory transactions
+
+Problem description
+===================
+
+Currently it is common for virtualisation host platforms to exhibit multi NUMA
+node characteristics.
+
+An optimal configuration would be where the guests assigned PCI device and RAM
+allocation are associated with the same NUMA node. This will ensure there is
+no cross NUMA node memory traffic.
+
+To reach a remote NUMA node the memory request must traverse the inter CPU
+link and use the remote NUMA nodes associated memory controller to access the
+remote node. This incurs a latency penalty on remote NUMA node memory access
+which is not desirable for NFV workloads.
+
+Openstack needs to offer more fine grained control of NUMA configuration in
+order to deliver higher performing, lower latency guest applications. The
+default guest placement policy is to use any available pCPU or NUMA node.
+
+Proposed change
+===============
+
+Libvirt now provides the numa node a PCI device is associated with, we will
+use this information to populate the nova DB. For versions of libvirt that do
+not provide this information we will add a fall back mechanism to query the
+host for this info.
+
+Logic will be added to nova scheduler to allow it decide on which host is best
+able satisfy a guests PCI NUMA node requirements.
+
+Logic, similar to what will be implemented in the nova scheduler will be added
+to the libvirt driver to allow it decide on which NUMA node to place the guest.
+
+Alternatives
+------------
+
+Libvirt supports integration with a NUMA daemon (numad) that monitors NUMA
+topology and usage. It attempts to locate guests for optimum NUMA locality,
+dynamically adjusting to changing system conditions.
+
+This is insufficient because we need this intelligence in nova for host
+selection and node deployment.
+
+Data model impact
+-----------------
+
+The PciDevice model will be extended to add a field identifying the NUMA node
+that PCI device is associated with.
+
+numa_node = Column(Integer, nullable=False, default="-1")
+
+A DB migration script will use ALTER_TABLE to add a new column to the
+pci_devices table in nova DB.
+
+REST API impact
+---------------
+
+There will be no change to the REST API.
+
+Security impact
+---------------
+
+This blueprint does not introduce any new security issues.
+
+Notifications impact
+--------------------
+
+This blueprint does not introduce new notifications.
+
+Other end user impact
+---------------------
+
+This blueprint adds no other end user impact.
+
+Performance Impact
+------------------
+
+The benefits of associating a guests PCI device and RAM allocation with the
+same NUMA node will provides an optimal configuration that will give improved
+I/O throughput and reduced memory latencies, compared with the default libvirt
+guest placement policy.
+
+This feature will add some scheduling overhead, but this overhead will deliver
+improved performance on the host.
+
+The optimisation described here is dependent on the guest CPU and RAM
+allocation being associated with the same NUMA node. This feature is described
+in the "Virt driver guest NUMA node placement & topology" blueprint referenced
+in the dependency section.
+
+Other deployer impact
+---------------------
+
+To use this feature the deployer must use HW that is capable of reporting
+numa related info to the OS.
+
+Developer impact
+----------------
+
+This blueprint will have no developer impact.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    James Chapman
+
+Other contributors:
+    Przemyslaw Czesnowicz
+    Sean Mooney
+    Adrian Hoban
+
+Work Items
+----------
+
+* Add a NUMA node attribute to the pci_device object
+* Use libvirt to discover hosts PCI device NUMA node association
+* Enable nova compute synchronise PCI device NUMA node associations with nova
+  DB
+* Enable libvirt driver configure guests with requested PCI device NUMA node
+  associations
+* Enable the nova scheduler decide on which host is best able to support
+  a guest
+* Enable libvirt driver decide on which NUMA node to place a guest
+
+Dependencies
+============
+
+The blueprint listed below will define a policy used by the scheduler to decide
+on which host to place a guest. We plan to respect this policy while extending
+it to add support for a PCI devices NUMA node association.
+
+Virt driver guest NUMA node placement & topology
+* https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
+
+The blueprint listed below will support use cases requiring SR-IOV NICs to
+participate in neutron managed networks.
+
+Enable a nova instance to be booted up with neutron SRIOV ports
+* https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
+
+Testing
+=======
+
+Scenario tests will be added to validate these modifications.
+
+Documentation Impact
+====================
+
+This feature will not add a new scheduling filter, but as it depends on the bp
+mentioned in the dependency section we will need to extend their filter. We
+will add documentation as required.
+
+References
+==========
+
+Support for NUMA and VCPU topology configuration
+* https://blueprints.launchpad.net/nova/+spec/virt-driver-guest-cpu-memory-placement
+
+Virt driver guest NUMA node placement & topology
+* https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
+
+Enable a nova instance to be booted up with neutron SRIOV ports
+* https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
diff --git a/specs/juno/approved/io-ops-weight.rst b/specs/juno/approved/io-ops-weight.rst
new file mode 100644
index 0000000..849897f
--- /dev/null
+++ b/specs/juno/approved/io-ops-weight.rst
@@ -0,0 +1,132 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================
+Create Nova Scheduler IO Ops Weighter
+=====================================
+
+https://blueprints.launchpad.net/nova/+spec/io-ops-weight
+
+Add a new nova scheduler weighter, sort the filter hosts according to host io
+ops number, aims to booting instances on light workload hosts.
+
+
+Problem description
+===================
+
+Currently, Nova scheduler can use host ram or metrics as hosts weight to choice
+host to booting instance, but have a large free ram host maybe have this many
+or more instances currently in build, resize, snapshot, migrate, rescue or
+unshelve task states, especially in some cases of the ram resource of compute
+hosts is very uneven. For example, We had two compute hosts, they had large
+enough free ram(hostA:64G and hostB:10G) to booting instances, by default Nova
+scheduler always choose hostA to booting instance and don't consider the
+concurrent instance task. The io_ops_filter can filter out the heavy workload
+hosts, but it can't help us to choose a most free compute host to booting.
+Using CONF.scheduler_host_subset_size can spread instances on suitable randomly
+compute hosts, but we think it's better that consider instance io ops as weight
+value.
+
+
+Proposed change
+===============
+
+Create a new scheduler weighter class 'IoOpsWeigher', use host_state.num_io_ops
+as weigh_object. Add a new CONF.io_ops_weight_multiplier, default value is
+-1.0.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+The new code itself will introduce some performance impact, new scheduler
+weighter 'IoOpsWeigher' add new calculation logic about hosts weight value.
+Direct use of the attribute 'num_io_ops' of HostState will not bring a big
+performance impact.
+
+Other deployer impact
+---------------------
+
+* Add a new weighter class 'IoOpsWeighter', it takes effect by default.
+* Add a new config option CONF.io_ops_weight_multiplier in nova.conf, default
+  value is -1.0, positive numbers mean to prior choose heavy workload compute
+  hosts.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <kiwik-chenrui>
+
+Work Items
+----------
+
+* Add new weighter class 'IoOpsWeighter'.
+* Add some unit tests and tempest.
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+New unit tests and tempest about 'IoOpsWeighter' will be added.
+
+
+Documentation Impact
+====================
+
+The docs about 'IoOpsWeighter' need to be drafted and new config option
+'io_ops_weight_multiplier' in nova.conf should be introduced, default value is
+-1.0, negative numbers mean to preference choose light workload compute hosts,
+positive numbers mean to the opposite thing.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/approved/libvirt-driver-class-refactor.rst b/specs/juno/approved/libvirt-driver-class-refactor.rst
new file mode 100644
index 0000000..d626a33
--- /dev/null
+++ b/specs/juno/approved/libvirt-driver-class-refactor.rst
@@ -0,0 +1,222 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================
+Libvirt driver class refactoring
+================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-driver-class-refactor
+
+The libvirt driver.py class is growing ever larger and more complicated.
+There are circular dependencies between this class and other libvirt
+classes. This work aims to split some of the functionality out into
+new classes
+
+Problem description
+===================
+
+The libvirt driver.py class is growing ever larger over time. This is
+increasing the complexity of the code and also resulting in larger
+test suites.
+
+The driver.py class is serving what are really a number of distinct
+use cases. Primarily it is the interface for the compute manager
+class to consume. It also, however, has alot of helper APIs for
+dealing with the libvirt connection and the host operating system,
+as well as helpers for dealing with guest instance configuration.
+A number of these helpers are required by other libvirt modules
+such as the vif, volume and image backend drivers. This has resulted
+in circular dependancies between the driver.py and the other libvirt
+modules. For example, LibvirtDriver uses NWFilterFirewall, but also
+has to pass a 'get_connection' callback so that NWFilterFirewall can
+obtain the libvirt connection from the LibvirtDriver class. There are
+a number of other similar deps.
+
+Proposed change
+===============
+
+The intention is to introduce two new modules to the codebase
+
+* host.py - this will encapsulate access to libvirt and the host
+  operating system state. It will contain a 'Host' class, which
+  manages a single libvirt connection. It will contain the methods
+  for connecting to libvirt, getting lists of domains, querying
+  host performance metrics and so on (see the work-items section
+  for specifics). This is not to be confused with the existing
+  HostState class which is just a trivial helper for the driver
+  'host_state' method.
+
+* guest.py - this will encapsulate interaction with libvirt guest
+  domain instances. It will contain a 'Guest' class, which manages
+  a single libvirt guest domain. It will contain all methods used
+  to construct the guest XML configuration during instance startup
+  that currently live in driver.py
+
+The code for host.py and guest.py will be pulled out of the
+existing driver.py class. Other libvirt modules will be updated
+as needed to access the new APIs. To minimize the risk of creating
+regressions changes to the methods being moved will be minimized,
+to just minor renames & fixups where appropriate.
+
+The intended end result is that none of the modules in the libvirt
+driver directory should need to access the driver.py file. They
+should be able to consume the host.py and guest.py APIs instead,
+thus breaking the circular dependancies. For example the
+NWFilterFirewall class can be given an instance of the Host class
+instead of a callback to LibvirtDriver.
+
+The new structure should also reduce the size of the test_driver.py
+file and make it possible to create simpler, self contained tests
+for the functionality that's in host.py and guest.py, since it will
+be isolated from the overall virt driver API.
+
+It is not anticipated that any configuration parameters will move.
+The high level desire is that the new APIs will not directly use
+any Nova configuration parameters. Instead the LibvirtDriver would
+be responsible for reading the config parameters and then setting
+attributes on the new class or passing method parameters where
+appropriate.
+
+At the end of the work there should be absolutely no functional
+change on the libvirt driver. This is intended to be purely
+refactoring work that is invisible to anyone except the people
+writing libvirt driver code.
+
+Alternatives
+------------
+
+Doing nothing is always an option, but it isn't very appealing
+because it leaves us with an ever growing monster ready to
+devour us at any moment.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+There are liable to be conflicts with any developers who have patches
+touching driver.py or test_driver.py
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  berrange
+
+Work Items
+----------
+
+* Create a host.py module and move the basic connection handling code
+  out of driver.py into the new Host class. This will cover the following
+  methods:
+
+  * _conn_has_min_version
+  * _has_min_version
+  * _native_thread
+  * _dispatch_thread
+  * _event_lifecycle_callback
+  * _queue_Event
+  * _dispatch_events
+  * _init_events_pipe
+  * _init_events
+  * _get_new_connection
+  * _close_callback
+  * _test_connection
+  * _connect
+
+* Move helpers used by HostState out into the Host class. This will
+  cover the following methods
+
+  * _get_vcpu_total
+  * _get_memory_mb_total
+  * _get_vcpu_used
+  * _get_memory_mb_used
+  * _get_hypervisor_type
+  * _get_hypervisor_version
+  * _get_hypervisor_hostname
+  * _get_cpu_info
+  * _get_disk_available_least
+
+* Create a guest.py module and move the code for creating the guest XML
+  configuration out of driver.py into the new Guest class. This will cover
+  the following methods
+
+  * _get_guest_cpu_model_config
+  * _get_guest_cpu_config
+  * _get_guest_disk_config
+  * _get_guest_storage_config
+  * _get_guest_config_sysinfo
+  * _get_guest_pci_device
+  * _get_guest_config
+  * _get_guest_xml
+
+* Move the code for listing domains into the new Host class. This
+  will cover the '_list_instance_domains' method.
+
+* Change NWFilterFirewall and LibvirtBaseVIFDriver so that they
+  accept a 'Host' object instance, instead of requiring a callback
+  to the LibvirtDriver class.
+
+* Anything else that appears relevant to move :-)
+
+Dependencies
+============
+
+* None
+
+Testing
+=======
+
+Since it is intended that there is no functional change in this work,
+the existing test coverage should be sufficient. The existing unit
+tests will need some refactoring as code is moved, and some more unit
+tests will be written where appropriate.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+None
diff --git a/specs/juno/approved/libvirt-sheepdog-backed-instances.rst b/specs/juno/approved/libvirt-sheepdog-backed-instances.rst
new file mode 100644
index 0000000..92d3b89
--- /dev/null
+++ b/specs/juno/approved/libvirt-sheepdog-backed-instances.rst
@@ -0,0 +1,181 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================
+Support Sheepdog ephemeral disks for libvirt driver
+====================================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-sheepdog-backed-instances
+
+Add support for Sheepdog instance ephemeral disks.
+
+Problem description
+===================
+
+Sheepdog block devices can already be attached to QEMU and KVM
+virtual machines.  Nova's libvirt driver supports most of the
+functionality. The only additional changes are to the image-backend
+drivers. Both Glance and Cinder support sheepdog backends, so this
+would complement the efforts made in those projects.
+
+
+Proposed change
+===============
+
+This change would extend several parts of the libvirt driver. In
+general, these changes are very similar to the changes required for
+the RBD driver. These changes would bring Sheepdog support into
+feature-parity with RBD, QEMU and other libvirt image drivers.
+
+- nova.virt.libvirt.driver would be extended with cleanup functions
+  for Sheepdog, in the same way that ``cleanup_rbd_instance`` does
+  for the RBD backend.
+
+- A new ``Image`` subclass class would be added to
+  ``nova.virt.libvirt.imagebackend`` for Sheepdog.
+
+- Helper functions would be added to ``nova.virt.libvirt.utils`` where needed.
+
+- /etc/nova/rootwrap.d/filters would be extended to support rootwrap
+  on the ``dog`` command used to interact with Sheepdog.
+
+- See Deployer impact for configuration changes.
+
+Alternatives
+------------
+
+Cinder has existing support for Sheepdog volumes. One alternative
+is to use that driver and only launch instances from volumes. There
+are two problems with this option. First, it would not support
+instance disks that are deleted after the instance is destroyed.
+Second, for the end-user it requires additional steps to provision
+the volume before the instance is booted.
+
+Data model impact
+-----------------
+None.
+
+REST API impact
+---------------
+
+None. This blueprint makes no REST API changes.
+
+Security impact
+---------------
+Rootwrap of 'dog' command on nova-compute machines.
+
+Notifications impact
+--------------------
+None. No plans for new notifications.
+
+Performance Impact
+------------------
+None.
+
+Other end user impact
+---------------------
+
+None. This blueprint has no impact on python-novaclient or any other
+end-user interface.
+
+Other deployer impact
+---------------------
+
+To use this feature, a deployer must first set up a sheepdog cluster and
+then make several configuration changes to nova on machines running the
+nova-compute process. If Sheepdog is to be used with Glance and Cinder,
+the deployer must also make the appropriate configuration changes for those
+services.
+
+To setup a Sheepdog cluster, follow the install guide provided by
+Sheepdog [1]_. After setting up a Sheepdog cluster, each nova-compute
+process must be configured. The following options must be set under
+the ``[libvirt]`` section of ``nova.conf`` or ``nova-compute.conf``:
+
+- ``images_type=sheepdog`` This must be set to indicate that sheepdog should
+  be used.
+
+- ``images_sheepdog_host=locahost`` Change this if the machine running the
+  nova-compute process is not a member of the sheepdog cluster, or is not
+  acting as a gateway node within the cluster.
+
+- ``images_sheepdog_port=7000``` Change this if the sheep process is listening
+  on a different port than the default.
+
+For sites doing continuous deployment, this change will have no impact until
+the deployer changes the ``images_type`` setting to deliberately switch a
+nova-compute machine to use Sheepdog.
+
+There are no database migrations required for this change.
+
+Developer impact
+----------------
+
+This change would add an additional disk-backend to the libvirt driver,
+slightly increasing code support costs.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <scott-devoid> Scott Devoid
+
+Other contributors:
+  <None>
+
+
+Work Items
+----------
+
+- Implement basic support for Sheepdog images booted from a Glance image.
+
+- Implement snapshot support.
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Devstack integration is required before tempest can run functional
+tests against the Sheepdog drivers for Nova, Glance and Cinder. A patch
+has been proposed which would use Sheepdog for each service. [2]
+
+This, I think, would result in many functional tests of the Sheepdog
+drivers via the Tempest tests. However, a Jenkins job would need
+to be defined and VMs would need to be provisioned to run the jobs.
+It is not clear if openstack-infra is willing or capable of committing
+to a proliferation of CI test runs. There is a Juno Summit scheduled for
+this. [3]
+
+
+Documentation Impact
+====================
+
+- Configuration reference would need to be updated with the new configuration
+  options. See the Deployer Impact section.
+
+- Cloud Administer or Operations guide should be updated with a section which
+  describes in detail how to configure Sheepdog for nova and what sort of
+  considerations should be taken into account, e.g. cluster size, Zookeeper vs
+  Corosync, the use of gateway nodes.
+
+These documentation changes would happen as part of this blueprint.
+
+
+References
+==========
+
+.. [1] https://github.com/sheepdog/sheepdog/wiki
+.. [2] https://review.openstack.org/#/c/90244/
+.. [3] http://summit.openstack.org/cfp/details/198
diff --git a/specs/juno/approved/libvirt-start-lxc-from-block-devices.rst b/specs/juno/approved/libvirt-start-lxc-from-block-devices.rst
new file mode 100644
index 0000000..f73495d
--- /dev/null
+++ b/specs/juno/approved/libvirt-start-lxc-from-block-devices.rst
@@ -0,0 +1,128 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Libvirt - Start LXC from a block device
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-start-lxc-from-block-devices
+
+The purpose of this blueprint is to enable the LXC containers
+to be started from a block device volumes.
+
+Problem description
+===================
+
+Currently, LXC containers can only be started from a Glance image.
+However, a minor adjustment is needed to support it's being booted
+using a block volume as its root OS filesystem.
+
+Proposed change
+===============
+
+Separate the lxc disk handling code from _create_domain() to
+_lxc_disk_handler context manager. It will use block_device_mapping
+to map the device that instance has been started from, otherwise,
+an image will be used.
+
+The _lxc_disk_handler will handle the "pre" and "post" lxc start actions
+on the disk, to mount it and clean the lxc namespace, after it starts.
+These actions are specific to LXC, both for images and volumes.
+
+The following layout of the volumes will be supported.
+
+ - Unpartitioned, filesystem across entire content.
+ - Partitioned. Only mount the filesystem in the first partition.
+   In case there are more than one partition present, only the first one
+   will be considered, while others will be ignored.
+
+The user may create a volume from and existing Glance image and boot
+LXC container in one command:
+
+    nova boot --flavor FLAVOR --block-device source=image,id=ID,dest=volume,\
+              size=SIZE,shutdown=PRESERVE,bootindex=0 NAME
+
+or booting the LXC container from an existing volume
+
+    nova boot --flavor FLAVOR --block-device source=volume,id=ID,dest=volume,\
+              size=SIZE,shutdown=PRESERVE,bootindex=0 NAME
+
+
+Alternatives
+------------
+None
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+As LXC will always share the host's kernel, between all instanances,
+any vulnerability in the kernel, maybe used to harm the host.
+In general, the kernel's filesystem drivers should be trusted to
+free of vulnerabilities that the user filesystem image may exploit.
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Vladik Romanovsky <vladik.romanovsky@enovance.com>
+
+Work Items
+----------
+ - Introduce a _lxc_disk_handler context manager method and
+   separate all lxc disk handling code from _create_domain()
+   to it.
+ - Add logic to the _lxc_disk_handler to mount the volumes,
+   using the provided block_device_mapping
+ - Remove the lxc specific mapping creation in blockinfo.py
+
+Dependencies
+============
+None
+
+Testing
+=======
+
+None
+
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+[1] https://review.openstack.org/#/c/74537
diff --git a/specs/juno/approved/log-request-id-mappings.rst b/specs/juno/approved/log-request-id-mappings.rst
new file mode 100644
index 0000000..bbdc9b3
--- /dev/null
+++ b/specs/juno/approved/log-request-id-mappings.rst
@@ -0,0 +1,167 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Log Request ID Mappings
+==========================================
+
+
+https://blueprints.launchpad.net/nova/+spec/log-request-id-mappings
+
+Tracking requests across multiple OpenStack services is difficult.
+
+
+Problem description
+===================
+
+Each OpenStack service sends a request ID header with HTTP responses. This
+value can be useful for tracking down problems in the logs. However, when
+operations cross service boundaries, this tracking becomes difficult, as
+services generate a new ID for each inbound request; a nova request ID cannot
+help users find debug info for other services that were called by nova while
+a request to nova was being fulfilled. This becomes especially problematic when
+many requests are coming at once, especially at the gate, where tempest is now
+running tests in parallel. Simply matching timestamps between service logs is
+no longer a feasible solution. Therefore, another method of request tracing
+is needed to aid debugging.
+
+
+Proposed change
+===============
+
+A request ID is generated at the start of request processing. This is the ID
+that a user will see when their request returns. Other OpenStack services
+that nova calls out to (such as glance) will send responses with their request
+ID as a header. By logging the mapping of the two request IDs (nova->glance), a
+user will be  able to easily lookup the request ID returned by glance in the
+n-api log. With the glance request ID in hand, a user can then check the glance
+logs for debug info which corresponds to the request made to nova.
+
+There are two request IDs required to form the log message: one generated by
+nova, and one included in the response from another service. The request ID
+generated by nova is in the context that is passed in to the python client
+wrappers. The request ID of the other service does not yet reach nova's
+client wrappers; this value being returned depends on some client blueprints
+being implemented (see Dependencies). Once both request IDs are available, the
+logging will be done using link_request_ids() from request_utils.
+
+Alternatives
+------------
+
+This idea surfaced in previous cycles[1], with the proposed solution being a
+unified request ID that would be passed between services. While work was
+started on this, the approach was eventually deemed unsatisfactory, as it
+would allow for meddling with the request ID value on the client side. A client
+reusing the same request ID would eliminate the benefit of request tracing.
+With a carefully chosen request ID, one user could even interfere with the
+debugging effort of another user. So having the request ID be generated by the
+server is essential.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+The request_utils module used will generate an INFO notification for each
+request ID mapping that is logged.
+
+Other end user impact
+---------------------
+
+The only places an end user will see this is in the nova logs or in the
+generated notifications.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+In order for nova to log the request ID value of the other service, the python
+client for that service needs to be passing back the request ID it gets from
+the HTTP response from the service. New releases of python-glanceclient,
+python-cinderclient, etc. will be necessary for this to occur. As such,
+deployers will need to be running newer versions of the client in order for
+the request ID mappings to be logged. If the client is not returning the
+request ID, no logging will occur.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  chris-buccella
+
+Work Items
+----------
+
+1. Sync request_utils module from oslo
+2. As various service's python clients are updated to return the request ID,
+   log the request ID mapping using request_utils.link_request_ids():
+   1. python-glanceclient
+   2. python-cinderclient
+   3. python-neutronclient
+3. Update requirements.txt with the new required versions of the clients
+
+Dependencies
+============
+
+* New versions of some python clients. The following blueprints need to be
+  completed first:
+
+  * https://blueprints.launchpad.net/python-glanceclient/+spec/return-req-id
+  * https://blueprints.launchpad.net/python-cinderclient/+spec/return-req-id
+  * https://blueprints.launchpad.net/python-neutronclient/+spec/return-req-id
+
+Testing
+=======
+
+A tempest test could be added to ensure that notifications are being generated
+in the correct format.
+
+
+Documentation Impact
+====================
+
+The Operations Guide should be updated particularly:
+
+* Chapter 11, under "Determining Which Component Is Broken"
+* Chapter 13, under "Tracing Instance Requests"
+
+
+References
+==========
+
+[0] Proposed change for nova<->glance logging from Icehouse cycle:
+https://review.openstack.org/#/c/68518
+
+[1] Discussion from the HK Summit:
+https://etherpad.openstack.org/p/icehouse-summit-nova-cross-project-request-ids
+
+[2] Refinements from the ML:
+http://lists.openstack.org/pipermail/openstack-dev/2013-December/020774.html
diff --git a/specs/juno/approved/lvm-ephemeral-storage-encryption.rst b/specs/juno/approved/lvm-ephemeral-storage-encryption.rst
new file mode 100644
index 0000000..bdd8db0
--- /dev/null
+++ b/specs/juno/approved/lvm-ephemeral-storage-encryption.rst
@@ -0,0 +1,204 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================================
+Ephemeral storage encryption for LVM backend
+============================================
+
+https://blueprints.launchpad.net/nova/+spec/lvm-ephemeral-storage-encryption
+
+The proposed feature will provide data-at-rest security for LVM backed,
+libvirt managed ephemeral storage devices attached to a VM instance.
+
+
+Problem description
+===================
+
+The current implementation of LVM ephemeral storage leaves user data vulnerable
+following instance shutdown, through disk block reuse (if data is not
+securely erased), improper storage media disposal and physical facility
+compromise.
+
+For example, if a compute node goes down without properly disposing of the
+active instances, when it is restarted, the disk blocks that held pre-reboot
+instances' data will be reallocated to new instances.  Since LVM storage is not
+sanitized before allocation this, in principle, permits recovery of other
+users' data.
+
+
+Proposed change
+===============
+
+User data can be protected against inadvertant disclosure by encrypting
+ephemeral storage disks with a unique key, accessible only via a secure key
+manager (most likely Barbican, currently in incubation) with proper
+credentials. The feature will be labeled optional until the status of Barbican
+key manager is finalized.
+
+This feature is part of a larger effort to add ephemeral storage encryption to
+OpenStack.
+
+
+Alternatives
+------------
+
+It is unlikely there is another solution that would cover all the cases such
+unexpected compute node events, preventing proper instance shutdown,
+improper storage media disposal, etc., covered by ephemeral storage encryption.
+
+For example, ephemeral disks could be sanitized before being attached to
+instances to prevent disclosure due to block storage reuse.  However, this
+would not protect users' data against improper storage media disposal.
+Moreover, data sanitization is expensive since the entire ephemeral disk,
+which can be sizeable, must be wiped.
+
+Data model impact
+-----------------
+
+All necessary data objects and database changes have already been made. See
+
+* https://blueprints.launchpad.net/nova/+spec/encrypt-ephemeral-storage
+
+* https://review.openstack.org/#/c/61544/
+
+* https://review.openstack.org/#/c/60621/
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+This feature will make LVM ephemeral storage more secure by providing
+data-at-rest security for user data.
+
+* A uniqe encryption key is created for each instance (or batch of
+  instances in case of a batch launch) in
+  compute.API._populate_instance_for_create() and stored securely using key
+  manager (e.g., Barbican).
+
+* The key is retrieved, using its uuid and the user's context, immediately
+  before the ephemeral disk is created to minimize the exposure.
+
+Potential security concerns:
+
+* Command cryptsetup will be added to the rootwrap filter.
+
+* User context will be passed to imagebackend.Lvm.create_image(),
+  LibvirtDriver.create_swap() and LibvirtDriver.create_ephemeral() from
+  LibvirtDriver._create_image()
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Certain instance operations:
+
+* instance rescue
+
+may not be immediately supported.
+
+Performance Impact
+------------------
+
+The optional encryption layer imposes a roughly 10% performance penalty
+on ephemeral storage I/O performance, according to measurements performed
+with the Phoronix test suite on a single-node DevStack cloud.
+
+Other deployer impact
+---------------------
+
+* LVM ephemeral storage encryption is controlled by 3 options collected in the
+  ephemeral_storage_encryption options group.  The name is deliberately generic
+  since the same options could be used to control encryption for other
+  backends.
+
+  ephemeral_storage_encryption options group
+
+  * enabled:Boolean -- enables/disables LVM ephemeral storage encryption;
+                       default is False
+
+  * cipher:String -- cipher-mode string to be passed to cryptsetup; the set of
+                     cipher-mode combinations available depends on kernel
+                     support; default is aes-xts-plain64
+
+  * key_size:Integer -- encryption key length in bits; default is 512
+
+  The default values have been chosen to provide a high level of
+  confidentiality.  (Note that in XTS mode only half of the key bits are
+  used for encryption key.)
+
+* Encryption is implemented using the cryptsetup utility, which is available
+  as a package on most Linux distributions.
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Dan Genin <daniel.genin@jhuapl.edu>
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+Two of the three components comprising the feature:
+
+* adding ephemeral storage encryption key uuid to Nova DB
+  (https://review.openstack.org/#/c/61544/)
+
+* dmcrypt module for interacting with cryptsetup
+  (https://review.openstack.org/#/c/60621/)
+
+have already merged in Icehouse.
+
+The final remaining item is to actually add encryption to imagebackend.Lvm.
+
+Dependencies
+============
+
+Depends on Barbican (https://review.openstack.org/#/c/94918/) for key
+management.
+
+Depends on cryptsetup being installed.
+
+
+Testing
+=======
+
+We will work to implement Tempest tests for the feature. However, Tempest
+testing will require Tempest support for LVM backed ephemeral storage as
+well as Barbican for key management. These changes may take some time to
+implement.
+
+Documentation Impact
+====================
+
+Ephemeral storage encryption configuration options and its dependencies,
+namely dmcrypt/cryptsetup and Barbican, will have to be documented.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/approved/make-resource-tracker-use-objects.rst b/specs/juno/approved/make-resource-tracker-use-objects.rst
new file mode 100644
index 0000000..79ff269
--- /dev/null
+++ b/specs/juno/approved/make-resource-tracker-use-objects.rst
@@ -0,0 +1,183 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Make Resource Tracker Use Objects
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/make-resource-tracker-use-objects
+
+This blueprint was approved for Icehouse but missed feature freeze.
+
+Nova is converting data structures it uses to communicate via RPC and through
+the database to use an object encapsulation called Nova Objects. This supports
+of multi-versioning for live-upgrade and database schema independence. This
+blueprint covers the conversion of the resource tracker to use Nova Objects.
+
+Problem description
+===================
+
+Conversion to Nova Objects will replace dict data structures that are currently
+communicated via the conductor API with Nova Object versions. Where necessary
+the Nova Objects will be extended to cover parameters that are not already
+implemented.
+
+Proposed change
+===============
+
+The Nova Object classes that will be used include:
+
+- ComputeNode
+- Instance
+- Migrations
+- Flavor
+- Service
+
+The ComputeNode object is currently missing some parameters that exist
+in the compute_nodes table and are used in the resource tracker. The
+following parameters will be added to the ComputeNode:
+
+- host_ip
+- pci_stats
+
+In addition, the following fields exist in the compute_nodes table but
+are not currently used by the resource tracker. We propose not to add fields
+to the ComputeNode object unless they are used, so these fields will not
+be added as part of this blueprint.
+
+- extra_resources
+- supported_instances
+
+When complete there will be direct calls to conductor in the resource tracker.
+
+Alternatives
+------------
+
+There is another effort to split the scheduler from Nova. When the split is
+complete the resource tracker may no longer interact with the scheduler via
+the database.  Initially, the scheduler-lib blueprint (see references) will
+make all compute node interaction with the scheduler go through a new
+scheduler library in the Juno-1 timeframe in preparation for the split.
+
+This suggests that it might be unnecessary to use the ComputeNode object at
+least. However, it is reasonable to continue using the ComputeNode object
+even if it is not used to persist data in the database, so we will go ahead
+with the existing plan to implement it.
+
+Data model impact
+-----------------
+
+The objects isolate the code from the database schema. They are written to
+operate with existing data model versions. At present the scheduler does not
+the ComputeNode object, so code there will need to tolerate changes in
+database schema or the format of data stored in fields.
+
+The fields that need to be added to the ComputeNode object are as follows:
+
+**host_ip**
+
+Database field type: varchar(39)
+
+Object field type: fields.IPAddressField()
+
+**pci_stats**
+
+Database field type: text
+
+Object field type: fields.ObjectField('PciDeviceList', nullable=True)
+
+The pci_stats field is currently populated with a PciDeviceList serialized
+as an object primitive. This is already the correct form for an object field.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+The objects are written to be compatible with the database schema used in
+Icehouse. There is no database migration associated with this blueprint and
+the format of data stored in the fields of the database will not change. This
+means that a combination of Icehouse and Juno versions of the compute nodes
+will be able to coexist and interact with the scheduler.
+
+Developer impact
+----------------
+
+Developers working on the resource tracker will be required to use the new
+objects instead of directly making database calls to conductor.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  pmurray
+
+Other contributors:
+  alexisl
+
+Work Items
+----------
+
+* Use flavor object in resource tracker - (merged in Icehouse)
+
+* Use Service object in resource tracker
+
+* Use Instance object in resource tracker
+
+* Use Migrations object in resource tracker
+
+* Use ComputeNode object in resource tracker
+
+Several of these work items are currently ready for review:
+https://review.openstack.org/#/q/topic:bp/make-resource-tracker-use-objects,n,z
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+This does not affect existing tempest tests. Unit tests will be
+added for each object and existing tests will be modified to deal
+with the new data structure.
+
+Documentation Impact
+====================
+
+No new features or API changes so no document impact.
+
+References
+==========
+
+https://blueprints.launchpad.net/nova/+spec/scheduler-lib
diff --git a/specs/juno/approved/migrate-libvirt-volumes.rst b/specs/juno/approved/migrate-libvirt-volumes.rst
new file mode 100644
index 0000000..ac028d2
--- /dev/null
+++ b/specs/juno/approved/migrate-libvirt-volumes.rst
@@ -0,0 +1,204 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================================
+Use Libvirt Storage Pool Methods to Migrate Libvirt Volumes
+===========================================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/migrate-libvirt-volumes
+
+Currently, the libvirt driver uses SSH (rsync or scp) to do cold migrations
+and resizes on non-shared storage.  This requires SSH permissions between
+compute nodes, which is problematic for a number of reasons.  Instead we can
+use the methods built in to libvirt's storage pool API to do migrations.
+
+NOTE: this proposal requires
+`Use libvirt storage pools`_
+(`Gerrit Spec Review <https://review.openstack.org/#/c/86947>`_).
+
+Problem description
+===================
+
+The primary issue is that, currently, the Nova libvirt driver requires
+SSH access between compute nodes to perform cold migrations and resizes on
+non-shared storage.  This presents several issues:
+
+* From a security perspective, providing SSH access between compute nodes
+  is sub-optimal.  Giving compute nodes SSH access could allow a compromised
+  node to compromise other nodes and potentially inflict harm on a cloud.
+
+* From a setup perspective, it adds several extra steps to a setup:
+  System administrators, or their setup tools, must generate a keypair
+  for each compute node, and upload the public key to each of the other
+  compute nodes.
+
+Proposed change
+===============
+
+As specified in blueprint mentioned above, Nova's disk images would be placed
+in a libvirt storage pool.  At migration time, a new volume would be created in
+the destination node's storage pool, and the methods virStorageVolDowload and
+virStorageVolUpload would be used to stream the contents of the disk between
+compute nodes
+(http://libvirt.org/html/libvirt-libvirt.html#virStorageVolUpload).
+
+In order to ensure secure migrations, libvirt should be configured to use one
+of the various forms of authentication and encryption that it supports, such as
+Kerberos (via SASL -- http://libvirt.org/auth.html) or TLS client certificates
+(http://libvirt.org/remote.html#Remote_libvirtd_configuration).
+
+Note that this would only apply to setups using the new image backend
+described in the previous blueprint; setups using the "legacy" image
+backends would continue to use the SSH method until the "legacy" image
+backends are removed.
+
+Alternatives
+------------
+
+* Requiring shared storage for cold migrations and resizes: there are many
+  OpenStack users who would like to be able to perform cold migrations and
+  resizes without having shared storage.
+
+* Just setting up SSH keys between compute nodes: see the problem description
+
+* Temporarily provisioning SSH keys for the duration of the migration:
+  While this somewhat mitigates the security issue and remove the extra setup
+  steps, it still provides a window where the compute nodes are vulnerable.
+  It would be much harder to secure, and would require having Nova be able
+  to securely generate SSH keys.
+
+* Using an rsync daemon: People seemed to be averse to requiring an rsync
+  daemon.  Additionally, rsync daemon connections are not encryptable out
+  of the box, although they can be run over stunnel.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+While this change does require two compute nodes' libvirt daemons to connect
+to each other, such a process is already done by live migration.  While the
+disks would no longer be encrypted by SSH while transfering, system
+administrators could simply secure the libvirt connections instead
+(http://libvirt.org/auth.html).  Libvirt supports TLS for encryption with x509
+client certificates for authentication, as well as SASL for GSSAPI/Kerberos
+encryption and authentication.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+There are a couple potential performance issues:
+
+* Rsync compresses compress the contents to be transfered, but AFAIK libvirt
+  does not (although this is being worked on in conjunction with the libvirt
+  developers).  This could result in more data being transfered over
+  the network.
+
+* The actually streaming process would be using python as an intermediary
+  (e.g. :code:`data = stream1.recv(1024*64); stream2.send(data)`, although
+  the actual code would properly support async IO, detection of partial sends,
+  etc).  While this would be less performant than having C code which would do
+  the transfer, I suspect there are ways in which we could optimize the code.
+
+Other deployer impact
+---------------------
+
+In order for the new method to work, deployers would have to enable the libvirt
+daemon on each compute node to listen for remote libvirt connections (if live
+migrations are enabled, this has already been done).  Such connections must be
+secured as noted in `Security Impact`_.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    sross-7
+
+Other contributors:
+    None
+
+Work Items
+----------
+
+1. Implement the virStorageVolUpload/virStorageVolDownload code in the
+   :code:`migrate_disk_and_power_off` method, replacing the existing calls
+   to :code:`libvirt_utils.copy_image`.
+
+2. Follow Up: remove the instances of SSH that create the instance directory
+   and detect shared storage.  These could easily be done in a pre-migration
+   method, similarly to how live-migration works currently.
+
+
+Dependencies
+============
+
+`Use libvirt storage pools`_
+(`Gerrit spec review <https://review.openstack.org/#/c/86947>`_)
+
+.. _Use libvirt storage pools:
+   https://blueprints.launchpad.net/nova/+spec/use-libvirt-storage-pools
+
+Testing
+=======
+
+Since this only changes how migration works under the hood, existing migration
+tests should be sufficient for the most part.
+
+
+Documentation Impact
+====================
+
+For the OpenStack Security Guide, we should document that SSH keys are no
+longer required between compute nodes, as well as provide instructions for
+securing remove libvirtd connections.
+
+In the Compute Admin Guide, we should provide instructions for how to enable
+remote libvirtd connections (as required for libvirt live migration), as well
+as noting that these connections need to be secured, as per the Security Guide.
+
+Since much of this documentation also applies to libvirt live migrations, it
+may be beneficial to place the instructions in a "general" section and link
+to it from both the libvirt cold migrations and libvirt live migrations
+documentation.
+
+
+References
+==========
+
+* http://libvirt.org/html/libvirt-libvirt.html#virStorageVolUpload
+
+* http://libvirt.org/auth.html)
+
+* http://libvirt.org/remote.html#Remote_libvirtd_configuration
diff --git a/specs/juno/approved/nova-pagination.rst b/specs/juno/approved/nova-pagination.rst
new file mode 100644
index 0000000..b18e509
--- /dev/null
+++ b/specs/juno/approved/nova-pagination.rst
@@ -0,0 +1,333 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================
+Nova REST API Sorting Enhancements
+====================================
+
+https://blueprints.launchpad.net/nova/+spec/nova-pagination
+
+Currently, the pagination support for Nova does not allow the caller to
+specify the sort order and direction of the data set. This blueprint
+enhances the pagination support for the /servers and /servers/detail
+APIs so that multiple sort keys and sort directions can be supplied on
+the request.
+
+
+Problem description
+===================
+
+There is no support for retrieving server data in a specific order, it is
+defaulted to descending sort order by the "created date" and "id" keys. In
+order to retrieve data in any sort order and direction, the REST APIs need
+to accept multiple sort keys and directions.
+
+Use Case: A UI that displays a table with only the page of data that it
+has retrieved from the server. The items in this table need to be sorted
+by status first and by display name second. In order to retrieve data in
+this order, the APIs must accept multiple sort keys/directions.
+
+
+Proposed change
+===============
+
+The /servers and /servers/detail APIs will support the following parameters
+being repeated on the request:
+
+* sort_key: Key used to determine sort order
+* sort_dir: Direction for with the associated sort key ("asc" or "desc")
+
+The caller can specify these parameters multiple times in order to generate
+a list of sort keys and sort directions. The first key listed is the primary
+key, the next key is the secondary key, etc.
+
+For example: /servers?sort_key=status&sort_dir=desc&sort_key=display_name&
+&sort_dir=desc&sort_key=created_at&sort_dir=desc
+
+Note: The "created_at" and "id" sort keys are always appended at the end of
+the key list if they are not already specified on the request.
+
+The database layer already supports multiple sort keys and directions. This
+blueprint will update the API layer to retrieve the sort information from
+the API request and pass that information down to the database layer.
+
+All sorting is handled in the sqlalchemy.utils.paginate_query function.  This
+function accepts an ORM model class as an argument and the only valid sort
+keys are attributes on the given model class.  Therefore, the valid sort
+keys are limited to the model attributes on the models.Instance class.
+
+For the v2 API a new 'os-server-sort-keys' API extension will be created to
+signify that the new sorting parameters proposed in this blueprint should be
+honored. The v3 API will support the new sorting parameters by default.
+
+Alternatives
+------------
+
+Repeating parameter key/values was chosen because glance already did it:
+
+https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L526
+
+However, the list of sort keys and directions could be built by splitting the
+associated parameter values.
+
+For example:
+/servers?sort_key=status,display_name,created_at&sort_dir=desc,desc,desc
+
+The downside of this approach is that it would require pre-defined token
+characters. I'm open to this solution if it is deemed better.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+A new v2 API extension will be created to signify that the new sorting
+parameters should be honored. Extension details:
+
+* Name: ServerSortKeys
+* Alias: os-server-sort-keys
+
+Note that this API extension will behave in the same manner as the current
+'os-user-data' extension. For example, the extension is defined as:
+
+http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute
+/contrib/user_data.py
+
+And it is checked in the v2 server API here:
+
+http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute/
+servers.py#n850
+
+The following existing v2 GET APIs will support the new sorting parameters
+if the 'os-server-sort-keys' API extenstion is loaded:
+
+* /v2/{tenant_id}/servers
+* /v2/{tenant_id}/servers/detail
+
+The following existing v3 GET APIs will support the new sorting parameters
+by default:
+
+* /v3/servers
+* /v3/servers/details
+
+Note that the design described in this blueprint could be applied to other GET
+REST APIs but this blueprint is scoped to only those listed above. Once this
+design is finalized, then the same approach could be applied to other APIs.
+
+The existing API documentation needs to be updated to include the following
+new Request Parameters:
+
++-----------+-------+--------+---------------------------------------------+
+| Parameter | Style | Type   | Description                                 |
++===========+=======+========+=============================================+
+| sort_key  | query | string | Sort key (repeated for multiple), keys      |
+|           |       |        | default to 'created_at' and 'id'            |
++-----------+-------+--------+---------------------------------------------+
+| sort_dir  | query | string | Sort direction, either 'asc' or 'desc'      |
+|           |       |        | (repeated for multiple), defaults to 'desc' |
++-----------+-------+--------+---------------------------------------------+
+
+Neither the API response format nor the return codes will be modified, only
+the order of the servers that are returned.
+
+In the event that an invalid sort key is specifed then a "badRequest" error
+response (code 400) will be returned with a message like "Invalid input
+received: Invalid sort key".
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The novaclient should be updated to accept sort keys and sort directions, new
+parameters:
+
++-------------+----------------------------------------------------------+
+| Parameter   | Description                                              |
++=============+==========================================================+
+| --sort-keys | Comma-separated list of sort keys used to specify server |
+|             | ordering. Each key must be paired with a sort direction  |
+|             | value.                                                   |
++-------------+----------------------------------------------------------+
+| --sort-dirs | Comma-separated list of sort directions used to specify  |
+|             | server ordering. Each key Must be paired with a sort key |
+|             | value. Valid values are 'asc' and 'desc'.                |
++-------------+----------------------------------------------------------+
+
+Performance Impact
+------------------
+
+All sorting will be done in the database. The choice of sort keys is limited
+to attributes on the models.Instance ORM class -- not every attribute key
+returned from a detailed query is a valid sort key.
+
+Performance data was gathered by running on a simple devstack VM with 2GB
+memory. 5000 instances were inserted into the DB. The data shows that the
+sort time on the main data table is dwarfed (see first table below) when
+running a detailed query -- most of the time is spent querying the the other
+tables for each item; therefore, the impact of the sort key on a detailed
+query is minimal.
+
+For example, the data below compares the processing time of a GET request for
+a non-detailed query to a detailed query with various limits using the default
+sort keys. The purpose of this table is to show the the processing time for a
+detailed query is dominated by getting the additional details for each item.
+
++-------+--------------------+----------------+---------------------------+
+| Limit | Non-Detailed (sec) | Detailed (sec) | Non-Detailed / Detailed % |
++=======+====================+================+===========================+
+| 50    | 0.0560             | 0.8621         | 6.5%                      |
++-------+--------------------+----------------+---------------------------+
+| 100   | 0.0813             | 1.6839         | 4.8%                      |
++-------+--------------------+----------------+---------------------------+
+| 150   | 0.0848             | 2.4705         | 3.4%                      |
++-------+--------------------+----------------+---------------------------+
+| 200   | 0.0874             | 3.2502         | 2.7%                      |
++-------+--------------------+----------------+---------------------------+
+| 250   | 0.0985             | 4.1237         | 2.4%                      |
++-------+--------------------+----------------+---------------------------+
+| 300   | 0.1229             | 4.8731         | 2.5%                      |
++-------+--------------------+----------------+---------------------------+
+| 350   | 0.1262             | 5.6366         | 2.2%                      |
++-------+--------------------+----------------+---------------------------+
+| 400   | 0.1282             | 6.5573         | 2.0%                      |
++-------+--------------------+----------------+---------------------------+
+| 450   | 0.1458             | 7.2921         | 2.0%                      |
++-------+--------------------+----------------+---------------------------+
+| 500   | 0.1770             | 8.1126         | 2.2%                      |
++-------+--------------------+----------------+---------------------------+
+| 1000  | 0.2589             | 16.0844        | 1.6%                      |
++-------+--------------------+----------------+---------------------------+
+
+Non-detailed query data was also gathered. The table below compares the
+processing time using default sort keys to the processing using display_name
+as the sort key. Items were added with a 40 character display_name that was
+generated in an out-of-alphabetical sort order.
+
++-------+--------------------+------------------------+------------+
+| Limit | Default keys (sec) | display_name key (sec) | Slowdown % |
++=======+====================+========================+============+
+| 50    | 0.0560             | 0.0600                 | 7.1%       |
++-------+--------------------+------------------------+------------+
+| 100   | 0.0813             | 0.0832                 | 2.3%       |
++-------+--------------------+------------------------+------------+
+| 150   | 0.0848             | 0.0879                 | 3.7%       |
++-------+--------------------+------------------------+------------+
+| 200   | 0.0874             | 0.0906                 | 3.7%       |
++-------+--------------------+------------------------+------------+
+| 250   | 0.0985             | 0.1031                 | 4.7%       |
++-------+--------------------+------------------------+------------+
+| 300   | 0.1229             | 0.1198                 | -2.5%      |
++-------+--------------------+------------------------+------------+
+| 350   | 0.1262             | 0.1319                 | 4.5%       |
++-------+--------------------+------------------------+------------+
+| 400   | 0.1282             | 0.1368                 | 6.7%       |
++-------+--------------------+------------------------+------------+
+| 450   | 0.1458             | 0.1458                 | 0.0%       |
++-------+--------------------+------------------------+------------+
+| 500   | 0.1770             | 0.1619                 | -8.5%      |
++-------+--------------------+------------------------+------------+
+| 1000  | 0.2589             | 0.2659                 | 2.7%       |
++-------+--------------------+------------------------+------------+
+
+In conclusion, the sort processing on the main data table has minimal impact
+on the overall processing time. For a detailed query, the sort time is dwarfed
+by other processing -- even if the sort time when up 3x it would only
+represent 4.8% of the total processing time for a detailed query with a limit
+of 1000 (and only increase the processing time by .11 sec with a limit of 50).
+
+Other deployer impact
+---------------------
+
+The choice of sort keys has a minimal impact on data retrieval performance
+(see performance data above). Therefore, the user should be allowed to
+retrieve data in whatever order they need to for creating their views (see
+use case in the Problem Description).
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Steven Kaufer
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+Ideally the logic for processing the sort parameters would be common to all
+components and would be done in oslo; a similar blueprint is also being
+proposed in cinder:
+https://blueprints.launchpad.net/cinder/+spec/cinder-pagination
+
+Therefore, I see the following work items:
+
+* Create common functions to process the API parameters and create a list of
+  sort keys and directions
+* Update v2 and v3 APIs to retrieve the sort information and pass down to the
+  DB layer (requires changes to compute/api.py, objects/instance.py,
+  db/api.py, and db\sqlalchemy\api.py)
+* Update the novaclient to accept and process multiple sort keys and sort
+  directions
+
+
+Dependencies
+============
+
+* Related (but independent) change being proposed in cinder:
+  https://blueprints.launchpad.net/cinder/+spec/cinder-pagination
+
+
+Testing
+=======
+
+Both unit and Tempest tests need to be created to ensure that the data is
+retrieved in the specified sort order. Tests should also verify that the
+default sort keys ("created_at" and "id") are always appended to the user
+supplied keys (if the user did not already specify them).
+
+Testing should be done against multiple backend database types.
+
+
+Documentation Impact
+====================
+
+The /servers and /servers/detail API documentation will need to be updated to:
+
+- Reflect the new sorting parameters and explain that these parameters will
+  affect the order in which the data is returned.
+- Explain how the default sort keys will always be added at the end of the
+  sort key list
+
+The documentation could also note that query performance will be affected by
+the choice of the sort key, noting which keys are indexed.
+
+
+References
+==========
+
+None
diff --git a/specs/juno/approved/persistent-resource-claim.rst b/specs/juno/approved/persistent-resource-claim.rst
new file mode 100644
index 0000000..ff702cd
--- /dev/null
+++ b/specs/juno/approved/persistent-resource-claim.rst
@@ -0,0 +1,201 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Persistent resource claims
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/persistent-resource-claim
+
+This blueprint plans to enhance the compute resource tracker to keep resource
+claim as persistent to across nova-compute restart. This will be helpful
+to move the resource claim process to the conductor.
+
+Problem description
+===================
+
+The resource tracker provides an interface to claim resources for an instance.
+However, the claim result is only kept in memory instead of kept persistently
+and a context manager is returned to the caller.
+
+There are several potential issue with this implementation. Firstly, it is
+not easy to support two-phases resources claim, because it return the claim
+as a context manager. Secondly without the persistent claim, the resource
+tracker has to recalculate the claims from instances and migration object,
+which requires more DB access and also requires locks to create the migration
+object and to set the instance's host/node. Thirdaly, it's not easy to move
+the _prep_resize() to the conductor because the claim is not persistent and
+can't be invoked remotely.
+
+Proposed change
+===============
+
+We suggest to change the resource tracker to track the resources claim and
+persist the resources claim.
+
+* When resources claim, the resource tracker will track the result claim. Each
+  claim will be identified by compute node and a unique ID in the node.
+
+* The resources claim is kept persistently by compute manager.
+
+  There are several solutions to persist the claim like keeping it in
+  central DB or in a local sqlite. In this spec, we will persistent the claim
+  in local sqlite only, and a claim table is created to track the resources
+  claim. We will enhance it later to be configurable as local sqlite or
+  central DB. A mechanism like service_group is used to make the future
+  extension easier.
+
+  See "Alternative" for more info.
+
+* The claim persistent code defines claim format version and upgrades the claim
+  table if new version is required, so that we can enhance the resources claim,
+  like support for extra_info, easily. A separate sqlite table is created to
+  save the current claim table's version information.
+
+  When a compute service is restarted after upgrade, it will check the claim
+  table version. If the claim table version is lower than the latest version
+  in the claim persistent code, it will upgrade the claim table to
+  latest version and then update the version table. The upgrade code knows
+  about the schema for each version so that we don't need keep schema
+  information in the sqlite.
+
+* When the compute node upgrades from non-persistent claim to persist
+  claim, the upgrade code will find the table does not exist and thus
+  will create the table from scratch based on the instance/migration
+  information.
+
+* The compute manager's periodic task will clean up any orphan claims. If
+  a resources claim has no corresponding instance or migration object in the
+  node, and it has been created for a specified period, it's an orphan claim
+  and will be cleaned. The 'specified' time is a configuration item.
+
+  Also if a server is evacuated when host is shutdown, the corresponding
+  resources claim will be released when the compute service is restarted.
+
+  In future, such clean up should happen in conductor which will take response
+  of garbage collector.
+
+Alternatives
+------------
+
+We had some discussions on how to keep the claims persistent. Originally it's
+proposed to keep the claims in central DB. central DB will provide a global
+view and will be more robust, but it will impact performance for each periodic
+task. Later, it's suggested to keep in sqlite first which will provide much
+better performance and can be extended to central DB in future.
+
+Data model impact
+-----------------
+
+A sqlite table (claim table) is created to keep the claim. Below is
+the data model to be used in sqlite. Although defined with sqlite
+type, but it should similar to central DB also:
+
+id: INTEGER
+host: TEXT
+node: TEXT
+instance_uuid: TEXT
+vcpus: INTEGER
+memory_mb: INTEGER
+disk_gb: INTEGER
+pci: TEXT
+resize_target: INTEGER
+created_at: TEXT
+
+The resize_target is to distinguish the resources claims in the same host for
+the same instance, when resize in the same host. It is in fact a boolean value
+stored as integers 0 (false) and 1 (true).
+
+The created_at is the timestamp when the claim is created. It's a ISO 8601
+format string.
+
+Another table is created to keep the claim format version information.
+table_name: TEXT
+version: INTEGER
+
+This table has only one entry, with the table_name as "claims" and the version
+is the version of the claims in the claim table. As stated above, the upgrade
+code knows about the schema of each version and knows how to upgrade between
+versions.
+
+REST API impact
+---------------
+
+No.
+
+Security impact
+---------------
+
+No.
+
+Notifications impact
+--------------------
+
+No.
+
+Other end user impact
+---------------------
+
+No.
+
+Performance Impact
+------------------
+
+No for this BP since we will not cover the DB solution.
+
+Other deployer impact
+---------------------
+
+A new configuration 'claim_db' will be added to define how the sqlite
+database is stored on disk. It will be a relative path to state_path
+configuration item. The default value is claim.sqlite, which means
+$state_path/claim.sqlite.
+
+A new configuration 'claim_expiry_time'. A claim that has been created for
+'claim_expiry_time' seconds and not is not associated with a instance or
+migration object is an orphan claim and will be released.
+The default value is 300 seconds.
+
+Developer impact
+----------------
+
+No
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  yunhong-jiang
+
+Work Items
+----------
+
+* Claims persistent code.
+* Update the resource tracker.
+
+Dependencies
+============
+No
+
+Testing
+=======
+
+We should have test code to check the sqlite is really populated correctly.
+
+Documentation Impact
+====================
+
+The documentation should be updated to describe the 'claim_db' configuration,
+where is the sqlite db lives now. Also the documents should describe how the
+upgrade works according to the "Proposed change" section.
+
+References
+==========
+
+https://wiki.openstack.org/wiki/Persistent_resource_claim
diff --git a/specs/juno/approved/quiesced-image-snapshots-with-qemu-guest-agent.rst b/specs/juno/approved/quiesced-image-snapshots-with-qemu-guest-agent.rst
new file mode 100644
index 0000000..3189326
--- /dev/null
+++ b/specs/juno/approved/quiesced-image-snapshots-with-qemu-guest-agent.rst
@@ -0,0 +1,131 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================================================
+Quiescing filesystems with QEMU guest agent during image snapshotting
+=====================================================================
+
+https://blueprints.launchpad.net/nova/+spec/quiesced-image-snapshots-with-qemu-guest-agent
+
+When QEMU Guest Agent is installed in a kvm instance, we can request the
+instance to freeze filesystems via libvirt during snapshotting to make the
+snapshot consistent.
+
+Problem description
+===================
+
+Currently we need to quiesce filesystems (fsfreeze) manually before
+snapshotting an image of active instances to create consistent backups.
+This should be automated when QEMU Guest Agent is enabled.
+
+Proposed change
+===============
+
+When QEMU Guest Agent is enabled in an instance, Nova-compute libvirt driver
+will request the agent to freeze the filesystems (and applications if
+fsfreeze-hook is installed) before taking snapshot of the image.
+After taking snapshot, the driver will request the agent to thaw the
+filesystems.
+
+The prerequisites of this feature are:
+
+1. the hypervisor is 'qemu' or 'kvm'
+
+2. libvirt >= 1.2.5 (which has fsFreeze/fsThaw API) is installed in the
+   hypervisor
+
+3. 'hw_qemu_guest_agent=yes' property in the image metadata is set to 'yes'
+   and QEMU Guest Agent is installed and enabled in the instance
+
+When quiesce is failed even though these conditions are satisfied
+(e.g. the agent is not responding), snapshotting may fail by exception
+not to get inconsistent snapshots.
+
+Alternatives
+------------
+
+Rewrite nova's snapshotting with libvirt's domain.reateSnapshot API with
+VIR_DOMAIN_SNAPSHOT_CREATE_QUIESCE flag, although it will change the current
+naming scheme of disk images. In addition, it cannot be leveraged to implement
+live snapshot of cinder volumes.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+While taking snapshots, disk writes from the instance are blocked.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  tsekiyama
+
+Work Items
+----------
+
+Implement the automatic quiesce during snapshotting when it is available.
+Now the code is ready to  review: https://review.openstack.org/#/c/72038/
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Live snapshotting with an image with qemu-guest-agent should be added to
+scenario tests.
+Note that it requires environment with libvirt >= 1.2.5.
+
+Documentation Impact
+====================
+
+Need to document how to use this feature in the operation guide (which
+currently recommends you use the fsfreze tool manually).
+
+References
+==========
+
+None
diff --git a/specs/juno/approved/restrict-image-isolation-with-defined-keys.rst b/specs/juno/approved/restrict-image-isolation-with-defined-keys.rst
new file mode 100644
index 0000000..d44cadd
--- /dev/null
+++ b/specs/juno/approved/restrict-image-isolation-with-defined-keys.rst
@@ -0,0 +1,177 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================================
+Strictly isolate group of hosts for an image
+============================================
+
+https://blueprints.launchpad.net/nova/+spec/restrict-image-isolation-with-defined-keys
+
+The aim of this blueprint is to improve the filter
+`AggregateImagePropertiesIsolation`
+
+An operator wants to schedule instances for a specific image on a
+pre-defined group of hosts. In addition, he wants to strictly isolate this
+group of hosts for the image only and accept images without key scheduled
+to other hosts.
+
+Problem description
+===================
+
+Currently with the filter `AggregateImagePropertiesIsolation` we have the
+possibility to define images that will be scheduled on a specific aggregate
+following this matrix:
+
++--------------+------------+----------+----------+
+| img \\ aggr  | key=foo    | key=xxx  | <empty>  |
++==============+============+==========+==========+
+| key=foo      | True       | False    | True     |
++--------------+------------+----------+----------+
+| key=bar      | False      | False    | True     |
++--------------+------------+----------+----------+
+| <empty>      | True       | True     | True     |
++--------------+------------+----------+----------+
+
+*Table 1: row are image properties, col are aggregate metadata.*
+
+The problem is:
+ * An image without key can still be scheduled in a tagged aggregate
+ * The hosts outside aggregates or in a no-tagged aggregate can still accept a
+   tagged image
+
+Proposed change
+===============
+
+We would like to add an option to:
+ * Make tagged aggregate refuse not-tagged images
+ * Make not-tagged aggregate accept ONLY not-tagged images
+
++--------------+------------+----------+----------+
+| img \\ aggr  | key=foo    | key=xxx  | <empty>  |
++==============+============+==========+==========+
+| key=foo      | True       | False    | False    |
++--------------+------------+----------+----------+
+| key=bar      | False      | False    | False    |
++--------------+------------+----------+----------+
+| <empty>      | False      | False    | True     |
++--------------+------------+----------+----------+
+
+*Table 2: row are image properties, col are aggregate metadata*
+
+We propose to add global option `aggregate_image_filter_strict_isolation` in
+the filter which dictates strictness level of the isolation:
+
+ * aggregate_image_filter_strict_isolation = False:
+   the filter functions as before (Tab. 1)
+ * aggregate_image_filter_strict_isolation = True:
+   the filter functions as proposed decision (Tab. 2)
+
+*For backward compatibility this option will be set by default to False.*
+
+We also propose to add this option configurable in per-aggregate.
+
+
+Alternatives
+------------
+
+* An alternative solution would be to create a new filter that inherits from
+  `AggregateImagePropertiesIsolation`.
+* A more configurable solution could be to use two config options
+  `allow_untagged_images_in_tagged_aggregate=True` and
+  `allow_tagged_images_in_untagged_aggregate=True` but currently we cannot
+  find any cases of using this alternative.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+* Operator needs to update the scheduler's `nova.conf` to set the option
+  `aggregate_image_filter_strict_isolation`.
+
+::
+  aggregate_image_filter_strict_isolation=True
+
+* To configure per-aggregate Operator needs to set the metadata.
+
+::
+  nova aggregate-set-metadata aggrA
+  aggregate_image_filter_strict_isolation=True
+
+*Note: For existing system, instances will be not re-scheduled. The operator
+always have the possibility to do migration.*
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  sahid-ferdjaoui
+
+Work Items
+----------
+
+* Updating `AggregateImagePropertiesIsolation` to accept the new global option.
+* Updating `AggregateImagePropertiesIsolation` to accept per-aggregate
+  configuration.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+* Unit tests can validate the expected behavior.
+
+Documentation Impact
+====================
+
+We need to update the documentation:
+  'doc/source/devref/filter_scheduler.rst'
+
+References
+==========
+
+* http://docs.openstack.org/developer/nova/devref/filter_scheduler.html#filtering
+* https://review.openstack.org/#/c/80940/
diff --git a/specs/juno/approved/return-all-servers-during-multiple-create.rst b/specs/juno/approved/return-all-servers-during-multiple-create.rst
new file mode 100644
index 0000000..980950d
--- /dev/null
+++ b/specs/juno/approved/return-all-servers-during-multiple-create.rst
@@ -0,0 +1,225 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=========================================
+Return all servers during multiple create
+=========================================
+
+https://blueprints.launchpad.net/nova/+spec/return-all-servers-during-multiple-create
+
+In this blueprint, we propose to improve the server create API in V3 by
+including all of the created servers in the response instead of only the first
+server. In V2, servers[0] is returned to the caller in response to a create
+request that has specified min/max_count.
+
+Problem description
+===================
+
+End users of the server create API have the ability to create multiple
+instances in one batch by specifying min/max_count in the request. One reason
+to use this ability is to scale up a cloud-hosted application quickly or in
+response to increased load. Upon requesting multiple servers, the user needs to
+know the list of servers that have been created in order to work with them. It
+would be ideal to receive the list in the response for the create request.
+
+Proposed change
+===============
+
+In order to provide the end user with the list of created servers most
+efficiently, we propose to change the server create API response in V3 from
+returning only one server to returning a list of servers. In the case when the
+end user has requested creation of just one server, a list containing one
+server will be returned.
+
+Alternatives
+------------
+
+Callers can work around in V2 by specifying return_reservation_id=True in the
+request to receive a reservation ID which they can use to obtain the list of
+servers. This is also currently possible in V3, but it requires an additional
+API call to get the server list.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+V3 API specification:
+
+ * Description: Create one or more servers
+
+ * Method type: POST
+
+ * Normal http response code: 202
+
+ * Expected error http response codes:
+
+  * 400: Invalid request parameter, image/flavor not found
+
+  * 409: Port in use, no unique match
+
+  * 413: Quota or port limit exceeded
+
+ * URL: v3/servers
+
+ * Example JSON request (no change)::
+
+    {
+        "server": {
+            "name": "server-test-1",
+            "image_ref": "b5660a6e-4b46-4be3-9707-6b47221b454f",
+            "flavor_ref": "2",
+            "max_count": 2,
+            "min_count": 2
+    }
+
+ * Example JSON response (new format)::
+
+    {
+        "servers": [
+            {
+                "admin_password": "qpYU66rKxmnK",
+                "id": "215d1109-216d-48c3-af8e-998bb9bc3ca0",
+                "links": [
+                    {
+                        "href": "http://openstack.example.com/v3/servers/<id>",
+                        "rel": "self"
+                    },
+                    {
+                        "href": "http://openstack.example.com/servers/<id>",
+                        "rel": "bookmark"
+                    }
+                ]
+            },
+            {
+                "admin_password": "wfksH3GTTseP",
+                "id": "440cf918-3ee0-4143-b289-f63e1d2000e6",
+                "links": [
+                    {
+                        "href": "http://openstack.example.com/v3/servers/<id>",
+                        "rel": "self"
+                    },
+                    {
+                        "href": "http://openstack.example.com/servers/<id>",
+                        "rel": "bookmark"
+                    }
+                ]
+            }
+        ]
+    }
+
+ * Partial JSON response schema definition to show change::
+
+    create = {
+        'type': 'object',
+        'properties': {
+            'servers': {
+                'type': 'array',
+                'items': {
+                    'type': 'object',
+                    'properties: {
+                        'admin_password': {type': 'string'},
+                        'id': {'type': 'string'},
+                        'links': {
+                            'type': 'array',
+                            'items': {
+                                'type': 'object',
+                                'properties': {
+                                    'href': {'type': 'string'},
+                                    'rel': {'type': 'string'}
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The python-novaclient will have to be changed to handle the list of servers in
+the V3 API server create response and show the list to the user.
+
+Performance Impact
+------------------
+
+For a server create API request for multiple servers, instead of returning only
+the first server, all of the created server objects must be serialized and
+returned in the response instead of just the first one.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  melwitt
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+ * Change the V3 API response for server create to return a list of instances.
+
+ * Update tests in tempest to handle the changed response.
+
+Dependencies
+============
+
+This blueprint is related to the tasks API blueprint [1] because it needs to
+interact with how tasks will work in V3. Initial comments on this interaction
+are available in the original review [2].
+
+[1] https://blueprints.launchpad.net/nova/+spec/instance-tasks-api
+
+[2] https://review.openstack.org/#/c/54214/
+
+Testing
+=======
+
+Tempest tests must be updated to accept the changed server create API
+response format. Tempest tests already exercise the various server creation
+scenarios, but the response format has changed for V3.
+
+Documentation Impact
+====================
+
+The changed REST API response for server create, as represented by the
+jsonschema definition above, will need to be documented. The changed API
+response will be available as API samples generated from testing.
+
+References
+==========
+
+None
diff --git a/specs/juno/approved/selecting-subnet-when-creating-vm.rst b/specs/juno/approved/selecting-subnet-when-creating-vm.rst
new file mode 100644
index 0000000..98c5c27
--- /dev/null
+++ b/specs/juno/approved/selecting-subnet-when-creating-vm.rst
@@ -0,0 +1,214 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================
+CreateVM supports subnet specified
+==================================
+
+https://blueprints.launchpad.net/nova/+spec/selecting-subnet-when-creating-vm
+
+Currently the network info specified as part of server creation is limited to
+network-id, port-id, and ip address. When a network has multiple subnets
+then we need to also be able to specify a subnet-id.
+
+
+Problem description
+===================
+
+Currently the network info specified as part of server creation is limited to
+network-id, port-id, and ip address.
+
+So if an network has multiple subnets in it, it's impossible to select
+which of the possible subnets a VM should be created in.
+You only could choose an ip address in one subnet and then create an instance.
+But this is not a convenient way. Moreover, this method is also not available
+for bulk instances creation.
+
+
+Proposed change
+===============
+
+1. Add one optional param 'subnet-id' in networking structure of 'spawn'.
+
+2. This parameter will affect in 'allocate_for_instance()'
+   in nova/network/neutronv2/api.py.
+
+3. Bulk instances creation with 'subnet-id' will be supported,
+   as the 'net-uuid' is specified.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+The new 'spawn' rest API in v2::
+ /v2/{project_id}/servers
+
+    {
+        'server':{
+        ...
+        'networks': [
+        {
+        'subnet-id': '892b9731-044a-4c87-b003-1e75869028c0'
+        }
+        ...
+        ]
+        ...
+        }
+
+    }
+
+and in v3 it is like::
+ /v3/servers
+
+{
+    'server':{
+    ...
+    'networks': [
+    {
+    'subnet-id': '892b9731-044a-4c87-b003-1e75869028c0'
+    }
+    ...
+    ]
+    ...
+    }
+
+}
+
+* Here, the <string> 'subnet-id' means the subnet your instances
+  want to be created in. No default value.
+
+* If 'subnet-id' is not a string or uuid-like, a BadRequest exception
+  will be raised.(HTTP 400)
+
+* The status code will be HTTP 202 when the request succeeded as usual,
+  and the response body won't be changed.
+
+In the current implement in Nova, the network info specified is limited to
+network-id, port-id, and ip address, and port-id has the highest priority.
+So, we also need to point the priority during server creation.
+
+* The 'port' parameter still has the highest priority here.
+
+  That means, if both 'port' & 'subnet-id' are specified, 'port' will be used
+  and the 'subnet-id' won't effect here.
+
+* The 'subnet-id' has the same priority with 'v4-fixed-ip'/'v6-fixed-ip'.
+
+  That means, if both 'subnet-id' & 'v4-fixed-ip'/'v6-fixed-ip' are specified,
+  compatibility validation of these two arguments will be executed.
+  * If it passed, the ip address you assigned will be used as usual.
+  * If not, a BadRequest exception will be raised.(HTTP 400)
+
+* The 'net-uuid' parameter still has the lowest priority like before.
+
+  That means, if both 'subnet-id' & 'net-id' are specified, 'subnet-id'
+  will effect here and 'net-uuid' will be ignored like port specified.
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The related works in python-novaclient will also be added.
+After this modification, user could create instances with 'subnet-id' specified
+like 'net-uuid' does via CLI.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Assignee: wingwj <wingwj@gmail.com>
+
+Work Items
+----------
+
+In nova:
+
+  * Add 'subnet-id' to 'create' in API layer
+
+  * Use 'subnet-id' for 'allocate_for_instance()'
+    in nova/network/neutronv2/api.py
+
+  * Add related tests both API & nova-compute
+
+In python-novaclient:
+
+  * Add 'subnet-id' support in python-novaclient
+
+  * Add related tests in python-novaclient
+
+In tempest:
+
+  * Related test-cases will definitely be added here
+
+In doc:
+
+  * The API modification will also be registered in openstack-doc
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+The unit tests need to be added in each related projects like I described
+in <Work Items> part. After the modifications, all changed methods above
+will be verified together.
+
+
+Documentation Impact
+====================
+
+The 'server creation' in API & CLI documentations will need to be updated to:
+
+* Reflect the new 'subnet-id' parameter and explain its usage
+* Explain the priority of network info during server creation
+
+
+References
+==========
+
+None
\ No newline at end of file
diff --git a/specs/juno/approved/server-count-api.rst b/specs/juno/approved/server-count-api.rst
new file mode 100644
index 0000000..1038da6
--- /dev/null
+++ b/specs/juno/approved/server-count-api.rst
@@ -0,0 +1,315 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================
+Nova Server Count API Extension
+===============================
+
+https://blueprints.launchpad.net/nova/+spec/server-count-api
+
+This blueprint proposes a new REST API extension that returns the number of
+servers that match the specified search criteria.
+
+
+Problem description
+===================
+
+There is no current API that can retrieve summary count data for servers that
+match a variety of search filters. For example, getting the total number of
+servers in a given state.
+
+Retrieving all servers and then manually determining the count data does not
+scale because pagination queries must be implemented (see Alternatives section
+for a detailed explanation).
+
+The use cases that are driving this API extension are derived from a user's
+experience in a GUI.
+
+Use Case 1: A UI dashboard that contains servers in various states for a cloud
+administrator. A new API extension is needed to retrieve the server count data
+associated with various filters (ie, servers in active state, servers in
+building state, servers in error state, etc.) for the entire cloud.
+
+Assume that you have 5k instances in your cloud. The admin wants to see a
+summary of instances in each state -- this API extension will help them
+quickly determine if there is an issue that need attention; for example, if
+there are many instances in 'error'. It is likely that once the admin sees
+this count that they will then drill down into the data. However, without
+this new API extension, the admin will not know if there are unacceptable
+number of systems in a given state without drilling down into each set.
+
+From a deployer's perspective, creating this dashboard with the existing APIs
+is very painful since pagination is required (assume more then the default of
+1k items). Also, processing time to get this data using the existing APIs
+(even the non-detailed) is slow (and possibly inaccurate -- see #3) compared
+to the processing time to get and return a single number.
+
+Use Case 2: Showing filtered data in a table in the UI. Assume that the UI
+supports tables that show filtered data (ie, table just showing instances in
+'error' state) and uses pagination to get the data. Many users do not like
+"infinite scrolling" where they have no idea how many items really are in the
+list (more just show up as you scroll down or navigate to the next page).
+Using this new count API, the UI table can indicate how many total items are
+in the list (ie, showing 1-20 of 1000).
+
+Assume that you have 500 instances in error state and that you can open a UI
+table showing their details -- when creating the table, assume that the UI
+uses a page size of 100 and assume that there is no dashboard showing the
+'error' count. In this case, the admin logs into the UI and wants to know
+how many servers are in error state. In order to do this, the admin navigates
+to the 'servers in error state' table -- the UI only retrieves the first 100
+items so it impossible to know if there are 101 total items or 500 total
+items. As an admin, I would like to know what the total number of items in the
+table is.
+
+Use Case 3:  Inherent timing window when adding a new item with limit/marker
+processing. Assume that you are using pagination to iterate over the data to
+get a count. When you are getting page n, it is possible that page n-1 has a
+new item x that was just added. Due to the sorting of the data, limit/marker
+will not detect that this new item was added.
+
+While this timing window is small, it does exist so getting an accurate count
+using this method is not guaranteed to be accurate.
+
+I realize that you can argue that the count API may not handle this UI use case
+either. However, the count will always be accurate from the DB at the time that
+the .count() function was processed -- the same claim cannot be made about
+getting the count using limit/marker since multiple DB calls are being invoked
+to calculate the number.
+
+
+Proposed change
+===============
+
+The new count API extension must accept that same filter values as the
+existing /servers and /servers/details APIs and re-use the existing filter
+processing (once the common parts are refactored into utility methods that
+can be utilized by both paths). Once the filters are processed to create the
+query object, then the number of matching servers will be retrieved and
+returned from the database.
+
+The count API extension will be both per tenant and global (admin-only),
+similar to the existing /servers APIs. An admin can supply the 'all_tenants'
+parameter to signify that server count data should be retrieved globally.
+
+This new flow requires new functions to retrieve the count value in the
+compute API layer, in the instance layer, and in the database layers; all
+functions return an integer value. The naming conventions for the functions
+will follow the existing functions used for retrieving server instances, for
+example:
+
+* Compute API: get_count function
+
+* Instance layer (InstanceList class): get_count_by_filters function
+
+* DB layer: instance_count_by_filters function
+
+* Sqlalchemy layer: instance_count_by_filters function
+
+In the sqlalchemy DB layer, the filter processing (for processing exact name
+filters, regex filters, and tag filters) needs to be moved into a common
+function so that both the new count API extension and the existing get servers
+APIs can utilize it. Once the query object is created, then the count()
+function is invoked to retrieve the total number of matching servers for the
+given query.
+
+For the v2 API extension, the existing filtering pre-processing done in
+nova.api.openstack.compute.servers.Controller._get_servers needs to be moved
+into a static utility method so that the new count API extension can utilize
+it; this is critical so that the filtering support for the count API matches
+the filtering support for the /servers API.
+
+For the v3 API, a new count function (similiar to 'index' and 'detail') needs
+to be added to nova.api.openstack.compute.plugins.v3.servers directly. Common
+filter processing needs to broken out into utility functions (same idea as the
+v2 API). For v3, the 'count' GET API can be registered with the Servers
+extensions.V3APIExtensionBase directly.
+
+Alternatives
+------------
+
+Other APIs exist that return count data (quotas and limit) but they do not
+accept filter values.
+
+A user could accomplish the same result (less the timing window noted in Use
+Case #3) using the existing non-detailed /servers API with a filter and then
+count up the results. However, the primary use case for this blueprint is
+getting summary count data at scale.  For example, if the total cloud has 5k
+VMs then doing paginated queries to iterate over the non-detailed '/servers'
+API with a filter and limit/marker is really inefficient -- the API is going
+to return more data then the user cares about (and do a lot of processing to
+get it).  Assume that there are 2,500 instances in an active state; if the
+non-detailed query (and the default limit of 1k) is used then the application
+would have to make 3 separate REST API calls to get the all of the VMs and,
+at the DB layer, the marker processing would be used to find the correct page
+of data to return.  Since the user only cares about a summary count, then the
+most efficient mechanism to retrieve that data would be a single DB query
+using the count() function.
+
+Note that the default maximum page set is set on the server (default of 1k);
+therefore, a user MUST HANDLE pagination since the number of items being
+queried may be greater then the default.
+
+There are other options for how the v2 and v3 APIs can be registered. For v2,
+the new count API could be registered by modifying the API routing in
+nova.api.openstack.compute.__init__.APIRouter directly (to create the
+/servers/count API just like /server/detail). Since v3 is still experimental,
+this blueprint is proposing that the count API is baked into
+nova.api.openstack.compute.plugins.v3.servers directly.
+
+I cannot think of alternative implementations. The new API needs to utilitize
+the existing filter processing as the current /servers APIs in order to ensure
+consistency and prevent dual maintenance.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+The response for the existing /servers and /servers/detail REST APIs will
+not be affected.
+
+* New v2 API extension:
+
+  * Name: ServerCounts
+  * Alias: os-server-counts
+
+* NEW v2 URL: v2/{tenant_id}/servers/count
+
+* NEW v3 URL: v3/servers/count
+
+* Description: Get number of servers
+
+* Method type: GET
+
+* Normal Response Codes: Same as the 'v2/{tenant_id}/servers/detail' API):
+
+  * 200
+  * 203
+
+* Error Response Codes (same as the 'v2/{tenant_id}/servers/detail' API):
+
+  * computeFault (400, 500, ...)
+  * serviceUnavailable (503)
+  * badRequest (400)
+  * unauthorized (401)
+  * forbidden (403)
+  * badMethod (405)
+
+* Parameters (same as the 'v2/{tenant_id}/servers' API except the 'limit' and
+  'marker' parameters):
+
++---------------+-------+--------------+--------------------------------------+
+| Parameter     | Style | Type         | Description                          |
++===============+=======+==============+======================================+
+| all_tenants   | query | xsd:boolean  | Display server count information     |
+| (optional)    |       |              | from all tenants (Admin only).       |
++---------------+-------+--------------+--------------------------------------+
+| changes-since | query | xsd:dateTime | A time/date stamp for when the       |
+| (optional)    |       |              | serverlast changed status.           |
++---------------+-------+--------------+--------------------------------------+
+| image         | query | xsd:anyURI   | Name of the image in URL format.     |
+| (optional)    |       |              |                                      |
++---------------+-------+--------------+--------------------------------------+
+| flavor        | query | xsd:anyURI   | Name of the flavor in URL format.    |
+| (optional)    |       |              |                                      |
++---------------+-------+--------------+--------------------------------------+
+| name          | query | xsd:string   | Name of the server as a string.      |
+| (optional)    |       |              |                                      |
++---------------+-------+--------------+--------------------------------------+
+| status        | query | csapi:Server | Value of the status of the server so |
+| (optional)    |       | Status       | that you can filter on "ACTIVE" for  |
+|               |       |              | example.                             |
++---------------+-------+--------------+--------------------------------------+
+
+  * JSON schema definition for the body data: N/A
+
+  * JSON schema definition for the response data: {"count": <int>}
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None -- This new API is not introducing any new DB joins that would affect
+performance.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Steven Kaufer
+
+Other contributors:
+  <launchpad-id or None>
+
+Work Items
+----------
+
+* Move filter processing code into utility functions at the API layer and at
+  the DB sqlalchemy layer.
+* Create new API functions in the various layers to get the count data.
+* v2 API extension and v3 API updates to expose the new count API function.
+
+
+Dependencies
+============
+
+Related (but independent) change being proposed in cinder:
+https://blueprints.launchpad.net/cinder/+spec/volume-count-api
+
+
+Testing
+=======
+
+Both unit and Tempest tests need to be created to ensure that the count data
+is accurate for various filters.
+
+Testing should be done against multiple backend database types.
+
+
+Documentation Impact
+====================
+
+Document the new v2 API extension and v3 API updates (see "REST API impact"
+section for details).
+
+
+References
+==========
+
+None
+
diff --git a/specs/juno/approved/standardize-nova-image.rst b/specs/juno/approved/standardize-nova-image.rst
new file mode 100644
index 0000000..41ea3e8
--- /dev/null
+++ b/specs/juno/approved/standardize-nova-image.rst
@@ -0,0 +1,125 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================
+Standardize Nova Image
+======================
+
+https://blueprints.launchpad.net/nova/+spec/standardize-nova-image
+
+Standardize Nova's nova.image module to work like nova.network.api
+and nova.volume.cinder.
+
+Problem description
+===================
+
+For some reason, nova.image does things differently than nova.volume and
+nova.network. Instead of nova.compute.manager instantiating a
+self.image_api object, like it does for self.network_api and
+self.volume_api, the compute manager calls an obtuse collection of
+nova.image.glance module calls.
+
+This blueprint is around the work to make a new nova.image.api module
+and have it called like other submodule "internal APIs" in Nova.
+
+Proposed change
+===============
+
+A new nova.image.api module shall be created, following in the style
+of nova.network.api and nova.volume.cinder. There will be an API class
+in the nova.image.api module that follows identical conventions as the
+nova.volume.cinder.API class, with methods for listing (get_all), showing
+(get), creating (create), updating (update), and removing (delete)
+images from the backend image store. There will be a nova.image.driver
+module with a base driver class.
+
+The nova.image.glance module will be updated to subclass the base driver
+class.
+
+Alternatives
+------------
+
+There is a series of patches in review up for Nova that tries to add
+support for Glance's V2 API operations:
+
+https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/use-glance-v2-api,n,z
+
+Unfortunately, I believe this patch series further muddies the image
+service inside Nova instead of making it cleaner and standardized with
+the rest of Nova's external API interfaces.
+
+The idea of this blueprint is to lay a good foundation for future V2
+Glance API work by first standardizing the image API inside of Nova.
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+
+See above link to Eddie Sheffield's patch series that would be affected by the
+code in this blueprint. Hopefully, however, once the image API is brought in
+line with the other internal-to-external Nova APIs, the work on V2 Glance API
+should be quite a bit easier.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jaypipes
+
+Work Items
+----------
+
+ * Create the nova.image.api module that instantiates a driver
+ * Create the base image driver class, modeling after the new
+   nova.network.driver class created by the refactor-network-api blueprint
+   code.
+ * Move the existing glance code into a subclassed driver
+
+Dependencies
+============
+None
+
+Testing
+=======
+None
+
+Documentation Impact
+====================
+None
+
+References
+==========
+None
diff --git a/specs/juno/approved/string-field-max-length.rst b/specs/juno/approved/string-field-max-length.rst
new file mode 100644
index 0000000..3aeee19
--- /dev/null
+++ b/specs/juno/approved/string-field-max-length.rst
@@ -0,0 +1,174 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================
+Allow StringField to enforce max length
+=======================================
+
+https://blueprints.launchpad.net/nova/+spec/string-field-max-length
+
+This blueprint aims to add a max length constraint to the
+`nova.objects.fields.StringField` class.
+
+Problem description
+===================
+
+Currently, the nova object framework revolves around the use of field type
+classes that describe the schema of an object. Each object model is simply
+a collection of fields, each of which have a particular type, such as
+IntegerField or StringField.
+
+In much the same way that a SQL database schema describes the constraints
+that a given column in a table must adhere to -- e.g. the length of characters
+possible in a CHAR field, or a valid DATETIME string -- the nova objects
+should be self-validating.
+
+Proposed change
+===============
+
+This specification proposes to change the `coerce` method of the
+`String` class to validate on the number of characters in the
+field's string value.
+
+The `StringField` concrete field class shall have a new `max_length` kwarg
+added to its constructor that will control the validation. The default
+value will be None, and no `StringField` objects defined in the schemas of
+any of the nova object models shall be changed in this spec.
+
+Alternatives
+------------
+
+None (keep things the way they are now)
+
+Data model impact
+-----------------
+
+None (the existing models themselves won't be changed in this specification
+at all)
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Roughly, the code the `String` field type class would change from this:
+
+.. code:: python
+
+    class String(FieldType):
+        @staticmethod
+        def coerce(obj, attr, value):
+            # FIXME(danms): We should really try to avoid the need to do this
+            if isinstance(value, (six.string_types, int, long, float,
+                                  datetime.datetime)):
+                return unicode(value)
+            else:
+                raise ValueError(_('A string is required here, not %s') %
+                                 value.__class__.__name__)
+
+to this:
+
+.. code:: python
+
+    class String(FieldType):
+
+        def __init__(self, max_length=None):
+            """
+            :param max_length: Optional constraint on the number of Unicode
+                               characters the string value can be.
+            """
+            self._max_length = max_length
+
+        @staticmethod
+        def coerce(self, obj, attr, value):
+            # FIXME(danms): We should really try to avoid the need to do this
+            if isinstance(value, (six.string_types, int, long, float,
+                                  datetime.datetime)):
+                result = unicode(value)
+            else:
+                raise ValueError(_('A string is required here, not %s') %
+                                 value.__class__.__name__)
+            if self._max_length is not None:
+                if len(value) > self._max_length):
+                    msg = _("String %(result)s is longer than maximum allowed "
+                            "length of %(max_length)d.")
+                    msg = msg % dict(result=result,
+                                     max_length=self._max_length)
+                    raise ValueError(msg)
+            return result
+
+The StringField class would then need to be modified to allow passing the
+max_length parameter along to its type class.
+
+Work Items
+----------
+
+N/A
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jaypipes
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Would need new unit tests. No need for any integration test changes.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+The server-instance-tagging work will likely be the first work to use
+this functionality, as the tag string has a max length associated with
+it and we need to be very careful about changing existing model fields' string
+length validation code, so a new field like the tag field is an ideal place to
+begin with this implementation.
+
+http://git.openstack.org/cgit/openstack/nova-specs/tree/specs/juno/tag-instances.rst
diff --git a/specs/juno/approved/support-console-log-migration.rst b/specs/juno/approved/support-console-log-migration.rst
new file mode 100644
index 0000000..867cd12
--- /dev/null
+++ b/specs/juno/approved/support-console-log-migration.rst
@@ -0,0 +1,140 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================================
+Support Console Log migration during Live-migration
+===================================================
+
+https://blueprints.launchpad.net/nova/+spec/support-console-log-migration
+
+Implement console log migration during live-migration in the libvirt driver
+
+
+Problem description
+===================
+
+Currently, in libvirt driver with a kvm hypervisor, console output is written
+to console.log. Nova responds to a get-console-log request with the contents
+of this file and this information is useful for debugging issues during boot
+process. However, during a live-migration the contents of the file in the
+source node is discarded.
+
+There are two issues which play a role in this.
+
+* The new kvm process in the destination would have already started using
+  an empty console log.
+
+* While the migration progresses the VM in the source node will continue
+  to write to the console log.
+
+Proposed change
+===============
+
+We propose the following in this blueprint to solve this issue without
+depending on kvm.
+
+* Require that VIR_MIGRATE_UNDEFINE_SOURCE is not set. Instead wait for the
+  condition that the instance is shutoff at the source.
+
+* During post-live-migration copy the console log be from source node and save
+  in the destination node as console.log.1. If log rotation is implemented,
+  all the rotated files need to be rotated once.
+
+* Change get-console-log function such that console.log and console.log.1 are
+  merged in the response (within the MAX_CONSOLE_BYTES limit). It log rotation
+  is implemented then the function needs to read as many files as it takes to
+  fill up the MAX_CONSOLE_BYTES limit.
+
+* The source VM would get undefined by the periodic task once the database is
+  updated with the new hostname.
+
+Alternatives
+------------
+
+* Change qemu to move the file content
+* Stream console output to a shared location
+* If spec/libvirt-serial-console is implemented we can leverage on that
+  mechanism and trigger a rotation and move it to destination.
+
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+
+* There's a brief window between the time the VM is activated in the
+  destination and before post-live-migration is completed. Any nova
+  console-log requests will return almost empty content during this window.
+
+Other deployer impact
+---------------------
+
+* If people are using VIR_MIGRATE_UNDEFINE_SOURCE then they need to remove this
+  option to get this feature. If this flag exists we will fallback to not
+  having the console log migrated.
+
+Developer impact
+----------------
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  parthipan
+
+Work Items
+----------
+
+* Change live-migration to wait for shutoff state if flag
+  VIR_MIGRATE_UNDEFINE_SOURCE is not set.
+* Change get_console_log to handle rotated log files
+* Implement console log migration during post-live-migration
+
+Dependencies
+============
+None
+
+Testing
+=======
+
+Tempest tests should be added to test that the console logs are merged in the
+response and catch other corner-cases.
+
+Documentation Impact
+====================
+
+We expect to have the following documentation changes:
+
+* The migration flag changes to get console logs migrated
+* Expected empty console log during the VM offline period in the final stages
+  of the migration
+
+References
+==========
+
+* https://bugs.launchpad.net/nova/+bug/1203193
diff --git a/specs/juno/approved/tag-instances.rst b/specs/juno/approved/tag-instances.rst
new file mode 100644
index 0000000..debc962
--- /dev/null
+++ b/specs/juno/approved/tag-instances.rst
@@ -0,0 +1,318 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================
+Allow simple string tagging of instances
+========================================
+
+https://blueprints.launchpad.net/nova/+spec/tag-instances
+
+This blueprint aims to add support for a simple string tagging mechanism
+for the instance object in the Nova domain model.
+
+Problem description
+===================
+
+In most popular REST API interfaces, objects in the domain model can be
+"tagged" with zero or more simple strings. These strings may then be used
+to group and categorize objects in the domain model.
+
+In order to align Nova's REST API with the Internet's common understanding
+of `resource tagging`_, we can add an API extension that allows normal users
+to add, remove and list tags for an instance.
+
+.. _resource tagging: http://en.wikipedia.org/wiki/Tag_(metadata)
+
+Proposed change
+===============
+
+No changes to existing metadata, system_metadata or extra_specs functionality
+are being proposed. This is *specfically* for adding a new API for *normal
+users* to be able to tag their instances with simple strings.
+
+Add a v2[.1] API extension that allows a user to add, remove, and list tags
+for an instance.
+
+Add a v2[.1] API extension to allow searching for instances based on one
+or more string tags.
+
+Alternatives
+------------
+
+Alternatives to simple string tagging are already available in Nova through
+the instance metadata key/value pairs API extension. However, these existing
+approaches suffer from a few issues:
+
+* The key/value pairs in the existing server metadata API extension are
+  all exposed via the nova-metadata endpoint, and therefore some people
+  think they are limited to being queried only from the 169.254.169.254
+  address.
+* It is not clear in the API that some metadata key/value pairs are added by
+  the user and some are added by Nova, Glance, or some external system. Part
+  of the idea behind this simple string tagging proposal is to have a way
+  to tag instances that is *only* for normal users.
+* Finally, and *most importantly*, the direction that the Glance program is
+  taking is to use simple string tagging for **user-side categorization of
+  resources**, and to use key/value pairs, hierarchical metadata, and property
+  bags for describing system-side metadata about resources. Property bags are
+  basically enumerated types for metadata, with a key and a constrained list of
+  value choices. The proposed Catalog program will be following a strategy
+  used by the Graffiti project that is designed to handle metadata/catalog data
+  of various formats in a structured way, and leave user-focused taxonomy as
+  simple-string tags only. This blueprint aligns with that direction.
+
+Data model impact
+-----------------
+
+The `nova.objects.instance.Instance` object would have a new `tags` field
+of type `nova.objects.fields.ListOfStrings` that would be populated on-demand
+(i.e. not eager-loaded).
+
+A tag shall be defined as a Unicode bytestring no longer than 60 bytes in
+length. (This length is entirely arbitrary and could be reduced or expanded
+depending on review discussion...)
+
+The tag is an opaque string and is not intended to be interpreted or even
+read by the virt drivers. In the REST API changes below, non-URL-safe
+characters in tags will need to be urlencoded if referred in the URI (for
+example, doing a DELETE /servers/{server}/tags/{tag}, the {tag} would need
+to be urlencoded.
+
+.. note::
+
+    Glance already has object tagging functionality, and the database schema
+    in that project uses a VARCHAR(255) length for the tag value. I would
+    greatly prefer to keep a shorter-than-255 length. There
+    are a number of performance reasons (including the fact that MySQL
+    converts all varchar columns to fixed-width columns when doing aggregation
+    and temporary tables containing the varchar columns). In addition, if the
+    tags are UTF-8 (as proposed above), the 255 width will actually be 765
+    bytes wide (which exacerbates the fixed-width problems on MySQL).
+
+For the database schema, the following table constructs would suffice ::
+
+    CREATE TABLE tags (
+        resource_id CHAR(32) NOT NULL PRIMARY KEY,
+        tag VARCHAR(80) NOT NULL CHARACTER SET utf8
+         COLLATION utf8_ci PRIMARY KEY
+    );
+
+There shall be a new hard-coded limit of 50 for the number of tags a user can
+use on a server. No need to make this configurable or use the quota system at
+this point.
+
+REST API impact
+---------------
+
+This proposal would add a v2[.1] API extension for retrieving and setting tags
+against an instance. In addition, it would add an API extension to allow the
+searching/listing of instances based on one or more string tags.
+
+The tag CRUD operations API extension would look like the following:
+
+Return list of tags for a server ::
+
+    GET /v2/{project_id}/servers/{server_id}/tags
+
+returns ::
+
+    [
+        'tag-one',
+        'tag-two'
+    ]
+
+JSONSchema document for response ::
+
+    {
+        "title": "Server tags",
+        "type": "array",
+        "items": {
+            "type": "string"
+        },
+    }
+
+Replace set of tags on a server ::
+
+    POST /v2/{project_id}/servers/{server_id}/tags
+
+with request payload ::
+
+    [
+        'tag-one',
+        'tag-three'
+    ]
+
+JSONSchema document for request ::
+
+    {
+        "title": "Server tags",
+        "type": "array",
+        "items": {
+            "$ref": "#/definitions/tag"
+        },
+        "maxItems": 50,
+        "definitions": {
+            "tag": {
+                "type": "string",
+                "maxLength": 60
+            }
+        }
+    }
+
+Returns a `200 OK`. If the number of tags exceeds the limit of tags per
+server, shall return a `403 Forbidden`
+
+Add a single tag on a server ::
+
+    PUT /v2/{project_id}/servers/{server_id}/tags/{tag}
+
+Returns `204 No Content`.
+
+If the tag already exists, no error is raised, it just returns the
+`204 No Content`
+
+If the number of tags would exceed the per-server limit, shall return a
+`403 Forbidden`
+
+Remove a single tag on a server ::
+
+    DELETE /v2/{project_id}/servers/{server_id}/tags/{tag}
+
+Returns `204 No Content` upon success. Returns a `404 Not Found` if you
+attempt to delete a tag that does not exist.
+
+Remove all tags on a server ::
+
+    DELETE /v2/{project_id}/servers/{server_id}/tags
+
+Returns `204 No Content`.
+
+The API extension that would allow searching/filtering of the `GET /servers`
+REST API call would add the following query parameters:
+
+* `tag` -- One or more strings that will be used to filter results in an
+  AND expression.
+* `tag-any` -- One or more strings that will be used to filter results in
+  an OR expression.
+
+Get all servers having a single tag ::
+
+    GET /v2/{project_id}/servers?tag={tag}
+
+Would return the servers having the `{tag}` tag. No change is needed to the
+JSON response for the `GET /v2/{project_id}/servers/` call.
+
+Get all servers having either of two tags ::
+
+    GET /v2/{project_id}/servers?tag-any={tag_a}&tag-any={tag_b}
+
+Would return the servers having either the `{tag_a}` or the `{tag_b}` tag.
+No change is needed to the JSON response for the
+`GET /v2/{project_id}/servers/` call.
+
+Get all servers having *both* tag A and tag B::
+
+    GET /v2/{project_id}/servers?tag={tag_a}&tag={tag_b}
+
+Would return the servers having both the `{tag_a}` AND the `{tag_b}` tag.
+No change is needed to the JSON response for the
+`GET /v2/{project_id}/servers/` call.
+
+Mixing of `tag` and `tag-any` is perfectly fine. All `tag-any` tags will
+be grouped into a single OR'd expression that is AND'd to the expression
+built from all of the `tag` tags. For example::
+
+    GET /v2/{project_id}/servers?tag=A&tag=B&tag-any=C&tag-any=D
+
+Would yield servers that were tagged with "A", "B", and either "C" or "D".
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None, though REGEXP-based querying on some fields might be modified to
+use a faster tag-list filtering query.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+See `Work Items`_ section below.
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jaypipes
+
+Other contributors:
+  snikitin
+
+Work Items
+----------
+
+Changes would be made, in order, to:
+
+1. the database API layer to add support for CRUD operations on instance tags
+2. the database API layer to add tag-list filtering support to
+   `instance_get_all_by_filters`
+3. the nova.objects layer to add support for a tags field of the Instance
+   object
+4. the API extension for CRUD operations on the tag list
+
+Dependencies
+============
+
+Soft dependency on specification for adding field type validation to nova
+objects. I say soft because technically this blueprint can be implemented
+with the tag string length validation done at the database schema level:
+
+https://blueprints.launchpad.net/nova/+spec/field-type-validation
+
+Note that the above is NOT a hard dependency and the work for this blueprint
+should not be held up for it. Hard-coded database schema string size limits
+are usable in this blueprint for the tag string length constraint.
+
+Testing
+=======
+
+Would need new Tempest and unit tests.
+
+Documentation Impact
+====================
+
+Docs needed for new API extension and usage.
+
+References
+==========
+
+Mailing list discussions:
+
+http://lists.openstack.org/pipermail/openstack-dev/2014-April/033222.html
+http://www.mail-archive.com/openstack-dev@lists.openstack.org/msg23310.html
diff --git a/specs/juno/approved/use-libvirt-storage-pools.rst b/specs/juno/approved/use-libvirt-storage-pools.rst
new file mode 100644
index 0000000..8c54b78
--- /dev/null
+++ b/specs/juno/approved/use-libvirt-storage-pools.rst
@@ -0,0 +1,291 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=========================
+Use libvirt Storage Pools
+=========================
+
+https://blueprints.launchpad.net/nova/+spec/use-libvirt-storage-pools
+
+Currently, the libvirt driver does not make use of libvirt's storage pools
+and volumes.  Using libvirt storage pools would simplify adding support for
+new image backends, as well as facilitating cold migrations (see follow up
+blueprint).
+
+
+Problem description
+===================
+
+Currently, Nova's libvirt driver does not make any use of libvirt volumes
+and storage pools.
+
+This means that, for the image backends, we have a lot
+of code that deals directly with various images backend formats, and we have
+to manually deal with a variety of different situations via various command
+line tools and libraries.
+
+However, much of this functionality is already present in libvirt, in the form
+of libvirt storage pools, so the libvirt driver duplicates functionality
+already present in libvirt itself.
+
+Proposed change
+===============
+
+The cache of images downloaded from Glance would be placed into a volume pool
+(:code:`nova-base-images-pool`).  This is done simply by instructing libvirt
+that Nova's image cache directory (e.g. :code:`/var/lib/nova/_base`) is a
+volume pool, and as such does not affect directory layout (and is thus
+compatible with both the legacy image backends and the new image backend
+proposed below).
+
+A new image backend, :code:`LibvirtStorage`, would be introduced.  This would
+support being used in place of all of the current types (with the exeception of
+RBD support, which for the time being would need a subclass [1]_).
+
+If we are not using COW, the libvirt :code:`pool.createXMLFrom` method
+could be used to appropriately copy the template image from the source pool,
+:code:`nova-base-images-pool`, into the target image in the target pool
+`nova-disks-pool`.
+
+If we are using COW, the libvirt :code:`pool.createXML` method could be used
+with a :code:`backingStore` element, which will appropriately create the new
+QCOW2 file with the backing file as the file in the image cache.
+
+This has the additional benefit of paving the way for the simplification of the
+image cache manager -- instead of having to run an external executable to check
+if an image is in the qcow2 format and has a backing store, we can simply check
+the :code:`backingStore` element's :code:`path` subelement for each
+libvirt volume (this also makes the code less brittle, should we decide to
+support other formats with backing stores) [2]_.
+
+A similar approach could be used with :code:`extract_snapshot` -- use
+:code:`createXMLFrom` to duplicate the libvirt volume (the new XML we pass
+in can handle compression, etc).
+
+In order to associate images with instances, the volumes in `nova-disks-pool`
+would have a name of the form `{instance-uuid}_{name}` (with :code:`name` being
+"disk", "kernel", etc, depending on the name passed to the image creation
+method).  This way, it still remains easy to find the disk image associated
+with a particular instance.
+
+The use of this new backend would become the default for new installations.
+However, the legacy backends would be left in place to maintain the live
+upgrade functionality (e.g. Icehouse->Juno). See the `Other deployer impact`_
+section below for more information.
+
+For the :code:`disk` XML element in the :code:`domain` element supplied to
+libvirt on instance creation, a type of :code:`volume` can be supplied, with
+the :code:`<source>` element specifying the pool name and volume name [3]_.
+
+.. [1] Currently, libvirt does not have support for the createXMLFrom operation
+   for RBD-backed pools, so for RDB support, we would have to subclass the new
+   backend and add in code to manually upload the template image.  This
+   functionality should be present in a future version of libvirt. See
+   `Red Hat BZ 1089079 <https://bugzilla.redhat.com/show_bug.cgi?id=1089079>`_.
+
+.. [2] Note that this functionality will most likely have to wait until the
+   OpenStack K release to be enabled by default, since such functionality would
+   be difficult to implement while supporting instances using both the legacy
+   and new backend -- see the `Other deployer impact`_ section below.  It could
+   be enabled in Juno by setting the :code:`images_type` configuration option
+   to 'libvirt-storage', which would imply that the deployer didn't want the
+   transitional functionality described in the aforementioned section.
+
+.. [3] Note that this XML is only available in libvirt version 1.0.5 and up,
+   so if we wish to support a version less than that for Juno, we
+   would simply have to rely on the current code (with some slight tweaks -- we
+   no longer have to try to detect the format, etc ourselves, as libvirt will
+   give it to us via the libvirt volume XML specification).
+
+Alternatives
+------------
+
+The setup described in this document calls for using a single storage pool
+for all VMs on a system.
+
+When using a file-based backend, this would require storing disk images in a
+single directory (such as :code:`/var/lib/nova/instance/disks`) instead of the
+current setup, where the disk images are stored in the instance directory
+(:code:`/var/lib/nova/instances/{instance-id}`).  This is due to the way that
+the libvirt :code:`dir` storage pool works.
+
+While it would be possible to create a new storage pool for each instance,
+this would only be applicable for file-based backends.  Having different
+functionality between file-based backends and other backends would complicate
+the code and reduce the abstraction introduced by this blueprint.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+Since the :code:`createXMLFrom` is actually intelligent about creating and
+copying image files (for instance, it calls :code:`qemu-img` under the hood
+when appropriate), there should be no performance impact.  As per what is
+mentioned in the `Proposed change`_ section, we would maintain current image
+cache functionality, including support for COW (via QCOW2), while paving the
+road for other file formats that libvirt supports as well.
+
+Other deployer impact
+---------------------
+
+For live migration/upgrade from OpenStack Icehouse to OpenStack Juno, the
+legacy image backends (and support for them in Nova's image cache) will be left
+in place for the next release (Juno), but will be marked as deprecated.  In
+the K release, the legacy backends will be removed (as well as support for
+them in the image cache manager).
+
+To allow existing installations to easily transition to the new backend,
+existing instances would be left on the legacy backend, while all new instances
+would be created to use the new backend.  Whether or not an instance was using
+a legacy backend could be determined by checking the instance directory for
+images (if they are present, the instance is using a legacy backend, if not the
+instance is using the new backend).
+
+During operations which allow the changing of libvirt XML, such as cold
+migrations, resizes, reboots, and live migrations, instances would be
+automatically transitioned to using the new system [5]_.  This would allow
+deployers to move to the new system at their leisure, since they could either
+choose to bulk-restart the VMs themselves, or simply ask the VMs owners to do
+so when convinient.  For instances still on the legacy system, a warning would
+be issued on compute node startup.
+
+.. [5] This would entail telling libvirt to use the volume as the disk source.
+   In the case of live migrations with shared storage, resizes to the same
+   host, and reboots, a couple extra steps would be taken for deployments using
+   the local-file-based legacy backends.  For reboots and resizes, we can
+   simply move the disk image file to the directory pool location while the VM
+   is shut off.  In the case of shared storage which supports hard-linking, a
+   hard link pointing to the disk image file would be placed into the storage
+   pool directory.  Once the live migration finishes, the original location
+   would be deleted, leaving the new hard link as the only remaining reference
+   to the disk image file.  For filesystems where hard linking isn't supported,
+   a block live migration would be necessary to migrate the VM to the new image
+   backend.
+
+Developer impact
+----------------
+
+Currently, file-based images for a particular instance are stored in the
+instance directory (:code:`/var/lib/nova/instances/{instance-id}`).  In order
+to have one storage pool per compute node, libvirt's directory-based storage
+pool would require all of the disk images to be stored in one directory, so
+the images themselves would no longer be in
+:code:`/var/lib/nova/instances/{instance-id}`, but instead in something
+to the effect of :code:`/var/lib/nova/instance/disks`.
+
+Should it be desired to have different disk types (e.g. main disk vs swap)
+stored differently [6]_, we could simply create a pool for each type, and place
+the images into the appropriate pool based on their name.  An advantage to
+using pools is that Nova doesn't actually need to know the underlying details
+about the pool, only its name.  Thus, if a deployer wanted to move a particular
+pool to a different location, device, etc, no XML changes would be needed,
+assuming the same pool name was kept.
+
+.. [6] As suggested in
+   `this blueprint <https://review.openstack.org/#/c/83727>`_, for instance
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    sross-7
+
+Other contributors:
+    None
+
+Work Items
+----------
+
+1. Modify the code which downloads images from Glance into a cache to
+   create a storage pool in the cache directory and refresh the cache
+   when a new image is downloaded.
+
+2. Implement the new image backend (and subclass it for RBD as long as it's not
+   supported natively as per [1]_) and sections in the XML config builder to
+   accept the :code:`volume` type for disk elements.
+
+3. Implement the functionality required to support transitional installations
+   (detecting legacy backend use, adding code to migration and reboots to
+   transition into new backend use).
+
+4. Implement functionality in the image cache manager to take advantage of the
+   new data about backing files stored in libvirt's volume information XML
+   (this would be disabled in Juno unless :code:`images_type` was set to
+   'libvirt-storage', implying the deployer didn't want the transitional
+   functionality mentioned above).
+
+
+Dependencies
+============
+
+No new libraries are required for this change.  However, the XML changes
+discussed above require a libvirt version > 1.0.5 (the actual storage pools do
+not, however).  While this is not strictly needed (as we can simply use the
+existing code for determining the correct XML for a given image), it does
+simplify the section of the code responsible for XML generation.  Since we
+will most likely be increasing the minimum libvirt version for Juno, however,
+this should not be problematic.
+
+Testing
+=======
+
+We will want to duplicate the existing tests for the various image backends to
+ensure that the new backend covers all of the existing functionality.
+Additionally, new tests should be introduced for:
+
+* the XML changes
+
+* storage pool management
+
+* migrating existing instances to the new backend and the supporting
+  transitional functionality
+
+Documentation Impact
+====================
+
+We should warn about the deprecation of the legacy image backends,
+and note the change to the new backend.  It should also be noted that
+migrations and cold resizes are the preferred method to transition existing
+instances to the new backend.
+
+
+References
+==========
+
+* http://libvirt.org/formatdomain.html#elementsDisks
+
+* http://libvirt.org/formatstorage.html
+
+* http://libvirt.org/storage.html
+
+* http://libvirt.org/html/libvirt-libvirt.html#virStorageVolCreateXMLFrom
diff --git a/specs/juno/approved/vif-vhostuser.rst b/specs/juno/approved/vif-vhostuser.rst
new file mode 100644
index 0000000..5e3164b
--- /dev/null
+++ b/specs/juno/approved/vif-vhostuser.rst
@@ -0,0 +1,163 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================
+Create VIF_VHOSTUSER
+====================
+
+https://blueprints.launchpad.net/nova/+spec/vif-vhostuser
+
+We propose to add a new VIF type to support the new QEMU vhost-user
+feature. vhost-user is a new QEMU feature that supports efficient
+Virtio-net I/O between a guest and a user-space vswitch. vhost-user is
+the userspace equivalent to /dev/vhost-net and is based on a Unix
+socket for communication instead of a kernel device file.
+
+
+Problem description
+===================
+
+QEMU has a new type of network interface, vhost-user, and we want to
+make this available to Neutron drivers. This will support deploying
+high-throughput userspace vswitches for OpenStack-based NFV
+applications. (This is the reason that vhost-user was developed.)
+
+
+Proposed change
+===============
+
+This change defines nova.network.model.VIF_TYPE_VHOSTUSER.
+
+We propose to add VIF_VHOSTUSER to Nova for creating network
+interfaces based on vhost-user. This VIF type would be enabled by
+Neutron drivers that want to assign certain ports to a userspace agent
+(vswitch) that is based on vhost-user.
+
+VIF_VHOSTUSER is to be implemented by extending the Libvirt driver.
+Libvirt support for vhost-user is currently under review and we expect
+it to be merged in time for Juno. We see that upstream Libvirt support
+for vhost-user is a dependency for merging the VIF_VHOSTUSER
+implementation into Nova.
+
+
+Alternatives
+------------
+
+Intel DPDK has a separate mechanism for accessing vhost from
+userspace, based on replacing /dev/vhost-net with a FUSE-based device
+file that traps ioctls into userspace. However, vhost-user is the new
+standard way to achieve this and is upstream in QEMU.
+
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+vhost-user will make OpenStack compatible with vswitches supporting N
+x 10G Virtio-net workloads.
+
+
+Other deployer impact
+---------------------
+
+VIF_VHOSTUSER does not have to be enabled by the deployer. Neutron
+drivers will automatically enable VIF_VHOSTUSER via port binding if
+this is the appropriate choice for the agent on the compute host.
+
+VIF_VHOSTUSER will require a version of QEMU with vhost-user support,
+which is currently upstream and will be released in QEMU 2.1.
+
+VIF_VHOSTUSER will also require a version of Libvirt with vhost-user
+support.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Luke Gorrie <lukego>
+
+Other contributors:
+  m.paolino
+
+Work Items
+----------
+
+* Add vhost-user support to the Libvirt driver.
+* Add VIF_VHOSTUSER support to Nova.
+
+Dependencies
+============
+
+* Libvirt must add support for vhost-user. Current patch under review:
+  http://www.redhat.com/archives/libvir-list/2014-July/msg00111.html
+
+* VIF_VHOSTUSER will enable the Neutron driver for Snabb NFV:
+  https://blueprints.launchpad.net/neutron/+spec/snabb-nfv-mech-driver
+  http://snabb.co/nfv.html
+  http://github.com/SnabbCo/snabbswitch
+
+
+Testing
+=======
+
+VIF_VHOSTUSER will be Tempest-tested by the planned 3rd party CI
+integration for the Snabb NFV mech driver.
+
+
+Documentation Impact
+====================
+
+No documentation changes for Nova are anticipated. VIF_VHOSTUSER will
+be automatically enabled by Neutron where appropriate.
+
+
+References
+==========
+
+* vhost-user:
+  http://www.virtualopensystems.com/en/solutions/guides/snabbswitch-qemu/
+
+* Snabb NFV (initial vswitch supporting vhost-user): http://snabb.co/nfv.html
+
+* Deutsche Telekom TeraStream project (initial user of VIF_VHOSTUSER):
+  http://blog.ipspace.net/2013/11/deutsche-telekom-terastream-designed.html
+
+* Discussion from NFV BoF (Atlanta) etherpad:
+  https://etherpad.openstack.org/p/juno-nfv-bof
+
diff --git a/specs/juno/approved/virt-driver-cpu-pinning.rst b/specs/juno/approved/virt-driver-cpu-pinning.rst
new file mode 100644
index 0000000..24890f0
--- /dev/null
+++ b/specs/juno/approved/virt-driver-cpu-pinning.rst
@@ -0,0 +1,224 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================
+Virt driver pinning guest vCPUs to host pCPUs
+=============================================
+
+https://blueprints.launchpad.net/nova/+spec/virt-driver-cpu-pinning
+
+This feature aims to improve the libvirt driver so that it is able to strictly
+pin guest vCPUS to host pCPUs. This provides the concept of "dedicated CPU"
+guest instances.
+
+Problem description
+===================
+
+If a host is permitting overcommit of CPUs, there can be prolonged time
+periods where a guest vCPU is not scheduled by the host, if another guest is
+competing for the CPU time. This means that workloads executing in a guest can
+have unpredictable latency, which may be unacceptable for the type of
+application being run.
+
+Depending on the workload being executed the end user or admin may wish to
+have control over how the guest used hyperthreads. To maximise cache
+efficiency, the guest may wish to be pinned to thread siblings. Conversely
+the guest may wish to avoid thread siblings (ie only pin to 1 sibling)
+or even avoid hosts with threads entirely.
+
+Proposed change
+===============
+
+The flavour extra specs will be enhanced to support two new parameters
+
+* hw:cpu_policy=shared|dedicated
+* hw:cpu_threads_policy=avoid|separate|isolate|prefer
+
+If the policy is set to 'shared' no change will be made compared to the current
+default guest CPU placement policy. The guest vCPUs will be allowed to freely
+float across host pCPUs, albeit potentially constrained by NUMA policy. If the
+policy is set to 'dedicated' then the guest vCPUs will be strictly pinned to a
+set of host pCPUs. In the absence of an explicit vCPU topology request, the
+virt drivers typically expose all vCPUs as sockets with 1 core and 1 thread.
+When strict CPU pinning is in effect the guest CPU topology will be setup to
+match the topology of the CPUs to which it is pinned. ie if a 2 vCPU guest is
+pinned to a single host core with 2 threads, then the guest will get a topology
+of 1 socket, 1 core, 2 threads.
+
+The threads policy will control how the scheduler / virt driver place guests
+wrt CPU threads. It will only apply if the sheduler policy is 'dedicated'
+
+ - avoid: the scheduler will not place the guest on a host which has
+   hyperthreads.
+ - separate: if the host has threads, each vCPU will be placed on a
+   different core. ie no two vCPUs will be placed on thread siblings
+ - isolate: if the host has threads, each vCPU will be placed on a
+   different core and no vCPUs from other guests will be able to be
+   placed on the same core. ie one thread sibling is guaranteed to
+   always be unused,
+ - prefer: if the host has threads, vCPU will be placed on the same
+   core, so they are thread siblings.
+
+The image metadata properties will also allow specification of the
+threads policy
+
+* hw_cpu_threads_policy=avoid|separate|isolate|prefer
+
+This will only be honoured if the flavour does not already have a threads
+policy set. This ensures the cloud administrator can have absolute control
+over threads policy if desired.
+
+The schedular will have to be enhanced so that it considers the usage of CPUs
+by existing guests. Use of a dedicated CPU policy will have to be accompanied
+by the setup of aggregates to split the hosts into two groups, one allowing
+overcommit of shared pCPUs and the other only allowing dedicated CPU guests.
+ie we do not want a situation with dedicated CPU and shared CPU guests on the
+same host. It is likely that the administrator will already need to setup host
+aggregates for the purpose of using huge pages for guest RAM. The same grouping
+will be usable for both dedicated RAM (via huge pages) and dedicated CPUs (via
+pinning).
+
+The compute host already has a notion of CPU sockets which are reserved for
+execution of base operating system services. This facility will be preserved
+unchanged. ie dedicated CPU guests will only be placed on CPUs which are not
+marked as reserved for the base OS.
+
+Alternatives
+------------
+
+There is no alternative way to ensure that a guest has predictable execution
+latency free of cache effects from other guests working on the host, that does
+not involve CPU pinning.
+
+The proposed solution is to use host aggregates for grouping compute hosts into
+those for dedicated vs overcommit CPU policy. An alternative would be to allow
+compute hosts to have both dedicated and overcommit guests, splitting them onto
+separate sockets. ie if there were for sockets, two sockets could be used for
+dedicated CPU guests while two sockets could be used for overcommit guests,
+with usage determined on a first-come, first-served basis. A problem with this
+approach is that there is not strict workload isolation even if separate
+sockets are used. Cached effects can be observed, and they will also contend
+for memory access, so the overcommit guests can negatively impact performance
+of the dedicated CPU guests even if on separate sockets. So while this would
+be simpler from an administrative POV, it would not give the same performance
+guarantees that are important for NFV use cases. It would none the less be
+possible to enhance the design in the future, so that overcommit & dedicated
+CPU guests could co-exist on the same host for those use cases where admin
+simplicity is more important than perfect performance isolation. It is believed
+that it is better to start off with the simpler to implement design based on
+host aggregates for the first iteration of this feature.
+
+Data model impact
+-----------------
+
+No impact.
+
+The new data items are stored in the existing flavour extra specs data model
+and in the host state metadata model.
+
+REST API impact
+---------------
+
+No impact.
+
+The existing APIs already support arbitrary data in the flavour extra specs.
+
+Security impact
+---------------
+
+No impact.
+
+Notifications impact
+--------------------
+
+No impact.
+
+The notifications system is not used by this change.
+
+Other end user impact
+---------------------
+
+There are no changes that directly impact the end user, other than the fact
+that their guest should have more predictable CPU execution latency.
+
+Performance Impact
+------------------
+
+The schedular will incur small further overhead if a threads policy is set
+on the image or flavour. This overhead will be negligible compared to that
+implied by the enhancements to support NUMA policy and huge pages. It is
+anticipated that dedicated CPU guests will typically be used in conjunction
+with huge pages.
+
+Other deployer impact
+---------------------
+
+The cloud administrator will gain the ability to define flavours which offer
+dedicated CPU resources. The administrator will have to place hosts into groups
+using aggregates such that the schedular can separate placement of guests with
+dedicated vs shared CPUs. Although not required by this design, it is expected
+that the administrator will commonly use the same host aggregates to group
+hosts for both CPU pinning and large page usage, since these concepts are
+complementary and expected to be used together. This will minimise the
+administrative burden of configuring host aggregates.
+
+Developer impact
+----------------
+
+It is expected that most hypervisors will have the ability to setup dedicated
+pCPUs for guests vs shared pCPUs. The flavour parameter is simple enough that
+any Nova driver would be able to support it.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  berrange
+
+Other contributors:
+  ndipanov
+
+Work Items
+----------
+
+* Enhance libvirt to support setup of strict CPU pinning for guests when the
+  appropriate policy is set in the flavour
+
+* Enhance the schedular to take account of threads policy when choosing
+  which host to place the guest on.
+
+Dependencies
+============
+
+* Virt driver guest NUMA node placement & topology
+
+   https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
+
+Testing
+=======
+
+It is unknown at this time if the gate hosts have sufficient pCPUs available
+to allow this feature to be effectively tested by tempest.
+
+Documentation Impact
+====================
+
+The new flavour parameter available to the cloud administrator needs to be
+documented along with recommendations about effective usage. The docs will
+also need to mention the compute host deployment pre-requisites such as the
+need to setup aggregates.
+
+References
+==========
+
+Current "big picture" research and design for the topic of CPU and memory
+resource utilization and placement. vCPU topology is a subset of this
+work
+
+* https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement
diff --git a/specs/juno/approved/virt-driver-large-pages.rst b/specs/juno/approved/virt-driver-large-pages.rst
new file mode 100644
index 0000000..f549067
--- /dev/null
+++ b/specs/juno/approved/virt-driver-large-pages.rst
@@ -0,0 +1,340 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================
+Virt driver large page allocation for guest RAM
+===============================================
+
+https://blueprints.launchpad.net/nova/+spec/virt-driver-large-pages
+
+This feature aims to improve the libvirt driver so that it can use large pages
+for backing the guest RAM allocation. This will improve the performance of
+guest workloads by increasing TLB cache efficiency. It will ensure that the
+guest has 100% dedicated RAM that will never be swapped out.
+
+Problem description
+===================
+
+Most modern virtualization hosts support a variety of memory page sizes. On
+x86 the smallest, used by the kernel by default, is 4kb, while large sizes
+include 2MB and 1GB. The CPU TLB cache has a limited size, so when there is a
+very large amount of RAM present and utilized, the cache efficiency can be
+fairly low which in turn increases memory access latency. By using larger page
+sizes, there are fewer entries needed in the TLB and thus its efficiency goes
+up.
+
+The use of huge pages for backing guests implies that the guest is running with
+a dedicated resource allocation. ie the concept of memory overcommit is no
+longer possible to provide. This is a tradeoff that cloud administrators may
+be willing to make to support workloads that require predictable memory access
+times, such as NFV.
+
+While large pages are better than small pages, it can't be assumed that the
+benefit increases as the page size increases. In some workloads, a 2 MB page
+size can be better overall than 1 GB page sizes. Also the choice of page size
+affects the granularity of guest RAM size. ie a 1.5 GB guest would not be able
+to use 1 GB pages since RAM is not a multiple of the page size.
+
+Although it is theoretically possible to reserve large pages on the fly, after
+a host has been booted for a period of time, physical memory will have become
+very fragmented. This means that even if the host has lots of free memory, it
+may be unable to find contiguous chunks required to provide large pages. This
+is a particular problem for 1 GB sized pages. To deal with this problem, it is
+usual practice to reserve all required large pages upfront at host boot time,
+by specifying a reservation count on the kernel command line of the host. This
+would be a one-time setup task done when deploying new compute node hosts.
+
+Proposed change
+===============
+
+The flavour extra specs will be enhanced to support a new parameter
+
+* hw:mem_page_size=large|small|any|2MB|1GB
+
+In absence of any page size setting in the flavour, the current behaviour of
+using the small, default, page size will continue. A setting of 'large' says
+to only use larger page sizes for guest RAM, eg either 2MB or 1GB on x86;
+'small' says to only use the small page sizes, eg 4k on x86, and is the
+default; 'any' means to leave policy upto the compute driver implementation to
+decide. When seeing 'any' the libvirt driver might try to find large pages,
+but fallback to small pages, but other drivers may choose alternate policies
+for 'any'. Finally an explicit page size can be set if the workload has very
+precise requirements for a specific large page size. It is expected that the
+common case would be to use page_size=large or page_size=any. The
+specification of explicit page sizes would be something that NFV workloads
+would require.
+
+The property defined for the flavour can also be set against the image, but
+the use of large pages would only be honoured if the flavour already had a
+policy or 'large' or 'any'. ie if the flavour said 'small', or a specific
+numeric page size, the image would not be permitted to override this to access
+other large page sizes. Such invalid override in the image would result in
+an exception being raised and the attempt to boot the instance resulting in
+an error. While ultimate validation is done in the virt driver, this can also
+be caught and reported at the at the API layer.
+
+If the flavor memory size is not a multiple of the specified huge page size
+this would be considered an error which would cause the instance to fail to
+boot. If the page size is 'large' or 'any', then the compute driver would
+obviously attempt to pick a page size which was a multiple of the RAM size
+rather than erroring. This is only likely to be a significant problem when
+when using 1 GB page sizes, which imply that ram size must be in 1 GB
+increments.
+
+The libvirt driver will be enhanced to honour this parameter when configuring
+the guest RAM allocation policy. This will effectively introduce the concept
+of a "dedicated memory" guest, since large pages must be 1-to-1 associated with
+guests - there's not facility to over commit by allowing one large page to be
+used with multiple guests or to swap large pages.
+
+The libvirt driver will be enhanced to report on large page availability per
+NUMA node, building on previously added NUMA topology reporting.
+
+The scheduler will be enhanced to take account of the page size setting on the
+flavour and pick hosts which have sufficient large pages available when
+scheduling the instance. Conversely if large pages are not requested, then the
+scheduler needs to avoid placing the instance on a host which has pre-reserved
+large pages. The enhancements for the scheduler will be done as part of the
+new filter that is implemented as part of the NUMA topology blueprint. This
+involves altering the logic done in that blueprint, so that instead of just
+looking at free memory in each NUMA node, it instead looks at the free page
+count for the desired page size.
+
+As illustrated later in this document each host will be reporting on all
+page sizes available and this information will be available to the scheduler.
+So when it interprets 'small', it will consider the smallest page size
+reported by the compute node. Conversely when intepreting 'large' it will
+consider any page size except the smallest one. This obviously implies that
+there is potential for 'large' and 'small' to have different meanings
+depending on the host being considered. For the use cases where this would
+be a problem, an explicit page size would be requested instead of using
+these symbolic named sizes. It will also have to consider whether the page
+size is a multiple of the flavor memory size. If the instance is using
+multiple NUMA nodes, it will have to consider whether the RAM in each
+guest node is a multiple of the page size, rather than the total memory
+size.
+
+Alternatives
+------------
+
+Recent Linux hosts have a concept of "transparent huge pages" where the kernel
+will opportunistically allocate large pages for guest VMs. The problem with
+this is that over time, the kernel's memory allocations get very fragmented
+making it increasingly hard to find contiguous blocks of RAM to use for large
+pages. This makes transparent large pages impractical for use with 1 GB page
+sizes. The opportunistic approach also means that users do not have any hard
+guarantee that their instance will be backed by large pages. This makes it an
+unusable approach for NFV workloads which require hard guarantees.
+
+Data model impact
+-----------------
+
+The previously added data in the host state structure for reporting NUMA
+topology would be enhanced to further include information on page size
+availability per node. So it would then look like
+
+::
+
+  hw_numa = {
+     nodes = [
+         {
+            id = 0
+            cpus = 0, 2, 4, 6
+            mem = {
+               total = 10737418240
+               free = 3221225472
+            },
+            mempages = {
+               4096 = {
+                  total = 262144
+                  free = 262144
+               }
+               2097152 = {
+                  total = 1024
+                  free = 1024
+               }
+               1073741824 = {
+                  total = 7
+                  free = 0
+               }
+            }
+            distances = [ 10, 20],
+         },
+         {
+            id = 1
+            cpus = 1, 3, 5, 7
+            mem = {
+               total = 10737418240
+               free = 5368709120
+            },
+            mempages = {
+               4096 = {
+                  total = 262144
+                  free = 262144
+               }
+               2097152 = {
+                  total = 1024
+                  free = 1024
+               }
+               1073741824 = {
+                  total = 7
+                  free = 2
+               }
+            }
+            distances = [ 20, 10],
+         }
+     ],
+  }
+
+The data provided to the extensible resource tracker would be similarly
+enhanced to include this page info in a flattened format, which can be
+efficiently queried based on the key name:
+
+* hw_numa_nodes=2
+* hw_numa_node0_cpus=4
+* hw_numa_node0_mem_total=10737418240
+* hw_numa_node0_mem_avail=3221225472
+* hw_numa_node0_mem_page_total_4=262144
+* hw_numa_node0_mem_page_avail_4=262144
+* hw_numa_node0_mem_page_total_2048=1024
+* hw_numa_node0_mem_page_avail_2048=1024
+* hw_numa_node0_mem_page_total_1048576=7
+* hw_numa_node0_mem_page_avail_1048576=0
+* hw_numa_node0_distance_node0=10
+* hw_numa_node0_distance_node1=20
+* hw_numa_node1_cpus=4
+* hw_numa_node1_mem_total=10737418240
+* hw_numa_node1_mem_avail=5368709120
+* hw_numa_node1_mem_page_total_4=262144
+* hw_numa_node1_mem_page_avail_4=262144
+* hw_numa_node1_mem_page_total_2048=1024
+* hw_numa_node1_mem_page_avail_2048=1024
+* hw_numa_node1_mem_page_total_1048576=7
+* hw_numa_node1_mem_page_avail_1048576=2
+* hw_numa_node1_distance_node0=20
+* hw_numa_node1_distance_node1=10
+
+REST API impact
+---------------
+
+No impact.
+
+The existing APIs already support arbitrary data in the flavour extra specs.
+
+Security impact
+---------------
+
+No impact.
+
+Notifications impact
+--------------------
+
+No impact.
+
+The notifications system is not used by this change.
+
+Other end user impact
+---------------------
+
+There are no changes that directly impact the end user, other than the fact
+that their guest should have more predictable memory access latency.
+
+Performance Impact
+------------------
+
+The scheduler will have more logic added to take into account large page
+availability per NUMA node when placing guests. Most of this impact will have
+already been incurred when initial NUMA support was added to the scheduler.
+This change is merely altering the NUMA support such that it considers the
+free large pages instead of overall RAM size.
+
+Other deployer impact
+---------------------
+
+The cloud administrator will gain the ability to set large page policy on the
+flavours they configured. The administrator will also have to configure their
+compute hosts to reserve large pages at boot time, and place those hosts into a
+group using aggregates.
+
+It is possible that there might be a need to expose information on the page
+counts to host administrators via the Nova API. Such a need can be considered
+for followup work once the work refernced in this basic spec is completed
+
+Developer impact
+----------------
+
+If other hypervisors allow the control over large page usage, they could be
+enhanced to support the same flavour extra specs settings. If the hypervisor
+has self-determined control over large page usage, then it is valid to simply
+ignore this new flavour setting. ie do nothing.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  berrange
+
+Other contributors:
+  ndipanov
+
+Work Items
+----------
+
+* Enhance libvirt driver to report available large pages per NUMA node in the
+  host state data
+* Enhance libvirt driver to configure guests based on the flavour parameter
+  for page sizes
+* Add support to scheduler to place instances on hosts according to the
+  availability of required large pages
+
+Dependencies
+============
+
+* Virt driver guest NUMA node placement & topology. This blueprint is going
+  to be an extension of the work done in the compute driver and scheduler
+  for NUMA placement, since large pages must be allocated from matching
+  guest & host NUMA node to avoid cross-node memory access
+
+   https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
+
+* Libvirt / KVM need to be enhanced to allow Nova to indicate that large
+  pages should be allocated from specific NUMA nodes on the host. This is not
+  a blocker to supporting large pages in Nova, since it can use the more
+  general large page support in libvirt, however, the performance benefits
+  won't be fully realized until per-NUMA node large page allocation can be
+  done.
+
+* Extensible resource tracker
+
+  https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
+
+Testing
+=======
+
+Testing this in the gate would be difficult since the hosts which run the
+gate tests would have to be pre-configured with large pages allocated at
+initial OS boot time. This in turn would preclude running gate tests with
+guests that do not want to use large pages.
+
+Documentation Impact
+====================
+
+The new flavour parameter available to the cloud administrator needs to be
+documented along with recommendations about effective usage. The docs will
+also need to mention the compute host deployment pre-requisites such as the
+need to pre-allocate large pages at boot time and setup aggregates.
+
+References
+==========
+
+Current "big picture" research and design for the topic of CPU and memory
+resource utilization and placement. vCPU topology is a subset of this
+work
+
+* https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement
diff --git a/specs/juno/approved/vmware-driver-ova-support.rst b/specs/juno/approved/vmware-driver-ova-support.rst
new file mode 100644
index 0000000..f152c44
--- /dev/null
+++ b/specs/juno/approved/vmware-driver-ova-support.rst
@@ -0,0 +1,200 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================
+VMware: Support spawning from OVA images
+========================================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-driver-ova-support
+
+This blueprint proposes to add support of spawning an instance from the disk
+embedded in an OVA (Open Virtualization Application) glance image.
+
+
+Problem description
+===================
+
+Given that the best practice for obtaining a compact, portable template of a
+virtual machine in the vSphere platform is to export it into an OVF folder or
+an OVA file (http://www.dmtf.org/standards/ovf), a frequent customer ask is to
+be able to deploy them in OpenStack as Glance images and spawn new instances
+with them.
+
+In addition, OVF/OVA contains virtual disks that are converted to the
+streamOptimized format, and streamOptimized disks are the only disk type
+deployable on vSAN datastores (see
+https://blueprints.launchpad.net/nova/+spec/vmware-vsan-support)
+Since exporting a virtual machine to OVA/OVF remains one of the most convenient
+means to obtain streamOptimized disks, providing support for spawning using OVA
+glance images will streamline the process of providing images for vSAN use.
+
+
+Proposed change
+===============
+
+An OVF contains additional information about the virtual machine beyond its
+disks - it has an .ovf XML descriptor file that describes the virtual machine
+configuration (memory, CPU settings, virtual devices, etc).  But for the
+purpose of this blueprint, it is treated essential as a container of a root
+disk targetted for the spawn process.
+
+Note: An OVA is essentially a tarball of an OVF bundle.  Given the current
+image-as-a-single-file nature of glance images, it is more straightforward to
+support the uploading/download of OVA as a Glance image.
+
+The blueprint propose to support spawning of an image of container format 'ova'
+and disk format 'vmdk'. The driver expects the image to be an OVA tar bundle.
+
+While much of the information in the XML descriptor file could prove useful in
+the proper configuration of the spawned virtual machine in the future, the
+implementation of this blueprint will only perform minimal processing of the
+XML file solely for the purpose of obtaining the right disk file to use for
+spawn as well as type of the virtual disk adapter that the disk should be
+attached to by default. The disk adapter type used will continue to be
+overridable by specifying the "vmware_adaptertype" property in the spawn
+operation.
+
+
+Alternatives
+------------
+
+* When implemented, the vmware-vsan-support blueprint will allow spawning of
+  streamOptimized disk. An alternative is to force all users to extract the
+  streamOptimized disk from any OVA/OVF they intend to deploy in OpenStack and
+  have the compute driver only support spawning of a streamOptimized disk
+  image. This that puts unnecesary burden on the user.
+
+* Use the Task framework under proposal in Glance to provide on-the-fly
+  conversion of a supplied OVF/OVA into some other appropriate forms. This is
+  closely related to the previous alternative, as it may provide a more
+  streamlined workflow in glance to degenerate an incoming OVF into a single
+  streamOptimized disk.
+
+* Add support for OVF folder as the portable vSphere VM image. Since an OVF is
+  a folder with multiple files, it does not work well with existing the glance
+  model.
+
+* There are other proposals that involves using images that references data in
+  the hypervisor's datastore, or storing images directly on the datastore.
+  These are welcome optimizations that will reduce the amount of glance<->nova
+  nova transfers, but they do not address the issue of providing portable
+  image data that can be deployed in other vCenter installations.
+
+* Continue to force customers to upload images using the flat and sparse disk
+  variants. Because there is no straightforward way of obtaining disk images of
+  these type while still adopting the best practice of exporting virtual
+  machines first, this leads a separate, lengthier and more error-prone
+  workflow for preparing images for OpenStack use.
+
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+OVA and streamOptimized disks are more space efficient and streamable, this
+means less storage use in glance and faster first-time deployment times (as
+compared to a flat or sparse disk image).
+
+Other deployer impact
+---------------------
+
+This change will allow deployment of existing libraries of exported OVA images,
+with little or no additional transformations. Existing image using flat/sparse
+disk types may be deprecated/deleted in favor of OVA (or standalone
+streamOptimized disks).
+
+Developer impact
+----------------
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  vui
+
+Other contributors:
+  arnaudleg
+
+Work Items
+----------
+
+* Download OVA, process embedded .ovf descriptor file for the path to the
+  root disk in the OVA, and spawn using data from said disk.
+
+Dependencies
+============
+
+* https://blueprints.launchpad.net/nova/+spec/use-oslo-vmware. The oslo.vmware
+  library provides functionality not available in the current vmware nova
+  driver that is required by this blueprint.
+
+* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor. Work
+  related to this blueprint will likely cause non-trivial changes to the
+  patches for this blueprint since several of them involve
+  the spawn operation.
+
+* https://blueprints.launchpad.net/nova/+spec/vmware-vsan-support. This work
+  introduces support for streamOptimized images, a prerequisite for being able
+  to use OVA as images.
+
+Testing
+=======
+
+Since Tempest in general does not support driver-specific tests, the proposal
+is to update the MineSweeper CI
+(https://wiki.openstack.org/wiki/NovaVMware/Minesweeper) with additional tests
+to verify spawning of instances using OVA images uploaded to glance with the
+'ova' container format.
+
+
+Documentation Impact
+====================
+
+In addition, new information in the vmware driver section of the Nova
+documentation will have to be added to document:
+
+* The parameters to use when uploading an OVA image.
+* The scope of the information contained in the OVA that is used in the spawn
+  process (essentially information pertaining to obtaining the root disk and
+  not much else)
+
+References
+==========
+
+* http://www.dmtf.org/standards/ovf
+* https://blueprints.launchpad.net/nova/+spec/use-oslo-vmware
+* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
+* https://blueprints.launchpad.net/nova/+spec/vmware-vsan-support
+* https://wiki.openstack.org/wiki/NovaVMware/Minesweeper
+* https://bugs.launchpad.net/glance/+bug/1286375
diff --git a/specs/juno/approved/vmware-ephemeral-disk-support.rst b/specs/juno/approved/vmware-ephemeral-disk-support.rst
new file mode 100644
index 0000000..8601616
--- /dev/null
+++ b/specs/juno/approved/vmware-ephemeral-disk-support.rst
@@ -0,0 +1,117 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================
+VMware Ephemeral Disk Support
+=============================
+
+https://blueprints.launchpad.net/nova/+spec/improve-vmware-disk-usage
+
+The blueprint adds support for support ephemeral disks to the VMware driver.
+
+Problem description
+===================
+
+The VMware driver does not support ephemeral disks.
+
+Proposed change
+===============
+
+The change will add ephemeral disk support to the VMware driver. The commit
+acec2579b796d101f732916bfab557a66cebe512 added in a method create_virtual_disk.
+This method will be used to create the ephemeral disk for the instance.
+
+The method will create an ephemeral disk for the instance on the datastore.
+This will be done according to the size defined in the instance flavor.
+
+Alternatives
+------------
+
+* Do not implement the feature.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+* Users will be able to use ephemeral disks for the vCenter driver.
+
+Performance Impact
+------------------
+
+A modest increase in network traffic will slow down spawn operations as we
+create the ephemeral disk, size it, and place it for mounting in the vSphere
+virtual machine.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+This work was completed during IceHouse-1 and merely needs to be ported to
+the Juno release.
+
+Primary assignee:
+  tjones
+  heut2008
+  garyk
+
+Work Items
+----------
+
+* refactor and port https://review.openstack.org/#/c/51793/ for Juno
+
+Dependencies
+============
+
+blueprint vmware-spawn-refactor
+
+Testing
+=======
+
+* Minesweeper tests involving ephemeral disks will be turned on or written
+
+
+Documentation Impact
+====================
+
+After this blueprint the vmware driver will support ephemeral disks. This will
+need some additional documentation and changes to supported feature lists.
+
+References
+==========
+
+None
diff --git a/specs/juno/approved/vmware-spbm-support.rst b/specs/juno/approved/vmware-spbm-support.rst
new file mode 100644
index 0000000..e9f5030
--- /dev/null
+++ b/specs/juno/approved/vmware-spbm-support.rst
@@ -0,0 +1,213 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================
+Storage Policy Based Management (SPBM)
+======================================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-spbm-support
+
+The feature will enable an OpenStack environment to take advantage of
+backend storage policies to provide differential services to tenants.
+
+Problem description
+===================
+
+Enable administrators and tenants to take advantage of backend storage
+policies. The storage admin first creates storage profiles in VC based
+on the storage vendor provided capabilities and/or tag based capabilities
+of the underlying storage infrastructure. Refer to
+http://pubs.vmware.com/vsphere-55/topic/com.vmware.vsphere.storage.doc/GUID-A8BA9141-31F1-4555-A554-4B5B04D75E54.html
+to learn more about storage profiles on VC.
+
+The disk(s) of the virtual machine will be placed on the storage that
+matches the storage policy. This can for example provide preferential
+services to the user. For example the user will have an option of
+selecting 'gold', 'silver' or 'bronze' storage. 'gold' can be for
+applications that require fast and reliable results. 'bronze' can be
+for a background VM running in the evening doing maintenance.
+
+The spawn method currently selects the ‘best’ datastore to use. The
+administrator is able to select one or more datastores for selection
+by configuring a datastore regular expression. This logic will not
+be required if the instance flavor contain extra spec information
+that is relevant to the SPBM. That is, the SPBM information will be
+used for the datastore selection.
+
+Proposed change
+===============
+
+In order for Nova to provide SPBM we will need to address the following:
+
+* Enabling the tenant to make use of storage policies. The goal here
+  will be to provide the administrator with the necessary tools to
+  provide differential storage services to the tenant. More specifically
+  the administrator will be able to leverage capabilities provided by the
+  storage infrastructure. There are two parts:
+
+  * Configuration. The admin will need to do the following:
+
+    * Configure a default SPBM policy
+
+    * Create a flavor(s) for the tenants that will enable them to make use
+      of the various storage policies.
+
+  * Tenant usage. The tenant will be able to select a flavor that has
+    a storage policy.
+
+* Driver support for the storage policies.
+
+  * This entails using the information passed by the tenant to the driver.
+    More specifically the storage policy will be passed as flavor metadata.
+
+The driver will need to make use of a different endpoint to access the storage
+policies on the VC. This will require a new configuration variable, that is,
+the PBM WSDL location will need to be defined.
+
+NOTE: all of the nodes will share the same storage so there will not be any
+issues regarding rescheduling.
+
+The change will not affect the cached images. This is only where the disk
+for the VM will be placed.
+
+The flavor extra spec ‘image:storage_policy’ will drive the datastore
+selection. In the event that this flag is not present and the pbm_enabled
+is set in the configuration file then we will make use of a configured default
+policy. That is, if this is present then it will be used to get the list of
+datastores that can be used for selection. If not then we will use the list of
+datastores that can be accessed by the cluster.
+
+If this exists then we will validate that the policy exists.
+If not then an exception will be thrown. We will then proceed to get the moref
+and datastore of the datastore that is relevant to this policy
+
+pseudo code::
+        profile_ids = pbmServiceContent.profileManager.pbmQueryProfile()
+        profiles = pbmRetrieveContent(profile_ids)
+        profiles.find(name=profile_name)
+
+Query Matching ‘datastore’ entities for the profile. API :-
+pbmQueryMatchingHub
+
+If this does not exist then we will proceed to the select the datastore as
+before.
+
+The list of datastores will be processed by the existing code to select the
+best fit.
+
+Alternatives
+------------
+
+At the moment there is no way that a administrator can provide differential
+storage services to a tenant.
+
+Data model impact
+-----------------
+
+There are no data model changes. The information is passed from the tenant to
+the driver via flavor metadata (extraspecs). The driver in turn will use this
+information to assign the correct storage.
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The cloud provider will provide a flavor to the tenant that will enable them
+to have preferential storage capabilities.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+There are 3 new configuration variables (both in the vmware section):
+* pbm_wsdl_location - PBM service WSDL file location URL. e.g.
+file:///opt/SDK/spbm/wsdl/pbmService.wsdl. This will be optional. This
+value is a string. The default is None (not set).
+* pbm_enabled - status of storage policy based placement of instances.
+This value is a boolean. Default is False.
+* pbm_default_policy - The PBM default policy. If pbm_enabled
+is set and there is no defined storage policy for the specific request
+then this policy will be used. This value is a string. The default policy
+is defined out of band by the administrator on the Virtual Center. The
+default is None (not set).
+
+An admin user will create a new flavor either via the dashboard or the CLI.
+The flavor extra spec will have a key ‘image:storage_policy’. The admin
+will associate this this a predfined storage policy on the VC.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+None
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    garyk
+    smurugesan
+
+Other contributors:
+    rgerganov
+
+Work Items
+----------
+
+Code was posted in the Icehouse cycle:
+* SPBM support (part of oslo integration)
+* Add support for default pbm policy
+* Get storage policy from flavor
+* Use storage policy in datastore selection
+* Associate instance with storage policy
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+This requires 3rd party testing. It is not possible to be tested by the current
+gate.
+
+
+Documentation Impact
+====================
+
+Configuration variables and their usage need to be documented.
+Flavor creation and management should be discussed too. That is, the flavor
+extra spec will need to contain the policy. The key will be:
+'image:storage_policy' and the values can be for example 'gold', 'silver',
+etc.
+
+References
+==========
+
+https://docs.google.com/document/d/14Fr76WsFxBPfQJHRdy389IxlxZHXq-Kr83PeCXgDP1M/edit
diff --git a/specs/juno/approved/vmware-vsan-support.rst b/specs/juno/approved/vmware-vsan-support.rst
new file mode 100644
index 0000000..ab40aab
--- /dev/null
+++ b/specs/juno/approved/vmware-vsan-support.rst
@@ -0,0 +1,193 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================
+VMware: Support for vSAN Datastores
+===================================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-vsan-support
+
+Currently the vmwareapi compute driver only supports deploying instances to NFS
+and VMFS datastores. This blueprint proposes to add support for using vSAN
+storage as well.
+
+Explanation of terminology used:
+
+The term "datastore" as referred to in the spec and the driver refers to
+the vSphere concept a logical storage container, a place where VM data (among
+other things) is kept. The purpose of this abstraction is to provide a uniform
+way for vSphere clients to access said VM data regardless of what hardware, I/O
+protocols or transport protocols are used by the underlying storage.
+
+All vSphere datastores until recently has been broadly divided into two types,
+VMFS and NFS. The vmwareapi driver has been supporting the use of both since
+its inception, without having to distinguish between either, largely because of
+this datastore abstraction.
+
+vSAN storage is a third type of datastore introduced in vSphere. It is
+a software-defined distributed storage that aggregates disks (magnetic for
+capacity, SSD for cache/performance) attached to a group of hosts into a
+single storage pool. That pool is once again exposed as a single datastore.
+
+Problem description
+===================
+
+Currently datastores with type "vsan" is ignored by compute driver entirely.
+One obstacle to using this type of datastore is that virtual disk data files
+(the "-flat.vmdk" files) are not directly addressable as datastore paths. Since
+both the spawn and snapshot workflow in the vmware driver addresses the data
+files in some way, they will have to be changed to support vSAN datastores.
+
+Proposed change
+===============
+
+The change is divided into two areas:
+
+* Recognize and use datastores of a new type ("vsan").
+* Update existing code involved in exporting and importing Glance images to
+  use alternate vSphere APIs that does not address disk data files directly.
+
+The second area of change is mainly provided by the image-transfer
+functionality in the oslo.vmware library [*]_. The proposal is to update the
+code to use said library.
+
+However, the only disk format that these alternate APIs support is the
+'streamOptimized' format. (The streamOptimized format is a sparse, compressed,
+and stream-friendly version of the VMDK disk that is well suited for
+import/export use cases, such as the glance<->hypervisor exchanges described
+above). This implies that only streamOptimized disk images are deployable on
+vSAN. The driver will be modified to recognize Glance vmdk images tagged
+with the property vmware_disktype='streamOptimized' as disks of such format,
+and only use the alternate APIs when handling disks of this format.
+
+.. [*] To import a disk image to a vSAN datastore, oslo.vmware uses the
+   ImportVApp vSphere API is used to import the image as a shadow virtual
+   machine (a VM container to hold a reference to the base disk disk, and is
+   not meant to be powered on). Likewise, to export the disk image, the library
+   uses the ExportVM vSphere API.  These APIs do not reference the virtual disk
+   data file paths directly and are hence compatible with vSAN storage.
+
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+The compute driver will require the oslo.vmware library. (See "Dependencies"
+section).
+
+There is a new configuration option under the [vmware] section,
+"image_transfer_timeout_secs", which configures how long an image transfer can
+proceed before timing out.
+
+In order to deploy existing VMDK images to vSAN, these images will have to be
+converted to streamOptimized and reimported to glance.
+
+
+Developer impact
+----------------
+
+Minimal. The changes related to blueprint are mostly isolated in the areas of
+handling a new vmdk format type and add recognition and use of an additional
+datastore type called "vsan".
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  vui
+
+Work Items
+----------
+
+Much of the work was done and proposed in the Icehouse cycle, but did not make
+the release due to time constraints. That work should should continue to be
+considered for this blueprint. The work is broadly decomposed into:
+
+* use oslo.vmware image_transfer module to handle download of images
+* use oslo.vmware image_transfer module to handle upload of image snapshot
+* update driver to allow the use of datastores of type vSAN.
+* update driver to recognized a new vmdk format (streamOptimized)
+
+
+Dependencies
+============
+
+* https://blueprints.launchpad.net/nova/+spec/use-oslo-vmware. The oslo.vmware
+  library provides functionality not available in the current vmware nova
+  driver that is required by this blueprint.
+
+* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor. Work
+  related to this blueprint will likely cause non-trivial changes to the
+  patches for this blueprint since several of them involve
+  the spawn operation.
+
+Testing
+=======
+
+Since Tempest in general does not support driver-specific tests, the proposal
+is to update the MineSweeper CI
+(https://wiki.openstack.org/wiki/NovaVMware/Minesweeper), to provide
+vCenter with vSAN storage and additional tests to verify existing Tempest
+tests passes when invoked against compute nodes using it.
+
+
+Documentation Impact
+====================
+
+New information in the vmware driver section of the Nova documentation will
+have to be added to document:
+
+* How to configure a compute node for vSAN use.
+* The virtual disk format requirement ("streamOptimized" only) when using vSAN
+  storage.
+* The new "image_transfer_timeout_secs" configuration option.
+* How to obtain a streamOptimized disk from a virtual machine or vmdk disk in a
+  non-streamOptimized format.
+
+
+References
+==========
+
+* https://github.com/openstack/oslo.vmware
+* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
+* https://wiki.openstack.org/wiki/NovaVMware/Minesweeper
diff --git a/specs/juno/approved/websocket-proxy-to-host-security.rst b/specs/juno/approved/websocket-proxy-to-host-security.rst
new file mode 100644
index 0000000..e21c45f
--- /dev/null
+++ b/specs/juno/approved/websocket-proxy-to-host-security.rst
@@ -0,0 +1,215 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================================================
+Support Proxying of Encryption and Authentication in WebSocketProxy
+===================================================================
+
+https://blueprints.launchpad.net/nova/+spec/websocket-proxy-to-host-security
+
+Currently, while the noVNC and HTML5 SPICE clients can use TLS-encrypted
+WebSockets to communicate with Websockify (and authenticate with Nova console
+tokens), the encryption and authentication ends there.  There are neither
+encryption nor authentication between Websockify and the hypervisors'
+VNC and SPICE servers.
+
+This blueprint would propose introducing a generic framework for supporting
+proxying security for Websockify to use between itself and the compute nodes.
+
+Problem description
+===================
+
+Currently, there are neither authentication nor encryption between Websockify
+and the hypervisors' SPICE and VNC servers.  Were a malicious entity to gain
+access to the "internal" network of an OpenStack deployment he or she could:
+
+* "Listen" to VNC and SPICE traffic (lack of encryption)
+
+* Connect freely to the SPICE and VNC servers of VMs (lack of authentication)
+
+For example, suppose Alice starts a VM, which gets placed on "hypervisor-a".
+Carol could then use Wireshark or the like to watch what Alice is doing with
+her VM's console.  Furthermore, Carol could point her VNC client at
+"hypervisor-a:5900" and actually access the VM's console.
+
+Proposed change
+===============
+
+This blueprint would introduce a generic framework performing proxying of
+authentication and encryption.  When establishing a connection, the proxy would
+act as a client to the server and a server to the client, performing different
+steps for each during the security negotiation phase of the respective
+protocols.
+
+The proxy would then wrap the server socket in an encryption layer that
+respected the standard python socket class (much like python's :code:`ssl`
+library does) and pass the resulting wrapped socket off to the normal proxy
+code.
+
+Authentication drivers would have a class for SPICE as well as for VNC
+(since VNC has to do some extra negotiation as part of the RFB protocol).
+Deployers could then point Nova to the appropriate driver and options via
+configuration options.
+
+A base driver for TLS [1]_ (VeNCrypt for VNC, plain TLS for SPICE) would be
+included as an example implementation, although it would be beneficial to
+develop further drivers, such as a SASL driver [2]_.
+
+.. [1] To ensure only the correct clients connect, the proxy would send
+       the hypervisor x509 client certificates, and the server would reject
+       any certificates not signed by the specified CA (authentication).  To
+       prevent evesdroppers, the actual data stream would use TLS encryption.
+       While both of these are supported for VNC by QEMU (and thus KVM, Xen,
+       etc), it would appear that SPICE only supports the encryption.  If a
+       deployment is using SPICE, another driver should be used.
+
+.. [2] Such a driver would most likely use the GSSAPI mechanism, which would
+       provide Kerberos encryption and authentication for the connections.
+       However, SASL supports other mechanisms, so non-GSSAPI drivers could
+       be written.  Some mechanisms do not support encryption ("data-layer
+       security" in SASL terms), so TLS should be used to provide encryption
+       with those.  SASL connections are by both SPICE and VNC on QEMU fully.
+
+Alternatives
+------------
+
+* Doing end-to-end security: this would require supporting more advanced
+  encryption and authentication in the HTML5 clients.  Unfortunately, this
+  requires doing cryptography in the browser, which is not really feasible
+  until more browsers start implementing the HTML5 WebCrypto API.
+
+* Using a tool like stunnel: There are a couple of issues with this.  The first
+  is that it locks us in to a particular authentication mechanism -- stunnel
+  works fine for TLS, but will not work if we want to use SASL instead.
+  The second issue is that it bypasses normal VNC security negotation, which
+  does the initial handshake in the clear, and then moves on to security
+  negotiation later.  It is desired to stay within the confines of the standard
+  RFB (VNC) specification.  The third issue is that this would sidestep the
+  issue of authentication -- a malicous entity could still connect directly to
+  the unauthenticated port, unless you explicitly set up your firewall to block
+  remote connections to the normal VNC ports (which requires more setup on the
+  part of the deployer -- we want to make it fairly easy to use this).
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+The actual crypto done would depend on the driver being used.  It will be
+important to ensure that the libraries used behind any implemented drivers
+are actually secure.
+
+Assuming the driver is secure and implements both authentication and
+encryption, the security of the deployment would be strengthened.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+Minimal.  The extra encryption will most likely be performed via a C-based
+python library, so there will be relatively low overhead.
+
+Other deployer impact
+---------------------
+
+First, a deployer would have to choose the driver that he or she wished to use:
+:code:`console_proxy_security_driver = driver_name`.  Then, the particular
+driver would be have configuration options under its own section in the
+configuration file.  For instance, the x509/TLS driver would appear as the
+following:
+
+.. code::
+
+   [console_proxy_tls]
+   ca_certificate = /path/to/ca.cert
+   client_certificate = /path/to/client.cert
+
+Finally, most drivers will require extra setup outside of Nova.  For instance,
+the x509/TLS driver will reqiure generating CA, client, and server
+certificates, distributing the CA and client certificates, and configuring
+libvirt to require x509/TLS encryption and authentication when connecting to
+VNC and SPICE consoles (see `References`_).
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    sross-7
+
+Other contributors:
+    None
+
+Work Items
+----------
+
+1. Implement the base framework for proxying authentication and
+   encryption.
+
+2. Implement a No-op driver
+
+3. Implement the basic x509/TLS driver
+
+
+Dependencies
+============
+
+While individual drivers might introduce new dependencies,
+the actual framework would not.
+
+
+Testing
+=======
+
+We should test that the framework is callable correctly.  Additionally,
+it will be necessary to work with infra to ensure that we can test the actual
+drivers (for instance, for x509/TLS, we will need to generate certificates,
+etc).
+
+
+Documentation Impact
+====================
+
+We will need to document the new configuration options, as well as how to
+generate certificates for the TLS driver (See `Other deployer impact`_).
+
+
+References
+==========
+
+* The most recent version of the VeNCrypt specification can be found in
+  this thread http://sourceforge.net/p/tigervnc/mailman/message/25748057/ --
+  http://sourceforge.net/p/tigervnc/mailman/attachment/20100720083109.GA3303%40evileye.atkac.brq.redhat.com/1/
+
+* SPICE TLS: http://www.spice-space.org/docs/spice_user_manual.pdf -- page 11
+
+* libvirt TLS setup:
+  VNC: http://wiki.libvirt.org/page/VNCTLSSetup,
+  SPICE: http://people.freedesktop.org/~teuf/spice-doc/html/ch02s08.html
diff --git a/specs/juno/approved/xenapi-set-ipxe-url-as-img-metadata.rst b/specs/juno/approved/xenapi-set-ipxe-url-as-img-metadata.rst
new file mode 100644
index 0000000..7d2c4b0
--- /dev/null
+++ b/specs/juno/approved/xenapi-set-ipxe-url-as-img-metadata.rst
@@ -0,0 +1,117 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================================
+Set ipxe url as image metadata instead of config option
+=======================================================
+
+https://blueprints.launchpad.net/nova/+spec/xenapi-set-ipxe-url-as-img-metadata
+
+Move xenapi_ipxe_boot_menu_url to a image property so that it is user
+configurable.
+
+Problem description
+===================
+
+Currently the xenapi iPXE URL is specified as a configuration option in Nova.
+Because it is a configuration option, users are unable to specify their own
+iPXE URL on their own images.  The proposal is to allow the iPXE URL to be
+specified as an image property.  By doing this, a customer can upload an iPXE
+ISO, with the iPXE URL specified as a metadata option and boot from their own
+custom configurations.
+
+Proposed change
+===============
+
+Add the ability to specify ipxe_boot_menu_url as an image metadata property
+which can override the nova configuration of xenapi_ipxe_boot_menu_url.
+
+Alternatives
+------------
+
+Remove the main configuration option of xenapi_ipxe_boot_menu_url and rely on
+the image property to populate the configuration.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Users will need to specify the ipxe_boot_menu_url in order to boot from their
+iPXE configuration.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Because the settings set on the image properties would override the Nova
+configuration settings, an operator could prevent users from overriding the
+ipxe settings by setting policies to restrict usage of the various flags like
+ipxe_boot and ipxe_boot_menu_url.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  antonym
+
+Work Items
+----------
+
+* Create ipxe_boot_menu_url image metadata configuration to be used when
+  generating iPXE ISO image.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Testing of this feature will be covered by the XenServer CI.
+
+Documentation Impact
+====================
+
+Change documentation to reflect that ipxe_boot_menu_url can now be specified as
+an image property which will override the default configuration.
+
+References
+==========
+
+* Original iPXE implementation:
+  https://blueprints.launchpad.net/nova/+spec/xenapi-ipxe-iso-boot-support
diff --git a/specs/juno/approved/xenapi-vcpu-topology.rst b/specs/juno/approved/xenapi-vcpu-topology.rst
new file mode 100644
index 0000000..e50a219
--- /dev/null
+++ b/specs/juno/approved/xenapi-vcpu-topology.rst
@@ -0,0 +1,152 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================
+XenAPI vCPU Topology
+====================
+
+https://blueprints.launchpad.net/nova/+spec/xenapi-vcpu-topology
+
+The proposal is to add support for vCPU topology for XenAPI.  It will utilize
+the work done on the virt-driver-vcpu-toplogy blueprint.  Most of this
+blueprint has been copied from the virt-driver-vcpu-topology blueprint as
+they align well but differ slightly on implementations per hypervisor.
+
+Problem description
+===================
+
+See Virt Driver VCPU Topology spec referenced at the end of blueprint.
+
+Proposed change
+===============
+
+See Virt Driver VCPU Topology spec referenced at the end of blueprint.
+
+For XenServer implementation the following configurations will be used:
+
+* hw:cpu_sockets=NN - preferred number of sockets to expose to the guest
+* hw:cpu_cores=NN - preferred number of cores to expose to the guest
+* hw:cpu_max_sockets=NN - maximum number of sockets to expose to the guest
+* hw:cpu_max_cores=NN - maximum number of cores to expose to the guest
+
+At the image level the exact same set of parameters will be permitted,
+with the exception that image properties will use underscores throughout
+instead of an initial colon.
+
+* hw_cpu_sockets=NN - preferred number of sockets to expose to the guest
+* hw_cpu_cores=NN - preferred number of cores to expose to the guest
+* hw_cpu_max_sockets=NN - maximum number of sockets to expose to the guest
+* hw_cpu_max_cores=NN - maximum number of cores to expose to the guest
+
+Note that XenServer does not have a specific setting for number of threads
+so setting threads will not function on XenServer currently.
+
+Alternatives
+------------
+
+None, will utilize existing work done in virt-driver-vcpu-topology blueprint
+
+Data model impact
+-----------------
+
+No impact.
+
+The new properties will use the existing flavour extra specs and image
+property storage models.
+
+REST API impact
+---------------
+
+No impact.
+
+The new properties will use the existing flavour extra specs and image
+property API facilities.
+
+Security impact
+---------------
+
+The choice of sockets vs cores can have an impact on host resource utilization
+when NUMA is involved, since over use of cores will prevent a guest being
+split across multiple NUMA nodes. This feature addresses this by allowing the
+flavour administrator to define hard caps, and ensuring the flavour will
+always take priority over the image settings.
+
+Notifications impact
+--------------------
+
+No impact.
+
+There is no need for this feature to integrate with notifications.
+
+Other end user impact
+---------------------
+
+The user will gain the ability to control aspects of the vCPU topology used
+by their guest. They will achieve this by setting image properties in glance.
+
+Performance Impact
+------------------
+
+The cores vs sockets vs threads decision does not involve any scheduler
+interaction, since this design is not attempting to match host topology
+to guest topology. A later blueprint on CPU pinning will make it possible
+todo such host to guest topology matching, and its performance impact
+will be considered there.
+
+Other deployer impact
+---------------------
+
+The flavour extra specs will gain new parameters in extra specs which a
+cloud administrator can choose to use. If none are set then the default
+behaviour is unchanged from previous releases.
+
+Developer impact
+----------------
+
+Implementation will add support for XenAPI drivers.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignees:
+  antonym
+  johngarbutt
+
+Work Items
+----------
+
+* Add XenAPI driver support for choosing a CPU topology solution based on
+  the given hw_cpu_* parameters.
+
+Dependencies
+============
+
+No external dependencies
+
+Testing
+=======
+
+Testing of this feature will be covered by the XenServer CI.
+
+Documentation Impact
+====================
+
+The new flavour extra specs and image properties will need to be documented.
+Guidance should be given to cloud administrators on how to make most
+effective use of the new features. Guidance should be given to the end user
+on how to use the new features to address their use cases.
+
+References
+==========
+
+* Virt Driver VCPU Topology:
+  https://blueprints.launchpad.net/nova/+spec/virt-driver-vcpu-topology
+
+* Information on cores-per-socket in XenServer:
+  https://support.citrix.com/article/CTX126524
diff --git a/specs/juno/backportable-db-migrations-juno.rst b/specs/juno/backportable-db-migrations-juno.rst
deleted file mode 100644
index 175247b..0000000
--- a/specs/juno/backportable-db-migrations-juno.rst
+++ /dev/null
@@ -1,135 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Allow DB migration backports to Icehouse
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations-juno
-
-Just as we did at the beginning of the Havana and Icehouse dev cycles, we need
-to reserve a range of DB migrations as the first DB change in Juno. This will
-allow a range to be used for migration backports to Icehouse if needed.
-
-
-Problem description
-===================
-
-Normally, it is not possible to backport a change that requires a database
-migration due to the linear versioned nature of the migrations.  For the last
-two releases (Havana and Icehouse), we have reserved a set of empty migrations
-as placeholders to allow for migration backports if needed.
-
-
-Proposed change
-===============
-
-The proposed change is to reserve 10 migrations for Icehouse backports. These
-migrations would be no-ops and would simply result in an increment of the
-schema version.
-
-Alternatives
-------------
-
-When figuring out ways to allow database migrations, alternatives usually
-involve discussion of drastic changes to the way we manage migrations.  For
-example, it could require moving to a new framework.  This proposal works for
-our current use of sqlalchemy-migrate.  This will also be the third release
-we've used this approach, so it's fairly well understood at this point.
-
-Data model impact
------------------
-
-There's no changes to the data model as a part of this effort.  It simply gives
-us the ability to backport data model changes to Icehouse.
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-These migrations have minimal cost and can be run against a database without
-taking down Nova services.
-
-Other deployer impact
----------------------
-
-This set of changes requires doing database migrations.  However, they can be
-done without any Nova downtime.
-
-Developer impact
-----------------
-
-This must be the first set of migrations merged into Juno, or it doesn't work.
-
-Developers must also be very careful when writing migrations that may be
-backported.  They must be idempotent.  For example, if migration 115 is
-backported to 107 in the previous release, someone who has executed the
-backported migration must not suffer any trouble when the migration runs again
-after an upgrade.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  russellb
-
-Work Items
-----------
-
-* Create 10 placeholder migrations.
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-The existing unit tests will cover this.  Both the normal devstack based CI
-systems, as well as the "turbo-hipster" DB CI system will provide functional
-test coverage of these placeholder migrations.
-
-
-Documentation Impact
-====================
-
-None.
-
-
-References
-==========
-
-* https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations-icehouse
-
-* https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
-
-* http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html
diff --git a/specs/juno/better-support-for-multiple-networks.rst b/specs/juno/better-support-for-multiple-networks.rst
deleted file mode 100644
index 6f3e685..0000000
--- a/specs/juno/better-support-for-multiple-networks.rst
+++ /dev/null
@@ -1,207 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-====================================================
-Better Support for Multiple Networks in nova-network
-====================================================
-
-https://blueprints.launchpad.net/nova/+spec/better-support-for-multiple-networks
-
-Since nova-network is staying around, it needs a few updates for multiple
-networks. There are various settings that are automatically determined or set
-via flags, that should be explicitly set per network. This spec is about adding
-a few options to the networks table and converting the network manager and
-linux_net code to support multiple networks.
-
-Problem description
-===================
-
-Currently it is impossible to have multiple networks with different mtu
-settings or to have some networks that share ips or have external gateways and
-others that do not. If you have a single network it is possible to specify a
-different (external) gateway for that network by adding a custom dnsmasq.conf,
-but this breaks down when you have multiple networks.
-
-Proposed change
-===============
-
-This change proposes adding four fields to the networks table:
-
- * mtu
- * dhcp_server
- * enable_dhcp
- * share_address
-
-Each of the new fields will be used in place of existing config options or
-automatic value interpretation. The defaults for these options will mean there
-is no difference to users if they are not specified.
-
-It will also modify network create to allow these fields to be modified. An api
-extension will be added so one can determine if extra network fields are
-available.
-
-Alternatives
-------------
-
-Supporting this functionality without changing the data model would require
-some pretty complex config options. For example mtu could be a list of network
-names and mtus, but this is extremely unweildy.
-
-Data model impact
------------------
-
-This adds four new fields to the network model. The fallback for these fields
-will use the existing config options and defaults. These config options will be
-marked deprecated but will still work by default.
-
-The four new fields will be added to the object model, and they will be cut out
-for older versions.
-
-REST API impact
----------------
-
-The current network create api allows extra values to be passed in and they are
-silently ignored. In order to provide information about whether the new fields
-are supported, a dummy api extension will be created and the extra fields will
-only be accepted/returned if the api extension is enabled.
-
-The json for a network create call would currently look like::
-
-    {
-        "network": {
-            "label": "new net 111",
-            "cidr": "10.20.105.0/24",
-            ...
-        }
-    }
-
-With the new fields it would support::
-
-    {
-        "network": {
-            "label": "new net 111",
-            "cidr": "10.20.105.0/24"
-            "mtu": 9000,
-            "enable_dhcp": "true",
-            "dhcp_server": "10.20.105.2",
-            "share_address": true,
-            ...
-        }
-    }
-
-These fields will also be returned in the show command::
-
-    {
-        "network": {
-            "bridge": "br100",
-            "bridge_interface": "eth0",
-            "broadcast": "10.0.0.7",
-            "cidr": "10.0.0.0/29",
-            "cidr_v6": null,
-            "created_at": "2011-08-15T06:19:19.387525",
-            "deleted": false,
-            "deleted_at": null,
-            "dhcp_start": "10.0.0.3",
-            "dns1": null,
-            "dns2": null,
-            "gateway": "10.0.0.1",
-            "gateway_v6": null,
-            "host": "nsokolov-desktop",
-            "id": "20c8acc0-f747-4d71-a389-46d078ebf047",
-            "injected": false,
-            "label": "mynet_0",
-            "multi_host": false,
-            "netmask": "255.255.255.248",
-            "netmask_v6": null,
-            "priority": null,
-            "project_id": "1234",
-            "rxtx_base": null,
-            "updated_at": "2011-08-16T09:26:13.048257",
-            "vlan": 100,
-            "vpn_private_address": "10.0.0.2",
-            "vpn_public_address": "127.0.0.1",
-            "vpn_public_port": 1000,
-            "mtu": 9000,
-            "dhcp_server": "10.20.105.2",
-            "enable_dhcp": true,
-            "share_address": true
-        }
-    }
-
-Security impact
----------------
-
-This change doesn't have any security impact.
-
-Notifications impact
---------------------
-
-This change doesn't impact notifications.
-
-Other end user impact
----------------------
-
-This change will also include a modification to python-novaclient network
-create to allow users to create networks specifying the additional fields.
-
-Performance Impact
-------------------
-
-The performance impact of this change is negligible.
-
-Other deployer impact
----------------------
-
-Deployers should start using the network fields in place of the config options,
-but there is no requirement for them to move right away.
-
-Developer impact
-----------------
-
-This change should not affect developers.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  vishvananda
-
-Other contributors:
-  None
-
-Work Items
-----------
-
-Nova code addtions
-Python-novaclient code addtions
-Tempest test additions
-
-Dependencies
-============
-
-There are no new dependencies for this feature.
-
-
-Testing
-=======
-
-There are currently no tempest tests for the create network call. A test for
-create network including the new fields  will be added.  The internal
-modifications will be covered by unit tests.
-
-
-Documentation Impact
-====================
-
-The new additions to the network create call need to be documented.
-
-References
-==========
-
-None
diff --git a/specs/juno/clean-logs.rst b/specs/juno/clean-logs.rst
deleted file mode 100644
index 2469029..0000000
--- a/specs/juno/clean-logs.rst
+++ /dev/null
@@ -1,149 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==================================================
-Nova logs shouldn't have ERRORs or TRACEs in them
-==================================================
-
-https://blueprints.launchpad.net/nova/+spec/clean-logs
-
-Nova logs should be readable, sensible, and contain only errors and
-traces in exceptional situations.
-
-Problem description
-===================
-
-During a normal, successful, run of Tempest in the OpenStack gate we
-get a large number of ERRORs and stack traces in the logs. This is for
-passing results, which means the cloud should have been operating
-normally.
-
-Stack traces and errors in the logs under normal conditions make it
-very difficult for operators to actually determine when real issues
-are happening with their OpenStack cloud. We've seen this even as part
-of normal development where people will be tricked in debugging
-OpenStack issues by the ERRORs, when the real issue is masked.
-
-Proposed change
-===============
-We should clean up all the instances of Stack Traces and Errors
-happening under a normal Tempest run. This means addressing the bugs
-that this currently exposes, as well as changing some logging levels
-where we are logging Exceptions at log.exception level that are
-actually expected (and thus should be a log.debug or deleted
-entirely).
-
-See Testing section below for completion criteria.
-
-Alternatives
-------------
-
-None.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-This will change some log messages for clarity. Users that built
-filters around the old error messages will have to adjust their
-filters. As they were probably filtering those messages out, this
-should be minimal.
-
-Performance Impact
-------------------
-
-None. (Possibly miniscule, and largely undetectable, boost because of
-not dumping stack traces so much.)
-
-Other deployer impact
----------------------
-
-None.
-
-Developer impact
-----------------
-
-Developers will have to be more careful about doing arbitrary
-log.exception calls inside Nova code once this is enforcing, and will
-need to be more careful on catching appropriate exceptions for
-expected conditions.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  sdague
-
-Work Items
-----------
-
-The services should be tackled in this order (consider cleaning of
-each one a work item):
-
- * n-sched
- * n-net
- * n-api
- * n-cpu
-
-n-sched and n-net are currently the most critical to clean up as they
-are services that surface testing don't hit directly (only indirectly
-through n-api calls). Ensuring that they don't have unexpect behavior
-in dumping stack traces will provide extra verification that those
-services are working as expected.
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-Testing will be accomplished by the tempest check_logs.py script
-currently running in the gate. Once we are confident that we have
-cleaned up a service, we remove that service from the allowed_dirty
-list
-https://github.com/openstack/tempest/blob/master/tools/check_logs.py#L33. After
-that any change which causes there to be a stack trace or error in the
-logs for that service will cause the tempest tests to fail, thus
-blocking the change from merging.
-
-Documentation Impact
-====================
-
-There will be a related effort in overall logging standards (to be
-presented as a Juno cross project session) that will need to be
-fleshed out in conjunction with this.
-
-References
-==========
-
- * Initial thread on Log Harmonization -
-   http://lists.openstack.org/pipermail/openstack-dev/2013-October/017300.html
diff --git a/specs/juno/cold-migration-with-target.rst b/specs/juno/cold-migration-with-target.rst
deleted file mode 100644
index ca40a6f..0000000
--- a/specs/juno/cold-migration-with-target.rst
+++ /dev/null
@@ -1,145 +0,0 @@
-
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===========================================
-Enable cold migration with target host
-===========================================
-
-https://blueprints.launchpad.net/nova/+spec/cold-migration-with-target
-
-The aim of this feature is to let operators cold migrate instances with
-target host manually.
-
-
-Problem description
-===================
-
-I have a customized HA plugin which automatically performs migrations under
-certain conditions, and the HA plugin is able to intelligently pick up
-destinations, but only live migrations support specifying destinations.
-
-At the moment cold migration do not support migrate a VM instance with target
-host, this blueprint want to add this feature to nova so that above scenario
-can be satisified.
-
-It also make cold migration consistent with live-migrate operations as live
-migration support migration with and w/o target host.
-
-
-Proposed change
-===============
-
-Modify the current resize_instance flow to let the api can specify the target
-host for cold migration.
-
-
-Alternatives
-------------
-None
-
-Data model impact
------------------
-None
-
-REST API impact
----------------
-
-* For V2 API, a new extension will be added as:
-  alias: os-extended-admin-actions
-  name: ExtendedAdminActions
-  namespace:
-  http://docs.openstack.org/compute/ext/extended_admin_actions/api/v1.1
-
-  When the new extension "os-extended-admin-actions" is loaded, the api of
-  _migrate() wil support cold migration with target host.
-
-* For a later microversion of v2.1 API, no new extension needed, the
-  existing cold migration API will be updated to support this.
-
-* URL: existed admin actions extension as:
-       * /v2/{tenant_id}/servers/actions:
-       * /v2.1/servers/actions:
-
-  JSON request body::
-
-    {
-        "migrate":
-        {
-            "host": "fake_host"
-        }
-    }
-
-Security impact
----------------
-None
-
-Notifications impact
---------------------
-None
-
-Other end user impact
----------------------
-
-python-novaclient will be modified to have target_host argument as
-optional.
-
-The user can trigger this feature by:
-nova migrate my_server target_host
-
-Performance Impact
-------------------
-None
-
-Other deployer impact
----------------------
-None
-
-Developer impact
-----------------
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  jay-lau-513
-
-Work Items
-----------
-
-* Add logic to select target host for cold migration
-* Add API v2/v2.1
-* Set target host optional on nova-client
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-Add unit test in nova to cover the case of cold migration with target host,
-also we probably need to think about adding functionnal tests in tempest.
-
-
-Documentation Impact
-====================
-
-* Api Docs to reflect that target host field is optional.
-* Client docs ( due to optional arg)
-* Admin User Guide on cold migration topic.
-
-
-References
-==========
-None
diff --git a/specs/juno/compute-manager-objects-juno.rst b/specs/juno/compute-manager-objects-juno.rst
deleted file mode 100644
index ddd6bd3..0000000
--- a/specs/juno/compute-manager-objects-juno.rst
+++ /dev/null
@@ -1,160 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===========================================
-Compute Manager Objects Support (Juno Work)
-===========================================
-
-https://blueprints.launchpad.net/nova/+spec/compute-manager-objects-juno
-
-This blueprint represents the remaining work to be done in Juno around
-moving the compute manager (and associated modules, like
-nova.compute.utils) to using objects instead of raw conductor
-methods. This is important because objects provide versioning of the
-actual data, which supports our upgrade goals.
-
-Problem description
-===================
-
-The nova compute manager still sends unversioned bundles of data using
-conductor and compute RPC methods, which is problematic during an
-upgrade where the format of the data has changed across releases.
-This is especially important for compute manager, because it is likely
-that it will be speaking to a newer conductor and compute node at
-times. During an upgrade, migrate and live-migrate operations are
-expected, and by nature will involve compute nodes running different
-versions of the code to communicate.
-
-Proposed change
-===============
-
-Migrate uses of raw condutor methods in the compute manager to
-objects. For example consider this::
-
-  service_ref = self.conductor_api.service_get_by_compute_host(
-          context, self.host)
-  self.conductor_api.compute_node_delete(context, service_ref['compute_node'])
-
-would become::
-
-  service = service_obj.Service.get_by_compute_host(context,
-                                                    self.host)
-  service.compute_node.destroy()
-
-Alternatives
-------------
-
-This is the accepted direction of the project to solve this
-problem. However, alternatives would be:
-
-1. Don't solve the problem and continue using unversioned data
-2. Attempt to enforce version bumps of individual methods when any
-   data (including nested downstream data) has changed
-
-Data model impact
------------------
-
-The low-level data model (i.e. the SQLAlchemy models) will not need to
-change. However, additional high-level objects may be added where
-necessary to provide versioned wrappers around the low-level models.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-In general, conversion of code to use objects does not affect
-notifications. However, at times, emission of notifications is
-embedded into an object method to achieve higher consistency about
-when and how the notifications are sent. No such changes are
-antitipated in this work, but it's always a possibility.
-
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-None.
-
-Other deployer impact
----------------------
-
-Moving to objects enhances the ability for deployers to incrementally
-roll out new code. It is, however, largely transparent for them.
-
-Developer impact
-----------------
-
-This is normal refactoring, so the impact is minimal. In general,
-objects-based code is easier to work with, so long-term it is a win
-for the developers.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  danms
-
-Work Items
-----------
-
-* check_can_live_migrate_destination
-* check_can_live_migrate_source
-* live_migration
-* _post_live_migration
-* _rollback_live_migration
-* _rollback_live_migration_at_destination
-* refresh_instance_security_rules
-* run_instance
-* detach_volume
-* Remaining uses of instance[attr] in compute/manager.py
-
-Dependencies
-============
-
-There is a cross-dependency between this blueprint and the following:
-
-  https://blueprints.launchpad.net/nova/+spec/virt-objects-juno
-
-At times, a virt driver will need to be modified to accept an object
-from the compute manager before the manager method can be fully
-converted.
-
-Testing
-=======
-
-In general, unit tests require minimal change when this happens,
-depending on how the tests are structured. Ideally, they are already
-mocking out database calls, which means the change to objects is a
-transparent one. In reality, this usually means minor tweaking to the
-tests to return whole data models, etc.
-
-Documentation Impact
-====================
-
-None.
-
-References
-==========
-
-* https://blueprints.launchpad.net/nova/+spec/compute-manager-objects
-* https://blueprints.launchpad.net/nova/+spec/virt-objects
-* https://blueprints.launchpad.net/nova/+spec/unified-object-model
diff --git a/specs/juno/config-drive-image-property.rst b/specs/juno/config-drive-image-property.rst
deleted file mode 100644
index 424ea0e..0000000
--- a/specs/juno/config-drive-image-property.rst
+++ /dev/null
@@ -1,155 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Config drive based on image property
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/config-drive-image-property
-
-When creating an instance, check the image property to decide if a config
-drive should be created.
-
-Problem description
-===================
-
-Currently Nova decides if config drive is created for a server
-based on:
-
-a) If the user specifies the config-drive option in server create API
-   request, or,
-
-b) If the server is scheduled to a compute node with force_config_drive
-   option set.
-
-But we need consider the image requirement also. Some images may explicitly
-require config drive.
-
-Proposed change
-===============
-
-* Add an image property as "img_config_drive", the value of the
-  img_config_drive can be:
-
-  * img_config_drive=mandatory|optional
-
-  where these mean:
-
-  * mandatory == instance must always have a config drive
-
-  * optional == instance can use a config drive, but can still work if missing
-
-  Any other value will be treated as error. If no option specified, the default
-  value is optional.
-
-  In future, this property may be extended to include more choices like
-  'disable' to disable config_drive. A mechanism should be presented at that
-  time to make sure the 'disable' option is not treated as error.
-
-* The rule of config drive decision is described as followed table. A config
-  drive will be created whenever user specified in API, required in image
-  property or compute node configuration option specified it.
-
-    +-----------+------------------------+----------------+-----------+
-    |   API     |  Image Property        | Compute Config | Result    |
-    +===========+========================+================+===========+
-    |    No     |  Mandatory             | Set or Unset   | Yes       |
-    +-----------+------------------------+----------------+-----------+
-    |    No     |  Optional              | Set            | Yes       |
-    +-----------+------------------------+----------------+-----------+
-    |    No     |  Optional              | Unset          | No        |
-    +-----------+------------------------+----------------+-----------+
-    | Specified |  Mandatory or Optional | Set or Unset   | Yes       |
-    +-----------+------------------------+----------------+-----------+
-
-
-Alternatives
-------------
-
-Another option is to combine the API option and image property into one
-instance property in the API layer, but this is not clean IMHO.
-
-Data model impact
------------------
-
-No
-
-REST API impact
----------------
-
-No
-
-Security impact
----------------
-
-No
-
-Notifications impact
---------------------
-
-No
-
-Other end user impact
----------------------
-
-This BP will add one more image property, so user should be aware of that.
-
-Performance Impact
-------------------
-
-There will be no performance impact.
-
-Other deployer impact
----------------------
-
-It's recommended that deployers update all compute nodes before they add the
-config drive property to any images. Otherwise, the image property is not
-checked by compute node w/o this features.
-
-Developer impact
-----------------
-
-No
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  yunhong-jiang
-
-Work Items
-----------
-
-* Change the virt/configdrive.py to check image property also.
-
-Dependencies
-============
-
-There are some discussion of the enhancement of image property as in
-https://blueprints.launchpad.net/nova/+spec/convert-image-meta-into-nova-object
-and the discussion is on-going.
-
-This proposal is not conflict with that proposal, we just need make sure the
-new config drive property will be defined in the VirtProperties. It will be a
-small effort no matter which proposal lands firstly.
-
-Testing
-=======
-
-Tempest tests will be added so that we can make sure the image config drive
-property is treated correctly.
-
-Documentation Impact
-====================
-
-Document change needed for the new image property.
-
-References
-==========
-No
diff --git a/specs/juno/convert_ec2_api_to_use_nova_objects.rst b/specs/juno/convert_ec2_api_to_use_nova_objects.rst
deleted file mode 100644
index 0def439..0000000
--- a/specs/juno/convert_ec2_api_to_use_nova_objects.rst
+++ /dev/null
@@ -1,143 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===================================
-Convert EC2 API to use nova objects
-===================================
-
-https://blueprints.launchpad.net/nova/+spec/ec2-api-objects
-
-This blueprint covers updating EC2 API and related functions
-to use the Nova object model for all database interaction,
-like implementation in compute manager & nova-network now.
-
-Problem description
-===================
-
-Currently EC2 API use original raw db APIs to fetch data from the database.
-
-Proposed change
-===============
-
-The files need to be modified include:
-
-* nova/api/ec2/cloud.py
-* nova/api/ec2/ec2utils.py
-* nova/tests/api/ec2/test_cinder_cloud.py
-* nova/tests/api/ec2/test_cloud.py
-* nova/tests/api/ec2/test_ec2_validate.py
-
-
-Alternatives
-------------
-
-None
-
-
-Data model impact
------------------
-
-Four parts are included,
-EC2SnapshotIdMapping, EC2VolumeIdMapping, EC2S3Image, EC2InstanceIdMapping.
-
-All of them need to be modified to make use of the object
-instead of using the db API directly for managing UUID to EC2 ID.
-
-* Now 'EC2VolumeMapping' & 'EC2InstanceMapping' need to co-ordinate work
-  with russellb working on objects.
-* 'EC2SnapshotIdMapping' & 'EC2S3Image' object
-  need to be added and implemented in nova/objects.ec2.py later.
-
-REST API impact
----------------
-
-None
-
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  wingwj
-
-Other contributors:
-  russellb
-
-Work Items
-----------
-
-* Add 'EC2VolumeMapping' object - (needs to co-ordinate work with russellb)
-
-* Add 'EC2InstanceMapping' object - (needs to co-ordinate work with russellb)
-
-* Add 'EC2SnapshotIdMapping' & 'EC2S3Image' object in /nova/objects/ec2.py
-
-* Use 'EC2VolumeMapping' in EC2 API & related tests
-
-* Use 'EC2InstanceMapping' in EC2 API & related tests
-
-* Use 'EC2SnapshotIdMapping' in EC2 API & related tests
-
-* Use 'EC2S3Image' in EC2 API & related tests
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-The original unit tests also need to rewrite using nova objects.
-After the modifications, all changed APIs will be verified together.
-
-Documentation Impact
-====================
-
-None
-
-
-References
-==========
-
-None
\ No newline at end of file
diff --git a/specs/juno/cross-service-request-id.rst b/specs/juno/cross-service-request-id.rst
deleted file mode 100644
index 31165c9..0000000
--- a/specs/juno/cross-service-request-id.rst
+++ /dev/null
@@ -1,147 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-API v3: Add x-openstack-request-id header
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/cross-service-request-id
-
-The various OpenStack services are standardizing on a common header name to
-use for the request ID: x-openstack-request-id. Nova currently uses the header
-x-compute-request-id.
-
-Problem description
-===================
-
-nova sends the request ID as x-compute-request-id. Other services (cinder,
-glance, neutron) send x-openstack-request-id.
-
-
-Proposed change
-===============
-
-Use x-openstack-request-id when handling v3 requests for nova. There is
-existing middleware in oslo to generate the ID and attach the header to
-the response.
-
-Alternatives
-------------
-
-The current approach -- keeping the existing header name -- is the alternative.
-This will perpetuate header name discontinuity among OpenStack services.
-
-Another alternative is to include the new header name for both v2 and v3. But
-the benefits of doing so is not great enough to justify altering the behavior
-of the existing API.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-This change will add a new header to HTTP responses. The new header,
-x-openstack-request-id, will have the same value as x-compute-request-id.
-After this blueprint is implemented, v2 will continue to return
-x-compute-request-id. For v3, only x-openstack-request-id will be returned.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-Users making requests using the v3 API will only receive the new header,
-x-openstack-request-id. python-novaclient uses x-compute-request-id (if
-present) when reporting an HTTPError; this will need to be updated to use the
-new header name when novaclient is using v3. Other clients moving from v2 to v3
-will need to consider the header name change.
-
-Performance Impact
-------------------
-
-None.
-
-Other deployer impact
----------------------
-
-This change has an UpgradeImpact, since it relies on adding middleware to the
-pipeline in api-paste.ini. Since the middleware is taking over the task of
-attaching the header to the response, not updating api-paste.ini will cause
-responses to be returned without the x-openstack-request-id header.
-Additionally, when using the v2 API, the x-compute-request-id header will also
-be missing. The impact of this will be missing request ID information in
-error output by novaclient, as alluded to in a previous section.
-
-Developer impact
-----------------
-
-None.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-chris-buccella
-
-Work Items
-----------
-
-1) Sync request_id middleware from oslo (complete)
-2) Use request_id middleware to add x-openstack-request-id to both the v3
-   pipeline in api-paste.ini
-3) Write middleware to attach x-compute-request-id. Add this to the v2 pipeline
-   only.
-4) Remove existing x-compute-request-id header manipulation code from
-   api/openstack/wsgi.py
-
-
-Dependencies
-============
-
-None.
-
-
-Testing
-=======
-
-Due to the header name change, api/compute/v3/servers/test_instance_actions
-will be affected, as it references the current header name. We already have
-a skip in place for this, and will update the test to use the new name after
-this blueprint is completed.
-
-
-Documentation Impact
-====================
-
-v3 responses of the API will only include x-openstack-request-id, not
-x-compute-request-id.
-
-
-References
-==========
-
-Discussion from the HK Summit:
-https://etherpad.openstack.org/p/icehouse-summit-nova-cross-project-request-ids
-
-Refinements from the ML:
-http://lists.openstack.org/pipermail/openstack-dev/2013-December/020774.html
-
-Existing change:
-https://review.openstack.org/#/c/66903/
diff --git a/specs/juno/db2-database.rst b/specs/juno/db2-database.rst
deleted file mode 100644
index e5014ad..0000000
--- a/specs/juno/db2-database.rst
+++ /dev/null
@@ -1,267 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=====================================
-Add Support for DB2 (v10.5+)
-=====================================
-
-https://blueprints.launchpad.net/nova/+spec/db2-database
-
-The community currently supports MySQL and PostgreSQL production databases.
-Several other core projects already support DB2 (Keystone, Glance, Neutron,
-Ceilometer, Heat). This blueprint adds support to Nova for DB2 as a production
-database.
-
-
-Problem description
-===================
-
-* Currently there is no support in the community for a deployer to run Nova
-  against a DB2 backend database.
-
-* For anyone running applications against an existing DB2 database that wants
-  to move to OpenStack, they'd have to use a different database engine to
-  run Nova in OpenStack.
-
-* There is currently an inconsistent support matrix across the core projects
-  since the majority of core projects support DB2 but Nova does not yet.
-
-
-Proposed change
-===============
-
-Add code to support migrating the Nova database against a DB2 backend. This
-would require a fresh deployment of Nova since there are no plans to migrate
-an existing Nova database from another engine, e.g. MySQL, to DB2.
-
-Unit test code would also be updated to support running tests against a DB2
-backend with the ibm_db_sa driver and all Nova patches will be tested against a
-Tempest full run with 3rd party CI running DB2 that IBM will maintain.
-
-There is already some code in Oslo's db.api layer to support common function
-with DB2 like duplicate entry error handling and connection trace, so that is
-not part of this spec.
-
-Alternatives
-------------
-
-Deployers can use other supported database backends like MySQL or PostgreSQL,
-but this may not be an ideal option for customers already running applications
-with DB2 that want to integrate with OpenStack. In addition, you could run
-other core projects with multiple schemas in a single DB2 OpenStack database,
-but you'd have to run Nova separately which is a maintenance/configuration
-problem.
-
-Data model impact
------------------
-
-#. The 216 migration will be updated to handle conditions with DB2 like index
-   and foreign key creation. The main issue here is that DB2 does not support
-   unique constraints over nullable columns, it will instead create a unique
-   index that excludes null keys. Most unique constraints created in Nova are
-   on non-nullable columns, but the instances.uuid column is nullable and the
-   216 migration creates a unique index on it, but this will not allow any
-   foreign keys on the instances.uuid column to be created with DB2 since the
-   reference column has to be a unique or primary key constraint.
-#. In order to support creating the same foreign keys that reference the
-   instances.uuid column as other database engines, the instances.uuid column
-   must be made non-nullable and a unique constraint must be created on it.
-   The dependent blueprint "Enforce unique instance uuid in data model" is
-   used to handle this change.
-#. Finally, add another migration script which creates the previously excluded
-   foreign keys from the 216 migration script for DB2.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-The only performance impact on existing deployments is in the migration
-script changes which would be tested with turbo-hipster.
-
-Other deployer impact
----------------------
-
-The new database migration which creates the missing foreign keys since the
-control node needs to be down when running the migration. However, the new
-migration only creates foreign keys if the backend is DB2, which would be a new
-installation as noted in the "Proposed change" section so the impact should be
-minimal.
-
-Developer impact
-----------------
-
-The only impact on developers is if they are adding DB API code or migrations
-that do not work with DB2 they will have to adjust those appropriately, just
-like we do today with MySQL and PostgreSQL. IBM active technical contributors
-would provide support/guidance on issues like this which require specific
-conditions for DB2, although for the most part the DB2 InfoCenter provides
-adequate detail on how to work with the engine and provides details on error
-codes.
-
-* DB2 SQL error message explanations can be found here:
-  http://pic.dhe.ibm.com/infocenter/db2luw/v10r5/index.jsp?topic=%2Fcom.ibm.db2.luw.messages.sql.doc%2Fdoc%2Frsqlmsg.html
-
-* Information on developing with DB2 using python can be found here:
-  http://pic.dhe.ibm.com/infocenter/db2luw/v10r5/index.jsp?topic=%2Fcom.ibm.swg.im.dbclient.python.doc%2Fdoc%2Fc0054366.html
-
-* Main contacts for DB2 questions in OpenStack:
-
-   * Matt Riedemann (mriedem@us.ibm.com) - Nova core member
-   * Brant Knudson (bknudson@us.ibm.com) - Keystone core member
-   * Jay Bryant (jsbryant@us.ibm.com) - Cinder core member
-   * Rahul Priyadarshi (rahul.priyadarshi@in.ibm.com) - ibm_db_sa maintainer
-
-* The DB2 CI wiki page also provides contact information for issues with third
-  party testing failures:
-  https://wiki.openstack.org/wiki/IBM/DB2-TEST
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  mriedem@us.ibm.com
-
-Work Items
-----------
-
-#. Change the 216 migration to work with DB2.
-#. Add a new migration to create the excluded foreign keys from the 216 script
-   for DB2.
-#. Make the test_migrations.py module work with a configured DB2 backend for
-   running unit tests.
-
-See the WIP patch for details: https://review.openstack.org/#/c/69047/
-
-
-Dependencies
-============
-
-* Blueprint "Enforce unique instance uuid in data model":
-  https://blueprints.launchpad.net/nova/+spec/enforce-unique-instance-uuid-in-db
-
-* DB2 10.5 support was added to sqlalchemy-migrate 0.9 during Icehouse:
-  https://blueprints.launchpad.net/sqlalchemy-migrate/+spec/add-db2-support
-
-* There are no requirements changes in Nova for the unit tests to work. The
-  runtime requirements are the ibm-db-sa and ibm_db modules, which are both
-  available from pypi. sqlalchemy-migrate optionally imports ibm-db-sa. The
-  ibm-db-sa module requires a natively compiled ibm_db which has the c binding
-  that talks to the DB2 ODBC/CLI driver.
-
-* Note that only DB2 10.5+ is supported since that's what added unique index
-  support over nullable columns which is how sqlalchemy-migrate handles unique
-  constraints over nullable columns.
-
-
-Testing
-=======
-
-There are three types of testing requirements, Tempest, unit test and
-turbo-hipster performance/scale tests. Each have different timelines for when
-they are proposed to be implemented.
-
-* IBM is already running 3rd party CI for DB2 on the existing Nova WIP patch
-  that adds DB2 support. The same 3rd party CI is running against all
-  sqlalchemy-migrate changes with DB2 on py26/py27 and runs Tempest against
-  Keystone/Glance/Cinder/Heat patches with a DB2 backend. Once the DB2 support
-  is merged the DB2 3rd party CI would run against all Nova patches with a full
-  Tempest run. This is considered required testing for this blueprint to merge
-  in the Juno release.
-
-* While code will be added to make the Nova unit tests work against a DB2
-  backend, running Nova unit tests against DB2 with third party CI is not
-  considered in the scope of this blueprint for Juno, but long-term this is
-  something IBM wants to get running for additional QA coverage for DB2 in
-  Nova. This is something that would be worked on after getting Tempest
-  running. The plan for delivering third party unit test coverage is in the
-  K release.
-
-* Running 3rd party turbo-hipster CI against DB2 is not in plan for this
-  blueprint in Juno but like running unit tests against DB2 in 3rd party CI,
-  running turbo-hipster against DB2 in 3rd party CI would be a long-term goal
-  for QA and the IBM team will work on that after Tempest is running and after
-  unit test CI is worked on. The plan for delivering third party turbo-hipster
-  performance test coverage is in the K release.
-
-* The proposed penalty for failing to deliver third party unit test and/or
-  turbo-hipster performance test coverage in the K release is that the Nova
-  team will turn off voting/reporting of DB2 third party CI and not allow DB2
-  fixes to Nova until the third party CI is available.
-
-* More discussion in the mailing list here:
-  http://lists.openstack.org/pipermail/openstack-dev/2014-May/035009.html
-
-
-Documentation Impact
-====================
-
-* The install guides in the community do not go into specifics about setting up
-  the database.  The RHEL/Fedora install guide says to use the openstack-db
-  script provided by openstack-utils in RDO which uses MySQL.  The other
-  install guides just say that SQLite3, MySQL and PostgreSQL are widely used
-  databases. So for the install guides, those generic statements about
-  supported databases would be updated to add DB2 to the list. Similar generic
-  statements are also made in the following places which would be updated as
-  well:
-
-   * http://docs.openstack.org/training-guides/content/developer-getting-started.html
-   * http://docs.openstack.org/admin-guide-cloud/content/compute-service.html
-   * http://docs.openstack.org/trunk/openstack-ops/content/cloud_controller_design.html
-
-* There are database topics in the security guide, chapters 32-34, so there
-  would be DB2 considerations there as well, specifically:
-
-   * http://docs.openstack.org/security-guide/content/ch041_database-backend-considerations.html
-   * http://docs.openstack.org/security-guide/content/ch042_database-overview.html
-   * http://docs.openstack.org/security-guide/content/ch043_database-transport-security.html
-
-
-References
-==========
-
-* Work in progress nova patch: https://review.openstack.org/#/c/69047/
-
-* "Enforce unique instance uuid in data model" blueprint spec review:
-  https://review.openstack.org/#/c/97300/
-
-* There are Chef cookbooks on stackforge which support configuring OpenStack
-  to run with an existing DB2 installation:
-  http://git.openstack.org/cgit/stackforge/cookbook-openstack-common/
-
-* Mailing list thread on third party testing:
-  http://lists.openstack.org/pipermail/openstack-dev/2014-May/035009.html
-
-* DB2 10.5 InfoCenter: http://pic.dhe.ibm.com/infocenter/db2luw/v10r5/index.jsp
-
-* Some older manual setup instructions for DB2 with OpenStack:
-  http://www.ibm.com/developerworks/cloud/library/cl-openstackdb2/index.html
-
-* ibm-db-sa: https://code.google.com/p/ibm-db/source/clones?repo=ibm-db-sa
-
-* DB2 Third Party CI Wiki: https://wiki.openstack.org/wiki/IBM/DB2-TEST
diff --git a/specs/juno/ec2-volume-and-snapshot-tags.rst b/specs/juno/ec2-volume-and-snapshot-tags.rst
deleted file mode 100644
index dc4e0ff..0000000
--- a/specs/juno/ec2-volume-and-snapshot-tags.rst
+++ /dev/null
@@ -1,161 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-========================================================
-Tags support in EC2 API for volumes and volume snapshots
-========================================================
-
-https://blueprints.launchpad.net/nova/+spec/ec2-volume-and-snapshot-tags
-
-Expose volume and volume snapshot metadata as EC2 tags in the EC2 API.
-
-Problem description
-===================
-
-OpenStack's EC2 API has little support for 'tags' (resource metadata).
-Only instance metadata are exposed in the EC2 API, so a user
-can create, delete and list only instance metadata. OpenStack Cinder API
-has support for metadata as well, for both volumes and volume snapshots,
-and we just need to expose it into the EC2 API. This blueprint aims to
-do just that.
-
-
-Proposed change
-===============
-
-* EC2 API's 'CreateTags' method only used to work when one is creating
-  tags for an instance resource. After this patch, one will be also
-  able to create tags for volume and volume snapshot resources.
-
-* A user will be able to call the 'DeleteTags' API to delete any tag
-  associated with a volume or a volume snapshot.
-
-* While calling the 'DescribeTags' API, tags of volumes and volume
-  snapshots will be listed along with instance tags (provided tags
-  for these resources are present, obviously).
-
-* Support for specifying volume and volume snapshot IDs, and 'volume'
-  and 'snapshot' as resources as parameters while calling 'DescribeTags'
-  is added.
-
-* As this is the first time the supported resources for tags are becoming
-  plural in number, the code is made more generic so as to allow addition
-  of further resources easier.
-
-* Implementation detail: In the DescribeInstances API, user can specify
-  both resource ID and resource type as filters. If the query says filter
-  by resource IDs (vol-00000001 and ami-00000001) and also filter by
-  resource type (instances and volumes), the current implementation takes
-  the intersection of the resources (volumes in this case) and then checks
-  if those resources are implemented.
-
-Alternatives
-------------
-
-Alternative is: EC2 tags be different from volume tags by using scoped keys.
-So a user creating a tag stack=beta in EC2 API will, in the Cinder API, see
-it as EC2:stack=beta. This way, a user using the Cinder APIs will be able
-to clearly see which metadata entries are created using EC2 API and which
-are created by the OpenStack API.
-
-I think it makes sense to keep the EC2 API layer as transparent as possible.
-This means not going with the alternative proposed above. This also falls in
-line with what we have presently for instance metadata.
-
-Regarding the implementation detail specified above, an alternative is:
-Do not allow resource IDs and resource type to be specified in the same
-query.
-
-There is no doc of AWS which says such an API call is not allowed (atleast I
-can't find it). This implementation is easier, but IMO the way in which
-it is implemented right now gives a better user experience. Probably logging
-would help if we are dropping out a resource ID or resource type from the
-query.
-
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-Only EC2 API will be affected. Affected API calls are: CreateTags, DeleteTags
-DescribeTags.
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-Insignificant. Note that in the case of DescribeTags, as we keep on adding
-resources, an API call will be made to all of them (e.g. Glance, Cinder, etc)
-when a DescribeTags call is made without specifying a resource type.
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  rushiagr (Rushi Agrawal)
-
-Work Items
-----------
-
-* Implement support for volume and snapshot tags.
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-Comprehensive unit tests to test the functionality will be written.
-
-Documentation Impact
-====================
-
-EC2 API document should be updated to reflect the changes done to the EC2 API
-under this blueprint.
-
-
-References
-==========
-
-EC2 API reference:
-* CreateTags http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-CreateTags.html
-* DeleteTags http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-DeleteTags.html
-* DescribeTags http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-DescribeTags.html
diff --git a/specs/juno/enabled-qemu-memballoon-stats.rst b/specs/juno/enabled-qemu-memballoon-stats.rst
deleted file mode 100644
index 17c8800..0000000
--- a/specs/juno/enabled-qemu-memballoon-stats.rst
+++ /dev/null
@@ -1,164 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=====================================================
-Enabled qemu memory balloon stats when boot instance
-=====================================================
-
-https://blueprints.launchpad.net/nova/+spec/enabled-qemu-memballoon-stats
-
-We can get vm memory stats from libvirt API 'virDomainMemoryStats', it help
-telemetry module like as: Ceilometer to collect vm memory usage, but by
-default the memory statistical feature is disable in qemu, we need to add
-stats period in order to enabled memory statistical.
-
-Problem description
-===================
-
-By default, the memory statistical feature is disable in qemu, we need to
-add stats period in order to enabled memory statistical, like this::
-
-    <memballoon model='virtio'>
-      <stats period='10'/>
-    </memballoon>
-
-Add memballoon device stat period in libvirt.xml when boot instance.
-
-Actual memory statistical works on libvirt 1.1.1+ and qemu 1.5+, and need a
-guest driver that supports the feature, but booting instance with memory stats
-period does not lead to be failure on libvirt 0.9.6+ and qemu 1.0+.
-
-Refer to [1] for libvirt API 'virDomainMemoryStats' details.
-
-Refer to [2] for memballoon details in libvirt.xml.
-
-Details of enabled memory stats: [3]
-
-
-Proposed change
-===============
-
-* Add the option 'mem_stats_period_seconds' into nova.conf(libvirt section).
-* Enable stats period of memballoon device, if user boot instance when
-  mem_stats_period_seconds > 0. mem_stats_period_seconds is number of seconds
-  to memory usage statistics period. By default mem_stats_period_seconds=10.
-
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-User need to prepare suitable balloon driver in image, particularly for windows
-guests, most modern Linuxes have it built in. Booting instance will be
-successful without image balloon driver, just can't get guest memory stat from
-'virDomainMemoryStats' API.
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-Add a new option 'mem_stats_period_seconds' in nova.conf libvirt section.
-By default mem_stats_period_seconds=10, the stats feature is enable,
-mem_stats_period_seconds is number of seconds to memory usage statistics
-period. If mem_stats_period_seconds <= 0, the feature is disable.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  <kiwik-chenrui>
-
-Work Items
-----------
-
-* Add a LibvirtConfigMemoryBalloon class inherit from LibvirtConfigGuestDevice.
-* Changes to be made to the libvirt driver get_guest_config method to check
-  the option 'mem_stats_period_seconds' in nova.conf, during the boot of the
-  instance.
-* If mem_stats_period_seconds>0, set stats period of memory balloon device in
-  the instance.
-
-
-Dependencies
-============
-
-* libvirt 1.1.1+
-* qemu 1.5+
-* guest driver that supports memory balloon stats
-
-
-Testing
-=======
-
-Unit tests and tempest tests will verify this function. Compatibility will be
-verified, boot instance with 'mem_stats_period_seconds' on current devstack
-environment(libvirt0.9.8 and qemu1.0.0).
-
-Memory stats don't work in current gate environment, see details in
-Dependencies section. Full test need to ensure the devstack VM gate has updated
-libvirt, qemu versions and guest driver compatibility.
-
-
-Documentation Impact
-====================
-
-1. By default this feature is enabled, 'mem_stats_period_seconds'=10. If you
-   want to change the stat period, please modify nova.conf.
-
-2. mem_stats_period_seconds is number of seconds to memory usage statistics
-   period.
-
-3. If you set mem_stats_period_seconds<=0, the memory stats will be disabled,
-   by default mem_stats_period_seconds=10.
-
-This blueprint just add stats period into memory balloon device, it is not
-sufficient to guarantee this feature will work because you need to meet the
-requirements in dependencies section, and you need to handle the case where
-the API 'virDomainMemoryStats' call returns no data(not in scope of this bp).
-
-
-References
-==========
-
-* [1] http://libvirt.org/html/libvirt-libvirt.html#virDomainMemoryStats
-* [2] http://libvirt.org/formatdomain.html#elementsMemBalloon
-* [3] http://paste.openstack.org/show/78624/
diff --git a/specs/juno/encryption-with-barbican.rst b/specs/juno/encryption-with-barbican.rst
deleted file mode 100644
index bd3be5a..0000000
--- a/specs/juno/encryption-with-barbican.rst
+++ /dev/null
@@ -1,162 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-======================================================
-Make key manager interface interoperable with Barbican
-======================================================
-
-URL to Launchpad blueprint:
-
-https://blueprints.launchpad.net/nova/+spec/encryption-with-barbican
-
-The volume encryption feature added in the Havana release currently can only
-operate with a single key that is hardcoded in.  A much more flexible and
-secure solution would be to generate and store keys in Barbican, a cohesive and
-secure Linux-based key management system
-https://github.com/cloudkeep/barbican/wiki, which is now in the OpenStack
-incubation process.
-
-
-Problem description
-===================
-
-Problem 1: The OpenStack Volume Encryption feature currently cannot provide its
-designed level of security due to the absence of a key management service.
-Only a placeholder is available now, which isn't sufficient for the volume
-encryption feature to be used in an enterprise environment.  Keys cannot be
-stored, and only one hard-coded key is presented for all volumes. The proposed
-outcome would provide the ability to create and safely store dedicated keys for
-individual users or tenants.
-
-Problem 2: An ephemeral disk encryption feature supporting LVM was not accepted
-into the Icehouse release due to the lack of a key manager. For security
-reasons, since the disk is in close proximity to the virtual host, ephemeral
-disk encryption must use a key that's safely stored outside of the virtual host
-environment.
-
-An enterprise-grade key manager is needed for both cases, and Barbican
-(approved for incubation on 3/10/14) is becoming the default key manager that
-is slated to support OpenStack volume encryption, ephemeral disk storage
-encryption, and other potential security features.
-https://wiki.openstack.org/wiki/Barbican/Incubation. In order for Barbican to
-support these two storage encryption features, an interface between the
-existing key manager interface (nova/keymgr/key_mgr.py) used for volume
-encryption and the Barbican key manager needs to be developed.
-
-
-Proposed change
-===============
-
-Create an interface that will call python-barbicanclient, allowing Barbican to
-securely generate, store, and present encryption keys to Nova in support of the
-volume encryption feature.  The adapter will be a modification of the present
-key management abstraction layer in the volume encryption feature supporting
-block storage encryption on Cinder and ephemeral disk encryption.
-
-Alternatives
-------------
-
-Instead of implementing the existing key manager interface,
-python-barbicanclient could be invoked directly, but the additional indirection
-allows more extensibility if a different key manager needs to be integrated
-later.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-Use of a bonafide key manager greatly improves the security posture of the
-volume encryption and upcoming ephemeral disk encryption features.  When each
-user or tenant use a unique key instead of a common key, and when it is stored
-in a separate server, it will be much more difficult for an attacker to access
-stored encrypted data owned by a user or group of collective users within a
-tenant.
-
-Though the wrapper will be handling encryption keys, the security risk is
-considered minimal since the host must be trusted, and the wrapper is only
-holding the key temporarily.
-
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-The additional storage write and read time to initially query Barbican for the
-encryption key should be negligible.
-
-Other deployer impact
----------------------
-
-Assuming that Barbican is the default key manager, then no impact.  If it's not
-the default, then a configuration flag in Nova will need to be added.
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  hadi-esiely-barrera
-
-Other contributors:
-  brianna-poulos
-  bruce-benjamin
-
-Work Items
-----------
-
-Develop simple translation of existing key manager interface methods (e.g.,
-get_key) into the corresponding python-barbicanclient calls.
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-Tempest testing should be performed to ensure that the wrapper works correctly.
-
-
-Documentation Impact
-====================
-
-The use of Barbican as the default key manager for the storage encryption will
-need to be documented.
-
-
-References
-==========
-
-None
-
diff --git a/specs/juno/enforce-unique-instance-uuid-in-db.rst b/specs/juno/enforce-unique-instance-uuid-in-db.rst
deleted file mode 100644
index 458e7c2..0000000
--- a/specs/juno/enforce-unique-instance-uuid-in-db.rst
+++ /dev/null
@@ -1,164 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Enforce unique instance uuid in data model
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/enforce-unique-instance-uuid-in-db
-
-The instances.uuid column in the data model is not unique but by definition a
-UUID should be unique, and given how it's used within nova and across other
-openstack services like glance, neutron, ceilometer, etc, it should be unique.
-
-Furthermore, there are Foreign Keys created against instances.uuid so it should
-be unique.
-
-
-Problem description
-===================
-
-* Uniqueness for instances.uuid is not enforced in the data model.
-
-* There are foreign keys created on the instances.uuid column so it should be
-  unique.
-
-
-Proposed change
-===============
-
-Add a database migration that checks for existing records where the
-instances.uuid or related instance_uuid column is NULL and if found, fails the
-migration until those are deleted.
-
-A tool will be provided to scan the database for these records and list them,
-then prompt the user to delete them.  A --force option could also be provided
-in the tool to ignore any prompts and just delete the records if found.
-
-The new migration would be blocked until the records are deleted.  Once there
-are no records left, the migration will make those columns non-nullable via
-SQLAlchemy and create a UniqueConstraint on the instances.uuid column.
-
-Note that the fixed_ips table is the exception here since it can, by design,
-contain NULL instance_uuid records to indicate deallocated and disassociated
-fixed IPs.
-
-Alternatives
-------------
-
-Do nothing and leave the nova objects layer to enforce unique instance uuid
-entries, but this does not help with the foreign key issue in the data model.
-
-Data model impact
------------------
-
-#. NULL instances.uuid/instance_uuid records must be deleted, except in table
-   fixed_ips as described above.
-#. The instances.uuid column will be made non-nullable.
-#. A UniqueConstraint will be created on the instances.uuid column.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-The only performance impact on existing deployments is in the migration
-script changes which would be tested with turbo-hipster.
-
-Other deployer impact
----------------------
-
-The main impacts to deployers are:
-
-#. The biggest impact is the new migration. Migrations are potentially slow and
-   require the controller service to be down when run.
-#. The hope is that existing deployments are not carrying records where
-   instances.uuid or instance_uuid are None so the NULL queries in the new
-   migration script would not yield large result sets. However, the impact to
-   the deployer here is that they would be forced to manually prune those
-   records before the migration can continue. Note that it's expected that
-   those cases are exceptional and they are only the result of an inconsistent
-   database. So finding these records is not expected, but if it is a problem
-   the migration will fail clearly with instructions on how to fix the problem.
-
-Developer impact
-----------------
-
-None.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  mriedem@us.ibm.com
-
-Work Items
-----------
-
-#. Add a new migration to make instances.uuid non-nullable and put a unique
-   constraint on that column.
-#. Write a tool to check for null instance_uuid records within the database
-   for operators to use before the actual migration.
-
-See the WIP patch for details: https://review.openstack.org/#/c/97946/
-
-
-Dependencies
-============
-
-None.
-
-
-Testing
-=======
-
-* Unit tests for the new database migration will be added to stub a database
-  with NULL instance_uuid records to make sure the migration fails when those
-  records are found and then test that when they are removed, the migration
-  completes successfully and the unique constraint is created. Similarly the
-  downgrade path will be unit tested.
-* Unit tests will also be written for the scan tool used to run outside of the
-  actual database migrations. This will mock out the backend database but will
-  be used to test the CLI and logic.
-* It is expected that turbo-hipster will cover scale testing the new migration
-  for MySQL.
-
-
-Documentation Impact
-====================
-
-None.
-
-
-References
-==========
-
-* Work in progress nova patch: https://review.openstack.org/#/c/97946/
-
-* Mailing list thread on making instances.uuid non-nullable:
-  http://lists.openstack.org/pipermail/openstack-dev/2014-March/029467.html
diff --git a/specs/juno/extensible-resource-tracking.rst b/specs/juno/extensible-resource-tracking.rst
deleted file mode 100644
index cebc92d..0000000
--- a/specs/juno/extensible-resource-tracking.rst
+++ /dev/null
@@ -1,284 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Extensible Resource Tracking
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
-
-This blueprint introduces plugins to track resource allocation to allow the
-operator to select the resources they wish to track and to allow developers
-to add resource types without changing the existing code.
-
-Problem description
-===================
-
-The set of allocated compute resources is hard coded in the resource tracker,
-Allocation of these resources is always tracked regardless of their relevance
-to the cloud operator. In many cases the operator would like to track the use
-of different resources or account for their use in a different way.
-
-To support this requirement we need a way to easily develop additional
-resource tracking components that meet the operators preference and to make
-these optional so that only operators interested in them or are willing to
-incur any performance impact related to them, have to use them.
-
-The following is an example use case based on the CPU Entitlement
-blueprint referenced in the dependencies section below.
-
-As an operator I want to define a parameter for flavors called cu (compute
-unit). For users, cu represents cpu performance delivered by an instance
-using that flavor. Internally, cu represents a proportion of physical cpu
-capacity that should be assigned to the instance. I want to schedule
-instances to servers according to the available cpu capacity measured in cu.
-
-This use case describes a measure for cpu that is different to vcpu and
-cannot be implemented in terms of vcpu. The resource tracker needs to track
-the quantity of cu used at the host and report cu capacity to the scheduler.
-Note that the proportion of physical cpu mapped to cu depends on the
-performance of the processor. So in this case the operator would not use
-vcpu but would use cu. Other choices may be made in respect of other
-resources.
-
-Proposed change
-===============
-
-The proposed solution is to provide a plugin mechanism for resource tracking
-and make the selection of plugins configurable. This will include plugins
-at the resource tracker to represent compute resources, to track their usage,
-test availability in claims, and to communicate resource information to
-the scheduler. It will also include plugins for the host manager at the
-scheduler to interpret and handle the resource information received.
-
-Currently the means to make compute resource information available to the
-scheduler is via the compute_nodes table in the database. A field in this
-table will be used to communicate a dictionary of values representing the
-resource information.
-
-The existing extra_specs parameter of flavors already supports addition of
-resource requirements as key value/pairs, so no change is required in the
-APIs. However, the extra_specs parameter is not currently retained in the
-instances or instance_system_metadata tables so it will be added.
-
-A base class will be defined for a compute resource plugin for the resource
-tracker with methods to:
-
-* initialize the plugin
-
-* add and remove instances
-
-* test for sufficient resources to support a new instance
-
-* report resource information
-
-Plugins will be loaded by the resource tracker at start up using stevedore
-and called at existing points in the resource tracker code path. Exceptions
-occurring during method execution will be handled and logged.
-
-Plugins will be:
-
-* defined as entry points in the names space: **nova.compute.resources**
-
-* selected by name in the resource tracker configuration option:
-  **compute_resources**
-
-The resource information from the plugins will be recorded in the
-compute_nodes table in the database in **stats** field.
-
-A base class will be defined for a resource consumer plugin for the host
-manager with methods to:
-
-* read resource information
-
-* update resource information to reflect scheduler decisions
-
-Plugins will be loaded by the host manager at start up using stevedore and
-called at existing points in the host manager code path to make the resource
-information available in the host state. Exceptions occurring during method
-execution will be handled and logged.
-
-Plungins will be:
-
-* defined as entry points in the name space: **nova.scheduler.consumers**
-
-* selected by name in the host manager configuration option:
-  **scheduler_consumers**
-
-The new resource information can be exploited by filters and weights in the
-filter scheduler. The filters also have access to flavor extra_specs
-providing the ability to define new resource requirements that can be
-compared to the new resource information in the host state.
-
-By the nature of a distributed system configuration it is possible that an
-inconsistent set of resource, consumer, filter and weight plugins are loaded.
-Plugin developers are responsible for the behavior of the plugins in the
-event of missing or unexpected information. The exception handling around
-plugin method invocation will provide general error handling and reporting.
-
-Alternatives
-------------
-
-Our proposed solution defines two types of plugin: compute resource for the
-resource tracker and resource consumer for the host manager. The logic to
-add an instance to the compute resource plugin and to consume resources in
-the resource consumer plugin is essentially the same. These could be
-implemented as a single plugin that is loaded in both places. The dual
-plugin approach has been taken to avoid sharing code between the scheduler
-and the rest of nova in preparation for splitting the scheduler out from the
-rest of nova.
-
-When this blueprint was first implemented in the Icehouse cycle it was
-decided that the resource data would be communicated in a field called
-**extra_resources**. That field was created for this purpose and merged in
-Icehouse-2. Subsequently a separate change was made to remove the
-compute_node_stats table and put stats information in the compute_nodes
-table as well. The **stats** field was created for that purpose.
-
-Since the creation of the stats field there has been a debate over the
-future of the extra_resources field and which field should be used for this
-blueprint. There is an intention to refactor stats as resource plugins when
-this blueprint has been implemented. A key factor in the decision is how
-to do that refactor.
-
-It is possible to migrate stats handling to resource and consumer plugins
-without changing the representation of stats data in the database. So to ease
-the migration we propose to use the stats field and drop the extra_resources
-field.
-
-Data model impact
------------------
-
-The blueprint used the extra_resources field in the compute node table to
-communicate the resource tracking information. This field was added to the
-database in Icehouse-2 but has not yet been used. As discussed above, this
-will be removed and the existing stats field will be used instead.
-
-The extra_specs field will be added to the instances table.
-
-REST API impact
----------------
-
-This blueprint does not affect the existing REST APIs. New resource
-requirements can be set for flavors using the existing extra_specs API
-extension.
-
-Security impact
----------------
-
-This blueprint does not introduce any new security issues. The selection of
-plugins will be determined by operators and they will operate on data
-communicated through an existing path. Developers are able to make their
-plugins more robust by checking the integrity of the data they operate on.
-
-Notifications impact
---------------------
-
-This blueprint does not introduce new notifications.
-
-Other end user impact
----------------------
-
-This blueprint provides an extended resource management capability to the
-operator. It does not affect end users beyond the placement of their
-instances.
-
-Performance Impact
-------------------
-
-The plugin mechanism has no inherent performance impact, but performance may
-be impacted by the quantity of data exchanged by plugins and the performance
-of any operations they perform in the plugin methods.
-
-The compute resource plugins are called when instances are created, resized
-or migrated, and when the compute node executes its periodic resource update.
-
-The consumer plugins at the scheduler are called to interpret data received
-and to update host state when an instance placement decision is made. These
-are likely to be light weight operations.
-
-Other deployer impact
----------------------
-
-The plugins will be configured in the following ways:
-
-* the nova setup.cfg file will contain the entry points for plugins
-
-* the compute_resources config option select compute resource plugins
-
-* the scheduler_consumers config option select resource consumer plugins
-
-The default config options will be empty lists so no plugins will be loaded.
-This will ensure that this new feature only has effect if it is explicitly
-configured.
-
-Developer impact
-----------------
-
-Developers will be able to add new plugins for this feature.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  pmurray
-
-Other contributors:
-  andrea-rosa-m
-
-Work Items
-----------
-
-see:
-https://review.openstack.org/#q,topic:bp/extensible-resource-tracking,n,z
-
-The first two work items have patches are ready for review:
-
-* Add the resource plugin mechanism to resource tracker
-
-* Add the resource consumer plubin mechanism to the host manager
-
-* Add extra_specs to the instances table and write it to
-  instance_system_metadata
-
-The following work item is for house keeping:
-
-* The extra_resources field for the compute_nodes table was merged in
-  Icehouse-2. It will now be removed due to adopting the new stats field
-
-Dependencies
-============
-
-The following blueprints have a dependency on this one:
-
-* https://blueprints.launchpad.net/nova/+spec/cpu-entitlement
-
-* https://blueprints.launchpad.net/nova/+spec/network-bandwidth-entitlement
-
-* https://blueprints.launchpad.net/nova/+spec/cache-qos-monitoring
-
-Testing
-=======
-
-Unit tests are sufficient to cover feature changes.
-
-Documentation Impact
-====================
-
-Configuration options are derived automatically. New plugins
-should be listed as they are implemented.
-
-References
-==========
-
-Original blueprint for refactoring compute node stats:
-https://blueprints.launchpad.net/nova/+spec/stats-as-rt-extension
-
-Original specification that accompanied this blueprint in the Icehouse cycle:
-https://wiki.openstack.org/wiki/ExtensibleResourceTracking
diff --git a/specs/juno/find-host-and-evacuate-instance.rst b/specs/juno/find-host-and-evacuate-instance.rst
deleted file mode 100644
index 78b7da0..0000000
--- a/specs/juno/find-host-and-evacuate-instance.rst
+++ /dev/null
@@ -1,182 +0,0 @@
-
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===========================================
-API: Evacuate instance to a scheduled host
-===========================================
-
-https://blueprints.launchpad.net/nova/+spec/find-host-and-evacuate-instance
-
-The aim of this feature is to let operators evacuate instances without
-selecting a target host manually. The scheduler will select the best
-target instead.
-
-
-Problem description
-===================
-
-In the event of a unrecoverable hardware failure (compute-node down),
-Operators need to evacuate the instances by selecting a target
-compute host.
-
-This may work for temporary pre-selected failover-hosts, but if they
-just want to evacuate/rebuild the instance without taking further
-action, Operators must check each instance/flavor metadata and select
-target hosts that match the specs individually for each evacuation.
-
-In case of using external tools to trigger the evacuation, logic about
-the compute-hosts has to be there to appropriately call the API.
-
-It also make it consistent with migrate and live-migrate operations.
-
-
-Proposed change
-===============
-
-Modify the current rebuild_instance flow to let the scheduler pick up the best
-target host for the instance being evacuate.
-
-
-Alternatives
-------------
-
-Something external to pick up the proper host when
-nova can already do it.
-
-Data model impact
------------------
-None
-
-REST API impact
----------------
-
-The current evacuate API v2/v3 will be modified to accept body data without
-target host field and this change will be advertised through a new
-extension ExtendedEvacuateFindHost in case of v2.
-If the field is present but empty old behavior will be applied to be
-able to determine if it's an missing due to input error or not.
-
-* Evacuate an instance to another compute-host.
-     * POST
-     * Normal Response Code: 200
-     * Expected error http response code(s)
-           - 404: Compute host (if provided)/instance not found
-           - 400: Compute service in use
-           - 409: Invalid instance state
-     * v2|v3/servers/id/action
-     * Schema definition for V3::
-
-        evacuate = {
-        'type': 'object',
-        'properties': {
-            'evacuate': {
-                'type': 'object',
-                'properties': {
-                    'on_shared_storage': parameter_types.boolean,
-                    'admin_password': parameter_types.admin_password,
-                },
-                'required': ['on_shared_storage']
-                'additionalProperties': False,
-            },
-        },
-        'required': ['evacuate'],
-        'additionalProperties': False,
-        }
-
-     * Sample request::
-
-        { "evacuate": { "adminPass": "%(adminPass)s",
-                        "onSharedStorage": "%(onSharedStorage)s" }}
-
-     * Sample Response::
-
-        {  "adminPass": "%(password)s" }}
-
-
-
-Security impact
----------------
-None
-
-Notifications impact
---------------------
-None
-
-Other end user impact
----------------------
-
-python-novaclient will be modified to have target_host argument as
-optional.
-
-The user can trigger this feature by:
-nova evacuate my_server
-
-
-Performance Impact
-------------------
-None
-
-Other deployer impact
----------------------
-None
-
-Developer impact
-----------------
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  leandro-i-constantino
-
-Other contributors:
-  juan-m-olle
-
-
-Work Items
-----------
-
-* Move rebuild instance to conductor task to unify rebuild/evacuate logic
-* Add logic to select target host
-* Add APIv2/v3
-* Set target-host optional on nova-client
-* Allow evacuating instances in an 'affinity' group, allowing the scheduler to
-  pick the destination
-
-Dependencies
-============
-
-For a complete use-case the following bp will be required
-https://blueprints.launchpad.net/nova/+spec/validate-targethost-live-migration,
-since we can retrieve the original scheduler hints from that a particular
-instance and let the  scheduler select the best host based on that.
-Until then, instances launched without any scheduler hint could still be
-selected by the scheduler by using flavor specs.
-
-
-Testing
-=======
-
-Tempest do not currently support multi-node tests, so it will be added
-after CI can run those kind of tests.
-
-Documentation Impact
-====================
-
-* Api Docs to reflect that host field is now optional. If not present
-  in the body the new feature will be triggered.
-* Client docs ( due to optional arg)
-* Admin User Guide on evacuation topic.
-
-
-References
-==========
-None
diff --git a/specs/juno/hyper-v-console-log.rst b/specs/juno/hyper-v-console-log.rst
deleted file mode 100644
index 237bea6..0000000
--- a/specs/juno/hyper-v-console-log.rst
+++ /dev/null
@@ -1,127 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Hyper-V serial console log
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/hyper-v-console-log
-
-This blueprint introduces serial console log in the Nova Hyper-V driver.
-
-Problem description
-===================
-
-The Hyper-V driver is currently not providing a serial console log unlike
-other compute drivers (e.g. libvirt). This feature is particularly useful
-for the troubleshooting of both Linux and Windows instances.
-
-Proposed change
-===============
-
-Console log support in the Hyper-V nova driver will be obtained by implementing
-the "get_console_output" method inherited from nova.virt.driver.ComputeDriver.
-
-Hyper-V supports virtual serial ports in the guests, which can be redirected
-to a dedicated named pipe on the host.
-
-The driver will setup and connect the pipe upon starting or resuming a VM and
-closing it when stopping, suspending or live migrating.
-
-Data read from the pipe will be written in a file placed in the instance
-directory, capped to a maximum size.
-
-In case of live migration the console file must be moved to the destination
-server.
-
-A call to "get_console_output" for a given instance will return the content of
-the file.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  petrutlucian94
-
-Other contributors:
-  alexpilotti
-
-Work Items
-----------
-
-* Hyper-V Nova driver feature implementation
-* Unit tests
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-* Unit tests
-* Additional Tempest tests can be evaluated
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-* Initial discussion (Juno design summit):
-  https://etherpad.openstack.org/p/nova-hyperv-juno
diff --git a/specs/juno/hyper-v-soft-reboot.rst b/specs/juno/hyper-v-soft-reboot.rst
deleted file mode 100644
index 5be3c5f..0000000
--- a/specs/juno/hyper-v-soft-reboot.rst
+++ /dev/null
@@ -1,116 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Hyper-V soft reboot
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/hyper-v-soft-reboot
-
-This blueprint introduces soft reboot support in the Nova Hyper-V driver.
-
-Problem description
-===================
-
-Currently both "nova reboot" and "nova reboot --hard" cause a hard reset on
-Hyper-V instances. The driver needs to perform a soft reboot in the former case
-for consistency with the API specifications.
-
-Proposed change
-===============
-
-This feature can be implemented by invoking the "InitiateShutdown" method of
-the "Msvm_ShutdownComponent" class, waiting for the VM to reach a powered off
-status and powering it on again.
-
-For consistency with the libvirt driver, if a soft reboot fails then a hard
-reboot is attempted.
-
-Hyper-V provides an API to execute a soft shutdown but not a direct API to
-execute a soft reboot, hence the need to wait for the shutdown to be completed.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  alexpilotti
-
-Work Items
-----------
-
-* Hyper-V Nova driver feature implementation
-* Unit tests
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-* Unit tests
-* Additional Tempest tests can be evaluated
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-* Initial discussion (Juno design summit):
-  https://etherpad.openstack.org/p/nova-hyperv-juno
diff --git a/specs/juno/i18n-enablement.rst b/specs/juno/i18n-enablement.rst
deleted file mode 100644
index d101db0..0000000
--- a/specs/juno/i18n-enablement.rst
+++ /dev/null
@@ -1,185 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-i18n Enablement for Nova
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/i18n-enablement
-
-This BluePrint/Spec proposes completing the enablement of i18n
-(internationalization) support for Nova by turning on the "lazy" translation
-support from Oslo i18n and updating Nova to adhere to the restrictions this
-adds to translatable strings.
-
-Internationalization implementation has been an on-going effort in OpenStack
-during recent releases.  The original blueprint for the Oslo support was
-included in Havana:
-https://blueprints.launchpad.net/oslo/+spec/delayed-message-translation
-
-Blueprints for this support in Nova have been approved and worked on in
-previous releases
-(https://blueprints.launchpad.net/nova/+spec/user-locale-api).
-During the Icehouse release, the foundational support for internationalization
-was merged into Nova.  Specifically the update of Oslo's gettextutils and the
-pre-existing work of explicitly importing '_' from gettextutils.
-
-To finalize this work in Juno we need to enable the "lazy" translation
-provided in gettextutils and change how messages are manipulated.  Enablement
-of lazy translation will allow end users to not only have logs produced in
-multiple languages, but adds the ability for REST API messages to also be
-returned in the language chosen by the user.  This functionality is important
-to support the use of OpenStack by the international community.
-
-
-Problem description
-===================
-
-Today all users of Nova must agree on a common locale to use to translate
-messages.  This is because messages are translated when they are created.
-There is a need for different Nova users to be able to use different
-translations simultaneously.
-
-Proposed change
-===============
-
-This proposal is to use the i18n support provided as part of Oslo in order
-to enable "lazy" translation of messages.  This support, instead of
-immediately translating the messages, creates a Message object which
-holds the message and replacement text until the message can be translated
-using the locale associated with the Accept-Language Header from the
-user request.
-
-The code changes will be done as a series of patches that culminate in a
-patch that adds a call to 'gettextutils.enable_lazy()' in
-nova/cmd/__init__.py.
-
-A few prepratory patches will be required due to the limitations of the
-i18n support:
-
-* The Message class does not support str(), so use of str() on translatable
-  messages must be removed.  The most common case being when it is used on an
-  exception that is being put into another translatable message or logged.
-  This is due to the requirement by logging in Python 2.6 that str() return
-  a UnicodeError.
-* The Message class does not support concatenation of translatable messages,
-  so concatenation of translatable messages must be replaced with formatting.
-  This is due to the complexity caused by trying to concatenate two
-  independent Message instances potentially with overlapping replacement keys.
-  There are very few of these and the use of formatting allows for better
-  translation by translators.
-
-Alternatives
-------------
-
-None.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-There is no additional changes to the REST API other than the fact
-that the change enables the user to specify the language they
-wish REST API responses to be returned in using the Accept-Language
-option.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-None.
-
-Other deployer impact
----------------------
-
-Once merged this feature is immediately available to users.
-
-
-Developer impact
-----------------
-
-The developer impacts have already been in place for some time.  Developers
-have been using _() around messages that need translation.
-
-Note, however, that with the relatively new policy of not translating debug
-log messages, concatenating strings and exceptions will need care since the
-strings have to be cast to unicode. See https://review.openstack.org/#/c/78095/
-for examples. Cleaning this up is listed in the Work Items section.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  <jecarey@us.ibm.com>
-
-Work Items
-----------
-
-I am planning to implement this as three patches in this order:
-
-* Remove concatenations of translatable messages
-* Remove use of str() on translatable messages
-* Add enable_lazy to nova/cmd/__init__.py
-* Investigate and add hacking checks to catch i18n unfriendly practices
-
-Dependencies
-============
-
-None.
-
-* Note that gettextutil was synced with the latest oslo-incubator via
-  commit 185e4562df47a101cf41d1e66d75de2644c78022.
-
-
-Testing
-=======
-
-* There will be a tempest test added for Nova that will ensure that
-  lazy translation is working properly.
-
-* Hacking checks will be investigated and added for failures caused when
-  enabling lazy translation.
-
-  * For example the changes in https://review.openstack.org/#/c/78095/ and
-    https://review.openstack.org/#/c/78096/ which includes using str()
-    (or six.text_type) on an exception used as replacement text.
-
-
-Documentation Impact
-====================
-
-None.
-
-
-References
-==========
-
-* Mailing list discussion initiated by FFE rejected request for adding i18n to
-  Icehouse:
-  https://www.mail-archive.com/openstack-dev@lists.openstack.org/msg18617.html
-* Accept-Language header: http://www.w3.org/International/questions/qa-accept-lang-locale
diff --git a/specs/juno/implemented/add-differencing-vhdx-resize-support.rst b/specs/juno/implemented/add-differencing-vhdx-resize-support.rst
new file mode 100644
index 0000000..f3bdffd
--- /dev/null
+++ b/specs/juno/implemented/add-differencing-vhdx-resize-support.rst
@@ -0,0 +1,132 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================
+Add differencing vhdx resize support in Hyper-V Driver
+======================================================
+
+https://blueprints.launchpad.net/nova/+spec/add-differencing-vhdx-resize-support
+
+Differencing VHDX images can be resized, unlike differencing VHD images. Even
+so, the Nova Hyper-V driver currently does not support this.
+
+This feature is required for resizing existing instances which use CoW VHDX
+images and also in order to resize the root disk image when spawning a new
+instance.
+
+Problem description
+===================
+
+Currently, when using the Hyper-V Nova Driver and differencing (CoW) VHDX
+images for the instances, the differencing image will not get resized according
+to the flavor size. Instead, the VM root image will keep having the same size
+as the base image used when spawning a new instance.
+
+Also, when trying to resize such an instance, not only that the disk image will
+not get resized, but this will actually raise an exception as currently the
+method which gets the internal maximum size of a vhd/vhdx does not support
+differencing images.
+
+
+Proposed change
+===============
+
+The solution is simply passing the desired size when creating a new
+differencing vhdx image. Not passing it will result in the new disk having the
+same size as the base image.
+
+Also, it is required that the method which gets the internal maximum size of
+a vhdx to lookup the parent disk and return the according size instead of
+raising an exception.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <lpetrut@cloudbasesolutions.com>
+
+Work Items
+----------
+
+Add a "size" argument to the create_differencing_vhd method.
+
+Adapt the vmops module to specify the new size only if resize is required
+when booting a new instance using CoW vhdx images.
+
+Lookup for the parent image and get the according size when getting the
+maximum internal size of a vhdx.
+
+Adapt the vhdutils according methods in order to have the same method
+signatures and keep consistency.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Testing this feature will be covered by the Hyper-V CI.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+Official VHDX format specs:
+http://www.microsoft.com/en-us/download/details.aspx?id=34750
diff --git a/specs/juno/implemented/add-ironic-driver.rst b/specs/juno/implemented/add-ironic-driver.rst
new file mode 100644
index 0000000..c234799
--- /dev/null
+++ b/specs/juno/implemented/add-ironic-driver.rst
@@ -0,0 +1,285 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================
+Add a virt driver for Ironic
+============================
+
+https://blueprints.launchpad.net/nova/+spec/add-ironic-driver
+
+This specification proposes to add a virt driver to enable Nova to deploy
+images to bare metal resources by using the OpenStack Bare Metal Provisioning
+Service ("Ironic").
+
+Problem description
+===================
+
+The community has split out the functionality of provisioning bare metal
+servers into a separate program, which includes the ironic and
+python-ironicclient projects. The original intent of the
+nova.virt.baremetal driver was two-fold:
+
+- to provide physical machines more suitable to HPC-style workloads,
+  eg where virtualization overhead is too high;
+- to be an experimental proof-of-concept for enabling the TripleO project.
+
+In order to address scalability and architectural concerns affecting both
+use-cases, this driver was split out into a separate OpenStack Program,
+and developed over the last year as such.
+
+Proposed change
+===============
+
+This proposal aims to enable Nova to use Ironic to perform the same functions
+which it is currently able to perform via the nova.virt.baremetal driver.
+This abstracts the details of physical hardware within Ironic, such that the
+user interacts with Nova in the same way when deploying instances to virtual or
+physical machines. The hardware-specific details are only exposed to the cloud
+operators.
+
+Specifically, this will:
+
+* add the nova.virt.ironic driver, which will use the python-ironicclient
+  library to interact with Ironic's REST API for the purpose of provisioning
+  physical machines.
+
+* add new IronicHostManager class, similar to BaremetalHostManager, which
+  fills the same purpose but is specific to Ironic. Namely, this provides
+  several customizations to Nova's HostManager, tailoring it to consuming
+  discrete and non-subdivisible physical resources.
+
+* add exact-match scheduler filters, to facilitate users who wish to match
+  nova flavor to hardware specifications exactly. The best matching possible
+  today is greater-than-or-equal, which is often undesirable (eg. because
+  a machine with 128GB of RAM could be selected to fulfil a request for an
+  instance with 16GB of RAM).
+
+This driver will initially implement a subset of the Nova virt driver API
+sufficient to support the same functionality that the nova.virt.baremetal
+driver supported. Over time, additional functionality will be added, as
+appropriate and possible for physical hardware. It is expected that some
+operations may never be added to this driver, eg when the operation is not
+possible where there is no local hypervisor.
+
+This driver will expose the complete resources of the ironic service it is
+connected to. Therefor, running multiple nova-compute processes within a
+single cell or region will not be possible, and HA for the nova-compute
+service must be achieved externally, eg. via pacemaker+corosync. Scale-out
+may be achieved by running multiple ironic clusters, with a single n-cpu
+connected to each ironic end-point. This is not optimal, and is a result
+of a current limitation within Nova. See the Alternatives section below
+for a summary of the discussion which has occurred around this limitation.
+
+Alternatives
+------------
+
+One alternative would be for users to directly interact with Ironic's API,
+circumventing Nova when deploying instances to bare metal. This would require
+Ironic to duplicate a significant amount of functionality present in Nova, and
+violate the abstraction layer. Note that giving end-users direct access to
+Ironic's API may present security concerns for some operators; see the
+Security Impact section below for a discussion of this.
+
+Instead of creating a new IronicHostManager class, the existing
+BaremetalHostManager class could be refactored to support both drivers.
+
+Instead of adding exact-match scheduler filters, we could create a new
+scheduler that is specifically geared towards non-divisible resources.
+However, this approach is sufficient for many use cases, and does not prevent
+the later creation of another scheduler.
+
+An alternative was proposed which would allow multiple nova-compute processes
+to proxy for the same Ironic service end-point at the same time. This could be
+done by setting the same 'host' property on each nova-compute service, such
+that they expose the same set of resources.  Therefor, certain operations would
+need to be skipped when starting the nova-compute process (eg, so it doesn't
+trample over an ongoing operation on another compute host). This would be
+accomplished by creating a new ClusteredComputeManager class (subclassed from
+ComputeManager) which would override init_host() to avoid the call to
+InstanceList.get_by_host(), self._destroy_evacuated_instances() and
+self._init_instance(). This proposal was denied due to architectural concerns
+within Nova, specifically around @utils.synchronize(instance['uuid']) calls,
+and event callbacks that could be routed to a host other than the one waiting
+for the callback.
+
+Instead of overriding ComputeManager.init_host(), a significant rewrite of
+Nova's internal resource model could be undertaken -- eg, to remove the (host,
+hypervisor_hostname) tuple from all places within the code and make the
+nova-conductor process handle resource locking for clustered hypervisors, such
+as Ironic.  This would be a significant undertaking, and while merited, it was
+agreed that this work would not block the Ironic driver.
+
+
+Data model impact
+-----------------
+
+Adding the nova.virt.ironic driver will not impact the db model.
+
+Some additional extra_specs may be leveraged in faciliating better scheduling
+in the future. There is precedent in the way that the nova.virt.baremetal
+driver leverages extra_specs:baremetal:cpu_arch. Ironic may extend this to
+support additional hardware metadata in the future.
+
+REST API impact
+---------------
+
+The nova.virt.ironic driver will not add any REST API extensions or require
+changes to any of Nova's APIs.
+
+Security impact
+---------------
+
+Allowing Nova to provision physical hardware has significant security
+implications. The nova.virt.baremetal driver required direct access to the OOB
+management (IPMI) network of the hardware it managed. A compromise of
+nova-compute would expose that hardware's management interface.
+
+In a properly-secured OpenStack deployment, security will be improved by moving
+this functionality out of nova-compute and into ironic-conductor, because there
+is a strict API between the two services.
+
+The ironic-api should not be reachable or discoverable by end-users, and only
+the ironic-conductor service should have access to the hardware management
+interface.  The user of Nova who requests an instance on bare metal will thus
+have no direct access to the services managing that bare metal host. Should
+nova-compute be compromised, the malicious user would still need to gain access
+to the ironic-conductor host before having any access to the hardware
+management interface.
+
+Considering how often IPMI is not properly secured, and that in many cases it
+can not be secured, the OOB management network should be as isolated from users
+as possible.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+No impact on Nova itself.
+
+The performance profile of the nova.virt.ironic driver will be different than
+other virt drivers due to the nature of managing physical machines. For
+example, power cycling bare metal often takes more than five minutes as the
+hardware must complete a POST cycle. Thus, a deploy may be expected to take a
+minimum of ten minutes, though depending on the hardware, it may be more or
+less.
+
+Other deployer impact
+---------------------
+
+Deploying Nova with the nova.virt.ironic driver will be considerably different
+to deploying Nova with other virt drivers, and also different from the
+nova.virt.baremetal driver. Main areas of difference are:
+
+* different system libraries will be required. No local hypervisor needs be
+  installed, and none of the system libraries to enable baremetal need to be
+  installed on the compute host itself.
+
+* the OpenStack Ironic services must be properly set up and discoverable
+  via Keystone in order for the nova.virt.ironic driver to function properly.
+
+* Nova must be supplied with admin credentials capable of interacting
+  with Ironic.
+
+An upgrade path from the nova.virt.baremetal driver to the nova.virt.ironic
+driver will be provided. The details of that are proposed in another document:
+
+  https://blueprints.launchpad.net/nova/+spec/deprecate-baremetal-driver
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+
+Primary assignee:
+  devananda
+
+Other contributors:
+  lucasagomes
+  nobodycam
+
+Work Items
+----------
+
+* Merge auxiliary components: HostManager and exact-match scheduler filters
+
+* Delete auxiliary components from Ironic's tree
+
+* Split the nova.virt.ironic driver into a series of patches, the sum of
+  which will pass unit and functional tests.
+
+* Delete driver from Ironic's tree after it has merged in Nova.
+
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+There is already tempest testing being done upstream against changes in
+ironic, nova, devstack, and tempest. However, it is non-voting today.
+The following paragraph describes how it works.
+
+Devstack creates a "mock" bare metal node, enrolls it with Ironic, and
+configures Nova appropriately to use the nova.virt.ironic driver. A tempest
+scenario test is then run against that devstack instance, which allows tempest
+to test functionality appropriate for this driver. Certain tests may be
+excluded when the functionality does not apply to bare metal (eg,
+live migrate). The current test is fairly simple: validate the boot process,
+network connectivity of the instance, and validate destroy. Additional tests
+have been proposed for more coverage, eg. "rebuild --preserve-ephemeral".
+
+Testing of functionality not exposed via the nova virt driver interface is done
+directly in Tempest via the Ironic API (eg, management operations) and is
+mentioned here only for completeness.
+
+Documentation Impact
+====================
+
+Documentation should be added to Nova stating the existence of the new driver,
+and should include links to the Ironic project's developer and deployer
+documentation.
+
+References
+==========
+
+Current code, in Ironic's git tree::
+  http://git.openstack.org/cgit/openstack/ironic/tree/ironic/nova
+
+Devstack support for testing this driver::
+  http://git.openstack.org/cgit/openstack-dev/devstack/tree/lib/ironic
+  http://git.openstack.org/cgit/openstack-dev/devstack/tree/tools/ironic
+
+Tempest test which deploys using the nova.virt.ironic driver::
+  http://git.openstack.org/cgit/openstack/tempest/tree/tempest/scenario/test_baremetal_basic_ops.py
+
+Juno summit etherpad discussing this::
+  https://etherpad.openstack.org/p/juno-nova-deprecating-baremetal
+
+Some best practices for IPMI sanity::
+  http://fish2.com/ipmi/bp.pdf
+
+Discussions of IPMI vulnerabilities::
+  http://fish2.com/ipmi/itrain.pdf
+  http://fish2.com/ipmi/river.pdf
diff --git a/specs/juno/implemented/allow-image-to-be-specified-during-rescue.rst b/specs/juno/implemented/allow-image-to-be-specified-during-rescue.rst
new file mode 100644
index 0000000..dab5410
--- /dev/null
+++ b/specs/juno/implemented/allow-image-to-be-specified-during-rescue.rst
@@ -0,0 +1,204 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Allow image to be specified during rescue
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/allow-image-to-be-specified-during-rescue
+
+In this blueprint we aim to add an additional optional parameter to the
+instance rescue API. This parameter will be used to specify the image to be
+used while rescuing the instance. If the parameter is not specified, the
+instance will be rescued using the base image.
+
+
+Problem description
+===================
+
+The custom image used during rescue might be corrupt, leading to errors,
+or too large, leading to timeouts.
+Also, if the base image is deleted, the image ref on the
+instance_system_metadata will be invalid, leading to the rescue operation
+failing.
+This feature can also be used in the case where the customer wants to rescue
+the instance with a specific image, rather the default one. This would provide
+more flexibility to the feature.
+
+
+Proposed change
+===============
+
+In order to implement this I propose that we allow the user to specify which
+image is to be used for rescue. (could be a default base image or a custom
+image)
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+API for specifying image to be used to rescue an instance:
+
+Scenarios:
+Case 1: If image_ref is specified as part of the rescue request, that image
+will be used.
+Case 2: If image_ref is not specified as part of the rescue request,
+image_base_image_ref on the system_metadata of the instance will be used.
+(Default behavior)
+
+V2 API specification:
+POST: v2/{tenant_id}/servers/{server_id}/action
+
+V3 API specification:
+POST: v3/servers/{server_id}/action
+
+Request parameters:
+* tenant_id: The ID for the tenant or account in a multi-tenancy cloud.
+* server_id: The UUID for the server of interest to you.
+* rescue: Specify the rescue action in the request body.
+* adminPass(Optional): Use this password for the rescued instance.
+Generate a new password if none is provided.
+* rescue_image_ref(Optional): Use this image_ref for rescue.
+
+JSON request:
+{"rescue": {"adminPass": "MySecretPass",
+"rescue_image_ref": "848b39fb-6904-46d6-af3c-baa3eefedffc"}}
+
+JSON response:
+{"adminPass": "MySecretPass"}
+
+Sample v2 request:
+POST: /v2/d1b123/servers/7d14f8123/action -d '{"rescue":
+{"rescue_image_ref": "848b39fb-6904-46d6-af3c-baa3eefedffc"}}'
+
+Sample v3 request:
+POST: /v3/servers/7d14f8123/action -d '{"rescue":
+{"rescue_image_ref": "848b39fb-6904-46d6-af3c-baa3eefedffc"}}'
+
+This would use image with ref "848b39fb-6904-46d6-af3c-baa3eefedffc" to
+rescue instance with uuid "7d14f8123"
+
+JSON schema definition::
+
+    rescue = {
+        'type': 'object',
+        'properties': {
+            'rescue': {
+                'type': ['object', 'null'],
+                'properties': {
+                    'admin_password': parameter_types.admin_password,
+                    'rescue_image_ref': parameter_types.image_ref,
+                },
+                'additionalProperties': False,
+            },
+        },
+        'required': ['rescue'],
+        'additionalProperties': False,
+    }
+
+HTTP response codes:
+v2:
+Normal HTTP Response Code: 200 on success
+v3:
+Normal HTTP Response Code: 202 on success
+(Will check whether these can be made consistent in v2 and v3 during
+implementation.)
+
+Validation:
+'rescue_image_ref' must be of a uuid-str format.
+Failure Response Code: HTTPBadRequest with "Invalid image ref format" message.
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The rescue call in python-novaclient will have to include the additional
+optional parameter
+
+Optional argument:
+--rescue_image_ref <image_ref> ID of image to be used for rescue
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+The parameter will be optional, so no other code needs to be changed.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    aditirav
+
+Work Items
+----------
+
+* Changes to be made to the compute manager rescue method to use the
+  image ref passed in, during the rescue of the instance.
+* Add an extension to the V2 API to make rescue take in the optional parameter
+  'rescue_image_ref
+* Changes to the V3 API to take in the optional parameter 'rescue_image_ref'
+* Include tests in tempest to check the behavior of rescue instance with
+  the image ref passed in through the API call.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Tempest tests to be added to check if rescue of the instance uses the image
+specified in the API call.
+
+
+Documentation Impact
+====================
+
+Changes to be made to the rescue API documentation to include the additional
+parameter 'rescue_image_ref' that can be passed in.
+
+
+References
+==========
+
+None
+
diff --git a/specs/juno/implemented/backportable-db-migrations-juno.rst b/specs/juno/implemented/backportable-db-migrations-juno.rst
new file mode 100644
index 0000000..175247b
--- /dev/null
+++ b/specs/juno/implemented/backportable-db-migrations-juno.rst
@@ -0,0 +1,135 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Allow DB migration backports to Icehouse
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations-juno
+
+Just as we did at the beginning of the Havana and Icehouse dev cycles, we need
+to reserve a range of DB migrations as the first DB change in Juno. This will
+allow a range to be used for migration backports to Icehouse if needed.
+
+
+Problem description
+===================
+
+Normally, it is not possible to backport a change that requires a database
+migration due to the linear versioned nature of the migrations.  For the last
+two releases (Havana and Icehouse), we have reserved a set of empty migrations
+as placeholders to allow for migration backports if needed.
+
+
+Proposed change
+===============
+
+The proposed change is to reserve 10 migrations for Icehouse backports. These
+migrations would be no-ops and would simply result in an increment of the
+schema version.
+
+Alternatives
+------------
+
+When figuring out ways to allow database migrations, alternatives usually
+involve discussion of drastic changes to the way we manage migrations.  For
+example, it could require moving to a new framework.  This proposal works for
+our current use of sqlalchemy-migrate.  This will also be the third release
+we've used this approach, so it's fairly well understood at this point.
+
+Data model impact
+-----------------
+
+There's no changes to the data model as a part of this effort.  It simply gives
+us the ability to backport data model changes to Icehouse.
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+These migrations have minimal cost and can be run against a database without
+taking down Nova services.
+
+Other deployer impact
+---------------------
+
+This set of changes requires doing database migrations.  However, they can be
+done without any Nova downtime.
+
+Developer impact
+----------------
+
+This must be the first set of migrations merged into Juno, or it doesn't work.
+
+Developers must also be very careful when writing migrations that may be
+backported.  They must be idempotent.  For example, if migration 115 is
+backported to 107 in the previous release, someone who has executed the
+backported migration must not suffer any trouble when the migration runs again
+after an upgrade.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  russellb
+
+Work Items
+----------
+
+* Create 10 placeholder migrations.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+The existing unit tests will cover this.  Both the normal devstack based CI
+systems, as well as the "turbo-hipster" DB CI system will provide functional
+test coverage of these placeholder migrations.
+
+
+Documentation Impact
+====================
+
+None.
+
+
+References
+==========
+
+* https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations-icehouse
+
+* https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
+
+* http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html
diff --git a/specs/juno/implemented/better-support-for-multiple-networks.rst b/specs/juno/implemented/better-support-for-multiple-networks.rst
new file mode 100644
index 0000000..6f3e685
--- /dev/null
+++ b/specs/juno/implemented/better-support-for-multiple-networks.rst
@@ -0,0 +1,207 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================
+Better Support for Multiple Networks in nova-network
+====================================================
+
+https://blueprints.launchpad.net/nova/+spec/better-support-for-multiple-networks
+
+Since nova-network is staying around, it needs a few updates for multiple
+networks. There are various settings that are automatically determined or set
+via flags, that should be explicitly set per network. This spec is about adding
+a few options to the networks table and converting the network manager and
+linux_net code to support multiple networks.
+
+Problem description
+===================
+
+Currently it is impossible to have multiple networks with different mtu
+settings or to have some networks that share ips or have external gateways and
+others that do not. If you have a single network it is possible to specify a
+different (external) gateway for that network by adding a custom dnsmasq.conf,
+but this breaks down when you have multiple networks.
+
+Proposed change
+===============
+
+This change proposes adding four fields to the networks table:
+
+ * mtu
+ * dhcp_server
+ * enable_dhcp
+ * share_address
+
+Each of the new fields will be used in place of existing config options or
+automatic value interpretation. The defaults for these options will mean there
+is no difference to users if they are not specified.
+
+It will also modify network create to allow these fields to be modified. An api
+extension will be added so one can determine if extra network fields are
+available.
+
+Alternatives
+------------
+
+Supporting this functionality without changing the data model would require
+some pretty complex config options. For example mtu could be a list of network
+names and mtus, but this is extremely unweildy.
+
+Data model impact
+-----------------
+
+This adds four new fields to the network model. The fallback for these fields
+will use the existing config options and defaults. These config options will be
+marked deprecated but will still work by default.
+
+The four new fields will be added to the object model, and they will be cut out
+for older versions.
+
+REST API impact
+---------------
+
+The current network create api allows extra values to be passed in and they are
+silently ignored. In order to provide information about whether the new fields
+are supported, a dummy api extension will be created and the extra fields will
+only be accepted/returned if the api extension is enabled.
+
+The json for a network create call would currently look like::
+
+    {
+        "network": {
+            "label": "new net 111",
+            "cidr": "10.20.105.0/24",
+            ...
+        }
+    }
+
+With the new fields it would support::
+
+    {
+        "network": {
+            "label": "new net 111",
+            "cidr": "10.20.105.0/24"
+            "mtu": 9000,
+            "enable_dhcp": "true",
+            "dhcp_server": "10.20.105.2",
+            "share_address": true,
+            ...
+        }
+    }
+
+These fields will also be returned in the show command::
+
+    {
+        "network": {
+            "bridge": "br100",
+            "bridge_interface": "eth0",
+            "broadcast": "10.0.0.7",
+            "cidr": "10.0.0.0/29",
+            "cidr_v6": null,
+            "created_at": "2011-08-15T06:19:19.387525",
+            "deleted": false,
+            "deleted_at": null,
+            "dhcp_start": "10.0.0.3",
+            "dns1": null,
+            "dns2": null,
+            "gateway": "10.0.0.1",
+            "gateway_v6": null,
+            "host": "nsokolov-desktop",
+            "id": "20c8acc0-f747-4d71-a389-46d078ebf047",
+            "injected": false,
+            "label": "mynet_0",
+            "multi_host": false,
+            "netmask": "255.255.255.248",
+            "netmask_v6": null,
+            "priority": null,
+            "project_id": "1234",
+            "rxtx_base": null,
+            "updated_at": "2011-08-16T09:26:13.048257",
+            "vlan": 100,
+            "vpn_private_address": "10.0.0.2",
+            "vpn_public_address": "127.0.0.1",
+            "vpn_public_port": 1000,
+            "mtu": 9000,
+            "dhcp_server": "10.20.105.2",
+            "enable_dhcp": true,
+            "share_address": true
+        }
+    }
+
+Security impact
+---------------
+
+This change doesn't have any security impact.
+
+Notifications impact
+--------------------
+
+This change doesn't impact notifications.
+
+Other end user impact
+---------------------
+
+This change will also include a modification to python-novaclient network
+create to allow users to create networks specifying the additional fields.
+
+Performance Impact
+------------------
+
+The performance impact of this change is negligible.
+
+Other deployer impact
+---------------------
+
+Deployers should start using the network fields in place of the config options,
+but there is no requirement for them to move right away.
+
+Developer impact
+----------------
+
+This change should not affect developers.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  vishvananda
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+Nova code addtions
+Python-novaclient code addtions
+Tempest test additions
+
+Dependencies
+============
+
+There are no new dependencies for this feature.
+
+
+Testing
+=======
+
+There are currently no tempest tests for the create network call. A test for
+create network including the new fields  will be added.  The internal
+modifications will be covered by unit tests.
+
+
+Documentation Impact
+====================
+
+The new additions to the network create call need to be documented.
+
+References
+==========
+
+None
diff --git a/specs/juno/implemented/compute-manager-objects-juno.rst b/specs/juno/implemented/compute-manager-objects-juno.rst
new file mode 100644
index 0000000..ddd6bd3
--- /dev/null
+++ b/specs/juno/implemented/compute-manager-objects-juno.rst
@@ -0,0 +1,160 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================
+Compute Manager Objects Support (Juno Work)
+===========================================
+
+https://blueprints.launchpad.net/nova/+spec/compute-manager-objects-juno
+
+This blueprint represents the remaining work to be done in Juno around
+moving the compute manager (and associated modules, like
+nova.compute.utils) to using objects instead of raw conductor
+methods. This is important because objects provide versioning of the
+actual data, which supports our upgrade goals.
+
+Problem description
+===================
+
+The nova compute manager still sends unversioned bundles of data using
+conductor and compute RPC methods, which is problematic during an
+upgrade where the format of the data has changed across releases.
+This is especially important for compute manager, because it is likely
+that it will be speaking to a newer conductor and compute node at
+times. During an upgrade, migrate and live-migrate operations are
+expected, and by nature will involve compute nodes running different
+versions of the code to communicate.
+
+Proposed change
+===============
+
+Migrate uses of raw condutor methods in the compute manager to
+objects. For example consider this::
+
+  service_ref = self.conductor_api.service_get_by_compute_host(
+          context, self.host)
+  self.conductor_api.compute_node_delete(context, service_ref['compute_node'])
+
+would become::
+
+  service = service_obj.Service.get_by_compute_host(context,
+                                                    self.host)
+  service.compute_node.destroy()
+
+Alternatives
+------------
+
+This is the accepted direction of the project to solve this
+problem. However, alternatives would be:
+
+1. Don't solve the problem and continue using unversioned data
+2. Attempt to enforce version bumps of individual methods when any
+   data (including nested downstream data) has changed
+
+Data model impact
+-----------------
+
+The low-level data model (i.e. the SQLAlchemy models) will not need to
+change. However, additional high-level objects may be added where
+necessary to provide versioned wrappers around the low-level models.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+In general, conversion of code to use objects does not affect
+notifications. However, at times, emission of notifications is
+embedded into an object method to achieve higher consistency about
+when and how the notifications are sent. No such changes are
+antitipated in this work, but it's always a possibility.
+
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+Moving to objects enhances the ability for deployers to incrementally
+roll out new code. It is, however, largely transparent for them.
+
+Developer impact
+----------------
+
+This is normal refactoring, so the impact is minimal. In general,
+objects-based code is easier to work with, so long-term it is a win
+for the developers.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  danms
+
+Work Items
+----------
+
+* check_can_live_migrate_destination
+* check_can_live_migrate_source
+* live_migration
+* _post_live_migration
+* _rollback_live_migration
+* _rollback_live_migration_at_destination
+* refresh_instance_security_rules
+* run_instance
+* detach_volume
+* Remaining uses of instance[attr] in compute/manager.py
+
+Dependencies
+============
+
+There is a cross-dependency between this blueprint and the following:
+
+  https://blueprints.launchpad.net/nova/+spec/virt-objects-juno
+
+At times, a virt driver will need to be modified to accept an object
+from the compute manager before the manager method can be fully
+converted.
+
+Testing
+=======
+
+In general, unit tests require minimal change when this happens,
+depending on how the tests are structured. Ideally, they are already
+mocking out database calls, which means the change to objects is a
+transparent one. In reality, this usually means minor tweaking to the
+tests to return whole data models, etc.
+
+Documentation Impact
+====================
+
+None.
+
+References
+==========
+
+* https://blueprints.launchpad.net/nova/+spec/compute-manager-objects
+* https://blueprints.launchpad.net/nova/+spec/virt-objects
+* https://blueprints.launchpad.net/nova/+spec/unified-object-model
diff --git a/specs/juno/implemented/config-drive-image-property.rst b/specs/juno/implemented/config-drive-image-property.rst
new file mode 100644
index 0000000..424ea0e
--- /dev/null
+++ b/specs/juno/implemented/config-drive-image-property.rst
@@ -0,0 +1,155 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Config drive based on image property
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/config-drive-image-property
+
+When creating an instance, check the image property to decide if a config
+drive should be created.
+
+Problem description
+===================
+
+Currently Nova decides if config drive is created for a server
+based on:
+
+a) If the user specifies the config-drive option in server create API
+   request, or,
+
+b) If the server is scheduled to a compute node with force_config_drive
+   option set.
+
+But we need consider the image requirement also. Some images may explicitly
+require config drive.
+
+Proposed change
+===============
+
+* Add an image property as "img_config_drive", the value of the
+  img_config_drive can be:
+
+  * img_config_drive=mandatory|optional
+
+  where these mean:
+
+  * mandatory == instance must always have a config drive
+
+  * optional == instance can use a config drive, but can still work if missing
+
+  Any other value will be treated as error. If no option specified, the default
+  value is optional.
+
+  In future, this property may be extended to include more choices like
+  'disable' to disable config_drive. A mechanism should be presented at that
+  time to make sure the 'disable' option is not treated as error.
+
+* The rule of config drive decision is described as followed table. A config
+  drive will be created whenever user specified in API, required in image
+  property or compute node configuration option specified it.
+
+    +-----------+------------------------+----------------+-----------+
+    |   API     |  Image Property        | Compute Config | Result    |
+    +===========+========================+================+===========+
+    |    No     |  Mandatory             | Set or Unset   | Yes       |
+    +-----------+------------------------+----------------+-----------+
+    |    No     |  Optional              | Set            | Yes       |
+    +-----------+------------------------+----------------+-----------+
+    |    No     |  Optional              | Unset          | No        |
+    +-----------+------------------------+----------------+-----------+
+    | Specified |  Mandatory or Optional | Set or Unset   | Yes       |
+    +-----------+------------------------+----------------+-----------+
+
+
+Alternatives
+------------
+
+Another option is to combine the API option and image property into one
+instance property in the API layer, but this is not clean IMHO.
+
+Data model impact
+-----------------
+
+No
+
+REST API impact
+---------------
+
+No
+
+Security impact
+---------------
+
+No
+
+Notifications impact
+--------------------
+
+No
+
+Other end user impact
+---------------------
+
+This BP will add one more image property, so user should be aware of that.
+
+Performance Impact
+------------------
+
+There will be no performance impact.
+
+Other deployer impact
+---------------------
+
+It's recommended that deployers update all compute nodes before they add the
+config drive property to any images. Otherwise, the image property is not
+checked by compute node w/o this features.
+
+Developer impact
+----------------
+
+No
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  yunhong-jiang
+
+Work Items
+----------
+
+* Change the virt/configdrive.py to check image property also.
+
+Dependencies
+============
+
+There are some discussion of the enhancement of image property as in
+https://blueprints.launchpad.net/nova/+spec/convert-image-meta-into-nova-object
+and the discussion is on-going.
+
+This proposal is not conflict with that proposal, we just need make sure the
+new config drive property will be defined in the VirtProperties. It will be a
+small effort no matter which proposal lands firstly.
+
+Testing
+=======
+
+Tempest tests will be added so that we can make sure the image config drive
+property is treated correctly.
+
+Documentation Impact
+====================
+
+Document change needed for the new image property.
+
+References
+==========
+No
diff --git a/specs/juno/implemented/convert_ec2_api_to_use_nova_objects.rst b/specs/juno/implemented/convert_ec2_api_to_use_nova_objects.rst
new file mode 100644
index 0000000..0def439
--- /dev/null
+++ b/specs/juno/implemented/convert_ec2_api_to_use_nova_objects.rst
@@ -0,0 +1,143 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================
+Convert EC2 API to use nova objects
+===================================
+
+https://blueprints.launchpad.net/nova/+spec/ec2-api-objects
+
+This blueprint covers updating EC2 API and related functions
+to use the Nova object model for all database interaction,
+like implementation in compute manager & nova-network now.
+
+Problem description
+===================
+
+Currently EC2 API use original raw db APIs to fetch data from the database.
+
+Proposed change
+===============
+
+The files need to be modified include:
+
+* nova/api/ec2/cloud.py
+* nova/api/ec2/ec2utils.py
+* nova/tests/api/ec2/test_cinder_cloud.py
+* nova/tests/api/ec2/test_cloud.py
+* nova/tests/api/ec2/test_ec2_validate.py
+
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+Four parts are included,
+EC2SnapshotIdMapping, EC2VolumeIdMapping, EC2S3Image, EC2InstanceIdMapping.
+
+All of them need to be modified to make use of the object
+instead of using the db API directly for managing UUID to EC2 ID.
+
+* Now 'EC2VolumeMapping' & 'EC2InstanceMapping' need to co-ordinate work
+  with russellb working on objects.
+* 'EC2SnapshotIdMapping' & 'EC2S3Image' object
+  need to be added and implemented in nova/objects.ec2.py later.
+
+REST API impact
+---------------
+
+None
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  wingwj
+
+Other contributors:
+  russellb
+
+Work Items
+----------
+
+* Add 'EC2VolumeMapping' object - (needs to co-ordinate work with russellb)
+
+* Add 'EC2InstanceMapping' object - (needs to co-ordinate work with russellb)
+
+* Add 'EC2SnapshotIdMapping' & 'EC2S3Image' object in /nova/objects/ec2.py
+
+* Use 'EC2VolumeMapping' in EC2 API & related tests
+
+* Use 'EC2InstanceMapping' in EC2 API & related tests
+
+* Use 'EC2SnapshotIdMapping' in EC2 API & related tests
+
+* Use 'EC2S3Image' in EC2 API & related tests
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+The original unit tests also need to rewrite using nova objects.
+After the modifications, all changed APIs will be verified together.
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+None
\ No newline at end of file
diff --git a/specs/juno/implemented/cross-service-request-id.rst b/specs/juno/implemented/cross-service-request-id.rst
new file mode 100644
index 0000000..31165c9
--- /dev/null
+++ b/specs/juno/implemented/cross-service-request-id.rst
@@ -0,0 +1,147 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+API v3: Add x-openstack-request-id header
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/cross-service-request-id
+
+The various OpenStack services are standardizing on a common header name to
+use for the request ID: x-openstack-request-id. Nova currently uses the header
+x-compute-request-id.
+
+Problem description
+===================
+
+nova sends the request ID as x-compute-request-id. Other services (cinder,
+glance, neutron) send x-openstack-request-id.
+
+
+Proposed change
+===============
+
+Use x-openstack-request-id when handling v3 requests for nova. There is
+existing middleware in oslo to generate the ID and attach the header to
+the response.
+
+Alternatives
+------------
+
+The current approach -- keeping the existing header name -- is the alternative.
+This will perpetuate header name discontinuity among OpenStack services.
+
+Another alternative is to include the new header name for both v2 and v3. But
+the benefits of doing so is not great enough to justify altering the behavior
+of the existing API.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+This change will add a new header to HTTP responses. The new header,
+x-openstack-request-id, will have the same value as x-compute-request-id.
+After this blueprint is implemented, v2 will continue to return
+x-compute-request-id. For v3, only x-openstack-request-id will be returned.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+Users making requests using the v3 API will only receive the new header,
+x-openstack-request-id. python-novaclient uses x-compute-request-id (if
+present) when reporting an HTTPError; this will need to be updated to use the
+new header name when novaclient is using v3. Other clients moving from v2 to v3
+will need to consider the header name change.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+This change has an UpgradeImpact, since it relies on adding middleware to the
+pipeline in api-paste.ini. Since the middleware is taking over the task of
+attaching the header to the response, not updating api-paste.ini will cause
+responses to be returned without the x-openstack-request-id header.
+Additionally, when using the v2 API, the x-compute-request-id header will also
+be missing. The impact of this will be missing request ID information in
+error output by novaclient, as alluded to in a previous section.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+chris-buccella
+
+Work Items
+----------
+
+1) Sync request_id middleware from oslo (complete)
+2) Use request_id middleware to add x-openstack-request-id to both the v3
+   pipeline in api-paste.ini
+3) Write middleware to attach x-compute-request-id. Add this to the v2 pipeline
+   only.
+4) Remove existing x-compute-request-id header manipulation code from
+   api/openstack/wsgi.py
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+Due to the header name change, api/compute/v3/servers/test_instance_actions
+will be affected, as it references the current header name. We already have
+a skip in place for this, and will update the test to use the new name after
+this blueprint is completed.
+
+
+Documentation Impact
+====================
+
+v3 responses of the API will only include x-openstack-request-id, not
+x-compute-request-id.
+
+
+References
+==========
+
+Discussion from the HK Summit:
+https://etherpad.openstack.org/p/icehouse-summit-nova-cross-project-request-ids
+
+Refinements from the ML:
+http://lists.openstack.org/pipermail/openstack-dev/2013-December/020774.html
+
+Existing change:
+https://review.openstack.org/#/c/66903/
diff --git a/specs/juno/implemented/enabled-qemu-memballoon-stats.rst b/specs/juno/implemented/enabled-qemu-memballoon-stats.rst
new file mode 100644
index 0000000..17c8800
--- /dev/null
+++ b/specs/juno/implemented/enabled-qemu-memballoon-stats.rst
@@ -0,0 +1,164 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================================
+Enabled qemu memory balloon stats when boot instance
+=====================================================
+
+https://blueprints.launchpad.net/nova/+spec/enabled-qemu-memballoon-stats
+
+We can get vm memory stats from libvirt API 'virDomainMemoryStats', it help
+telemetry module like as: Ceilometer to collect vm memory usage, but by
+default the memory statistical feature is disable in qemu, we need to add
+stats period in order to enabled memory statistical.
+
+Problem description
+===================
+
+By default, the memory statistical feature is disable in qemu, we need to
+add stats period in order to enabled memory statistical, like this::
+
+    <memballoon model='virtio'>
+      <stats period='10'/>
+    </memballoon>
+
+Add memballoon device stat period in libvirt.xml when boot instance.
+
+Actual memory statistical works on libvirt 1.1.1+ and qemu 1.5+, and need a
+guest driver that supports the feature, but booting instance with memory stats
+period does not lead to be failure on libvirt 0.9.6+ and qemu 1.0+.
+
+Refer to [1] for libvirt API 'virDomainMemoryStats' details.
+
+Refer to [2] for memballoon details in libvirt.xml.
+
+Details of enabled memory stats: [3]
+
+
+Proposed change
+===============
+
+* Add the option 'mem_stats_period_seconds' into nova.conf(libvirt section).
+* Enable stats period of memballoon device, if user boot instance when
+  mem_stats_period_seconds > 0. mem_stats_period_seconds is number of seconds
+  to memory usage statistics period. By default mem_stats_period_seconds=10.
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+User need to prepare suitable balloon driver in image, particularly for windows
+guests, most modern Linuxes have it built in. Booting instance will be
+successful without image balloon driver, just can't get guest memory stat from
+'virDomainMemoryStats' API.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Add a new option 'mem_stats_period_seconds' in nova.conf libvirt section.
+By default mem_stats_period_seconds=10, the stats feature is enable,
+mem_stats_period_seconds is number of seconds to memory usage statistics
+period. If mem_stats_period_seconds <= 0, the feature is disable.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <kiwik-chenrui>
+
+Work Items
+----------
+
+* Add a LibvirtConfigMemoryBalloon class inherit from LibvirtConfigGuestDevice.
+* Changes to be made to the libvirt driver get_guest_config method to check
+  the option 'mem_stats_period_seconds' in nova.conf, during the boot of the
+  instance.
+* If mem_stats_period_seconds>0, set stats period of memory balloon device in
+  the instance.
+
+
+Dependencies
+============
+
+* libvirt 1.1.1+
+* qemu 1.5+
+* guest driver that supports memory balloon stats
+
+
+Testing
+=======
+
+Unit tests and tempest tests will verify this function. Compatibility will be
+verified, boot instance with 'mem_stats_period_seconds' on current devstack
+environment(libvirt0.9.8 and qemu1.0.0).
+
+Memory stats don't work in current gate environment, see details in
+Dependencies section. Full test need to ensure the devstack VM gate has updated
+libvirt, qemu versions and guest driver compatibility.
+
+
+Documentation Impact
+====================
+
+1. By default this feature is enabled, 'mem_stats_period_seconds'=10. If you
+   want to change the stat period, please modify nova.conf.
+
+2. mem_stats_period_seconds is number of seconds to memory usage statistics
+   period.
+
+3. If you set mem_stats_period_seconds<=0, the memory stats will be disabled,
+   by default mem_stats_period_seconds=10.
+
+This blueprint just add stats period into memory balloon device, it is not
+sufficient to guarantee this feature will work because you need to meet the
+requirements in dependencies section, and you need to handle the case where
+the API 'virDomainMemoryStats' call returns no data(not in scope of this bp).
+
+
+References
+==========
+
+* [1] http://libvirt.org/html/libvirt-libvirt.html#virDomainMemoryStats
+* [2] http://libvirt.org/formatdomain.html#elementsMemBalloon
+* [3] http://paste.openstack.org/show/78624/
diff --git a/specs/juno/implemented/extensible-resource-tracking.rst b/specs/juno/implemented/extensible-resource-tracking.rst
new file mode 100644
index 0000000..cebc92d
--- /dev/null
+++ b/specs/juno/implemented/extensible-resource-tracking.rst
@@ -0,0 +1,284 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Extensible Resource Tracking
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
+
+This blueprint introduces plugins to track resource allocation to allow the
+operator to select the resources they wish to track and to allow developers
+to add resource types without changing the existing code.
+
+Problem description
+===================
+
+The set of allocated compute resources is hard coded in the resource tracker,
+Allocation of these resources is always tracked regardless of their relevance
+to the cloud operator. In many cases the operator would like to track the use
+of different resources or account for their use in a different way.
+
+To support this requirement we need a way to easily develop additional
+resource tracking components that meet the operators preference and to make
+these optional so that only operators interested in them or are willing to
+incur any performance impact related to them, have to use them.
+
+The following is an example use case based on the CPU Entitlement
+blueprint referenced in the dependencies section below.
+
+As an operator I want to define a parameter for flavors called cu (compute
+unit). For users, cu represents cpu performance delivered by an instance
+using that flavor. Internally, cu represents a proportion of physical cpu
+capacity that should be assigned to the instance. I want to schedule
+instances to servers according to the available cpu capacity measured in cu.
+
+This use case describes a measure for cpu that is different to vcpu and
+cannot be implemented in terms of vcpu. The resource tracker needs to track
+the quantity of cu used at the host and report cu capacity to the scheduler.
+Note that the proportion of physical cpu mapped to cu depends on the
+performance of the processor. So in this case the operator would not use
+vcpu but would use cu. Other choices may be made in respect of other
+resources.
+
+Proposed change
+===============
+
+The proposed solution is to provide a plugin mechanism for resource tracking
+and make the selection of plugins configurable. This will include plugins
+at the resource tracker to represent compute resources, to track their usage,
+test availability in claims, and to communicate resource information to
+the scheduler. It will also include plugins for the host manager at the
+scheduler to interpret and handle the resource information received.
+
+Currently the means to make compute resource information available to the
+scheduler is via the compute_nodes table in the database. A field in this
+table will be used to communicate a dictionary of values representing the
+resource information.
+
+The existing extra_specs parameter of flavors already supports addition of
+resource requirements as key value/pairs, so no change is required in the
+APIs. However, the extra_specs parameter is not currently retained in the
+instances or instance_system_metadata tables so it will be added.
+
+A base class will be defined for a compute resource plugin for the resource
+tracker with methods to:
+
+* initialize the plugin
+
+* add and remove instances
+
+* test for sufficient resources to support a new instance
+
+* report resource information
+
+Plugins will be loaded by the resource tracker at start up using stevedore
+and called at existing points in the resource tracker code path. Exceptions
+occurring during method execution will be handled and logged.
+
+Plugins will be:
+
+* defined as entry points in the names space: **nova.compute.resources**
+
+* selected by name in the resource tracker configuration option:
+  **compute_resources**
+
+The resource information from the plugins will be recorded in the
+compute_nodes table in the database in **stats** field.
+
+A base class will be defined for a resource consumer plugin for the host
+manager with methods to:
+
+* read resource information
+
+* update resource information to reflect scheduler decisions
+
+Plugins will be loaded by the host manager at start up using stevedore and
+called at existing points in the host manager code path to make the resource
+information available in the host state. Exceptions occurring during method
+execution will be handled and logged.
+
+Plungins will be:
+
+* defined as entry points in the name space: **nova.scheduler.consumers**
+
+* selected by name in the host manager configuration option:
+  **scheduler_consumers**
+
+The new resource information can be exploited by filters and weights in the
+filter scheduler. The filters also have access to flavor extra_specs
+providing the ability to define new resource requirements that can be
+compared to the new resource information in the host state.
+
+By the nature of a distributed system configuration it is possible that an
+inconsistent set of resource, consumer, filter and weight plugins are loaded.
+Plugin developers are responsible for the behavior of the plugins in the
+event of missing or unexpected information. The exception handling around
+plugin method invocation will provide general error handling and reporting.
+
+Alternatives
+------------
+
+Our proposed solution defines two types of plugin: compute resource for the
+resource tracker and resource consumer for the host manager. The logic to
+add an instance to the compute resource plugin and to consume resources in
+the resource consumer plugin is essentially the same. These could be
+implemented as a single plugin that is loaded in both places. The dual
+plugin approach has been taken to avoid sharing code between the scheduler
+and the rest of nova in preparation for splitting the scheduler out from the
+rest of nova.
+
+When this blueprint was first implemented in the Icehouse cycle it was
+decided that the resource data would be communicated in a field called
+**extra_resources**. That field was created for this purpose and merged in
+Icehouse-2. Subsequently a separate change was made to remove the
+compute_node_stats table and put stats information in the compute_nodes
+table as well. The **stats** field was created for that purpose.
+
+Since the creation of the stats field there has been a debate over the
+future of the extra_resources field and which field should be used for this
+blueprint. There is an intention to refactor stats as resource plugins when
+this blueprint has been implemented. A key factor in the decision is how
+to do that refactor.
+
+It is possible to migrate stats handling to resource and consumer plugins
+without changing the representation of stats data in the database. So to ease
+the migration we propose to use the stats field and drop the extra_resources
+field.
+
+Data model impact
+-----------------
+
+The blueprint used the extra_resources field in the compute node table to
+communicate the resource tracking information. This field was added to the
+database in Icehouse-2 but has not yet been used. As discussed above, this
+will be removed and the existing stats field will be used instead.
+
+The extra_specs field will be added to the instances table.
+
+REST API impact
+---------------
+
+This blueprint does not affect the existing REST APIs. New resource
+requirements can be set for flavors using the existing extra_specs API
+extension.
+
+Security impact
+---------------
+
+This blueprint does not introduce any new security issues. The selection of
+plugins will be determined by operators and they will operate on data
+communicated through an existing path. Developers are able to make their
+plugins more robust by checking the integrity of the data they operate on.
+
+Notifications impact
+--------------------
+
+This blueprint does not introduce new notifications.
+
+Other end user impact
+---------------------
+
+This blueprint provides an extended resource management capability to the
+operator. It does not affect end users beyond the placement of their
+instances.
+
+Performance Impact
+------------------
+
+The plugin mechanism has no inherent performance impact, but performance may
+be impacted by the quantity of data exchanged by plugins and the performance
+of any operations they perform in the plugin methods.
+
+The compute resource plugins are called when instances are created, resized
+or migrated, and when the compute node executes its periodic resource update.
+
+The consumer plugins at the scheduler are called to interpret data received
+and to update host state when an instance placement decision is made. These
+are likely to be light weight operations.
+
+Other deployer impact
+---------------------
+
+The plugins will be configured in the following ways:
+
+* the nova setup.cfg file will contain the entry points for plugins
+
+* the compute_resources config option select compute resource plugins
+
+* the scheduler_consumers config option select resource consumer plugins
+
+The default config options will be empty lists so no plugins will be loaded.
+This will ensure that this new feature only has effect if it is explicitly
+configured.
+
+Developer impact
+----------------
+
+Developers will be able to add new plugins for this feature.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  pmurray
+
+Other contributors:
+  andrea-rosa-m
+
+Work Items
+----------
+
+see:
+https://review.openstack.org/#q,topic:bp/extensible-resource-tracking,n,z
+
+The first two work items have patches are ready for review:
+
+* Add the resource plugin mechanism to resource tracker
+
+* Add the resource consumer plubin mechanism to the host manager
+
+* Add extra_specs to the instances table and write it to
+  instance_system_metadata
+
+The following work item is for house keeping:
+
+* The extra_resources field for the compute_nodes table was merged in
+  Icehouse-2. It will now be removed due to adopting the new stats field
+
+Dependencies
+============
+
+The following blueprints have a dependency on this one:
+
+* https://blueprints.launchpad.net/nova/+spec/cpu-entitlement
+
+* https://blueprints.launchpad.net/nova/+spec/network-bandwidth-entitlement
+
+* https://blueprints.launchpad.net/nova/+spec/cache-qos-monitoring
+
+Testing
+=======
+
+Unit tests are sufficient to cover feature changes.
+
+Documentation Impact
+====================
+
+Configuration options are derived automatically. New plugins
+should be listed as they are implemented.
+
+References
+==========
+
+Original blueprint for refactoring compute node stats:
+https://blueprints.launchpad.net/nova/+spec/stats-as-rt-extension
+
+Original specification that accompanied this blueprint in the Icehouse cycle:
+https://wiki.openstack.org/wiki/ExtensibleResourceTracking
diff --git a/specs/juno/implemented/find-host-and-evacuate-instance.rst b/specs/juno/implemented/find-host-and-evacuate-instance.rst
new file mode 100644
index 0000000..78b7da0
--- /dev/null
+++ b/specs/juno/implemented/find-host-and-evacuate-instance.rst
@@ -0,0 +1,182 @@
+
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===========================================
+API: Evacuate instance to a scheduled host
+===========================================
+
+https://blueprints.launchpad.net/nova/+spec/find-host-and-evacuate-instance
+
+The aim of this feature is to let operators evacuate instances without
+selecting a target host manually. The scheduler will select the best
+target instead.
+
+
+Problem description
+===================
+
+In the event of a unrecoverable hardware failure (compute-node down),
+Operators need to evacuate the instances by selecting a target
+compute host.
+
+This may work for temporary pre-selected failover-hosts, but if they
+just want to evacuate/rebuild the instance without taking further
+action, Operators must check each instance/flavor metadata and select
+target hosts that match the specs individually for each evacuation.
+
+In case of using external tools to trigger the evacuation, logic about
+the compute-hosts has to be there to appropriately call the API.
+
+It also make it consistent with migrate and live-migrate operations.
+
+
+Proposed change
+===============
+
+Modify the current rebuild_instance flow to let the scheduler pick up the best
+target host for the instance being evacuate.
+
+
+Alternatives
+------------
+
+Something external to pick up the proper host when
+nova can already do it.
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+
+The current evacuate API v2/v3 will be modified to accept body data without
+target host field and this change will be advertised through a new
+extension ExtendedEvacuateFindHost in case of v2.
+If the field is present but empty old behavior will be applied to be
+able to determine if it's an missing due to input error or not.
+
+* Evacuate an instance to another compute-host.
+     * POST
+     * Normal Response Code: 200
+     * Expected error http response code(s)
+           - 404: Compute host (if provided)/instance not found
+           - 400: Compute service in use
+           - 409: Invalid instance state
+     * v2|v3/servers/id/action
+     * Schema definition for V3::
+
+        evacuate = {
+        'type': 'object',
+        'properties': {
+            'evacuate': {
+                'type': 'object',
+                'properties': {
+                    'on_shared_storage': parameter_types.boolean,
+                    'admin_password': parameter_types.admin_password,
+                },
+                'required': ['on_shared_storage']
+                'additionalProperties': False,
+            },
+        },
+        'required': ['evacuate'],
+        'additionalProperties': False,
+        }
+
+     * Sample request::
+
+        { "evacuate": { "adminPass": "%(adminPass)s",
+                        "onSharedStorage": "%(onSharedStorage)s" }}
+
+     * Sample Response::
+
+        {  "adminPass": "%(password)s" }}
+
+
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+
+python-novaclient will be modified to have target_host argument as
+optional.
+
+The user can trigger this feature by:
+nova evacuate my_server
+
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  leandro-i-constantino
+
+Other contributors:
+  juan-m-olle
+
+
+Work Items
+----------
+
+* Move rebuild instance to conductor task to unify rebuild/evacuate logic
+* Add logic to select target host
+* Add APIv2/v3
+* Set target-host optional on nova-client
+* Allow evacuating instances in an 'affinity' group, allowing the scheduler to
+  pick the destination
+
+Dependencies
+============
+
+For a complete use-case the following bp will be required
+https://blueprints.launchpad.net/nova/+spec/validate-targethost-live-migration,
+since we can retrieve the original scheduler hints from that a particular
+instance and let the  scheduler select the best host based on that.
+Until then, instances launched without any scheduler hint could still be
+selected by the scheduler by using flavor specs.
+
+
+Testing
+=======
+
+Tempest do not currently support multi-node tests, so it will be added
+after CI can run those kind of tests.
+
+Documentation Impact
+====================
+
+* Api Docs to reflect that host field is now optional. If not present
+  in the body the new feature will be triggered.
+* Client docs ( due to optional arg)
+* Admin User Guide on evacuation topic.
+
+
+References
+==========
+None
diff --git a/specs/juno/implemented/hyper-v-console-log.rst b/specs/juno/implemented/hyper-v-console-log.rst
new file mode 100644
index 0000000..237bea6
--- /dev/null
+++ b/specs/juno/implemented/hyper-v-console-log.rst
@@ -0,0 +1,127 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Hyper-V serial console log
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/hyper-v-console-log
+
+This blueprint introduces serial console log in the Nova Hyper-V driver.
+
+Problem description
+===================
+
+The Hyper-V driver is currently not providing a serial console log unlike
+other compute drivers (e.g. libvirt). This feature is particularly useful
+for the troubleshooting of both Linux and Windows instances.
+
+Proposed change
+===============
+
+Console log support in the Hyper-V nova driver will be obtained by implementing
+the "get_console_output" method inherited from nova.virt.driver.ComputeDriver.
+
+Hyper-V supports virtual serial ports in the guests, which can be redirected
+to a dedicated named pipe on the host.
+
+The driver will setup and connect the pipe upon starting or resuming a VM and
+closing it when stopping, suspending or live migrating.
+
+Data read from the pipe will be written in a file placed in the instance
+directory, capped to a maximum size.
+
+In case of live migration the console file must be moved to the destination
+server.
+
+A call to "get_console_output" for a given instance will return the content of
+the file.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  petrutlucian94
+
+Other contributors:
+  alexpilotti
+
+Work Items
+----------
+
+* Hyper-V Nova driver feature implementation
+* Unit tests
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+* Unit tests
+* Additional Tempest tests can be evaluated
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+* Initial discussion (Juno design summit):
+  https://etherpad.openstack.org/p/nova-hyperv-juno
diff --git a/specs/juno/implemented/hyper-v-soft-reboot.rst b/specs/juno/implemented/hyper-v-soft-reboot.rst
new file mode 100644
index 0000000..5be3c5f
--- /dev/null
+++ b/specs/juno/implemented/hyper-v-soft-reboot.rst
@@ -0,0 +1,116 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Hyper-V soft reboot
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/hyper-v-soft-reboot
+
+This blueprint introduces soft reboot support in the Nova Hyper-V driver.
+
+Problem description
+===================
+
+Currently both "nova reboot" and "nova reboot --hard" cause a hard reset on
+Hyper-V instances. The driver needs to perform a soft reboot in the former case
+for consistency with the API specifications.
+
+Proposed change
+===============
+
+This feature can be implemented by invoking the "InitiateShutdown" method of
+the "Msvm_ShutdownComponent" class, waiting for the VM to reach a powered off
+status and powering it on again.
+
+For consistency with the libvirt driver, if a soft reboot fails then a hard
+reboot is attempted.
+
+Hyper-V provides an API to execute a soft shutdown but not a direct API to
+execute a soft reboot, hence the need to wait for the shutdown to be completed.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  alexpilotti
+
+Work Items
+----------
+
+* Hyper-V Nova driver feature implementation
+* Unit tests
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+* Unit tests
+* Additional Tempest tests can be evaluated
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+* Initial discussion (Juno design summit):
+  https://etherpad.openstack.org/p/nova-hyperv-juno
diff --git a/specs/juno/implemented/i18n-enablement.rst b/specs/juno/implemented/i18n-enablement.rst
new file mode 100644
index 0000000..d101db0
--- /dev/null
+++ b/specs/juno/implemented/i18n-enablement.rst
@@ -0,0 +1,185 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+i18n Enablement for Nova
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/i18n-enablement
+
+This BluePrint/Spec proposes completing the enablement of i18n
+(internationalization) support for Nova by turning on the "lazy" translation
+support from Oslo i18n and updating Nova to adhere to the restrictions this
+adds to translatable strings.
+
+Internationalization implementation has been an on-going effort in OpenStack
+during recent releases.  The original blueprint for the Oslo support was
+included in Havana:
+https://blueprints.launchpad.net/oslo/+spec/delayed-message-translation
+
+Blueprints for this support in Nova have been approved and worked on in
+previous releases
+(https://blueprints.launchpad.net/nova/+spec/user-locale-api).
+During the Icehouse release, the foundational support for internationalization
+was merged into Nova.  Specifically the update of Oslo's gettextutils and the
+pre-existing work of explicitly importing '_' from gettextutils.
+
+To finalize this work in Juno we need to enable the "lazy" translation
+provided in gettextutils and change how messages are manipulated.  Enablement
+of lazy translation will allow end users to not only have logs produced in
+multiple languages, but adds the ability for REST API messages to also be
+returned in the language chosen by the user.  This functionality is important
+to support the use of OpenStack by the international community.
+
+
+Problem description
+===================
+
+Today all users of Nova must agree on a common locale to use to translate
+messages.  This is because messages are translated when they are created.
+There is a need for different Nova users to be able to use different
+translations simultaneously.
+
+Proposed change
+===============
+
+This proposal is to use the i18n support provided as part of Oslo in order
+to enable "lazy" translation of messages.  This support, instead of
+immediately translating the messages, creates a Message object which
+holds the message and replacement text until the message can be translated
+using the locale associated with the Accept-Language Header from the
+user request.
+
+The code changes will be done as a series of patches that culminate in a
+patch that adds a call to 'gettextutils.enable_lazy()' in
+nova/cmd/__init__.py.
+
+A few prepratory patches will be required due to the limitations of the
+i18n support:
+
+* The Message class does not support str(), so use of str() on translatable
+  messages must be removed.  The most common case being when it is used on an
+  exception that is being put into another translatable message or logged.
+  This is due to the requirement by logging in Python 2.6 that str() return
+  a UnicodeError.
+* The Message class does not support concatenation of translatable messages,
+  so concatenation of translatable messages must be replaced with formatting.
+  This is due to the complexity caused by trying to concatenate two
+  independent Message instances potentially with overlapping replacement keys.
+  There are very few of these and the use of formatting allows for better
+  translation by translators.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+There is no additional changes to the REST API other than the fact
+that the change enables the user to specify the language they
+wish REST API responses to be returned in using the Accept-Language
+option.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+Once merged this feature is immediately available to users.
+
+
+Developer impact
+----------------
+
+The developer impacts have already been in place for some time.  Developers
+have been using _() around messages that need translation.
+
+Note, however, that with the relatively new policy of not translating debug
+log messages, concatenating strings and exceptions will need care since the
+strings have to be cast to unicode. See https://review.openstack.org/#/c/78095/
+for examples. Cleaning this up is listed in the Work Items section.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <jecarey@us.ibm.com>
+
+Work Items
+----------
+
+I am planning to implement this as three patches in this order:
+
+* Remove concatenations of translatable messages
+* Remove use of str() on translatable messages
+* Add enable_lazy to nova/cmd/__init__.py
+* Investigate and add hacking checks to catch i18n unfriendly practices
+
+Dependencies
+============
+
+None.
+
+* Note that gettextutil was synced with the latest oslo-incubator via
+  commit 185e4562df47a101cf41d1e66d75de2644c78022.
+
+
+Testing
+=======
+
+* There will be a tempest test added for Nova that will ensure that
+  lazy translation is working properly.
+
+* Hacking checks will be investigated and added for failures caused when
+  enabling lazy translation.
+
+  * For example the changes in https://review.openstack.org/#/c/78095/ and
+    https://review.openstack.org/#/c/78096/ which includes using str()
+    (or six.text_type) on an exception used as replacement text.
+
+
+Documentation Impact
+====================
+
+None.
+
+
+References
+==========
+
+* Mailing list discussion initiated by FFE rejected request for adding i18n to
+  Icehouse:
+  https://www.mail-archive.com/openstack-dev@lists.openstack.org/msg18617.html
+* Accept-Language header: http://www.w3.org/International/questions/qa-accept-lang-locale
diff --git a/specs/juno/implemented/instance-network-info-hook.rst b/specs/juno/implemented/instance-network-info-hook.rst
new file mode 100644
index 0000000..c640a59
--- /dev/null
+++ b/specs/juno/implemented/instance-network-info-hook.rst
@@ -0,0 +1,126 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================================
+Add hook for update_instance_cache_with_nw_info
+===============================================
+
+https://blueprints.launchpad.net/nova/+spec/instance-network-info-hook
+
+A hook of the update_instance_cache_with_nw_info call will allow hooks access
+to valuable network information as soon as it becomes available. This will be
+useful for sending this data to scripts that can make informed tweaks to the
+networking on hosts.
+
+Problem description
+===================
+
+Right now there is no way to hook into the updating of network info.
+
+Usecase:
+* Deployer would be able to register a hook to send networking information
+to a script that could make informed tweaks to networking on hosts. This
+might include flows or QoS.
+
+
+Proposed change
+===============
+
+Add a hook to the update_instance_cache_with_nw_info call to allow hooks
+access to this information.
+
+Alternatives
+------------
+
+This information is stored in the database, and could be accessed from there.
+But, this would require giving access to the database to outside applications
+and could potentitally increase load on the database.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+The new code itself will not introduce any performance impact, but due to the
+nature of hooks, any deployer introduced hooks could have a performance impact.
+It will be up to the deployer to test their hooks for performance impact.
+
+Other deployer impact
+---------------------
+
+This change will introduce a new location for hooks. It will not immediately
+effect a deployment, as new hooks would need to be introduced that would
+take advantage of this new hook location.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  andrew-melton
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* Register a new hook for update_instance_cache_with_nw_info
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Unit testing to verify that you can register a hook for
+update_instance_cache_with_nw_info should be sufficient. The functional
+testing of the actual hooks should be left to the deployer.
+
+
+Documentation Impact
+====================
+
+If there is a list of hook locations, it will need to be updated to include
+this new location.
+
+References
+==========
+
+* Dev docs on nova hooks: http://docs.openstack.org/developer/nova/devref/hooks.html
+
diff --git a/specs/juno/implemented/juno-slaveification.rst b/specs/juno/implemented/juno-slaveification.rst
new file mode 100644
index 0000000..1cab37c
--- /dev/null
+++ b/specs/juno/implemented/juno-slaveification.rst
@@ -0,0 +1,157 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+More periodic tasks to slave for Juno
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/juno-slaveification
+
+In the Icehouse development cycle we gave deployers the option to offload
+most reads from nova-compute periodic tasks to a DB replication slave.
+We will continue this work in Juno by "slaveifying" the rest of the
+periodic tasks where appropriate.
+
+Problem description
+===================
+
+Currently the accepted way to scale the database for reads and writes in Nova
+is to do a multi-master setup or use some sort of database clustering. The
+problem with this approach is that while read scalability is potentially
+increased by making more hardware resources available (CPU, RAM, iops, etc).
+Write scalability is decreased and more operational complexity is inherited.
+
+Proposed change
+===============
+
+I would like to continue the work done in Icehouse by completing the
+"slaveification" of periodic tasks.
+
+Alternatives
+------------
+
+There are alternative ways to scale reads and writes both:
+
+-Handling scaling within the application through some sort of sharding scheme.
+-Handle scaling at the DB level.
+
+We have a sharding model, cells, in Nova currently. It could be argued that
+time would be better spent improving this approach rather than spending time
+trying to scale it using available DB technologies.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+No negative changes, hopefully this allows us to take some load off of
+a "write master" and offload them to a slave or slaves.
+
+Other deployer impact
+---------------------
+
+If a deployer changes the slave_connection configuration parameter in the
+database section it is assumed that they are accepting the behavior of
+having all reads from periodic tasks be sent to that connection. The
+deployer needs to be educated and aware of the implication of running a
+database replication slave and fetching actionable data from said slave.
+These include, but may not be limited to:
+
+-Need for monitoring of the slave status
+-Operational staff familiar with maintenance of replication slaves
+-Possibility to operate on data that is slightly out of date
+
+See https://wiki.openstack.org/wiki/Slave_usage
+
+
+Developer impact
+----------------
+
+Developers should consider which reads might benefit from optionally using
+a slave handle. When new reads are introduced, consider the context in which
+the code is called. Will it matter if this code operates on possibly out of
+date data? Is the benefit of offloading reads greater than an inconvenience
+caused by acting on old data?
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <geekinutah>
+
+Other contributors:
+  <None>
+
+Work Items
+----------
+
+Slaveify the following periodic tasks in nova/compute/manager.py
+
+update_available_resource
+_run_pending_deletes
+_instance_usage_audit
+_poll_bandwidth_usage
+_poll_volume_usage
+
+Dependencies
+============
+
+We will need to have an object for bw_usage, this is covered by
+https://blueprints.launchpad.net/nova/+spec/compute-manager-objects-juno
+
+Testing
+=======
+
+Currently there is no testing in Tempest for reads going to the alternate
+slave handle. We should add a replication slave to our test runs and test
+the periodic tasks with and without slave_connection enabled.
+
+Documentation Impact
+====================
+
+The operations guide should be updated and provide instructions with references
+to MySQL and Postgres documentation on setting up and maintaining slaves. We
+should also talk about HA possibilities with asynchronous slaves and various
+automation frameworks that deal with this problem. It would also be good to
+explain that while being able to specify a slave_connection is primarily a
+scaling feature, the ability to use it for availability purposes is there.
+
+References
+==========
+
+https://wiki.openstack.org/wiki/Slave_usage
+
+The original blueprint with code history and discussion:
+https://blueprints.launchpad.net/nova/+spec/db-slave-handle
+
+The Icehouse blueprint:
+https://blueprints.launchpad.net/nova/+spec/periodic-tasks-to-db-slave
diff --git a/specs/juno/implemented/libvirt-disk-discard-option.rst b/specs/juno/implemented/libvirt-disk-discard-option.rst
new file mode 100644
index 0000000..30faea9
--- /dev/null
+++ b/specs/juno/implemented/libvirt-disk-discard-option.rst
@@ -0,0 +1,147 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================================
+Libvirt-Enable support discard option for disk device
+======================================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-disk-discard-option
+
+Most SCSI devices have supported UNMAP command which is used to return unused
+or freed blocks back to the storage. And SSD drive have a similar command
+called "Trim" command.
+
+This blueprint aims to support setting discard option for instance's disk.
+If discard option is enabled, unused/freed blocks can be return back to the
+storage device.
+
+Qemu1.5 has supported setting discard option for the raw disk. In Qemu1.6,
+qcow2 file supported discard option too.
+
+Cinder support is out of scope for this spec. This spec only covers disks
+managed by nova.
+
+Problem description
+===================
+
+Thin provision volume
+---------------------
+When we write data to a thin provision volume, storage device will allocate
+blocks to it and it will grow.
+But without discard option supported, the blocks will not be freed to the
+storage device even if we deleted the data in the volume. The result is the
+thin volume can grow, but can't shrink.
+With discard option supported, when user deleted the data in the volume,
+the blocks will be freed to the storage device. The volume will shrink.
+
+It's useful for both ephemeral volume and cinder volume.
+
+SSD backed volume
+--------------------
+Freeing the unused blocks to the SSD storage is useful to improving the
+performance and prolonging the lifetime of SSD.
+
+Proposed change
+===============
+
+Add support for the deployer to specify the discard option in the nova.conf by
+"hw_disk_discard".
+
+There are two available values for "hw_disk_discard" now::
+
+  "unmap" : Discard requests("trim" or "unmap") are passed to the filesystem.
+  "ignore": Discard requests("trim" or "unmap") are ignored and aren't passed
+  to the filesystem.
+
+For example::
+
+  hw_disk_discard=unmap    #enable discard
+  hw_disk_discard=ignore   #disable discard, by default
+
+For an instance running on a host which has the discard option in nova.conf,
+nova will produce the XML with a discard option when the nova managed disk
+is attached.
+
+Alternatives
+------------
+None
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+Discard option maybe cause performance degradation and fragmentation.
+But for the storage based on SSD, discard option is good for the performance.
+
+The users have control over this behaviour in the guest os if they don't want
+it. They can use the mount parameter or the command tools to control the
+discard behaviour.
+
+Other deployer impact
+---------------------
+Initially, only the libvirt driver will support this function, and
+only with qemu/kvm as the hypervisor.
+
+A serious consideration is needed before enabling discard option.
+With discard option enabled, the freed blocks of the thin provision volume
+will be return to the storage and can be reused. But it also maybe cause
+performance degradation and fragments.
+So it's reasonable to enable the discard option only when you use the thin
+provision volume and the storage are UNMAP/TRIM-capable. For example the SSD,
+the disk-arrays or other distributed storage which supports the UNMAP/TRIM.
+
+Developer impact
+----------------
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+boh.ricky
+
+Work Items
+----------
+
+* Libvirt driver will create a discard option for a disk device which the
+  instance flavor has discard option.
+
+Dependencies
+============
+Libvirt(1.0.6) Qemu1.5(raw format) Qemu1.6(qcow2 format)
+
+Testing
+=======
+None
+
+Documentation Impact
+====================
+
+The document should be modified to reflect this new feature.
+
+References
+==========
+None
diff --git a/specs/juno/implemented/libvirt-domain-listing-speedup.rst b/specs/juno/implemented/libvirt-domain-listing-speedup.rst
new file mode 100644
index 0000000..a73a149
--- /dev/null
+++ b/specs/juno/implemented/libvirt-domain-listing-speedup.rst
@@ -0,0 +1,139 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================================
+Speedup listing of domains in libvirt driver
+============================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-domain-listing-speedup
+
+The libvirt driver currently uses the legacy libvirt APIs for getting
+lists of domains. These are inefficient and prone to race conditions,
+so have been replaced by much better designed APIs.
+
+Problem description
+===================
+
+The libvirt driver in Nova currently uses a combination of calls to
+numOfDomains, listDomainsID, numOfDefinedDomains, listDefinedDomains,
+lookupByID and lookupByName to list domains on a host. This is very
+inefficient as it requires O(N) libvirt API calls to list 'N' guests.
+It also has designed in race condition where Nova can loose a guest
+if it transitions from shutoff to running while the list of domains
+is being fetched.
+
+The 0.9.13 version of libvirt introduced a new method listAllDomains
+which can be used to replace all those calls with a single API call,
+thus providing a way to get a list of domains which has constant
+execution time regardless of how many domains there are.
+
+Proposed change
+===============
+
+A new method 'list_instance_domains' will be introduced that will
+attempt to use the listAllDomains method to fetch the list of domains,
+and fallback to using the old method if it is not supported by the
+libvirt version or the hypervisor driver in use.
+
+Rather than just returning a list of domain IDs, names or UUIDs,
+it will return a list of libvirt.virDomain object instances avoiding
+the need todo separate lookups.
+
+By default it will only return running instances which were originally
+launched by Nova. It can be optionally told to include inactive
+instances, instances launched by other systems (eg libguestfs) or
+the Xen Domain-0 instance.
+
+Everywhere in the libvirt driver which calls list_instances or
+list_instance_ids will then be changed to use this new method, thus
+significantly improving their scalability.
+
+Alternatives
+------------
+
+Continue to use current APIs, but this is inefficient and prone to
+race conditions, so not at all desirable
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+It will improve the performance of the libvirt driver when used against
+libvirt >= 0.9.13, particularly when there are lots of instances of the
+host.
+
+Other deployer impact
+---------------------
+
+Deployers are strongly recommended to use libvirt >= 0.9.13 to take
+advantage of the performance improvements.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  berrange
+
+Work Items
+----------
+
+ - Implement new list_instance_domains method
+ - Write test cases for list_instance_domains
+ - Convert libvirt driver to use list_instance_domains
+
+Dependencies
+============
+
+* Current min libvirt is 0.9.6, but this requires 0.9.13. Fallback
+  code will be provided for use with 0.9.6 versions.
+
+Testing
+=======
+
+The current tempest gate tests should fully exercise the new code
+paths.
+
+Documentation Impact
+====================
+
+Recommend deployment of libvirt >= 0.9.13 for best scalability.
+
+References
+==========
+
+None
diff --git a/specs/juno/implemented/libvirt-driver-domain-metadata.rst b/specs/juno/implemented/libvirt-driver-domain-metadata.rst
new file mode 100644
index 0000000..45e7c64
--- /dev/null
+++ b/specs/juno/implemented/libvirt-driver-domain-metadata.rst
@@ -0,0 +1,158 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================
+Libvirt driver domain metadata
+==============================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-driver-domain-metadata
+
+Metadata will be recorded in the libvirt domain XML configuration to provide
+information about the Nova instance that the domain corresponds to. The aim
+is to provide information that can be useful to administrators troubleshooting
+compute hosts.
+
+Problem description
+===================
+
+When troubleshooting a compute node there will be a number of running libvirt
+domains which correspond to Nova instances. There may also be other running
+domains which were not launched by Nova, for example, utility guests run by
+libguestfs for file injection. The libvirt domain uuid will match that of the
+Nova instance, but there is more information about a Nova instance that could
+usefully be provided to administrators. For example, the identity of the
+tenant who launched it, the original flavour name and/or settings, the time at
+which the domain was launched, and the version number of the Nova instance that
+launched it (can be relevant if Nova is upgraded while a VM is running).
+
+Proposed change
+===============
+
+The Libvirt domain XML configuration schema allows for applications to insert
+arbitrary metadata under a private XML namespace. The proposal is to make use
+of this to define some metadata that is relevant to Nova, specifically it will
+record
+
+ - The nova package version
+ - The display name of the instance (as matching 'nova list')
+ - The name of the flavor
+ - The creation time of the instance
+ - The user and project ID/name of owner
+ - The root disk glance image or cinder volume UUID
+
+This would correspond to the following XML blob
+
+::
+
+  <domain type='kvm'>
+    ...rest of domain XML config...
+    <metadata>
+      <nova:instance xmlns:nova="http://openstack.org/nova/instance/1">
+        <nova:package version="2014.2.3"/>
+        <nova:flavor name="m1.small">
+          <nova:memory>512</nova:memory>
+          <nova:disk>10</nova:disk>
+          ....
+        </nova:flavor>
+        <nova:name>demo1vm</nova:name>
+        <nova:creationTime>2014-12-25 12:03:20</nova:creationTime>
+        <nova:owner>
+          <nova:user uuid="85bd45c0...213684">joe</nova:user>
+          <nova:project uuid="d33b8c0e...342d69">acmecorp</nova:project>
+        </nova:owner>
+        <nova:root type="image|volume" uuid="69f2991b...f29a8bc"/>
+      </nova:instance>
+    </metadata>
+  </domain>
+
+Alternatives
+------------
+
+Administrators can ask libvirt for the UUID of the running instance and then
+attempt to trace all the information back via Nova APIs. If Nova itself is in
+some failure scenario though, this would not be possible. It also places more
+burden on the administrator to trace the info which could be provided directly
+in the Libvirt XML.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+The compute host administrator will be able to ask libvirt to provide the XML
+config for the running instance and from there find out various useful pieces
+of metadata about the instance.
+
+Developer impact
+----------------
+
+None, this is entirely within the libvirt driver impl
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  berrange
+
+Work Items
+----------
+
+* Extend the nova/virt/libvirt/config.py object model to represent the
+  proposed metadata schema for Nova
+* Extend the nova/virt/libvirt/driver.py get_guest_config() method to fill
+  in the metadata when generating guest XML config
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+None required beyond unit tests
+
+Documentation Impact
+====================
+
+Document that the libvirt XML config contains this metadata as an aid
+for administrators debugging compute nodes.
+
+References
+==========
+
+* Libvirt XML format docs http://libvirt.org/formatdomain.html#elementsMetadata
diff --git a/specs/juno/implemented/libvirt-lxc-user-namespaces.rst b/specs/juno/implemented/libvirt-lxc-user-namespaces.rst
new file mode 100644
index 0000000..b671209
--- /dev/null
+++ b/specs/juno/implemented/libvirt-lxc-user-namespaces.rst
@@ -0,0 +1,184 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================================
+Libvirt-lxc User Namespace Support
+==================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-lxc-user-namespaces
+
+User namespaces provide a way for a process running in a container to appear to
+be running as root, but are in fact running as a different user on the host.
+The objective of this feature is to allow deployers to enable and configure
+which users and groups are mapped between container and host.
+
+Problem description
+===================
+
+It is a security risk to allow user processes to run as root on container
+hosts. In order to mitigate this risk, it is a good idea to run processes in
+those containers as non-root users. The problem with this is some processes
+may like to run (or at least appear to run as root inside the container).
+For example, running an init system as the init process of the container.
+
+Further, to boot an image in a user namespaced environment, the contents of
+it's filesystem must be owned by the target user for root on the host.
+
+Proposed change
+===============
+
+User namespaces allow processes inside a container to appear to be run as root,
+but are in fact running as another user. Libvirt exposes this feature through
+idmaps. This change would introduce a set of elements on the instance's domain
+xml to indicate which user and group ids should map between container and host.
+
+To address the owning of the filesystem by the targeted root user, the image
+will be chowned by Nova at boot time.
+
+Config for this feature will be disabled by default. It will be up to the
+deployer to enable and configure it.
+
+New config options in libvirt group:
+
+* uid_maps: comma separated list of mappings, maximum of 5
+
+* gid_maps: comma separated list of mappings, maximum of 5
+
+Format for mappings is "guest-id:host-id:count,guest-id:host-id_count,..."
+
+Alternatives
+------------
+
+Alternative image chown points, with performance impact:
+
+* Chown by image creator: No performance impact
+
+  * Rejected as the end user shouldn't have to worry about it
+
+* Chown by Glance on import: Image will take longer to become active
+
+  * Not ideal is it introduces a dependency on import being configured properly
+    in glance.
+
+
+* Chown by Nova when cached: Initial boot on all hosts will take longer
+
+  * Rejected initially as there are too many changes going on around image
+    caching. Once activity around iamge caching slows down, this may be the
+    ideal option.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+This change will improve the security of containers in Nova significantly.
+Before this change, processes running in containers built by Nova will be run
+as the host's root user. After this change, a deployer can restrict which
+user(s) processes will be run as.
+
+It should be noted that this change is not meant to provide isolation between
+guests, but instead isolation between host and guest. It is out of the scope
+of this change, but is reasonable to assume that if a mechanism was created
+to ensure that containers all used different UID/GIDs, user namespacing could
+be used to provide further guest-guest separation. This change provides a base
+that could be extended in the future for that use case.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Images need to be deliberately created to be run in a user namespaced
+environment. The contents of an image's filesystem need to be owned by the
+target uid/gid. In this iteration of this feature, Nova will chown the
+image on boot.
+
+Performance Impact
+------------------
+
+Due to the chowning of the image's filesystem on boot by Nova, there will
+be a performance hit on boot depending on how many files are on the image's
+filesystem.
+
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  andrew-melton
+
+Other contributors:
+  rconradharris
+  thomas-maddox
+
+Work Items
+----------
+
+* Modify libvirt config.py to include new idmap xml
+
+* Create util function to chown rootfs
+
+* Actual setup of new instance
+
+Dependencies
+============
+
+* Linux 3.8+ kernel
+
+  * Early 3.8 kernels may be buggy. If user needs minimum kernel, user
+    should use latest 3.8 kernel possible.
+
+
+* Libvirt 1.1.1
+
+Testing
+=======
+
+Making sure that the nova config options are properly mapped to libvirt domain
+objects can easily be handled by unit testing. Functional testing for this will
+not be possible until libvirt-lxc is included in the CI environment. Depending
+on how chowning is implemented, functional testing could be a bit tricky.
+
+Documentation Impact
+====================
+
+New config options.
+
+References
+==========
+
+* http://libvirt.org/formatdomain.html#elementsOSContainer
+
+* http://libvirt.org/drvlxc.html#secureusers
+
+* https://lwn.net/Articles/532593/
diff --git a/specs/juno/implemented/libvirt-volume-snap-network-disk.rst b/specs/juno/implemented/libvirt-volume-snap-network-disk.rst
new file mode 100644
index 0000000..1ae8790
--- /dev/null
+++ b/specs/juno/implemented/libvirt-volume-snap-network-disk.rst
@@ -0,0 +1,160 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Volume Snapshots for Network-Backed Disks
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/libvirt-volume-snap-network-disk
+
+Nova currently supports creating and deleting snapshots of file-backed
+Cinder volumes via libvirt's snapshot mechanism.  This work extends
+that capability to create and delete snapshots for network-backed disks
+in a similar fashion.
+
+This enables more complete Cinder volume functionality for deployments using
+qemu network-backed volumes through a mechanism like libgfapi.
+
+
+Problem description
+===================
+
+Nova does not support creating a snapshot via libvirt for a network-backed
+Cinder volume that is attached to an instance.  Currently, attempting to
+snapshot a Cinder volume configured this way will result in a failed snapshot
+operation.
+
+This is important for deployers who use qemu network-backed storage for Cinder
+volumes.  (Typically for performance reasons.)
+
+
+Proposed change
+===============
+
+Nova needs to be able to construct a <domainsnapshot> XML entity with
+the required fields to snapshot a network-backed disk via libvirt.
+
+Nova similarly needs to be able to pass in arguments for libvirt's
+blockCommit and blockRebase operations to delete snapshots for network-backed
+disks.  libvirt is adding support for a different style of parameters to the
+blockjob APIs to support this, which allows referencing an existing item in
+the disk snapshot change by index rather than by path name.
+
+Alternatives
+------------
+
+There is no alternative for deployers wishing to use Nova-assisted snapshots
+of Cinder-backed storage.  Nova must be able to interact with libvirt to
+enable this functionality.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+This work is used by the os-assisted-volume-snapshots extension APIs with no
+API-level changes.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+End-user impact is that Cinder volume snapshots now work when Nova
+is configured to use libgfapi for the GlusterFS Cinder driver.
+(qemu_allowed_storage_drivers=['gluster'])
+
+Performance Impact
+------------------
+
+Deleting (merging) a GlusterFS volume snapshot may be more efficient,
+particularly for simultaneous snapshot deletes for different volumes, as
+this work uses qemu direct storage access (via libgfapi) rather than a
+FUSE-mounted file system.
+
+No direct performance impact within Nova itself.
+
+Other deployer impact
+---------------------
+
+This change is relevant when using the Cinder GlusterFS driver and Nova
+is configured with qemu_allowed_storage_drivers=['gluster'].
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  eharney
+
+Work Items
+----------
+
+* Support for creating a volume snapshot of a network-backed disk
+* Support for deleting a volume snapshot of a network-backed disk
+  - Parse backing chain information from libvirt's domain XML
+  - Pass new-style arguments to blockCommit and blockRebase
+
+Dependencies
+============
+
+* This functionality depends on libvirt changes which are currently targeted
+  for libvirt 1.2.6.
+  - The libvirt capability is detected without using the libvirt version.
+
+* The libvirt changes also require fixes within qemu (targeting 2.1).
+
+* Currently only relevant for GlusterFS Cinder deployments.
+
+
+Testing
+=======
+
+This should be tested via Tempest volume snapshot test cases.  Since it is
+dependent on having a GlusterFS deployment this is not currently tested in
+the gate.
+
+When third-party CI is enabled for the GlusterFS driver within Cinder, it
+should cover this.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+* Required libvirt changes:
+  - https://www.redhat.com/archives/libvir-list/2014-June/msg00492.html
+
+* Required QEMU changes:
+  - https://lists.gnu.org/archive/html/qemu-devel/2014-06/msg04058.html
+
+* Based on work done in
+  - https://blueprints.launchpad.net/nova/+spec/qemu-assisted-snapshots
+
+* Patch series: https://review.openstack.org/#/c/78748/
diff --git a/specs/juno/implemented/move-prep-resize-to-conductor.rst b/specs/juno/implemented/move-prep-resize-to-conductor.rst
new file mode 100644
index 0000000..e488d67
--- /dev/null
+++ b/specs/juno/implemented/move-prep-resize-to-conductor.rst
@@ -0,0 +1,125 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================
+ Move prep_resize to Conductor
+===============================
+
+https://blueprints.launchpad.net/nova/+spec/move-prep-resize-to-conductor
+
+So as to prepare the scheduler to be a separate project, we need to remove
+all proxy calls from the scheduler to compute nodes.
+prep_resize() is still in Scheduler V3 API, so we need to modify how cold
+migrations are retried.
+
+Problem description
+===================
+
+When a cold migration is requested, there is a direct call from conductor to
+compute.prep_resize() which is OK. The problem is when the cold migration is
+failing, where compute node is asking Scheduler to reschedule a new migration
+by calling scheduler.prep_resize(), which itself calls compute.prep_resize()
+after issuing a select_destinations().
+
+Proposed change
+===============
+
+The idea is to replace the call back by conductor.migrate_server instead of
+scheduler.prep_resize in the compute prep_resize method.
+
+
+Alternatives
+------------
+
+All prep_resize logic should be left to the conductor, but that's a bigger step
+than just moving the scheduler logic to conductor. With regards to small
+iterations, that blueprint is quicker to implement and less risky, so that
+another blueprint for placing cold and hot migrations to conductor [1] could
+use it as dependency.
+
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  sylvain-bauza
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+- Replace call to scheduler.prep_resize by call to conductor.migrate_server
+  in compute.prep_resize
+- Remove prep_resize in Scheduler RPC API and note it to be removed in manager
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Covered by existing tempest tests and CIs.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+* [1] https://blueprints.launchpad.net/nova/+spec/cold-migrations-to-conductor-final
diff --git a/specs/juno/implemented/nfv-multiple-if-1-net.rst b/specs/juno/implemented/nfv-multiple-if-1-net.rst
new file mode 100644
index 0000000..e05ccca
--- /dev/null
+++ b/specs/juno/implemented/nfv-multiple-if-1-net.rst
@@ -0,0 +1,184 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================================================
+Support multiple interfaces from one VM attached to the same network
+====================================================================
+
+https://blueprints.launchpad.net/nova/+spec/multiple-if-1-net
+
+Permit VMs to attach multiple interfaces to one network to facilitate use
+of common NFV network function VMs that require this form of attachment.
+
+Problem description
+===================
+
+At present, Nova only permits a single VIF from each VM to be attached
+to a given Neutron network.  If you attempt to attach multiple VIFs to
+the same network, an error is issued, meaning that the second network
+is not found from the list of networks remaining after the first
+network is not used.
+
+NFV functions occasionally require multiple interfaces to be attached
+to a single network from the same VM, for reasons described below in
+the 'use cases' section.  When this is required, the VNF generally
+cannot be used under Openstack.
+
+VNFs are often large, complex pieces of code, and may be supplied by third
+parties.  For various reasons, it is not uncommon that it is necessary to
+feed traffic out of an interface and into another interface (when the VNF
+implements multiple functions and the functions cannot be chained internally)
+or to feed traffic from e.g. the internet into multiple interfaces to run
+them through separate processing functions internally.
+
+The limitation can be seen as one of the VNF.  Clearly, the VNF could be
+changed to put multiple addresses or functions on a single port (to fix the
+incoming traffic issue) or to connect functions internally (to fix the
+passthrough problem.
+
+The problem with this solution is that the timescale for getting such a fix
+is often prohibitive.  VNFs are large, complex pieces of code, and often the
+supplier of the VNF is not the same organisation as that trying to use
+the VNF within Openstack, necessitating a feature change request which may
+well not be possible within reasonable timescales.
+
+We propose changing the code within Nova to remove this limitation.
+
+Proposed change
+===============
+
+We propose removing the limitation, which exists in Nova (Neutron has no such
+limitation), allowing any number of VIFs to be attached to the same network.
+
+The ordering in the nova 'boot' command or POST should be respected, so if
+multiple interfaces are in use on the same network they are attached to the VM
+in the order in which they are provided, as with other NICs.
+
+API changes
+-----------
+
+When the attempt is made to attach multiple interfaces to a single
+network, Nova will, instead of returning the error, attach multiple
+interfaces to the same network and return a normal success code to the
+'nova boot' attempt.
+
+('nova interface-attach' already permits a second attachment to the same
+network and needs no change.)
+
+Alternatives
+------------
+
+It may be possible to work around this limitation by using multiple
+ports on the same network and attach the VM to the ports, rather than
+the same network twice.  This has not been tested.  On the other hand,
+this indicates that the limitation is highly artificial and should, in
+any case, be removed.  (In any case we should confirm this is possible
+after the change and fix it if not.)
+
+It is possible to boot the VM and use 'nova interface-attach' to get
+multiple interfaces on the same network, but this requires the VM to support
+PCI hotplug.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+When the attempt is made to attach multiple interfaces to a single
+network, Nova will, instead of returning the error, attach all
+interfaces to the same network and return a normal success code to the
+'nova boot' attempt.
+
+Security impact
+---------------
+
+It is now going to be possible to bridge multiple interfaces together
+within a VM and cause a broadcast storm.  It was always possible to
+flood a Neutron network from a VM; this makes it easier.  It doesn't
+make a security issue in and of itself but it certainly does make it a
+little more straightforward to trigger one that arguably already
+exists.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.  An end user using API calls that currently succeed will see no change
+in behaviour in those APIs.  This only changes a case where an API currently
+fails.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  ijw-ubuntu
+
+Work Items
+----------
+
+* Change the Nova code, per the existing abandoned patch in
+  https://review.openstack.org/#/c/26370 - which requires porting
+  forward from the code in question to the current trunk.
+
+* Add unit tests, which are missing from the abandoned patch.
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+Independently of this spec, tests should be added to Tempest:
+
+* minimally, to ensure that traffic can be passed between the two
+  interfaces on a VM created in this fashion
+
+* optionally, traffic flow should be tested from another VM or
+  external packet supplier to either of the interfaces.
+
+Testing should be conducted with both the nova boot and nova
+interface-attach methods.
+
+Documentation Impact
+====================
+
+The change should be documented. No documentation exists for the
+current behaviour.  Documentation exists for nova-network multinic
+saying that VIFs are attached to separate networks but this is specific
+to nova-network.
+
+References
+==========
+
+* https://review.openstack.org/#/c/26370
+* https://bugs.launchpad.net/nova/+bug/1166110
diff --git a/specs/juno/implemented/object-subclassing.rst b/specs/juno/implemented/object-subclassing.rst
new file mode 100644
index 0000000..559380b
--- /dev/null
+++ b/specs/juno/implemented/object-subclassing.rst
@@ -0,0 +1,146 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Support subclassing objects
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/object-subclassing
+
+Implement support for subclassing objects properly. If some hook, extension,
+or alternative DB API backend subclasses one of the base objects, the new
+object should be registered and all code should end up using this new class.
+
+Problem description
+===================
+
+Subclassing objects may be necessary to implement alternative DB API backends.
+There are probably some other use cases where it may be necessary to override
+some default object behavior. There was a rough plan to support subclassing
+objects in trunk. However, it wasn't fully thought through before we started
+landing all of the current object code. All objects do get registered right
+now, however there is no checking of the versions the objects advertise.
+Additionally, all code directly references the base object classes under
+nova/objects right now.
+
+Proposed change
+===============
+
+As objects are registered, check the version to see if it already exists. If
+so, replace the original in the tracked object list with the new one. As
+objects are registered, set an attribute on the nova.objects module to point
+to the newest class for latest version of the object. Replace all code that
+directly references object classes in modules under nova/objects with code
+that uses the nova.objects attribute. This has a side-effect of cleaning up
+imports. Instead of importing a ton of nova.object modules, only nova.objects
+will be imported.
+
+NOTE: This spec does not cover adding a hook/entrypoint for allowing
+alternative object implementations. That will be proposed at some point as
+a separate spec. At the moment, someone could specify an alternative db_backend
+and register alternative object implementations that way, but I don't see that
+as being the correct way to do that in the longer term.
+
+Alternatives
+------------
+
+There are probably some alternatives to setting attributes on the nova.objects
+module, like creating a method that returns the newest object and calling to
+that method everywhere. That would result in slightly lower performance. I
+suppose another solution to avoid having to change code everywhere is to
+rename all object classes to Base<Object> and then somehow setattr the latest
+version to be <Object> on the current modules. But, I rather like how using
+objects.<Object> everywhere will look.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+The changes will touch a lot of files. Anywhere there is a reference to
+something like instance_obj.Instance, it will change to objects.Instance.
+There's high chance of conflicts to resolve in either the object-subclassing
+patches or in other patches up for review.
+
+New patchsets should never import the module defining an object to reference
+the object class in it, directly. One should always import nova.objects and use
+nova.objects.<Object>.
+
+Objects register themselves when the module that defines them is imported.
+With this change, since there's likely no need to import object modules in
+code that uses them (you'll import nova.objects, instead), you must make sure
+that object modules are imported within nova/objects/__init__.py's
+register_all() method.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  cbehrens
+
+Work Items
+----------
+
+* Fix object registration to track object classes properly and set attributes
+  on the nova.objects module
+* Switch code to use the nova.objects module. This will be broken up into
+  areas of nova like nova/api and nova/compute, etc.
+
+Dependencies
+============
+
+None.
+
+Testing
+=======
+
+Tests will be modified to use nova.objects as well.
+
+Documentation Impact
+====================
+
+None.
+
+References
+==========
+
+None.
diff --git a/specs/juno/implemented/on-demand-compute-update.rst b/specs/juno/implemented/on-demand-compute-update.rst
new file mode 100644
index 0000000..1d40519
--- /dev/null
+++ b/specs/juno/implemented/on-demand-compute-update.rst
@@ -0,0 +1,184 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================================
+Change compute updates from periodic to on demand
+=================================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/on-demand-compute-update
+
+Currently, all compute nodes update status info in the DB on a periodic
+basis (the period is currently 60 seconds). Given that the status of
+the node only changes at specific points (mainly image
+creation/destruction) this leads to significant DB overhead on a large
+system. This BP changes the update mechanism to only update the DB when
+a node state changes, specifically at node startup, instance creation
+and instance destruction.
+
+Problem description
+===================
+
+The status information about compute nodes is updated into the DB on
+a periodic basis.  This means that every compute node in the system
+updates a row in the DB once every 60 seconds (the default period for
+this update).  This is unnecessary and a scalability problem given that
+the status info is mostly static and doesn't change very often, mainly
+it changes when an instance is created or destroyed.
+
+Proposed change
+===============
+
+Compute node only sends an update if its status changes.  On a periodic
+interval (using the current default period of 60 seconds) the compute
+node will compare its status with the status saved from the last update.
+Only if the state or claims have changed will the DB be updated.
+
+The advantage to this method is that it should significantly cut down
+on the number of DB updates while changing almost nothing about the way
+the system currently works, compute node changes will still take 60
+seconds before they are updated but any change, for any reason, will
+ultimately be reported.
+
+One potential issue with this is a possible sensitivity concern, updates
+shouldn't be constantly sent if, for example, steady state system activity
+causes something like RAM usage to change slightly.  Some experiments will
+have to be run to decide if adding in a sensitivity control for certain
+status metrics is needed.  This can be a follow on optimization, if it's
+necessary, since this design is no worse than the current mechanism.
+
+Alternatives
+------------
+
+Another idea would be to only update the DB at certain well defined events.
+The update code would only be called at system start up, instance creation
+and instance deletion.  This would reduce the latency for status updates
+(the DB is modified as soon as the system state changes) but it suffers
+from some disadvantages:
+
+1)  Finding all of the appropriate events to record a status change.  Are
+statup/creation/destruction the only places where the system state changes,
+maybe there are other events that should be tracked.
+
+2)  Future changes could add new events that change status and recognizing
+that an update is needed is an easy mistake to miss.
+
+3)  Status could change unknown to the nova code, imagine something like a
+hot plug add of memory.
+
+Data model impact
+-----------------
+
+There is no change to the data that is being stored in the DB, all of the
+current fields are stored exactly as before, this BP is just changing the
+frequency at which those fields are updated.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+As measured by DB updates this change will clearly cause no more DB updates
+than the current technique and, assuming instances are created on a node
+at a rate of less then one every 60 seconds, should cause much fewer DB
+updates.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Don Dugger <donald.d.dugger@intel.com>
+
+Other contributors:
+
+Work Items
+----------
+
+Change should be fairly simple, change the current update code to only
+update the DB if a 'state_modified' routine returns true.  The
+'state_modified' routine returns false if the current state matches the
+last recorded state.  Otherwise, it saves the current state as the last
+recorded state and returns true.
+
+The 'state_modified' routine maintains an in memory copy of the current
+status of the compute node.  This copy of the state is compared with the
+current state and the update to the DB only happens if the copy and the
+current state differ.  Note that the in memory copy is initialized to
+zero values on node startup so that the first periodic update call will
+find a miss match between the two states and the DB will be updated.
+
+Based upon experiments it has been determined that a simple comparison
+of the entire current vs. the saved state is sufficient.  If, in the
+future, more rapidly changing data that shouldn't be stored in the DB
+is added to the compute node state then 'state_modified' can be easily
+changed to ignore such data that isn't needed in the DB.
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+A unit test will be created to make sure that the DB is updated when the
+compute node status changes and is not updated when the status doesn't
+change.
+
+
+Documentation Impact
+====================
+
+Section 5 of the Associate Training Guide
+
+http://docs.openstack.org/training-guides/content/associate-computer-node.html
+
+is slightly incorrect and should be fixed.  It currently says "All compute
+nodes (also known as hosts in terms of OpenStack) periodically publish their
+status, resources available and hardware capabilities to nova-scheduler
+through the queue."  This should be modified to reflect that the status is
+updated in the DB which is then queried by the scheduler.  (Note this is a
+generic fix that is really unrelated to this blueprint.)
+
+
+References
+==========
+
+None.
diff --git a/specs/juno/implemented/pci-passthrough-sriov.rst b/specs/juno/implemented/pci-passthrough-sriov.rst
new file mode 100644
index 0000000..46dba41
--- /dev/null
+++ b/specs/juno/implemented/pci-passthrough-sriov.rst
@@ -0,0 +1,414 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+PCI SR-IOV passthrough to nova instance
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
+
+Enable nova instance to be booted up with SR-IOV neutron ports.
+
+Problem description
+===================
+Right now it is possible to boot VM with general purpose PCI device passthrough
+by means of libvirt's managed hostdev device definition in the domain XML. A
+guide to use it can be found in [GPP_WIKI]_. However, it's not possible to
+request access to virtual network via SR-IOV NICs. Nova enhancments are
+required to support SR-IOV networking with Neutron.
+
+Traditionally, a neutron port is a virtual port that is either attached to a
+linux bridge or an openvswitch bridge on a compute node. With the introduction
+of SR-IOV based NIC (called vNIC), the virtual bridge is no longer required.
+Each SR-IOV port is associated with a virtual function (VF) that logically
+resides on a vNIC.  There exists two variants for SR-IOV networking. SR-IOV
+ports may be provided by Hardware-based Virtual Eithernet Bridging (HW VEB); or
+they may be extended to an upstream physical switch (IEEE 802.1br). In the
+latter case, port's configuration is enforced in the switch.  There are also
+two variants in connecting a SR-IOV port to its corresponding VF. A SR-IOV port
+may be directly connected to its VF. Or it may be connected with a macvtap
+device that resides on the host, which is then connected to the corresponding
+VF. Using a macvtap device makes live migration with SR-IOV possible.
+
+In the Icehouse release, a couple of blueprints from neutron side were approved
+and their associated patches were committed that enable the interactions
+between nova and neutron for SR-IOV networking. Refer to [VIF_DETA]_ and
+[BIND_PRF]_ for details about them.
+
+Another blueprint [VNIC_TYP]_ added the support in the neutron port API to
+allow users to specify vnic-type when creating a neutron port. The currently
+supported vnic-types are:
+
+* normal: a traditional virtual port that is either attached to a linux bridge
+  or an openvswitch bridge on a compute node.
+* direct: an SR-IOV port that is directly attached to a VM
+* macvtap: an SR-IOV port that is attached to a VM via a macvtap device.
+
+This specification attempts to build up on top of the above-mentioned neutron
+changes and address the following functionalities in Nova so that SR-IOV
+networking in openstack is fully functional end-to-end:
+
+1. Generating libvirt domain XML and network XML that enables SR-IOV for
+   networking.
+2. Scheduling based on SR-IOV port's network connectivity.
+
+The initial use case that is targeted in this specification and therefore for
+Juno is to boot a VM with one or more vNICs that may use different vnic-types.
+Particularly a user would do the following to boot a VM with SR-IOV vnics:
+
+* create one or more neutron ports. For example:
+
+::
+
+  neutron port-create <net-id> --binding:vnic-type direct
+
+* boot a VM with one or more neutron ports. For example:
+
+::
+
+  nova boot --flavor m1.large --image <image>
+            --nic port-id=<port1> --nic port-id=<port2>
+
+Note that in the nova boot API, users can specify either a port-id or a net-id.
+If it's the latter case, it's assumed that the user is requesting a normal
+virtual port (which is not a SR-IOV port).
+
+This specification will make use of the existing PCI passthrough
+implementation, and make a few enhancements to enable the above use cases.
+Therefore, the existing PCI passthrough support as documented by [GPP_WIKI]_
+works as it is for general-purpose PCI passthrough.
+
+Proposed change
+===============
+
+To schedule an instance with SR-IOV ports based on their network connectivity,
+the neutron ports' associated physical networks have to be used in making the
+scheduling decision. A VF has to be selected for each of the neutron port.
+Therefore, the VF's associated physical network has to be known to the system,
+and the selected VF's associated physical network has to match that from the
+neutron port. To make the above happen, this specification proposes associating
+an extra tag called *physical_network* to each networking VF. In addition, nova
+currently has no knowledge of a neutron port's associated physical network.
+Therefore, nova needs to make extra calls to neutron in order to retrieve this
+information from neutron. In the following, detailed changes in nova will be
+described on how to achieve that.
+
+Note that this specification only supports libvirt driver.
+
+PCI Whitelist
+-------------
+
+This specification introduces a few enhancements to the existing PCI whitelist:
+
+* allows aggregated declaration of PCI devices by using '*' and '.'
+* allows tags to be associated with PCI devices.
+
+Note that it's compatible with the previous PCI whitelist definition. And
+therefore, the existing functionalities associated with the PCI whitelist work
+as is.
+
+with '[' to indicate 0 or one time occurrence, '{' 0 or multiple occurrences,
+'|' mutually exclusive choice, a whitelist entry is defined as:
+
+::
+
+      ["device_id": "<id>",] ["product_id": "<id>",]
+      ["address": "[[[[<domain>]:]<bus>]:][<slot>][.[<function>]]" |
+       "devname": "PCI Device Name",]
+      {"tag":"<tag_value>",}
+
+*<id>* can be a '*' or a valid *device/product id* as displayed by the linux
+utility lspci. The *address* uses the same syntax as it's in lspci. Refer to
+lspci's manual for its description about the '-s' switch. The *devname* can be
+a valid PCI device name. The only device names that are supported in this
+specification are those that are displayed by the linux utility *ifconfig -a*
+and correspond to either a PF or a VF on a vNIC. There may be 0 or more tags
+associated with an entry.
+
+If the device defined by the *address* or *devname* corresponds to a SR-IOV PF,
+all the VFs under the PF will match the entry.
+
+For SR-IOV networking, a pre-defined tag "physical_network" is used to define
+the physical network that the devices are attached to. A whitelist entry is
+defined as:
+
+::
+
+      ["device_id": "<id>",] ["product_id": "<id>",]
+      ["address": "[[[[<domain>]:]<bus>]:][<slot>][.[<function>]]" |
+       "devname": "Ethernet Interface Name",]
+      "physical_network":"name string of the physical network"
+
+Multiple whitelist entries per host are supported as they already are. The
+fields *device_id*, *product_id*, and *address* or *devname* will be matched
+against PCI devices that are returned as a result of querying libvirt.
+
+Whitelist entries are defined in nova.conf in the format:
+
+::
+
+    pci_passthrough_whitelist = {<entry>}
+
+{<entry>} is a json dictionary and is defined as in above.
+*pci_passthrough_whitelist* is a plural configuration, and therefore can appear
+multiple times in nova.conf.
+
+Some examples are:
+
+::
+
+    pci_passthrough_whitelist = {"devname":"eth0",
+                                 "physical_network":"physnet"}
+
+    pci_passthrough_whitelist = {"address":"*:0a:00.*",
+                                 "physical_network":"physnet1"}
+
+    pci_passthrough_whitelist = {"address":":0a:00.",
+                                 "physical_network":"physnet1"}
+
+    pci_passthrough_whitelist = {"vendor_id":"1137","product_id":"0071"}
+
+    pci_passthrough_whitelist = {"vendor_id":"1137","product_id":"0071",
+                                 "address": "0000:0a:00.1",
+                                 "physical_network":"physnet1"}
+
+PCI stats
+---------
+
+On the compute node, PCI devices are matched against the PCI whitelist entries
+in the order as they are defined in the nova.conf file. Once a match is found,
+the device is placed in the corresponding PCI stats entry.
+
+If a device matches a PCI whitelist entry, and if the PCI whitelist entry is
+tagged, the tags together with *product_id* and *vendor_id* will be used as
+stats keys; otherwise, the existing predefined keys will be used.
+
+A PCI whitelist entry for SR-IOV networking will be tagged with a physical
+network name. Therefore, the physical network name is used as the stats key for
+SR-IOV networking devices. Conceptually speaking for SR-IOV networking, a PCI
+stats entry keeps track of the number of SR-IOV ports that are attached to a
+physical network on a compute node. And for scheduling purpose, it can be
+considered as a tuple of
+
+::
+
+    <host_name> <physical_network_name> <count>
+
+When a port is requested from a physical network, the compute nodes that host
+the physical network can be found from the stats entries. The existing PCI
+passthrough filter in nova scheduler works without requiring any change in
+support of SR-IOV networking.
+
+There is no change in how the stats entries are updated and persisted into the
+compute_nodes database table with the use of nova resource tracker.  Currently,
+a collumn called *pci_stats* in the compute_nodes database table is used to
+store the PCI stats as a JSON document. The PCI stats JSON document is
+basically a list of stats entries in the format of *<key1> <key2> ....<keyn>* :
+*<count>*. This will not be changed for SR-IOV networking. Specifically for
+SR-IOV networking, however, PCI stats records are keyed off with the tag
+*physical_network_name*, plus *product_id* and *vendor_id*. a stats entry for
+SR-IOV networking will look like:
+
+::
+
+   <physical_network_name>, <product_id>, <vendor_id> : <count>.
+
+requested_networks (NICs)
+-------------------------
+
+Currently, each requested network is a tuple of
+
+::
+
+    <neutron-net-id> <v4-fixed-ip> <neutron-port-id>
+
+Either neutron-net-id or neutron-port-id must have a valid value, and
+v4-fixed-ip can be None. For each --nic option specified in the *nova boot*
+command, a requested_network tuple is created. All the requested_network tuples
+are passed to the compute node, and the compute service running on the node
+uses the information to request neutron services. This specification proposes
+one additional field in the tuple: *pci-request-id*.
+
+Corresponding to each requested_network tuple, there is a neutron port with a
+valid vnic-type. If the vnic-type is direct or macvtap, a valid
+*pci_request_id* must be populated into the tuple (see below for details). The
+*pci-request-id* is later used to locate the PCI device from PCI manager that
+is allocated for the requested_network tuple (therefore the NIC).
+
+PCI Requests
+------------
+
+Currently, pci_requests as key and a JSON doc string as associated value are
+stored in the instance's system metadata. In addition, all the PCI devices
+allocated for PCI passthrough are treated the same in terms of generating
+libvirt xml. However, for SR-IOV networking, special libvirt xml is required.
+Further, we need a way to correlate the allocated device with the requested
+network (NIC) later on during the instance boot process. In this specification,
+we propose the use of *pci_request_id* for that purpose.
+
+Each PCI request is associated with a *pci_request_id* that is generated while
+creating/saving the PCI request to the instance's system metadata. The
+*pci_request_id* is used on the compute node to retrieve the allocated PCI
+device. Particularly for SR-IOV networking, a PCI request is expressed as
+
+::
+
+   "physical_network" : <name>
+   "count" : 1
+   "pci_request_id" : <request-uuid>
+
+For each --nic specified in the 'nova boot', nova-api creates a requested
+network tuple. For a SR-IOV NIC, it creates a PCI request and as a
+result a *pci_request_id* is generated and saved in the PCI request spec. The
+same *pci_request_id* is also saved in the requested_network (Refer to the last
+section).
+
+nova neutronv2 and VIF
+-------------------------------------
+
+Note that Nova network will not be enhanced to support SR-IOV. However, Nova
+modules that are responsible for interacting with neutron need to be enhanced.
+
+Refer to [BIND_PRF]_, [VIF_DETA]_, [VNIC_TYP]_ that has added the
+functionalities required to support SR-IOV ports in neutron. Accordingly, nova
+neutronv2 will be enhanced to work with them in support of SR-IOV ports.
+Particularly:
+
+* When nova processes the --nic options, physical network names will be
+  retrieved from neutron. This needs to be done by using neutron provider
+  extension with admin access. As a result, additional neutron calls will be
+  made to retrieve the physical network name.
+* When nova updates neutron ports, binding:profile needs to be populated with
+  pci information that includes pci_vendor_info, pci_slot, physical_network.
+* After nova successfully updates the neutron ports, it retrieves the ports'
+  information from neutron that are used to populate VIF objects. New
+  properties will be added in the VIF class in support of binding:profile,
+  binding:vif_details and binding:vnic_type.
+
+nova VIF driver
+---------------
+
+Each neutron port is associated with a vif-type. The following VIF types are
+related to SR-IOV support:
+
+* VIF_TYPE_802_QBH: corresponds to IEEE 802.1BR (used to be IEEE 802.1Qbh)
+* VIF_TYPE_HW_VEB: for vNIC adapters that supports virtual embedded bridging
+* VIF_TYPE_802_QBG: corresponds to IEEE 802.1QBG. However, this existing vif
+  type may not be useful now because the libvirt parameters for 1QBG
+  (managerid, typeidversion and instanceid) are not supported by known neutron
+  plugins that support SR-IOV.
+
+The nova generic libvirt VIF driver will be enhanced to support the first two
+VIF types. This includes populating the VIF config objects and generating the
+interface XMLs.
+
+Alternatives
+------------
+
+N/A
+
+Data model impact
+-----------------
+
+Currently, a nova object *PciDevice* is created for each PCI passthrough
+device. The database table *pci_devices* is used to persist the *PciDevice*
+nova objects. A new field *request_id* will be added in the *PciDevice* nova
+object. Correspondingly, a new column *request_id* is added in the database
+table *pci_devices*. Database migration script will be incorporated
+accordingly.
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+The physical network to which a port is connected needs to be retrieved from
+neutron, which requires additional calls to neutron. Particularly, nova will
+call neutron *show_port* to check the port's *vnic_type*. If the *vnic_type* is
+either *direct* or *macvtap*, it will call neutron *show_network* to retrieve
+the associated physical network. As a consequence, the number of calls to
+neutron will be slightly increased when *port-id* is specified in the --nic
+option in nova boot.
+
+Other deployer impact
+---------------------
+
+No known deployer impact other than configuring the PCI whitelist for SR-IOV
+networking devices.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  baoli
+
+Other contributors:
+  TBD
+
+Work Items
+----------
+
+* PCI whitelist
+* PCI request
+* PCI stats
+* DB change and the required migration script, PCI device object change
+* neutronv2
+* VIF
+* libvirt generic VIF driver and instance configuration
+* nova compute api retrieving physical network, change of requested_networks
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Both unit and tempest tests need to be created to ensure proper functioning of
+SR-IOV networking. For tempest testing, given the nature of SR-IOV depending on
+hardware, it may require vendor support and use of proper neutron ML2 mechanism
+drivers. Cisco Neutron CI and Mellanox External Testing need to be enhanced in
+support of SR-IOV tempest testing.
+
+Documentation Impact
+====================
+
+* document new whitelist configuration changes
+* a user guide/wiki on how to use SR-IOV networking in openstack
+
+References
+==========
+.. [GPP_WIKI] `Generic PCI Passthrough WIKI <https://wiki.openstack.org/wiki/Pci_passthrough>`_
+.. [VIF_DETA] `Extensible port attribute for plugin to provide details to VIF driver  <https://blueprints.launchpad.net/neutron/+spec/vif-details>`_
+.. [BIND_PRF] `Implement the binding:profile port attribute in ML2 <https://blueprints.launchpad.net/neutron/+spec/ml2-binding-profile>`_
+.. [VNIC_TYP] `Add support for vnic type request to be managed by ML3 mechanism drivers <https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type>`_
diff --git a/specs/juno/implemented/per-aggregate-filters.rst b/specs/juno/implemented/per-aggregate-filters.rst
new file mode 100644
index 0000000..2fa1051
--- /dev/null
+++ b/specs/juno/implemented/per-aggregate-filters.rst
@@ -0,0 +1,145 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================================
+Scheduler: Adds per-aggregate filters
+=====================================
+
+https://blueprints.launchpad.net/nova/+spec/per-aggregate-disk-allocation-ratio
+https://blueprints.launchpad.net/nova/+spec/per-aggregate-max-instances-per-host
+https://blueprints.launchpad.net/nova/+spec/per-aggregate-max-io-ops-per-host
+
+The aim of this bp is to add the ability to the filters DiskFilter,
+NumInstancesFilter and IoOpsFilter to set our options by aggregates.
+
+Problem description
+===================
+
+Operator wants to define different filtering options (disk_allocation_ratio,
+max_instances_per_host, max_io_ops_per_host) for a subset of compute hosts.
+
+Proposed change
+===============
+
+Create new filters that extend DiskFilter, NumInstancesFilter and
+IoOpsAggregateFilter to provide the ability to read the metadata from
+aggregates. If no valid values found fall back to the global default
+configurations set in nova.conf.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Not related with this bp but a performance impact has been reported
+to the bug 1300775_
+about using aggregate with the scheduler on large
+cluster.
+
+.. _1300775: https://bugs.launchpad.net/nova/+bug/1300775
+
+Other deployer impact
+---------------------
+
+The operator needs to update the scheduler's nova.conf to activate filters,
+also he has to set metadata of the aggregates with the configurations options
+disk_allocation_ratio, max_instances_per_host, max_io_ops_per_host.
+
+::
+
+  $ # This one provides for hosts in the aggregate 'agr1' the possibility to
+  $ # host 60 instances.
+  $ nova aggregate-set-metadata agr1 set metadata max_instances_per_host=60
+
+  $ # This one provides for hosts in the aggregate 'agr2' the possibility to
+  $ # oversubscribe disk allocation and configures the scheduler to ignore
+  $ # hosts that have currently more than 3 heavy operations.
+  $ nova aggregate-set-metadata agr2 set\
+  $   metadata max_io_ops_per_host=3
+  $   disk_allocation_ratio=3
+
+The operator also needs to reload the scheduler service to activate this new
+filter.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  sahid-ferdjaoui
+
+Other contributors:
+  <None>
+
+Work Items
+----------
+
+ * New filter AggregateNumInstancesFilter
+ * New filter AggregateDiskFilter
+ * New filter AggregateIoOpsFilter
+
+Dependencies
+============
+
+ * A bug has been open to factory per-aggregate logic.
+   https://bugs.launchpad.net/nova/+bug/1301340
+
+Testing
+=======
+
+We need to add unit tests in test_host_filters.py also we probably need to
+think about adding functional tests in tempest.
+
+Documentation Impact
+====================
+
+We need to refer these new filters in the documentation, also
+'doc/source/devref/filter_scheduler.rst' needs to be updated.
+
+References
+==========
+
+These blueprints was accepted for icehouse but because of a work started to add
+helper that provides utility methods to get metadata from aggregates and so
+remove duplicate code between filters (bug 1301340_). The blueprints was
+deferred.
+
+.. _1301340: https://bugs.launchpad.net/nova/+bug/1301340
diff --git a/specs/juno/implemented/rbd-clone-image-handler.rst b/specs/juno/implemented/rbd-clone-image-handler.rst
new file mode 100644
index 0000000..66c7186
--- /dev/null
+++ b/specs/juno/implemented/rbd-clone-image-handler.rst
@@ -0,0 +1,222 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================================
+Storage: Copy-on-write cloning for RBD-backed disks
+===================================================
+
+https://blueprints.launchpad.net/nova/+spec/rbd-clone-image-handler
+
+Currently RBD-backed ephemeral disks are created by downloading an image from
+Glance to a local file, then uploading that file into RBD. Even if the file is
+cached, uploading may take a long time, since 'rbd import' is synchronous and
+slow. If the image is already stored in RBD by Glance, there's no need for any
+local copies - it can be cloned to a new image for a new disk without copying
+the data at all.
+
+
+Problem description
+===================
+
+The primary use case that benefits from this change is launching an instance
+from a Glance image where Ceph RBD backend is enabled for both Glance and Nova,
+and Glance images are stored in RBD in RAW format.
+
+Following problems are addressed:
+
+* Disk space on compute nodes is wasted by caching an additional copy of the
+  image on each compute node that runs instances from that image.
+
+* Disk space in Ceph is wasted by uploading a full copy of an image instead of
+  creating a copy-on-write clone.
+
+* Network capacity is wasted by downloading the image from RBD to a compute
+  node the first time that node launches an instance from that image, and by
+  uploading the image to RBD every time a new instance is launched from the
+  same image.
+
+* Increased time required to launch an instance reduces elasticity of the cloud
+  environment and increases the number of in-flight operations that have to be
+  maintained by Nova.
+
+
+Proposed change
+===============
+
+Extract RBD specific utility code into a new file, align its structure and
+provided functionality in line with similar code in Cinder. This includes the
+volume cleanup code that should be converted from rbd CLI to using the RBD
+library.
+
+Add utility functions to support cloning, including checks whether image exists
+and whether it can be cloned.
+
+Add direct_fetch() method to nova.virt.libvirt.imagebackend, make its
+implementation in the Rbd subclass try to clone the image when possible.
+Following criteria are used to determine that the image can be cloned:
+
+* Image location uses the rbd:// schema and contains a valid reference to an
+  RBD snapshot;
+
+* Image location references the same Ceph cluster as Nova configuration;
+
+* Image disk format is 'raw';
+
+* RBD snapshot referenced by image location is accessible by Nova.
+
+Extend fetch_to_raw() in nova.virt.images to try direct_fetch() when a new
+optional backend parameter is passed. Make the libvirt driver pass the backend
+parameter.
+
+Instead of calling disk.get_disk_size() directly from verify_base_size(), which
+assumes the disk is stored locally, add a new method that is overridden by the
+Rbd subclass to get the disk size.
+
+Alternatives
+------------
+
+An alternative implementation based on the image-multiple-location blueprint
+(https://blueprints.launchpad.net/glance/+spec/multiple-image-locations) was
+tried in Icehouse. It was ultimately reverted, which can be attributed to a sum
+of multiple reasons:
+
+* The implementation in https://review.openstack.org/33409 took a long time to
+  stabilize, and didn't land until hours before Icehouse feature freeze.
+
+* The impact of https://review.openstack.org/33409 was significantly larger
+  than that of the ephemeral RBD clone change that was built on top of it.
+
+* The impact included exposing nova.image.glance._get_locations() method that
+  relies on Glance API v2 to code paths that assume Glance API v1, which caused
+  LP bug #1291014 (https://bugs.launchpad.net/nova/+bug/1291014).
+
+This design has a significantly smaller footprint, and is mostly isolated to
+the RBD image backend in the libvirt driver.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+When Ceph RBD backend is enabled for Glance and Nova, there will be a
+noticeable difference in time and resource consumption when launching instances
+from Glance images in RAW and non-RAW formats.
+
+Performance Impact
+------------------
+
+In the primary use case defined in the `Problem description`_ section above,
+there will be a significant performance improvement.
+
+In other use cases, libvirt driver will introduce one more API call to Glance
+to retrieve a list of image locations when RBD backend is enabled. The
+performance impact of that call is insignificant compared to the time and
+resources it takes to fetch a full image from Glance.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  jdurgin
+
+Other contributors:
+  angdraug
+
+Work Items
+----------
+
+Current implementation (see `References`_) consists of following changes:
+
+* Move libvirt RBD utilities to a new file
+
+* Use library instead of CLI to cleanup RBD volumes
+
+* Enable cloning for rbd-backed ephemeral disks
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+This is a non-functional change with no impact on the test cases that need to
+be covered.
+
+There is work currently going on to get all of tempest running against an
+environment using Ceph in the OpenStack CI environment.  The first step is ceph
+support for devstack, which you can see here:
+
+    https://review.openstack.org/#/c/65113
+
+There's also a test devstack patch with forces ceph to be enabled, which
+results in all of the devstack jobs being run with ceph enabled.  You can find
+that here:
+
+    https://review.openstack.org/#/c/107472/
+
+There are some tests failing (14 and 15 the first couple of runs).  However,
+that also means that the vast majority of tests that cover this code (anything
+that spawns an instance) are passing.  So, we at least have a way to run these
+tests on demand against master.  Once the devstack patch merges, we will enable
+a job that can run against patches in all projects (perhaps experimental to
+start with).
+
+Fuel CI also includes a suite of tests for OpenStack deployments with Ceph:
+https://github.com/stackforge/fuel-main/blob/master/fuelweb_test/tests/test_ceph.py
+
+
+Documentation Impact
+====================
+
+None.
+
+
+References
+==========
+
+Mailing list discussions:
+http://lists.openstack.org/pipermail/openstack-dev/2014-March/029127.html
+http://lists.ceph.com/pipermail/ceph-users-ceph.com/2014-March/008659.html
+
+Current implementation:
+https://github.com/angdraug/nova/tree/rbd-ephemeral-clone
+https://review.openstack.org/#/q/status:open+topic:bp/rbd-clone-image-handler,n,z
diff --git a/specs/juno/implemented/refactor-network-api.rst b/specs/juno/implemented/refactor-network-api.rst
new file mode 100644
index 0000000..04f824b
--- /dev/null
+++ b/specs/juno/implemented/refactor-network-api.rst
@@ -0,0 +1,109 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================
+Refactor network API
+======================
+
+https://blueprints.launchpad.net/nova/+spec/refactor-network-api
+
+To have a common API network base with all required methods so
+neutron / nova network api can inherit from.
+
+
+Problem description
+===================
+
+Right now network api's do not inherit from a common base, and if the
+functionality is not implemented developers may forget to add the
+method.
+The situation is that every time that functionality want to be accessed
+from the API an exception is thrown due to missing methods and not clear
+error is returned.
+
+Proposed change
+===============
+
+The idea is to create a network_base API that define all the possible
+methods and just throw NotImplementedError, so next time the user will
+see the proper error message.
+
+Also fields like sentinel object could be directly inherited in the base
+api.
+
+Alternatives
+------------
+
+The current way to do this is to manually add the missing methods to
+neutronv2 api for instance. Every time someone add a new method to one
+api has to do the same for the others and raise NotImplementedError if
+not supported.
+
+Data model impact
+-----------------
+None
+
+REST API impact
+---------------
+None
+
+Security impact
+---------------
+None
+
+Notifications impact
+--------------------
+None
+
+Other end user impact
+---------------------
+None
+
+Performance Impact
+------------------
+None
+
+Other deployer impact
+---------------------
+None
+
+Developer impact
+----------------
+
+If developers add new methods to neutronv2 or nova-network api,
+they must define it first on the new network base api.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  leandro-i-costantino
+
+Work Items
+----------
+
+ * Create a base network api files that has all the public methods
+   from current network api
+
+
+Dependencies
+============
+None
+
+Testing
+=======
+None
+
+Documentation Impact
+====================
+None
+
+References
+==========
+None
diff --git a/specs/juno/implemented/remove-cast-to-schedule-run-instance.rst b/specs/juno/implemented/remove-cast-to-schedule-run-instance.rst
new file mode 100644
index 0000000..1b7c51a
--- /dev/null
+++ b/specs/juno/implemented/remove-cast-to-schedule-run-instance.rst
@@ -0,0 +1,153 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+============================================
+Stop using the scheduler run_instance method
+============================================
+
+https://blueprints.launchpad.net/nova/+spec/remove-cast-to-schedule-run-instance
+
+Currently the scheduler is used to both pick a host for an instance to be built
+on and to handle some setup and failure conditions for booting an instance.
+The scheduler should be responsible for placement logic and everything else
+should be moved elsewhere.  This will make efforts to introduce new scheduler
+drivers or split the scheduler out of Nova easier to tackle by keeping a clean
+interface with a clear responsibility.
+
+
+Problem description
+===================
+
+The flow of execution for spawning an instance is complicated and highly
+distributed.  Some amount of distribution is necessary but there is work
+happening and decisions being made in unexpected parts of the code.  This makes
+it very difficult to look at separating the scheduler out, and means that it
+will need intimate integration with Nova that should be unnecessary.  It is
+also unecessarily difficult to reason about what is happening at which point in
+the code which makes it challenging to improve those parts of the code.
+
+
+Proposed change
+===============
+
+In Havana it became possible to query the scheduler for a list of hosts to
+provision an instance to.  The conductor service also emerged as a place to
+help orchestrate tasks that don't logically belong in either the api or compute
+nodes.  There has already been work to move some of the spawn instance workflow
+into the conductor and the final part of that effort is to have the conductor
+communicate with compute nodes rather than the scheduler.
+
+There is a new, currently unused, build_and_run_instance method in the compute
+manager which mimics the currently used run_instance method, but handles a
+failed build by sending an RPC cast to the conductor service rather than the
+scheduler.  The proposed change is to have the conductor query the scheduler
+and send a message to a compute which invokes the new build_and_run_instance
+method.  Because the new method is unused and therefore untested by Tempest
+there will likely be some work required to achieve full compatibility with the
+current run_instance method.
+
+Alternatives
+------------
+
+An alternative would be to rework the run_instance method to cast back to
+conductor rather than use the new build_and_run_instance method.  This was
+decided against because the amount of refactoring that would need to happen
+there meant it was easier to rebuild that method from scratch.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None.  The notifications being sent by the scheduler will be ported over to
+conductor to maintain the same behaviour.
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Some database updates that were occuring within the scheduler will be moved out
+to less performance critical sections of code.  This should speed up the
+scheduler.
+
+There may be a decrease in the amount of time to boot an instance if it needs
+to be rescheduled.  The new build_and_run_instance performs some pre-build
+checks earlier and doesn't generally deallocate and reallocate networks for a
+rescheduled instance.  It will deallocate/reallocate if the baremetal driver is
+in use as the networking there is host specific.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+Developers will need to be aware of the new code path being used.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  alaski
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+  * Change the conductor to query the scheduler and cast to a compute.
+
+    * Move notifications from the scheduler into the conductor.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+This is essentially a refactoring of the current spawn process.  So the current
+Tempest tests will act as good integration tests for this change since the new
+method will be used on every instance boot.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+None
diff --git a/specs/juno/implemented/rescue-attach-all-disks.rst b/specs/juno/implemented/rescue-attach-all-disks.rst
new file mode 100644
index 0000000..5436556
--- /dev/null
+++ b/specs/juno/implemented/rescue-attach-all-disks.rst
@@ -0,0 +1,135 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+====================================
+Attach All Local Disks During Rescue
+====================================
+
+https://blueprints.launchpad.net/nova/+spec/rescue-attach-all-disks
+
+Attach all local disks during rescue to allow users access to all of
+their data.
+
+
+Problem description
+===================
+
+Currently only the root disk of the original instance is attached to the
+rescue instance. If an instance is unbootable, then there is no way to
+salvage data off ephemeral or other local disks.
+
+
+Proposed change
+===============
+
+When an instance is placed into rescue, attach all local disks in addition
+to the root disk already attached.
+
+This explicitly does not attach any non-local disks, such as volumes. Any
+attempt to rescue a volume-backed instance will continue being
+rejected.
+
+
+Alternatives
+------------
+
+None
+
+
+Data model impact
+-----------------
+
+None
+
+
+REST API impact
+---------------
+
+None
+
+
+Security impact
+---------------
+
+None
+
+
+Notifications impact
+--------------------
+
+None
+
+
+Other end user impact
+---------------------
+
+None
+
+
+Performance Impact
+------------------
+
+None
+
+
+Other deployer impact
+---------------------
+
+None
+
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  johannes.erdfelt
+
+Other contributors:
+  None
+
+
+Work Items
+----------
+
+Implement feature for each virt driver.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Each virt driver will be expected to test that all disks are attached
+during rescue as part of the existing Nova tests.
+
+Tempest will be updated to assert that the original disks are attached
+during rescue.
+
+
+Documentation Impact
+====================
+
+It should be documented that this is a behavior change when rescuing
+instances.
+
+
+References
+==========
+
+https://bugs.launchpad.net/nova/+bug/1223396
diff --git a/specs/juno/implemented/return-status-for-hypervisor-node.rst b/specs/juno/implemented/return-status-for-hypervisor-node.rst
new file mode 100644
index 0000000..98689e4
--- /dev/null
+++ b/specs/juno/implemented/return-status-for-hypervisor-node.rst
@@ -0,0 +1,191 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Return hypervisor node status
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/return-status-for-hypervisor-node
+
+Problem description
+===================
+
+Currently when user show or list the hypervisor, it will have no idea of the
+status, possibly it's down or disabled already. Sometimes it will cause
+confusion like in bug https://bugs.launchpad.net/nova/+bug/1285259 .
+
+Proposed change
+===============
+
+Propose to return the service state/status when showing the hypervisor node.
+For v2 api, an extra extension is added. When the extension is loaded, we will
+return the service state/status. For a later microversion of v2.1 api, we will
+always return the state/status.
+
+When the service is disabled, add the disabled reason in the service
+information in the details/show endpoint.
+
+Alternatives
+------------
+
+There are several other options:
+
+* User first get the service information from hypervisor
+  node and then show the service status. But I think showing the hypervisor
+  status directly will be more straight forward. For example, like in
+  https://bugs.launchpad.net/nova/+bug/1285259 , user may trying to figure
+  out the instances in a compute node and didn't realize the node is disabled
+  already and the information is useless.
+
+* Currently the os-hypervisors extension already returns the service
+  information like host and service id. We can extend that field to include
+  all service state/status/disabled_reason information. However, it may be
+  better to  add the state/status to the list endpoint and only
+  disabled_reason to the service information.
+
+Data model impact
+-----------------
+
+No change on data model.
+
+REST API impact
+---------------
+
+* For V2 API, a new extension will be added as:
+  alias: os-hypervisor-status
+  name: HypervisorStatus
+  namespace: http://docs.openstack.org/compute/ext/hypervisor_status/api/v1.1
+
+  When the new extension "os-hypervisor-status" is loaded, a new field 'status'
+  will be added to the os-hypervisor API.
+
+* For a later microversion of v2.1 API, no new extension needed, the
+  existing hypervisor REST API will be updated to return the status.
+
+
+* URL: existed hypervisors extension as:
+       * /v2/{tenant_id}/os-hypervisors:
+       * /v2.1/os-hypervisors:
+
+  JSON response body::
+
+    {
+        "hypervisor": [
+        {
+            "state": "enabled",
+            "status": "up",
+            "id": 1,
+            "hypervisor_hostname": "otccloud06"
+         }]
+     }
+
+  The 'status' and 'state' are the new added fields, and are same as
+  service API.
+
+* URL: existed hypervisors extension as:
+       * /v2/{tenant_id}/os-hypervisors/{id}
+       * /v2.1/os-hypervisors/{id}
+
+  JSON response body::
+
+    {"hypervisor": {
+            "state": "enabled",
+            "status": "up",
+            "os-pci:pci_stats": [],
+            "service":
+            {
+                "host": "otccloud06",
+                "id": 3,
+                "disabled_reason": ""
+            },
+            "vcpus_used": 0,
+            "hypervisor_type": "QEMU",
+            "local_gb_used": 0,
+            "host_ip": "172.25.110.34",
+            "hypervisor_hostname": "otccloud06",
+            "memory_mb_used": 512,
+            "memory_mb": 128956,
+            "current_workload": 0,
+            "vcpus": 32,
+            "cpu_info": {"vendor": "Intel}
+            "running_vms": 0,
+            "free_disk_gb": 469,
+            "hypervisor_version": 1000000,
+            "disk_available_least": 408,
+            "local_gb": 469,
+            "free_ram_mb": 128444,
+            "id": 1}
+    }
+
+  The 'status', 'disabled_reason' and 'state' are the new added fields, and
+  are same as service API.
+
+Security impact
+---------------
+
+No
+
+Notifications impact
+--------------------
+
+No
+
+Other end user impact
+---------------------
+
+Yes, this will impact the python-novaclient. novaclient should show the status
+on the 'nova hypervisor list'.
+
+Performance Impact
+------------------
+
+No
+
+Other deployer impact
+---------------------
+
+For V2 api, the extension should be added.
+
+Developer impact
+----------------
+
+No
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+    yunhong-jiang
+
+Work Items
+----------
+
+* Changes to V2 API
+* Changes to V3 API
+
+
+Dependencies
+============
+
+No
+
+Testing
+=======
+
+Both unit and Tempest tests will be created to ensure the correct
+implementation.
+
+Documentation Impact
+====================
+
+Document the change to the REST API.
+
+References
+==========
+No
diff --git a/specs/juno/implemented/scheduler-lib.rst b/specs/juno/implemented/scheduler-lib.rst
new file mode 100644
index 0000000..563abf4
--- /dev/null
+++ b/specs/juno/implemented/scheduler-lib.rst
@@ -0,0 +1,221 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=================================
+ Create Scheduler Python Library
+=================================
+
+https://blueprints.launchpad.net/nova/+spec/scheduler-lib
+
+We want to split out nova-scheduler into gantt. To do this, we need to
+isolate the scheduler from the rest of nova.
+
+In this blueprint, we need to define in a clear library all accesses to the
+Scheduler code or data (compute_nodes DB table) from other Nova bits (conductor
+and ResourceTracker).
+
+No scheduler bits of code will be impacted by this blueprint, the change is
+only affecting other Nova components and provides a new module for Scheduler.
+
+
+Problem description
+===================
+
+To create the gantt project we need to introduce a much cleaner "seam" between
+nova-scheduler and the rest of Nova. This will allow the existing
+nova-scheduler code to remain in Nova, while at the same time giving us a clean
+way to test the new gantt scheduler.
+
+This split will also be useful to allow efforts such as the no-db-scheduler
+to evolve in a way that allows multiple patterns to co-exist, thus encouraging
+more innovation, while keeping the existing stable and pluggable solution.
+
+This change in approach for the gantt project was agreed at the Nova
+Icehouse mid-cycle meetup:
+https://etherpad.openstack.org/p/icehouse-external-scheduler
+
+
+Proposed change
+===============
+
+The basic points to note about this change are:
+
+* No change in behaviour. This is just a refactor.
+
+* Produce a scheduler lib, a prototype interface for python-ganttclient
+
+* Assume select_destinations will be the single call to the scheduler from nova
+  by the end of Juno. This is the first bit of the interface.
+
+* Move all accesses to the compute_nodes table behind the new scheduler lib.
+  This is the second part of the interface.
+
+Here we need to define a line in the sand by exposing a Scheduler interface
+that Nova can use (mostly the ResourceTracker) for updating stats to the
+Scheduler instead of directly calling DB for updating compute_nodes table.
+
+In addition, calls to the Scheduler RPC API will now go through the scheduler
+lib, so as to have all current interfaces going to the same module .But given
+the above assumptions, we need only do this for select_destinations.
+
+As said, all interfaces will go into a single module (nova.scheduler.client).
+
+The current interfaces we identify are ::
+
+    select_destinations(context, request_spec, filter_properties)
+        """Returns a list of resources based on request criterias.
+        """
+        :param context: security context
+        :param request_spec: specification of requested resources
+        :type requested_resources: dict
+        :param filter_properties: scheduler hints and instance spec
+
+    update_resource_stats(context, name, stats)
+        """Update Scheduler state for a set of resources."""
+        :param context: context
+        :param name: name, as returned by select_destinations
+        :type name: tuple or string
+        :param stats: dict of stats to send to scheduler
+
+If we still need to support the node and host distinction in nova, this can be
+done by passing a tuple (host, node) as the resource name, instead of a string.
+
+In a similar way, resource_request, will, for now, contain both
+request_spec and filter_properties in a generic dict.
+
+The stats parameter is planned to be 1:1 matched with conductor/DB
+compute_node_update() (or create()) values parameter, ie. a dict matching
+compute_nodes fields in a JSON way.
+
+
+This proposal is just drawing a line in the sand. In the future we will need to
+make more invasive changes that are not triggered for this blueprint, such as:
+
+* Adding more data into compute_nodes, so the scheduler doesn't need access to
+  any other Nova objects. For example, filters that need to know about the AZ,
+  that could be included in the stats that are added into compute_nodes
+
+* Having a data collection plugin system, so data is extracted and sent from
+  the resource tracker to the scheduler in a format that the matches the
+  filters and/or weights on the receiving end. Also ensuring, only the data
+  that is required for your particular set of filters and/or weights are sent.
+  This is very similar to the extensible resource tracker blueprint or could
+  leverage it.
+
+* Proxying select_destinations by another method for having it less Nova
+  specific and allowing in the future a python-ganttclient client to use it.
+
+
+Alternatives
+------------
+
+The other alternative would be to fork the scheduler code at a point in time to
+a separate Git repository, do the necessary changes within the code (unittests,
+imports). However neither syncing changes or having a code freeze on
+nova-scheduler seem like the best approach.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None. This effort is just refactoring, not splitting now into a separate
+repository.
+
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+Ideally:
+
+* All new operations will be scheduled using select_destinations.
+
+* ResourceTracker will only take use of update_resource_stats.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  sylvain-bauza
+
+Other contributors:
+  None
+
+Work Items
+----------
+
+* Create scheduler lib for calls to select_resources
+
+* Add update_resource_stats to lib
+
+
+Dependencies
+============
+
+* https://review.openstack.org/#/c/86988/
+  (bp/remove-cast-to-schedule-run-instance)
+
+
+Testing
+=======
+
+Covered by existing tempest tests and CIs.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+* Other effort related to RT using objects is not mandatory for this blueprint
+  but both efforts can mutally benefit
+  https://blueprints.launchpad.net/nova/+spec/make-resource-tracker-use-objects
+  (pmurray)
+
+* Cast to scheduler for running instances is mandatory for the Gantt forklift
+  but not for this blueprint
+  https://blueprints.launchpad.net/nova/+spec/remove-cast-to-schedule-run-instance
+  (alaski)
+
+* https://etherpad.openstack.org/p/icehouse-external-scheduler
+
+* http://eavesdrop.openstack.org/meetings/gantt/2014/gantt.2014-03-18-15.00.html
diff --git a/specs/juno/implemented/serial-ports.rst b/specs/juno/implemented/serial-ports.rst
new file mode 100644
index 0000000..f94d00c
--- /dev/null
+++ b/specs/juno/implemented/serial-ports.rst
@@ -0,0 +1,258 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+======================================
+Interactive web-based serial consoles
+======================================
+
+https://blueprints.launchpad.net/nova/+spec/serial-ports
+
+This blueprint is about exposing interactive web-based serial consoles to
+openstack VMs through a websocket proxy. It is mainly raised because of the
+problems openstack is facing with the serial console logs that are hard to
+maintain, grow indefinitely, etc. The point is not to eliminate the serial
+console logs, but to give the users another option besides logging to a file
+and to expose an interactive serial console.
+
+Problem description
+===================
+
+Right now the serial console has unsolved issues with the logging that have
+bounced from one release to another and no suitable solution was developed for
+them. Most of the issues are nicely summed up in the serial console log
+blueprint for juno https://review.openstack.org/#/c/80865/ however, this
+proposal doesn't deal with exposing an interactive serial console to the end
+user.
+
+Proposed change
+===============
+
+This blueprint proposes the addition of a new service - serialproxy (a
+websocket proxy) that would handle websocket connections to the serial
+consoles. The websocket proxy can be deployed on a machine other from the
+hypervisor, so unix domain sockets wouldn't do the trick. The best way to
+expose them would be by opening a TCP socket for every serial console.
+http://libvirt.org/formatdomain.html#elementsCharTCP
+This service would act similarly to the novncproxy and scale in more or less
+the same way.
+
+One serial port can be accessed only by one user at a time, i.e. it can't
+be muxed since none of the hypervisors have a 'clear this line' command
+separate from the 'connect' command (or a flag to integrate that with the
+original 'connect' call).
+The proposed scenario for multiple users accessing the same serial port is the
+following:
+If a user is already connected, then reject the attempt of a second user to
+access the console, but have an API to forceably disconnect an existing
+session. This would be particularly important to cope with hung sessions where
+the client network went away before the console was cleanly closed.
+
+To allow multiple clients to connect to serial ports we'd need to create the
+ports when the instance is booted, but we'd need to know the number of ports
+that would need to be created in advance. That number can be passed through a
+property in the image metadata, e.g. "serial_ports".
+Since the serial ports are exposed through TCP sockets we would also need a
+module that tests for free TCP ports and allocates them so that the libvirt
+driver can use them when creating the serial ports. This should be persistent,
+so that the ports that are already tested won't be tested again for a new
+serial port.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+The REST API would have one additional method to obtain the serial console URL
+for the end user or for displaying in the dashboard.
+
+V2 API specification:
+POST: v2/{tenant_id}/servers/{server_id}/get-serial-console
+
+V3 API specification:
+POST: v3/servers/{server_id}/get-serial-console
+
+Request parameters:
+
+* tenant_id: The ID for the tenant or account in a multi-tenancy cloud.
+* server_id: The UUID for the server to get the serial console for.
+
+JSON response
+::
+
+    {
+        "serial_console":
+        {
+            "url": "http://example.com:6083/serial.html?token=b40ac1c3-b640-4a6a-ae34-bf347ef089d6"
+        }
+    }
+
+JSON schema definition
+::
+
+    serial_console = {
+        'type': 'object',
+        'properties': {
+            'serial_console': {
+                'type': ['object', 'null'],
+                'properties': {},
+                'additionalProperties': False,
+            },
+        },
+        'additionalProperties': False,
+    }
+
+
+HTTP response codes:
+v2:
+
+* Normal HTTP Response Code: 200 on success
+
+v3:
+
+* Normal HTTP Response Code: 202 on success
+
+Security impact
+---------------
+
+The opening of TCP ports in the hypervisor node can enable anyone to gain
+access to any of the serial consoles by scanning for open ports if the ports
+specified in port_range config param are visible to the public.
+Usually the hypervisor ports aren't externally exposed, so this wouldn't be any
+better or worse than VNC.
+The insecurity of VNC is being tackled by a blueprint that will add strong auth
+to VNC on the internal network. That's not a reason to block this serial
+console feature though. We can work with the QEMU community at a later date to
+get SSL support for the character device sockets it exposes.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+The python-novaclient will have to implement a new command.
+
+Command:
+get-serial-console <server> <console-type>
+
+* param server: The name or Id of the server.
+
+
+Performance Impact
+------------------
+
+Using the serial consoles instead of a graphical console would be more optimal
+since it interacts with the instance through a text stream.
+
+Other deployer impact
+---------------------
+
+Config options that are being added in the serial_console group:
+[serial_console]
+- enabled (type=BoolOpt, default=False)
+- base_url (type=StrOpt, default='http://127.0.0.1:6083/serial.html')
+- listen (type=StrOpt, default='0.0.0.0')
+- proxyclient_address (type=StrOpt, default='127.0.0.1')
+- port_range (type=StrOpt, default='10000:20000')
+- record (type=BoolOpt, default=False)
+- daemon (type=BoolOpt, default=False)
+- ssl_only (type=BoolOpt, default=False)
+- source_is_ipv6 (type=BoolOpt, default=False)
+- cert (type=StrOpt, default='self.pem')
+- key (type=StrOpt)
+- web (type=StrOpt, default='/usr/share/serialproxy-static')
+
+The default value of the "enabled" confing param is False so there's no need
+to take something into account after this change gets merged.
+
+A new service - serialproxy is introduced which will need to be deployed
+separately in order for this feature to work with websockets.
+The command line params would be no different from novnc's which would override
+some of the config params specified in the config file).
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Vladan Popovic
+
+Other contributors:
+  Ian Wells
+  Sushma Korati
+
+Work Items
+----------
+
+**Websocket proxy**
+
+* Add a config param in nova that would enable the web-based serial console,
+  e.g. enabled=True|False where False would be the default.
+* Configure libvirt to open TCP channels on the ports
+  http://libvirt.org/formatdomain.html#elementsCharTCP
+* Add a port allocator module that would generate/test TCP ports and assign
+  them to the instance's libvirt config when it finds a free one.
+  This would require another config param in nova, e.g. port_range=10000:20000
+* Implement the serial console config generation and retreival in the libvirt
+  driver.
+* Add a method for obtaining the serial console in the compute manager.
+* Add methods in the consoleauth that would authorize the tokens.
+* Add API calls that would obtain the serial console URL with the generated
+  consoleauth token.
+* Add a serialproxy service that will serve as a wesocket proxy for serial
+  consoles
+* Add static files that will be serverd from the proxy, including a terminal
+  emulator, probably https://github.com/chjj/term.js/
+
+
+Dependencies
+============
+
+May require packaging of the static files for the websocket proxy and the
+terminal emulator.
+
+Testing
+=======
+
+Unit tests should be sufficient to cover libvirt and the API part.
+
+
+Documentation Impact
+====================
+
+Since tihs proposal introduces a new console and service the following things
+should be documented at least:
+
+* Deploying the serialproxy (with SSL/TLS support if possible)
+* Changes in the image metadata (if that solution fits the needs for multiuser
+  serial consoles)
+* Now to obtain a serial console URL from the API or from python-novaclient
+* Examples of managing the ports specified in the port_range so that they are
+  only accessible from the node where the serialproxy is deployed and not from
+  the outside.
+
+References
+==========
+
+None
diff --git a/specs/juno/implemented/server-group-quotas.rst b/specs/juno/implemented/server-group-quotas.rst
new file mode 100644
index 0000000..d8b2aa8
--- /dev/null
+++ b/specs/juno/implemented/server-group-quotas.rst
@@ -0,0 +1,330 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================
+Server Group Quotas
+===================
+
+https://blueprints.launchpad.net/nova/+spec/server-group-quotas
+
+Add quota values to constrain the number and size of server groups a
+users can create.
+
+Problem description
+===================
+
+Server groups can be used to control the affinity and anti-affinity scheduling
+policy for a group of servers (instances).  Whilst this is a useful mechanism
+for users such scheduling decisions need to be balanced by a deployers
+requirements to make effective use of the available capacity.
+
+For example it may be considered reasonable for a user to be able to request
+anti-affinity between a set of 10 servers to support a particular
+availability schematic.   However a user creating anti-affinity between 100
+servers would be in direct conflict with a stacking policy intended to
+avoid fragmentation of the overall cloud capacity.
+
+Unlimited anti-affinity could allow a user to derive information about the
+overall size of the cloud, which is generally considered private information
+of the cloud provider.
+
+Unlimited server groups could in themselves be used as a DoS attack against
+systems not protected by an API rate limiter, a user creating groups until
+the DB fills up.
+
+Proposed change
+===============
+
+Two new quota values will be introduced to limit the number of sever groups
+and the number of servers in a server group.
+
+These will follow the existing pattern for quotas (for example security
+groups and rules per security group) in that:
+
+* They are defined by config values, which also include the default value
+
+* They can be defined per project or per user within a project
+
+* A value of -1 for either quota will be treated as unlimited.
+
+* Defaults can be set via the quota groups API
+
+* Values may be changed at any time but will only take effect at the next
+  server group or server create.   Reducing the quota will not affect any
+  existing groups, but new servers will not be allowed into groups
+  that have become over quota.
+
+The new options will be defined as follows:
+
+cfg.IntOpt('quota_server_groups',
+           default=10,
+           help='Number of server groups per project')
+
+cfg.IntOpt('quota_server_group_members',
+           default=10,
+           help='Number of servers per server group')
+
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.  The quota values will be simply checked at the point when a server
+group is created or a server is created.
+
+REST API impact
+---------------
+
+Because this change introduces additional fields to existing API methods
+it will be controlled in V2 by the presence of a new api extension.
+
+Name = "ServerGroupQuotas"
+Alias = "os-server-group-quotas"
+
+
+Change in the response when getting the quotas for a user/tenant.
+* Method: GET
+* Path: /os-quota-sets/{tenant_id}
+* Resp: Normal Response Codes 200
+
+JSON response
+
+{
+ "quota_set": {
+  "cores": 20,
+  "fixed_ips": -1,
+  "floating_ips": 10,
+  "id": "fake_tenant",
+  "injected_file_content_bytes": 10240,
+  "injected_file_path_bytes": 255,
+  "injected_files": 5,
+  "instances": 10,
+  "key_pairs": 100,
+  "metadata_items": 128,
+  "ram": 51200,
+  "security_group_rules": 20,
+  "security_groups": 10,
+  "server_groups": 10,
+  "server_group_members": 10,
+
+ }
+
+}
+
+Change in the response when getting the default quotas.
+* Method: GET
+* Path: /os-quota-sets/defaults
+* Resp: Normal Response Codes 200
+
+JSON response
+
+{
+ "quota_set": {
+  "cores": 20,
+  "fixed_ips": -1,
+  "floating_ips": 10,
+  "id": "fake_tenant",
+  "injected_file_content_bytes": 10240,
+  "injected_file_path_bytes": 255,
+  "injected_files": 5,
+  "instances": 10,
+  "key_pairs": 100,
+  "metadata_items": 128,
+  "ram": 51200,
+  "security_group_rules": 20,
+  "security_groups": 10,
+  "server_groups": 10,
+  "server_group_members": 10,
+
+ }
+
+}
+
+Change in the request when updating the quotas for a user/tenant.
+* Method: POST
+* Path: /os-quota-sets/{tenant_id}/{user_id}
+* Resp: Normal Response Codes 200
+
+JSON response:
+
+{
+ "quota_set": {
+  "force": "True",
+  "instances": 9,
+  "server_groups": 10,
+  "server_group_members": 10,
+
+ }
+
+}
+
+JSON Schema:
+
+common_quota = {
+    'type': ['integer', 'string'],
+    'pattern': '^-?[0-9]+$',
+    'minimum': -1
+
+}
+
+update = {
+    'properties': {
+        'type': 'object',
+         'quota_set': {
+            'properties': {
+                'instances': common_quota,
+                'cores': common_quota,
+                'ram': common_quota,
+                'floating_ips': common_quota,
+                'fixed_ips': common_quota,
+                'metadata_items': common_quota,
+                'key_pairs': common_quota,
+                'security_groups': common_quota,
+                'security_group_rules': common_quota,
+                'server_groups': common_quota,
+                'server_group_members': common_quota,
+                'force': parameter_types.boolean,
+
+            },
+            'additionalProperties': False,
+
+        },
+
+    },
+    'required': ['quota_set'],
+    'additionalProperties': False,
+
+}
+
+Change in the response of the of limits request:
+
+
+JSON response:
+
+{
+    "limits": {
+        "rate": [
+
+        ],
+    "absolute": {
+        "maxServerMeta": 128,
+        "maxPersonality": 5,
+        "maxImageMeta": 128,
+        "maxPersonalitySize": 10240,
+        "maxSecurityGroupRules": 20,
+        "maxTotalKeypairs": 100,
+        "totalRAMUsed": 2048,
+        "totalInstancesUsed": 4,
+        "maxSecurityGroups": 10,
+        "totalFloatingIpsUsed": 0,
+        "maxTotalCores": 20,
+        "totalSecurityGroupsUsed": 1,
+        "maxTotalFloatingIps": 10,
+        "maxTotalInstances": 10,
+        "totalCoresUsed": 4,
+        "maxTotalRAMSize": 51200,
+        "maxServerGroups": 10,
+        "totalServerGroupsUsed": 2,
+        "maxServersPerServerGroups": 10,
+
+    }
+
+  }
+
+}
+
+Change in the response of ServerGroup API:
+
+Create can now return 413 "Quota Exceeded for server groups"
+
+
+
+Security impact
+---------------
+
+Improves the security of systems with the Server Groups API enabled
+by limiting the resources each project can consume.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+python-novaclient will be updated to support the new quota values.
+
+If the new values are not returned by the API (i.e the system has not yet
+been updated to include this change) then the client will return a value
+of -1 (unlimited)
+
+Performance Impact
+------------------
+
+None - the quota validation will be a minor additional step in the  API.
+
+Other deployer impact
+---------------------
+
+Quotas will only be validated for new requests, so it is possible (as with
+any default quota change) that some existing projects may already be over
+quota.  No existing groups will be affected, but users will be unable to
+create new groups and/or add servers to groups until they drop below their
+quota allowances.
+
+Deployers will have to consider what default quota values they want to
+configure, and if they want to configure any project specific quotas.
+
+The new quota checks will only be effective and vakues reported via the API
+when the new extension is loaded.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  philip-day
+
+Work Items
+----------
+
+The change will be submitted as a single patch set.
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Existing Tempest quota tests will be extended to cover the new values.
+
+
+Documentation Impact
+====================
+
+The new values will need to be included in the documentation.
+
+
+References
+==========
+
+None.
diff --git a/specs/juno/implemented/servers-list-support-multi-status.rst b/specs/juno/implemented/servers-list-support-multi-status.rst
new file mode 100644
index 0000000..193741d
--- /dev/null
+++ b/specs/juno/implemented/servers-list-support-multi-status.rst
@@ -0,0 +1,133 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================
+servers list API support specify multi-status
+==============================================
+
+https://blueprints.launchpad.net/nova/+spec/servers-list-support-multi-status
+
+Allow to specify multiple status value concurrently in the servers list API.
+
+Problem description
+===================
+
+Currently the service list API allows the user to specify an optional status
+value to use as a filter - for example to limit the list to only servers with
+a status of Active.
+
+However often the user wants to filter the list by a set of status values,
+for example list servers with a status of Active or Error,
+which requires two separate API calls.
+
+Allowing the API to accept a list of status values would reduce this to a
+single API call.
+
+Proposed change
+===============
+
+Enable servers list API to support to specify multiple status values
+concurrently.
+
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+Allow to specify status value for many times in a request.
+
+For example::
+
+    GET /v2/{tenant_id}/servers?status=ACTIVE&status=ERROR
+    GET /v3/servers?status=ACTIVE&status=ERROR
+
+V2 API extension::
+
+    {
+        "alias": "os-server-list-multi-status",
+        "description": "Allow to filter the
+            servers by a set of status values.",
+        "links": [],
+        "name": "ServerListMultiStatus",
+        "namespace": "http://docs.openstack.org/compute/ext/
+            os-server-list-multi-status/api/v2",
+        "updated": "2014-05-11T00:00:00Z"
+    }
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  boh.ricky
+
+Work Items
+----------
+
+Implement the support for servers list API to specify multiple status values
+concurrently.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+None
+
+Documentation Impact
+====================
+
+Need to document in the API document.
+
+References
+==========
+
+None
diff --git a/specs/juno/implemented/support-cinderclient-v2.rst b/specs/juno/implemented/support-cinderclient-v2.rst
new file mode 100644
index 0000000..394684d
--- /dev/null
+++ b/specs/juno/implemented/support-cinderclient-v2.rst
@@ -0,0 +1,155 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Cinder Client V2 Support
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/support-cinderclient-v2
+
+Cinder has a new API version 2 [1]. This version has existed since Grizzly [2]
+and has been available in devstack since Havana [3].
+
+The API provides:
+
+* More consistent responses like name, description instead of 'display_name',
+  etc.
+
+* Caching data between controllers instead of multiple database hits.
+
+* Filtering when listing information on volumes, snapshots and backups. This
+  would be great support to have in Nova so the full listing of resources
+  doesn't have to be given over the network for Nova to sort through. [4]
+
+Cinder is also deprecating version 1 in favor of 2, so it would be great to
+give users a transition period in other projects.
+
+Problem description
+===================
+
+Nova currently has a wrapper to the Cinder client in nova.volumes.cinder which
+supports version 1 and expects a variety of response keys like 'display_name'
+and 'display_description' which aren't available in version 2. These were
+changed to be consistent with other projects that just use 'name' and
+'description'.
+
+Proposed change
+===============
+
+Nova should use Cinder v2 client [5] which understands how to talk to the
+Cinder v2 API. Since v1 is deprecated, we can leave Cinder client v1 support
+in.
+
+`cinder_catalog_info` option in nova.conf should also be set to
+`volumev2:cinder:publicURL` which would default new users the v2 API which is
+on by default in Cinder since Grizzly.
+
+Making these changes to the wrapper won't require any change to its interface
+or changes to how it returns information. This is done by the wrapper doing the
+translation and still giving back the expected data structure as it would with
+v1.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Existing deployments will not need to make any changes to nova.conf in the Juno
+release. Cinder will just be deprecating v1 support, so they'll receive
+a warning on start up in the cinder-api service. If the deployer wants Nova to
+use Cinder v2, they'll need to change `cinder_catalog_info` to use the
+appropriate service_type they have Cinder v2 endpoint setup in the service
+catalog. It is acceptable to have a mix of Nova hosts talking to different
+versions of the Cinder API, assuming both v1 and v2 are enabled in Cinder.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  thingee
+
+Other contributors:
+  dzyu
+
+Work Items
+----------
+
+* Write changes in nova.volumes.cinder to support Cinder client v2, while
+  keeping support for v1. [6]
+* Add Cinder filtering support in Nova.
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest gate tests for compute will test against Cinder v2. Tempest has both
+versions available, so Nova's config option of cinder_catalog_info will be
+updated to the appropriate service_type of v2. If resources allow, we can also
+test against v1.
+
+Unit tests will test against Nova's wrapper which talks to Cinder client. This
+will specifically verify usage between v1 and v2 is handled on this layer and
+is transparent to the rest of Nova.
+
+Documentation Impact
+====================
+
+None
+
+References
+==========
+
+[1] - http://docs.openstack.org/api/openstack-block-storage/2.0/content/
+[2] - https://review.openstack.org/#/q/status:merged+project:openstack/cinder+branch:master+topic:bp/bp,n,z
+[3] - https://review.openstack.org/#/c/22489/
+[4] - https://github.com/openstack/cinder/commit/88e688317dc4066f2f0b4dfc454a3f049da4d0e3
+[5] - https://github.com/openstack/python-cinderclient/tree/master/cinderclient/v2
+[6] - https://review.openstack.org/#/c/43986/
diff --git a/specs/juno/implemented/use-oslo-vmware.rst b/specs/juno/implemented/use-oslo-vmware.rst
new file mode 100644
index 0000000..748d5a6
--- /dev/null
+++ b/specs/juno/implemented/use-oslo-vmware.rst
@@ -0,0 +1,152 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+========================================================
+Integrate the vmware driver with the oslo.vmware library
+========================================================
+
+https://blueprints.launchpad.net/nova/+spec/use-oslo-vmware
+
+Now that the oslo.vmware library has been released, the vmware driver should be
+updated to use it.
+
+
+Problem description
+===================
+
+Too much code duplication of vmware-related projects led to the creation of the
+oslo.vmware project (https://github.com/openstack/oslo.vmware). Now that it is
+released, and already started to be used by Glance and Ceilometer, it's time
+the nova driver does the same.
+
+
+Proposed change
+===============
+
+This means mostly adding new import lines, mechanical conversion of call sites
+and deleting existing code obsoleted by the library.  Most of the work has
+already be done and proposed in the icehouse cycle
+(https://review.openstack.org/#/c/70175/) so that can be used as the starting
+point of the patch.
+The changes are pure code reorganization, and has no externally visible impact.
+
+Alternatives
+------------
+
+None, unless we consider the undesirable option of keeping status quo as one.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Some version of the oslo.vmware library as eventually dictated by the
+the project requirements will have to be installed for the updated vmware
+driver to function.
+
+Developer impact
+----------------
+
+While the changes are mechanical, it touches many places in the vmwareapi
+driver code base, so it can cause a lot of conflict with other driver work.
+Once merged, it is likely all vmware driver related patches under review will
+have to be updated to account for it.
+
+On the flip side, there is developer impact of this change not being merged as
+well:
+
+Until this change is merged, driver changes/fixes to areas of functionality
+that oslo.vmware also provides means that a developer should almost always have
+to update both nova and oslo.vmware with similar patches.
+
+To migitate this issue of conflicts and code duplication, it is recommended
+that patches related to the vmware driver should be made dependent on this
+work.
+
+Changes to the nova driver may now require a change/release to oslo.vmware
+as a pre-requisite.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  vui
+
+Work Items
+----------
+
+Mostly https://review.openstack.org/#/c/70175/ plus some additional updates to
+account for recent code additions to the vmware driver code.
+
+
+Dependencies
+============
+
+Changes pertaining to
+https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
+will cause significant code churn, but given the mostly mechanical nature of
+the changes to this blueprint, reacting to the former should be fairly
+straightforward.
+
+Given that this work and that for the vmware-spawn-refactor blueprint are
+fairly orthogonal, and both necessary to facilitate additional changes to the
+driver, it is proposed that they be considered the highest-priority items for
+the vmware driver to be included in Juno-1.
+
+
+Testing
+=======
+
+Unit tests exercising the obsoleted code will be removed. Updating existing
+tests that currently mocks the obsoleted code to use use.vmware accordingly
+so that they pass should be sufficient to validate the change.
+
+No externally visible changes means no additional Tempest tests are needed.
+
+
+Documentation Impact
+====================
+
+None
+
+
+References
+==========
+
+* https://github.com/openstack/oslo.vmware
+* https://review.openstack.org/#/c/70175/
+* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
diff --git a/specs/juno/implemented/user-defined-shutdown.rst b/specs/juno/implemented/user-defined-shutdown.rst
new file mode 100644
index 0000000..b9ac215
--- /dev/null
+++ b/specs/juno/implemented/user-defined-shutdown.rst
@@ -0,0 +1,259 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================================================
+Allow controlled shutdown of GuestOS for operations which power off the VM
+==========================================================================
+
+https://blueprints.launchpad.net/nova/+spec/user-defined-shutdown
+
+The current behavior of powering off a VM without giving the Guest Operating
+system a chance to perform a controlled shutdown can lead to data corruption.
+
+
+Problem description
+===================
+
+Currently in libvirt operations which power off the VM (stop, rescue, shelve,
+resize) do so without giving the GuestOS a chance to shutdown gracefully.
+Some GuestOS's (for example Windows) do not react well to this type of virtual
+power failure, and so it would be better if these operations follow the
+same approach as soft_reboot and give the GuestOS a chance to shutdown
+gracefully.
+
+
+Proposed change
+===============
+
+The proposed changes will make the default behavior for stop, rescue, resize,
+and shelve to give the GuestOS a chance to perform a controlled shutdown
+before the VM is powered off.
+
+The change will encapsulate the complexity of signaling to and waiting for
+the GuestOS in the hypervisor, and allow image owners the ability to tune
+the associated timing via image metadata to take account of GuestOSs that
+require an extended period to shutdown (such as Windows).
+
+Users will be able to specify the shutdown behavior on a per operation basis
+via a new shutdown_type parameter where, in keeping with the current reboot
+operation, a "soft" shutdown will give the GuestOS a chance to perform a
+clean shutdown, and a "hard" shutdown will cause an immediate power off.  The
+default behavior will be a "soft" shutdown.
+
+An example of a user wanting to override the default behavior is Tempest
+which does not generally care if a GuestOS becomes corrupted and may
+prefer speed of execution over data integrity.
+
+At the hypervisor layer the shutdown behavior will be controlled by two
+values:
+
+* A timeout value specifying in seconds how long the hypervisor should
+  wait for the GuestOS to shutdown. If the GuestOS does not shutdown
+  within this period then the VM will be powered off anyway. A value of 0
+  will power off the VM without signaling the Guest to shutdown.
+
+* A retry interval specifying in seconds how frequently within that period the
+  hypervisor should signal the guest to shutdown.  This is a protection
+  against guests that may not be ready to process the shutdown signal
+  when it is first issued - a common problem if an instance is deleted
+  just after it has been created and the GuestOS is booting.
+
+For example if the overall timeout is set to 60 seconds and the retry interval
+is set to 10 seconds then the guest will be signaled up to six times before
+being powered off.
+
+These values will be passed into the virt driver by the compute manager,
+allowing the same values to be used for all hypervisors.
+
+The timeout value will be a Nova configuration parameter as different
+operators may want a different default.  The retry value will be implemented
+as a constent in the Nova code.  The timeout value can be overridden
+on a per image basis via image metadata settings.
+
+Alternatives
+------------
+
+An alternative approach would be to expose a new operation that only shuts
+down the GuestOS (with used defined timing parameters), expose the status of
+that operation via the API, and rely on the client for all retry logic.
+However we believe that a clean shutdown should be the default behavior in
+Nova and not have to be managed as a separate activity (which would have to
+be replicated in all API bindings).
+
+An alternative using a simpler single parameter to specify how long the
+hypervisor should wait was previously merged but had to be reverted
+because it added around 25 minutes to the tempest runs:
+https://review.openstack.org/#/c/35303/
+
+This was due to Tempest frequently stopping an instance immediately after
+it is created, in which case the ACPI signal is delivered before the GuestOS
+is in a state to process it.  This results in the shutdown waiting for the
+full duration of the timeout.
+
+The revised approach described above avoids this issue by periodically
+resending the shutdown signal to the GuestOS.
+
+Once this change has been merged Tempest could be optimized to avoid this delay
+(for example by setting the timeout to zero via image metadata or nova.conf).
+
+It could be argued that the delete operation should allow the same
+controlled shutdown schematics so that instances using and/or booting
+from volumes can also leave those file systems in a safe state.  However
+if the stop operation is modified to provide a controlled shutdown then
+users can achieve the required sequence by performing a stop prior to
+the delete.  This also avoids an issue of the http delete request not
+normally accepting a body.
+
+
+Data model impact
+-----------------
+
+None, the change is contained mainly within the interaction between the compute
+manager and the virt driver.
+
+REST API impact
+---------------
+
+The following API methods will be extended to accept an optional shutdown_type
+parameter:
+
+* Stop       POST servers/{server_id}/action
+                        {"os-stop": {"shutdown_type": "HARD|SOFT"}}
+
+* Rescue     POST servers/{server_id}/action
+                        {"rescue": {"shutdown_type": "HARD|SOFT"}}
+
+* Resize     POST servers/{server_id}/action
+                        {"resize": {"shutdown_type": "HARD|SOFT",
+                                    "flavor_id": <id>}}
+
+* Shelve     POST servers/{server_id}/action
+                        {"shelve": {"shutdown_type: "HARD|SOFT"}}
+
+* Migrate    POST servers/{server_id}/action
+                        {"migrate": {"shutdown_type: "HARD|SOFT"}}
+
+
+Security impact
+---------------
+
+None, the change doesn't change the set of operations that a user can perform.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+Users will be able to provide additional options to the stop, rescue, and
+delete.  These will be exposed in the python-novaclient:
+
+nova stop [--hard-shutdown]
+nova rescue [--hard-shutdown]
+nova resize [--hard-shutdown]
+nova shelve [--hard-shutdown]
+
+Note that "--hard-shutdown" is preferred here over the "--hard" option used
+for reboot since a "soft resize" might be interpreted to mean a soft change
+in allocated resources (such as disabling a cpu).
+
+To make the novaclient CLI reboot command consistent it will be also modified
+to accept --hard-shutdown as an alias for --hard.
+
+Performance Impact
+------------------
+
+The performance impact is limited to the changes in the processing path of the
+stop, rescue, and delete operations. When performing a clean shutdown
+these will take longer than before as the system waits for the GuestOS to
+shutdown. The overhead of polling to observe this change in state is
+negligible and the calling thread will sleep (yield) between each poll.
+
+Other deployer impact
+---------------------
+
+Once this set of changes has been merged the system will by default be
+configured to wait for instances to shutdown gracefully for stop, shelve,
+rescue, and resize operations.
+
+Deployers will need to consider if they want to modify the default timeout
+parameters, and/or to add override values to the metadata of existing images.
+
+The configuration parameters will be common to all hypervisors, but this
+BP will only deliver a libvirt implementation.
+
+
+Developer impact
+----------------
+
+Only the first stage of the implementation is hypervisor dependent, once
+that has merged other hypervisor implementations can be added.
+
+The remaining stages will apply to any hypervisor that implements the revised
+power_off options.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  philip-day
+
+Work Items
+----------
+
+* Add timeout parameters to virt power_off method of virt driver and provide
+  the libvirt implementation.   Implement clean_shutdown for stop() within
+  the compute manager as an initial example.
+* Add clean_shutdown option to compute manager Rescue, Resize, and Shelve
+  operations
+* Use image properties to override the timeout values
+* Expose clean shutdown via rpcapi
+* Expose clean shutdown via API
+
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+The methods that are being modified are already extensively tested by Tempest
+which will ensure no functional regression.
+
+The default behavior will be to perform a clean shutdown, although it's not
+easy to see how this can be verified by Tempest, since it needs specific
+support within the Guest, and the behavior of any GuestOS is generally
+considered outside the scope of Nova.  Likewise the ability to stop without a
+clean shutdown could be exercised from Tempest (it's possible that Tempest
+would want to make this its normal case), but its hard to see how that could
+be verified.  Input will be sought from the Tempest community to see what can
+be done to address these issues.
+
+
+Documentation Impact
+====================
+
+* The API specs will need to be updated.
+* The change in default behavior for stop, rescue, resize, and shelve (to wait
+  for the GuestOS to shutdown) will need to be documented.
+* The ability to override the shutdown timeouts on a per image basis will need
+  to be documented.
+
+References
+==========
+
+The code for the first work item is available for review
+https://review.openstack.org/#q,I432b0b0c09db82797f28deb5617f02ee45a4278c,n,z
+
diff --git a/specs/juno/implemented/v2-on-v3-api.rst b/specs/juno/implemented/v2-on-v3-api.rst
new file mode 100644
index 0000000..1663226
--- /dev/null
+++ b/specs/juno/implemented/v2-on-v3-api.rst
@@ -0,0 +1,262 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================
+Implement the v2.1 API on the V3 API codebase
+=============================================
+
+https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api
+
+Implement v2 compatible API based on v3 API infrastructure.
+
+Problem description
+===================
+
+On v3 API development, we have improved API infrastructure such as API
+plugin loading, input validation, policy check, etc. In addition, to fix
+inconsistent interfaces of v2 API, we have made a significant number of
+backwards incompatible changes of the Nova API (Change success status
+codes, API attribute names, and API URLs). There is a comprehensive
+description of the problems with the v2 API for users, operators and
+developers here:
+http://ozlabs.org/~cyeoh/V3_API.html
+
+However, there have been intensive discussions over the future of Nova
+and the maintenance overhead implications from having to support two
+APIs such as v2 and v3 simultaneously for a long period of time.
+
+Proposed change
+===============
+
+Through a lot of discussions, we have understood the advantages of v3 API
+infrastructure (API plugin loading, input validation, policy check, etc).
+However, their backwards incompatible interfaces seem a little premature at
+this time, because now we aren't sure that current v3 API is the best.
+That means we cannot be sure that any more backwards incompatible changes
+are unnecessary even if switching to current v3 API.
+
+This spec proposes the removal of backwards incompatible changes from v3 code.
+That means current v3 consistent interfaces would go back to v2 inconsistent
+ones like::
+
+  --- a/nova/api/openstack/compute/plugins/v3/servers.py
+  +++ b/nova/api/openstack/compute/plugins/v3/servers.py
+  @@ -752,7 +752,7 @@ class ServersController(wsgi.Controller):
+  The field image_ref is mandatory when no block devices have been
+  defined and must be a proper uuid when present.
+  """
+  - image_href = server_dict.get('image_ref')
+  + image_href = server_dict.get('imageRef')
+
+This proposal is painful for v3 API developers because they have worked hard
+to make consistent interfaces over a year and v3 interfaces are exactly better
+than v2 ones. However, through the discussions, we have known that backwards
+incompatible changes are very painful for users and we must pay attention to
+these changes.
+
+On this spec, we would provide v2 compatible API with the other v3 advantages
+as the first step. After this spec, we will provide consistent interfaces by
+defining API rules step by step. These rules will prevent the same backwards
+incompatible changes and keep consistent interfaces even if adding a lot of
+new APIs in the future. However, that is out of scope from this spec now.
+
+It is also agreed that we wont implement proxies for other OpenStack APIs such
+as glance, cinder or neutron as part of the initial v2.1 implementation. These
+will instead be added later, but before the removal of the original v2 code.
+
+Alternatives
+------------
+
+Through these discussions, we got an idea that we could support both v2 API
+and v3 API on the top of the v3 API codebase. On this idea, nova translates a
+v2 request to v3 request and passes it to v3 API method. After v3 API method
+operation, nova translates its v3 response to v2 response again and returns
+it to a client.
+However, there was an intensive discussion against this idea also because it
+would be difficult to debug API problems due to many translations when we have
+a lot of backwards incompatible changes in the long term.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+The V2.1 REST API presented will be identical to the V2 API except as
+noted above.
+
+Note however that V2.1 will not support the XML version of the V2 API,
+only the JSON one. However the XML version of the V2 API is currently
+marked as deprecated.
+
+Security impact
+---------------
+
+Better up front input validation will reduce the ability for malicious
+user input to exploit security bugs.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+Potentially it may be advantageous if python novaclient could talk to
+/v2.1 instead of /v2 but code changes may not be required to change
+this. It may be simpler just to do this through keystone configuration.
+The API itself remains identical.
+
+Performance Impact
+------------------
+
+More stringent input validation also means more work that is needed to
+be done in the API layer but overall this is a good thing.
+
+Other deployer impact
+---------------------
+
+If the deployer wanted to export the API as /v2 rather than /v2.1 then
+they would need to modify the api-paste.ini file (a couple of line
+change to disable the original V2 API and use the APIRouterV21 as
+the /v2 API.
+
+The long term goal would be to deprecate and eventually remove the
+original V2 API code when deployers and users are satisfied that v2.1
+satisfies their requirements.
+
+The process which we would use is:
+
+* V2.1 fully implemented with Tempest verification (including extra
+  verification that is being added in terms of response data)
+* Verification from multiple sources (cloud providers, users etc) that
+  V2.1 is compatible with V2
+
+  * This could be done in various ways
+
+    * Keystone changes so /v2.1 is advertised instead of /v2
+    * Exporting the V2.1 as /v2
+    * Combined with the possibility of putting V2.1 input validation into
+      a log rather than reject mode.
+
+* V2.1 is in an openstack release for N versions
+* After widespread confirmation that the V2.1 API is compatible, V2
+  would be marked as deprecated
+
+Developer impact
+----------------
+
+Long term advantages for developers are:
+
+* All the API implementations are on the new API framework
+
+* Reduction in maintenance overhead of supporting two major API
+  versions
+
+* Have a better framework for handling future backwards incompatible
+  changes.
+
+In the short term while the old V2 API code exists there will still be
+a dual maintenance overhead.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  cyeoh-0
+
+Other contributors:
+  oomichi
+  Alex Xu
+
+Work Items
+----------
+
+* Change v3 success status codes to v2 ones.
+
+* Change v3 API routings to v2 ones.
+
+  * Handle API URLs include a project id.
+  * Change the API resource paths. (e.g: /keypairs(v3) -> /os-keypairs(v2))
+  * Change action names. (e.g: migrate_live(v3) -> os-migrateLive(v2))
+
+* Change v3 API attribute names to v2 ones.
+
+  * Change the API parsers of v3 code.
+  * Change the API schemas of input validation.
+
+* Change v3 API behaviors to v2 ones.
+  On some APIs, there are different behaviors.
+  For example, v3 "create a private flavor" API adds a flavor access for its
+  own project automatically, but v2 one doesn't.
+
+The following work item is not mandatory and it is one of wishlist.
+
+* Change v3 plugin code path.
+  e.g::
+
+    nova/api/openstack/compute/plugins/v3/servers.py
+    -> nova/api/openstack/compute/plugins/servers.py
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Tempest has already contained a lot of v2 API tests, and that is a good test
+coverage now. For this v2.1 API, we need to run v2 API tests for both current
+v2 and v2.1 in parallel. As an idea, we will add v2.1 API tests by inheriting
+from the existing v2 API test classes and executing them against /v2.1.
+A spec for this idea has been already proposed:
+
+https://review.openstack.org/#/c/96661/
+
+Documentation Impact
+====================
+
+The documentation for the v2 API will essentially remain the same as the API
+will not change except for improvements in input validation. There will need
+to be some updates on possible error status codes.
+
+Longer term the improved infrastructure for input validation and the
+development of JSON schema for response validation will make it much
+easier to automate the generation of documentation for v2 rather relying
+on the current mostly manual process.
+
+References
+==========
+
+* Juno Mid-Cycle meetup https://etherpad.openstack.org/p/juno-nova-mid-cycle-meetup
+
+* Juno design summit discussion https://etherpad.openstack.org/p/juno-nova-v2-on-v3-api-poc
+
+* Mailing list discussions about the Nova V3 API and the maintenance
+  overhead
+
+  * http://lists.openstack.org/pipermail/openstack-dev/2014-March/028724.html
+  * http://lists.openstack.org/pipermail/openstack-dev/2014-February/027896.html
+
+* Etherpad page which discusses the V2 on V3 Proof of Concept and
+  keeps track of the ongoing work.
+
+  * https://etherpad.openstack.org/p/NovaV2OnV3POC
+
+* Document about the problems with the V2 API
+
+  * http://ozlabs.org/~cyeoh/V3_API.html
+
+* Document describing the current differences between the V2 and V3 API
+
+  * https://wiki.openstack.org/wiki/NovaAPIv2tov3
diff --git a/specs/juno/implemented/v3-api-schema.rst b/specs/juno/implemented/v3-api-schema.rst
new file mode 100644
index 0000000..d2873b1
--- /dev/null
+++ b/specs/juno/implemented/v3-api-schema.rst
@@ -0,0 +1,223 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================================
+Create JSON Schema definitions for Nova v3 API
+==============================================
+
+https://blueprints.launchpad.net/nova/+spec/v3-api-schema
+
+Complete JSON Schema definitions for Nova v3 API request bodies.
+
+Problem description
+===================
+
+Nova contains a lot of RESTful API, but not all API parameters of a request
+body are completely validated. To validate all parameters, an API validation
+framework has been implemented with JSON Schema library.
+After that, we needed to add JSON Schema definitions for each API but we could
+not complete them in Icehouse cycle.
+In Juno cycle, we need to implemented strong validation for v2.1 API as the
+design summit discussion. That means we need to implement strong validation
+for v3 API because v2.1 API is implemented on the top of v3 API implementation.
+
+Proposed change
+===============
+
+Each API definition should be added with the following ways:
+
+* Create definition files under ./nova/api/openstack/compute/schemas/v3/.
+* Each definition should be described with JSON Schema.
+* Each parameter of definitions(type, minLength, etc.) can be defined from
+  current validation code, DB schema, unit tests, Tempest code, or so on.
+* Reuse the existing predefined parameter types(name, hostname, boolean, etc.)
+  in nova/api/validation/parameter_types.py as possible.
+
+Alternatives
+------------
+
+Before the API validation framework, we needed to add the validation code into
+each API method in ad-hoc. These changes would make the API method code dirty
+and we needed to create multiple patches due to incomplete validation.
+For example, "create a flavor extraspec" API has been changed twice in Icehouse
+for its validation:
+
+* Enforce FlavorExtraSpecs Key format.
+  http://git.openstack.org/cgit/openstack/nova/commit/?id=050ce0e5891ba816baaef
+
+* Fix the validation of flavor_extraspecs v2 API
+  http://git.openstack.org/cgit/openstack/nova/commit/?id=8010c8faf9f030d2c0264
+
+If using JSON Schema definitions instead, acceptable request formats are clear
+and we don't need to do this ad-hoc works in the future.
+
+* Why not Pecan
+
+  Some projects(Ironic, Ceilometer, etc.) are implemented with Pecan/WSME
+  frameworks and we can get API documents automatically from the frameworks.
+  In WSME implementation, the developers should define API parameters for
+  each API. Pecan would make the implementations of API routes(URL, METHOD)
+  easy. And API documentation is generated from the combinations of these
+  definitions.
+  In Icehouse summit, Nova team decided to pick Pecan as Nova v3 API framework
+  with JSONSchema instead of WSME. because Nova contains complex APIs (API
+  extensions) and WSME could not cover them. In addition, Pecan implementation
+  (https://blueprints.launchpad.net/nova/+spec/v3-api-pecan) also was difficult
+  in the development and not completed. So now, Nova v3 API is implemented with
+  Nova's original WSGI framework and JSONSchema, we cannot use Pecan.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+By applying strict validation to every APIs, some values which are accepted
+in v2 API will be denied in v3 API. For example, here picks the server name
+of "create a server" API up.
+The string pattern of the server name is not validated in v2 API at all. We
+can specify UTF-8(non-ascii) characters as a server name through v2 API now.
+For strong/comprehensive validation, we will apply the predefined parameter
+type "name" to the server name also. The types allows "a-zA-Z0-9. _-" only as
+the string pattern and denies UTF-8 characters. In the worst cases we could
+relax input validation for names.
+
+Security impact
+---------------
+
+Better up front input validation will reduce the ability for malicious user
+input to exploit security bugs.
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+Nova will need some performance cost for this comprehensive validation, because
+the checks will be increased for API parameters which are not validated now.
+However, I believe this is necessary cost for public REST APIs and we need to
+pay it.
+
+Other deployer impact
+---------------------
+
+None
+
+Developer impact
+----------------
+
+Developers, who implement a new REST API, need to add JSON Schema definitions
+as the part of an API implementation.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  oomichi
+
+Other contributors:
+  aikawa
+  takada-yuiko
+  xu-haiwei
+
+Work Items
+----------
+
+This task requires a lot of patches, and the progress management is the key.
+They are tracked on https://etherpad.openstack.org/p/nova-v3-api-validation
+Now the implementations of most APIs have been done except some new APIs (
+instance-group and server-external-events) and we need to review them.
+
+
+Dependencies
+============
+
+* If porting nova-network to v3 API, we need to create some JSON Schema patces
+  for it.
+
+
+Testing
+=======
+
+Through this implementation, we need to improve the unit test coverage from
+the viewpoint of negative request cases. Current unit tests don't cover every
+negative cases and we will be able to add them because of making valid request
+format clear.
+In addition, we will be able to find original unit test bugs through this work.
+We have fixed some bugs of unit tets in Icehouse:
+
+* Fix the sample and unittest params of v3 scheduler-hints
+  http://git.openstack.org/cgit/openstack/nova/commit/?id=b699c703e00eda1c8368b
+
+* Fix the flavor_ref type of unit tests
+  http://git.openstack.org/cgit/openstack/nova/commit/?id=5191576c279dc9905e881
+
+* Change evacuate test hostnames to preferable ones
+  http://git.openstack.org/cgit/openstack/nova/commit/?id=9888f61128ed82d15d074
+
+Now Tempest contains the negative test generator. The generator operates the
+negative tests automatically based on the API definitions which are described
+with JSON Schema. By porting the API definitions of this blueprint from Nova
+to Tempest, we can improve the test coverage of Tempest also.
+
+
+Documentation Impact
+====================
+
+In long term, I hope this API definitions are used for API specification
+document auto-genaration also. We can get the trustable API document and
+it would be good for users and developers.
+As the first step, I have submitted the blueprint for generating API sample
+files from the API definitions. This is out of the scope of this description
+but I pick it up as a useful sample:
+https://blueprints.launchpad.net/nova/+spec/generate-api-sample-from-api-schema
+
+* Why not current template files
+
+  API samples are generated from template files which are fixed format like::
+
+    {
+        "evacuate": {
+            "host": "%(host)s",
+            "admin_password": "%(adminPass)s",
+            "on_shared_storage": "%(onSharedStorage)s"
+        }
+    }
+
+  API developers should write this kind of template file for API implementation
+  and they should generate API sample files from them.
+  As the result, API implementation review has many files and sometime these
+  files were wrong at broken indents, non-existent parameters(typo, etc.).
+  To improve this situation, I proposed to use JSONSchema definitions instead
+  of the template files. After that, we can remove the template files and
+  reviews will be more easy.
+
+References
+==========
+
+* Links to mailing list
+
+  * [Nova] What validation feature is necessary for Nova v3 API
+    http://lists.openstack.org/pipermail/openstack-dev/2013-October/016649.html
+
+* Links to notes from a summit session
+
+  * API Validation for the Nova V3 API
+    https://etherpad.openstack.org/p/icehouse-summit-nova-pecan-wsme
diff --git a/specs/juno/implemented/v3-diagnostics.rst b/specs/juno/implemented/v3-diagnostics.rst
new file mode 100644
index 0000000..cfa4699
--- /dev/null
+++ b/specs/juno/implemented/v3-diagnostics.rst
@@ -0,0 +1,339 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==============================
+V3 Diagnostics - common output
+==============================
+
+https://blueprints.launchpad.net/nova/+spec/v3-diagnostics
+
+Currently there is no defined format for VM diagnostics. This BP will ensure
+that all of the drivers that provide VM diagnostics will have a consistent
+format.
+
+**NOTE:** this cannot be used for V2 as there may be existing deployments that
+parse the current output of the V2 diagnostics.
+
+Problem description
+===================
+
+In V2 the VM diagnostics are a 'blob' of data returned by each hypervisor. The
+goal here is to have a formal definition of what output should be returned, if
+possible, by the drivers supporting the API. In additition to this a driver
+will be able to return additional data if they choose.
+
+Proposed change
+===============
+
+Introduce a new driver method that will return a predefined structure:
+get_instance_diagnostics(self, context, instance)
+
+This is a new driver method. The reason for this is that it is much cleaner
+to have a new method instead of having if's which indicate if it is new or
+legacy. We should also consider deprecating get_diagnostics. This should be
+documented in the virt driver API.
+
+The proposal is to have the drivers return the following information in a
+object class. A diagnostics Model() class will be introduced. This will
+be instantiated and populated by the virt drivers. The class will have a
+method to serialize to JSON so that the API interface can return a JSON
+format to the user. A field that is not populated by the driver will return
+a default value set in the aforementioned class.
+
+The table below has the key and a description of the value returned:
+
++------------------------+---------------------------------------------------+
+| Key                    | Description                                       |
++========================+===================================================+
+| state                  | The current state of the VM. Example values       |
+|                        | are: 'pending', 'running', 'paused', 'shutdown',  |
+|                        | 'crashed', 'suspended' and 'building' (String)    |
++------------------------+---------------------------------------------------+
+| driver                 | A string denoting the driver on which the VM is   |
+|                        | running. Examples may be: 'libvirt', 'xenapi',    |
+|                        | 'hyperv' and 'vmwareapi' (String) [Admin only -   |
+|                        | key will not appear if non admin]                 |
++------------------------+---------------------------------------------------+
+| hypervisor_os          | A string denoting the hypervisor OS (String)      |
+|                        | [Admin only - key will not appear if non admin]   |
++------------------------+---------------------------------------------------+
+| uptime                 | The amount of time in seconds that the VM has     |
+|                        | been running (Integer)                            |
++------------------------+---------------------------------------------------+
+| num_cpus               | The number of vCPUs (Integer)                     |
++------------------------+---------------------------------------------------+
+| num_nics               | The number of vNICS (Integer)                     |
++------------------------+---------------------------------------------------+
+| num_disks              | The number of disks (Integer)                     |
++------------------------+---------------------------------------------------+
+| cpu_details            | An array of details (a dictionary) per vCPU (see  |
+|                        | below)                                            |
++------------------------+---------------------------------------------------+
+| nic_details            | An array of details (a dictionary) per vNIC (see  |
+|                        | below)                                            |
++------------------------+---------------------------------------------------+
+| disk_details           | An array of details (a dictionary) per disk (see  |
+|                        | below)                                            |
++------------------------+---------------------------------------------------+
+| memory_details         | A dictionary of memory details (see below)        |
++------------------------+---------------------------------------------------+
+| config_drive           | Indicates if the config drive is supported on     |
+|                        | the instance (Boolean)                            |
++------------------------+---------------------------------------------------+
+| driver_private_data    | A dictionary of private data from the driver.     |
+|                        | This is driver specific and each driver can       |
+|                        | return information valuable for diagnosing VM     |
+|                        | issues. The raw data should versioned.            |
++------------------------+---------------------------------------------------+
+
+Note: A number of the above details are common to all drivers. These values
+will be filled in by the Nova compute manager prior to invoking the driver
+call. The ones that are virt driver specific will be filled, if possible, by
+the virt driver. If the virt driver is unable to provide a spcific field
+then that field will not be reported in the diagnostics.
+
+For example::
+
+    def get_instance_diagnostics(self, context, instance):
+        """Retrieve diagnostics for an instance on this host."""
+        current_power_state = self._get_power_state(context, instance)
+        if current_power_state == power_state.RUNNING:
+            LOG.audit(_("Retrieving diagnostics"), context=context,
+                      instance=instance)
+            diagnostics = {}
+            diagnostics['state'] = instance.vm_state
+            ...
+            driver_diags = self.driver.get_instance_diagnostics(instance)
+            diagnostics.update(driver_diags)
+            return diagnostics
+
+The cpu details will be an array of dictionaries per each virtual CPU.
+
++------------------------+---------------------------------------------------+
+| Key                    | Description                                       |
++========================+===================================================+
+| time                   | CPU Time in nano seconds (Integer)                |
++------------------------+---------------------------------------------------+
+
+The network details will be an array of dictionaries per each virtual NIC.
+
++------------------------+---------------------------------------------------+
+| Key                    | Description                                       |
++========================+===================================================+
+| mac_address            | Mac address of the interface (String)             |
++------------------------+---------------------------------------------------+
+| rx_octets              | Received octets (Integer)                         |
++------------------------+---------------------------------------------------+
+| rx_errors              | Received errors (Integer)                         |
++------------------------+---------------------------------------------------+
+| rx_drop                | Received packets dropped (Integer)                |
++------------------------+---------------------------------------------------+
+| rx_packets             | Received packets (Integer)                        |
++------------------------+---------------------------------------------------+
+| tx_octets              | Transmitted Octets (Integer)                      |
++------------------------+---------------------------------------------------+
+| tx_errors              | Transmit errors (Integer)                         |
++------------------------+---------------------------------------------------+
+| tx_drop                | Transmit dropped packets (Integer)                |
++------------------------+---------------------------------------------------+
+| tx_packets             | Transmit packets (Integer)                        |
++------------------------+---------------------------------------------------+
+
+The disk details will be an array of dictionaries per each virtual disk.
+
++------------------------+---------------------------------------------------+
+| Key                    | Description                                       |
++========================+===================================================+
+| id                     | Disk ID (String)                                  |
++------------------------+---------------------------------------------------+
+| read_bytes             | Disk reads in bytes(Integer)                      |
++------------------------+---------------------------------------------------+
+| read_requests          | Read requests (Integer)                           |
++------------------------+---------------------------------------------------+
+| write_bytes            | Disk writes in bytes (Integer)                    |
++------------------------+---------------------------------------------------+
+| write_requests         | Write requests (Integer)                          |
++------------------------+---------------------------------------------------+
+| errors_count           | Disk errors (Integer)                             |
++------------------------+---------------------------------------------------+
+
+The memory details is a dictionary.
+
++------------------------+---------------------------------------------------+
+| Key                    | Description                                       |
++========================+===================================================+
+| maximum                | Amount of memory provisioned for the VM in MB     |
+|                        | (Integer)                                         |
++------------------------+---------------------------------------------------+
+| used                   | Amount of memory used by the VM in MB (Integer)   |
++------------------------+---------------------------------------------------+
+
+Below is an example of the dictionary data returned by the fake driver::
+
+           {'state': 'running',
+            'driver': 'fake-driver',
+            'hypervisor_os': 'fake-os',
+            'uptime': 7,
+            'num_cpus': 1,
+            'num_vnics': 1,
+            'num_disks': 1,
+            'cpu_details': [{'time': 1024}]
+            'nic_details': [{'rx_octets': 0,
+                             'rx_errors': 0,
+                             'rx_drop': 0,
+                             'rx_packets': 0,
+                             'tx_octets': 0,
+                             'tx_errors': 0,
+                             'tx_drop': 0,
+                             'tx_packets': 0}],
+            'disk_details': [{'read_bytes':0,
+                              'read_requests': 0,
+                              'write_bytes': 0,
+                              'write_requests': 0,
+                              'errors_count': 0}],
+            'memory_details': {'maximum': 512, 'used': 256},
+            'driver_private_data': {'version': 1,
+                                    'memory': {'actual': 220160,
+                                               'rss': 200164}}
+
+Alternatives
+------------
+
+Continue with the same format that the V2 has. This is problematic as
+we are unable to build common user interface that can query VM states,
+for example in tempest.
+
+We can add an extension to the V2 API that will enable us to return
+the information defined in this spec.
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+The V3 diagnostics API will no longer return data defined by the
+driver but it will return common data defined in this spec.
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+It will make life easier - deployers will be able to get better insight into
+the state of VM and be able to troubleshoot.
+
+We should consider adding this support for V2. In order to support backward
+compatibility we can add a configuration flag. That is, we can
+introduce a flag for the legacy format.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  Gary Kotton - garyk
+
+Other contributors:
+  Bob Ball - bob-ball
+
+Work Items
+----------
+
+All work items were in review Icehouse. They were broken up as
+follows:
+
+* VM diagnostics (v3 API only)
+
+* XenAPI
+
+* libvirt
+
+* VMware
+
+Dependencies
+============
+
+None
+
+Testing
+=======
+
+Once the code is approved we will add tests to Tempest that will do the
+following for the V3 API (assuming that the underlying driver does
+not return NotImplemented (501), which may be the case if the driver
+does not support the method):
+
+* Check that the returned driver is one of the supported ones in tree (at
+  the moment only libvirt, vmware and xenapi support the v3 method).
+
+* Check that the number of CPU's matches the flavor.
+
+* Check that the disk data matches the flavor.
+
+* Check that the memory matches the flavor.
+
+* If a cinder volume has been attached then we check that there is the
+  correct amount of disks attached.
+
+* Check that the number of vNics matches the instance running.
+
+* If the private data is present then check that this is a dictionary and
+  has a key 'version'.
+
+In addition to this, if there are tests that fail then we can use the V3
+diagnostics to help debug. That is, we can get the diagnostics which may help
+isolate problems.
+
+Documentation Impact
+====================
+
+We can now at least document the fields that are returned and their meaning.
+
+If we do decide to update the v2 support we will need to update:
+
+Please also update:
+http://docs.openstack.org/admin-guide-cloud/content/instance_usage_statistics.html
+http://docs.openstack.org/user-guide/content/usage_statistics.html
+http://docs.openstack.org/user-guide/content/novaclient_commands.html
+http://docs.openstack.org/trunk/openstack-ops/content/lay_of_the_land.html#diagnose-compute
+
+We will need to make sure that we update all of the equivalent v3 docs.
+The information in the tables above will be what we add to the documentation.
+
+References
+==========
+
+https://wiki.openstack.org/wiki/Nova_VM_Diagnostics
+https://bugs.launchpad.net/nova/+bug/1240043
diff --git a/specs/juno/implemented/virt-driver-numa-placement.rst b/specs/juno/implemented/virt-driver-numa-placement.rst
new file mode 100644
index 0000000..6ba3239
--- /dev/null
+++ b/specs/juno/implemented/virt-driver-numa-placement.rst
@@ -0,0 +1,361 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+================================================
+Virt driver guest NUMA node placement & topology
+================================================
+
+https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
+
+This feature aims to enhance the libvirt driver to be able to do intelligent
+NUMA node placement for guests. This will increase the effective utilization
+of compute resources and decrease latency by avoiding cross-node memory
+accesses by guests.
+
+Problem description
+===================
+
+The vast majority of hardware used for virtualization compute nodes will
+exhibit NUMA characteristics. When running workloads on NUMA hosts it is
+important that the CPUs executing the processes are on the same node as the
+memory used. This ensures that all memory accesses are local to the NUMA node
+and thus not consumed the very limited cross-node memory bandwidth, which adds
+latency to memory accesses. PCI devices are directly associated with specific
+NUMA nodes for the purposes of DMA, so when using PCI device assignment it is
+also desirable that the guest be placed on the same NUMA node as any PCI device
+that is assigned to it.
+
+The libvirt driver does not currently attempt any NUMA placement, the guests
+are free to float across any host pCPUs and their RAM is allocated from any
+NUMA node. This is very wasteful of compute resources and increases memory
+access latency which is harmful for NFV use cases.
+
+If the RAM/vCPUs associated with a flavour are larger than any single NUMA
+node, it is important to expose NUMA topology to the guest so that the OS in
+the guest can intelligently schedule workloads it runs. For this to work the
+guest NUMA nodes must be directly associated with host NUMA nodes.
+
+Some guest workloads have very demanding requirements for memory access
+latency and/or bandwidth, which exceed that which is available from a
+single NUMA node. For such workloads, it will be beneficial to spread
+the guest across multiple host NUMA nodes, even if the guest RAM/vCPUs
+could theoretically fit in a single NUMA node.
+
+Forward planning to maximise the choice of target hosts for use with live
+migration may also cause an administrator to prefer splitting a guest
+across multiple nodes, even if it could potentially fit in a single node
+on some hosts.
+
+For these two reasons it is desirable to be able to explicitly indicate
+how many NUMA nodes to setup in a guest, and to specify how much RAM or
+how many vCPUs to place in each node.
+
+Proposed change
+===============
+
+The libvirt driver will be enhanced so that it looks at the resources available
+in each NUMA node and decides which is best able to run the guest. When
+launching the guest, it will tell libvirt to confine the guest to the chosen
+NUMA node.
+
+The compute driver host stats data will be extended to include information
+about the NUMA topology of the host and the availability of resources in the
+nodes.
+
+The scheduler will be enhanced such that it can consider the availability of
+NUMA resources when choosing the host to schedule on. The algorithm that the
+scheduler uses to decide if the host can run will need to be closely matched,
+if not identical to, the algorithm used by the libvirt driver itself. This
+will involve the creation of a new scheduler filter to match the flavor/image
+config specification against the NUMA resource availability reported by the
+compute hosts.
+
+The flavour extra specs will support the specification of guest NUMA topology.
+This is important when the RAM / vCPU count associated with a flavour is larger
+than any single NUMA node in compute hosts, by making it possible to have guest
+instances that span NUMA nodes. The compute driver will ensure that guest NUMA
+nodes are directly mapped to host NUMA nodes. It is expected that the default
+setup would be to not list any NUMA properties and just let the compute host
+and scheduler apply a sensible default placement logic. These properties would
+only need to be set in the sub-set of scenarios which require more precise
+control over the NUMA topology / fit characteristics.
+
+* hw:numa_nodes=NN - numa of NUMA nodes to expose to the guest.
+* hw:numa_mempolicy=preferred|strict - memory allocation policy
+* hw:numa_cpus.0=<cpu-list> - mapping of vCPUS N-M to NUMA node 0
+* hw:numa_cpus.1=<cpu-list> - mapping of vCPUS N-M to NUMA node 1
+* hw:numa_mem.0=<ram-size> - mapping N GB of RAM to NUMA node 0
+* hw:numa_mem.1=<ram-size> - mapping N GB of RAM to NUMA node 1
+
+The most common case will be that the admin only sets 'hw:numa_nodes' and then
+the flavour vCPUs and RAM will be divided equally across the NUMA nodes.
+
+The 'hw:numa_mempolicy' option allows specification of whether it is mandatory
+for the instance's RAM allocations to come from the NUMA nodes to which it is
+bound, or whether the kernel is free to fallback to using an alternative node.
+If 'hw:numa_nodes' is specified, then 'hw:numa_mempolicy' is assumed to default
+to 'strict'. It is useful to change it to 'preferred' when the 'hw:numa_nodes'
+parameter is being set to '1' to force disable use of NUMA by image property
+overrides.
+
+It should only be required to use the 'hw:numa_cpu.N' and 'hw:numa_mem.N'
+settings if the guest NUMA nodes should have asymetrical allocation of CPUs
+and RAM. This is important for some NFV workloads, but in general these will
+be rarely used tunables. If the 'hw:numa_cpu' or 'hw:numa_mem' settings are
+provided and their values do not sum to the total vcpu count / memory size,
+this is considered to be a configuration error. An exception will be raised
+by the compute driver when attempting to boot the instance. As an enhancement
+it might be possible to validate some of the data at the API level to allow
+for earlier error reporting to the user. Such checking is not a functional
+prerequisite for this work though so such work can be done out-of-band to
+the main development effort.
+
+When scheduling, if only the hw:numa_nodes=NNN property is set the scheduler
+will synthesize hw:numa_cpus.NN and hw:numa_mem.NN properties such that the
+flavour allocation is equally spread across the desired number of NUMA nodes.
+It will then look consider the available NUMA resources on hosts to find one
+that exactly matches the requirements of the guest. So, given an example
+config:
+
+* vcpus=8
+* mem=4
+* hw:numa_nodes=2 - numa of NUMA nodes to expose to the guest.
+* hw:numa_cpus.0=0,1,2,3,4,5
+* hw:numa_cpus.1=6,7
+* hw:numa_mem.0=3
+* hw:numa_mem.1=1
+
+The scheduler will look for a host with 2 NUMA nodes with the ability to run
+6 CPUs + 3 GB of RAM on one node, and 2 CPUS + 1 GB of RAM on another node.
+If a host has a single NUMA node with capability to run 8 CPUs and 4 GB of
+RAM it will not be considered a valid match. The same logic will be applied
+in the scheduler regardless of the hw:numa_mempolicy option setting.
+
+All of the properties described against the flavour could also be set against
+the image, with the leading ':' replaced by '_', as is normal for image
+property naming conventions:
+
+* hw_numa_nodes=NN - numa of NUMA nodes to expose to the guest.
+* hw_numa_mempolicy=strict|prefered - memory allocation policy
+* hw_numa_cpus.0=<cpu-list> - mapping of vCPUS N-M to NUMA node 0
+* hw_numa_cpus.1=<cpu-list> - mapping of vCPUS N-M to NUMA node 1
+* hw_numa_mem.0=<ram-size> - mapping N GB of RAM to NUMA node 0
+* hw_numa_mem.1=<ram-size> - mapping N GB of RAM to NUMA node 1
+
+This is useful if the application in the image requires very specific NUMA
+topology characteristics, which is expected to be used frequently with NFV
+images. The properties can only be set against the image, however, if they
+are not already set against the flavor. So for example, if the flavor sets
+'hw:numa_nodes=2' but does not set any 'hw:numa_cpus' / 'hw:numa_mem' values
+then the image can optionally set those. If the flavour has, however, set a
+specific property the image cannot override that. This allows the flavor
+admin to strictly lock down what is permitted if desired. They can force a
+non-NUMA topology by setting hw:numa_nodes=1 against the flavor.
+
+Alternatives
+------------
+
+Libvirt supports integration with a daemon called numad. This daemon can be
+given a RAM size + vCPU count and tells libvirt what NUMA node to place a
+guest on. It is also capable of shifting running guests between NUMA nodes to
+rebalance utilization. This is insufficient for Nova since it needs to have
+intelligence in the scheduler to pick hosts. The compute drivers then needs to
+be able to use the same logic when actually launching the guests. The numad
+system is not portable to other compute hypervisors. It does not deal with the
+problem of placing guests which span across NUMA nodes. Finally, it does not
+address the needs for NFV workloads which require guaranteed NUMA topology
+and placement policies, not merely dynamic best effort.
+
+Another alternative is to just do nothing, as we do today, and rely on the
+Linux kernel scheduler being enhanced to automatically place guests on
+appropriate NUMA nodes and rebalance them on demand. This shares most of the
+problems seen with using numad.
+
+Data model impact
+-----------------
+
+No impact.
+
+The reporting of NUMA topology will be integrated in the existing data
+structure used for host state reporting. This already supports arbitrary
+fields so no data model changes are anticipated for this part. This would
+appear as structured data
+
+::
+
+  hw_numa = {
+     nodes = [
+         {
+            id = 0
+            cpus = 0, 2, 4, 6
+            mem = {
+               total = 10737418240
+               free = 3221225472
+            },
+            distances = [ 10, 20],
+         },
+         {
+            id = 1
+            cpus = 1, 3, 5, 7
+            mem = {
+               total = 10737418240
+               free = 5368709120
+            },
+            distances = [ 20, 10],
+         }
+     ],
+  }
+
+To enable more efficient scheduling though, it would be desirable to also map
+NUMA topology into the extensible resource tracker schema. This would imply
+representing the hierarchical data above in a flattened format as a series of
+key, value pairs.
+
+* hw_numa_nodes=2
+* hw_numa_node0_cpus=4
+* hw_numa_node0_memtotal=10737418240
+* hw_numa_node0_memavail=3221225472
+* hw_numa_node0_distance_node0=10
+* hw_numa_node0_distance_node1=20
+* hw_numa_node1_cpus=4
+* hw_numa_node1_memtotal=10737418240
+* hw_numa_node1_memavail=5368709120
+* hw_numa_node1_distance_node0=20
+* hw_numa_node1_distance_node1=10
+
+
+REST API impact
+---------------
+
+No impact.
+
+The API for host state reporting already supports arbitrary data fields, so
+no change is anticipated from that POV. No new API calls will be required.
+
+Security impact
+---------------
+
+No impact.
+
+There are no new APIs involved which would imply a new security risk.
+
+Notifications impact
+--------------------
+
+No impact.
+
+There is no need for any use fo the notification system.
+
+Other end user impact
+---------------------
+
+Depending on the flavour chosen, the guest OS may see NUMA nodes backing its
+RAM allocation.
+
+There is no end user interaction in setting up NUMA policies of usage.
+
+The cloud administrator will gain the ability to set policies on flavours.
+
+Performance Impact
+------------------
+
+The new scheduler features will imply increased performance overhead when
+determining whether a host is able to fit the memory and vCPU needs of the
+flavour. ie the current logic which just checks the vCPU count and RAM
+requirement against the host free memory will need to take account of the
+availability of resources in specific NUMA nodes.
+
+Other deployer impact
+---------------------
+
+If the deployment has flavours whose RAM + vCPU allocations are larger than
+the size of the NUMA nodes in the compute hosts, the cloud administrator
+should strongly consider defining guest NUMA nodes in the flavour. This will
+enable the compute hosts to have better NUMA utilization and improve perf of
+the guest OS.
+
+Developer impact
+----------------
+
+The new flavour attributes could be used by any full machine virtualization
+hypervisor, however, it is not mandatory that they do so.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  berrange
+
+Other contributors:
+  ndipanov
+
+Work Items
+----------
+
+* Define a schema for reporting NUMA node resources and availability
+  for the host state API / extensible resource tracker
+* Enhance libvirt driver to report NUMA node resources & availability
+* Enhance libvirt driver to support setup of guest NUMA nodes.
+* Enhance libvirt driver to look at NUMA node availability when launching
+  guest instances and pin all guests to best NUMA node
+* Add support to schedular for picking hosts based on the NUMA availability
+  instead of simply considering the total RAM/vCPU availability.
+
+Dependencies
+============
+
+* The driver vCPU topology feature is a pre-requisite
+
+    https://blueprints.launchpad.net/nova/+spec/virt-driver-vcpu-topology
+
+* Supporting guest NUMA nodes will require completion of work in QEMU and
+  libvirt, to enable guest NUMA nodes to be pinned to specific host NUMA
+  nodes. In absence of libvirt/QEMU support, guest NUMA nodes can still be
+  used but it would not have any performance benefit, and may even hurt
+  performance.
+
+    https://www.redhat.com/archives/libvir-list/2014-June/msg00201.html
+
+* Extensible resource tracker
+
+  https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
+
+Testing
+=======
+
+There are various discrete parts of the work that can be tested in isolation
+of each other, fairly effectively using unit tests.
+
+The main area where unit tests might not be sufficient is the scheduler
+integration, where performance/scalability would be a concern. Testing the
+scalability of the scheduler in tempest though is not practical, since the
+issues would only become apparent with many compute hosts and many guests.
+ie a scale beyond that which tempest sets up.
+
+Documentation Impact
+====================
+
+The cloud administrator docs need to describe the new flavour parameters
+and make recommendations on how to effectively use them.
+
+The end user needs to be made aware of the fact that some flavours will cause
+the guest OS to see NUMA topology.
+
+References
+==========
+
+Current "big picture" research and design for the topic of CPU and memory
+resource utilization and placement. vCPU topology is a subset of this
+work
+
+* https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement
+
+OpenStack NFV team:
+
+* https://wiki.openstack.org/wiki/Teams/NFV
diff --git a/specs/juno/implemented/virt-driver-vcpu-topology.rst b/specs/juno/implemented/virt-driver-vcpu-topology.rst
new file mode 100644
index 0000000..f1f4110
--- /dev/null
+++ b/specs/juno/implemented/virt-driver-vcpu-topology.rst
@@ -0,0 +1,273 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=============================================
+Virt driver guest vCPU topology configuration
+=============================================
+
+https://blueprints.launchpad.net/nova/+spec/virt-driver-vcpu-topology
+
+This feature aims to give users and administrators the ability to control
+the vCPU topology exposed to guests. This enables them to avoid hitting
+limitations on vCPU topologies that OS vendors place on their products.
+
+Problem description
+===================
+
+When a guest is given multiple vCPUs, these are typically exposed in the
+hardware model as discrete sockets. Some operating system vendors will
+place artificial limits on the topologies that their product will support.
+So for example, a Windows guest may support 8 vCPUs only if it is exposed
+as 2 sockets with 4 cores each. If the vCPUs were exposed as 8 sockets
+with 1 core each, some of the vCPUs will be inaccessible to the guest.
+It is thus desirable to be able to control the mixture of cores and
+sockets exposed to the guest. The cloud administrator needs to be able
+to define topologies for flavours, to override the hypervisor defaults,
+such that commonly used OS' will not encounter their socket count limits.
+The end user also needs to be able to express preferences for topologies
+to use with their images.
+
+While the choice of sockets vs cores does not have a significant impact
+on performance, if a guest is given threads or is running on host OS
+CPUs which are thread siblings, this can have a notable performance impact.
+It only makes sense to expose a value of threads > 1 to a guest if all the
+guest vCPUs are strictly pinned to host pCPUs and some of the host pCPUs
+are thread siblings. While this blueprint will describe how to set the
+threads count, it will only make sense to set this to a value > 1 once
+the CPU pinning feature is integrated in Nova.
+
+If the flavour admin wishes to define flavours which avoid scheduling on
+hosts which have pCPUs with threads > 1, then can use scheduler aggregates
+to setup host groups.
+
+Proposed change
+===============
+
+The proposal is to add support for configuration of aspects of vCPU topology
+at multiple levels.
+
+At the flavour there will be the ability to define default parameters for the
+vCPU topology using flavour extra specs
+
+* hw:cpu_sockets=NN - preferred number of sockets to expose to the guest
+* hw:cpu_cores=NN - preferred number of cores to expose to the guest
+* hw:cpu_threads=NN - preferred number of threads to expose to the guest
+* hw:cpu_max_sockets=NN - maximum number of sockets to expose to the guest
+* hw:cpu_max_cores=NN - maximum number of cores to expose to the guest
+* hw:cpu_max_threads=NN - maximum number of threads to expose to the guest
+
+It is not expected that administrators will set all these parameters against
+every flavour. The simplest expected use case will be for the cloud admin to
+set "hw:cpu_max_sockets=2" to prevent the flavour exceeding 2 sockets. The
+virtualization driver will calculate the exact number of cores/sockets/threads
+based on the flavour vCPU count and this maximum sockets constraint.
+
+For larger vCPU counts there may be many possible configurations, so the
+"hw:cpu_sockets", "hw:cpu_cores", "hw:cpu_threads" parameters enable the
+cloud administrator to express their preferred choice from the large set.
+
+The "hw:max_cores" parameter allows the cloud administrator to place an upper
+limit on the number of cores used, which can be useful to ensure a socket
+count greater than 1 and thus enable a VM to be spread across NUMA nodes.
+
+The "hw:max_sockets", "hw:max_cores" & "hw:max_threads" settings allow the
+cloud admin to set mandatory upper limits on the permitted configurations
+that the user can override with properties against the image.
+
+At the image level the exact same set of parameters will be permitted,
+with the exception that image properties will use underscores throughout
+instead of an initial colon.
+
+* hw_cpu_sockets=NN - preferred number of sockets to expose to the guest
+* hw_cpu_cores=NN - preferred number of cores to expose to the guest
+* hw_cpu_threads=NN - preferred number of threads to expose to the guest
+* hw_cpu_max_sockets=NN - maximum number of sockets to expose to the guest
+* hw_cpu_max_cores=NN - maximum number of cores to expose to the guest
+* hw_cpu_max_threads=NN - maximum number of threads to expose to the guest
+
+If the user sets "hw_cpu_max_sockets", "hw_cpu_max_cores", or
+"hw_cpu_max_threads", these must be strictly lower than the values
+already set against the flavour. The purpose of this is to allow the
+user to further restrict the range of possible topologies that the compute
+host will consider using for the instance.
+
+The "hw_cpu_sockets", "hw_cpu_cores" & "hw_cpu_threads" values
+against the image may not exceed the "hw_cpu_max_sockets", "hw_cpu_max_cores"
+& "hw_cpu_max_threads" values set against the flavour or image. If the
+upper bounds are exceeded, this will be considered a configuration error
+and the instance will go into an error state and not boot.
+
+If there are multiple possible topology solutions implied by the set of
+parameters defined against the flavour or image, then the hypervisor will
+prefer the solution that uses a greater number of sockets. This preference
+will likely be further refined when integrating support for NUMA placement
+in a later blueprint.
+
+If the user wants their settings to be used unchanged by the compute
+host they should set "hw_cpu_sockets" == "hw_cpu_max_sockets",
+"hw_cpu_cores" == "hw_cpu_max_cores", and "hw_cpu_threads" ==
+"hw_cpu_max_threads" on the image. This will force use of the exact
+specified topology.
+
+Note that there is no requirement in this design or implementation for
+the compute host topologies to match what is being exposed to the guest.
+ie this will allow a flavour to be given sockets=2,cores=2 and still
+be used to launch instances on a host with sockets=16,cores=1. If the
+admin wishes to optionally control this, they will be able todo so by
+setting up host aggregates.
+
+The intent is to implement this for the libvirt driver, targeting QEMU /
+KVM hypervisors. Conceptually it is applicable to all other full machine
+virtualization hypervisors such as Xen and VMWare.
+
+Alternatives
+------------
+
+The virtualization driver could hard code a different default topology, so
+that all guest always use
+
+   cores==2, sockets==nvcpus/cores
+
+instead of
+
+   cores==1, sockets==nvcpus
+
+While this would address the immediate need of current Windows OS', this is
+not likely to be sufficiently flexible for the longer term, as it forces all
+OS into using cores, even if they don't have any similar licensing
+restrictions. The over-use of cores will limit the ability to do an effective
+job at NUMA placement, so it is desirable to use cores as little as possible.
+
+The settings could be defined exclusively against the images, and not make
+any use of flavour extra specs. This is undesirable because to have best
+NUMA utilization, the cloud administrator will need to be able to constrain
+what topologies the user is allowed to use. The administrator would also
+like to have the ability to set up define behaviour so that guest can get
+a specific topology without requiring every single image uploaded to glance
+to be tagged with the same repeated set of properties.
+
+A more fine grained configuration option would be to allow the specification
+of the core and thread count for each separate socket. This would allow for
+asymmetrical topologies eg
+
+  socket0:cores=2,threads=2,socket1:cores=4,threads=1
+
+It is noted, however, that at time of writing, no virtualization technology
+provides any way to configure such asymmetrical topologies. Thus Nova is
+better served by ignoring this purely theoretical possibility and keeping
+its syntax simpler to match real-world capabilities that already exist.
+
+Data model impact
+-----------------
+
+No impact.
+
+The new properties will use the existing flavour extra specs and image
+property storage models.
+
+REST API impact
+---------------
+
+No impact.
+
+The new properties will use the existing flavour extra specs and image
+property API facilities.
+
+Security impact
+---------------
+
+The choice of sockets vs cores can have an impact on host resource utilization
+when NUMA is involved, since over use of cores will prevent a guest being
+split across multiple NUMA nodes. This feature addresses this by allowing the
+flavour administrator to define hard caps, and ensuring the flavour will
+always take priority over the image settings.
+
+Notifications impact
+--------------------
+
+No impact.
+
+There is no need for this feature to integrate with notifications.
+
+Other end user impact
+---------------------
+
+The user will gain the ability to control aspects of the vCPU topology used
+by their guest. They will achieve this by setting image properties in glance.
+
+Performance Impact
+------------------
+
+The cores vs sockets vs threads decision does not involve any scheduler
+interaction, since this design is not attempting to match host topology
+to guest topology. A later blueprint on CPU pinning will make it possible
+todo such host to guest topology matching, and its performance impact
+will be considered there.
+
+Other deployer impact
+---------------------
+
+The flavour extra specs will gain new parameters in extra specs which a
+cloud administrator can choose to use. If none are set then the default
+behaviour is unchanged from previous releases.
+
+Developer impact
+----------------
+
+The initial implementation will be done for libvirt with QEMU/KVM. It should
+be possible to add support for using the cores/sockets/threads parameters in
+the XenAPI and VMWare drivers.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+
+  berrange
+
+Work Items
+----------
+
+* Provide helper methods against the computer driver base class for
+  calculating valid CPU topology solutions for the given hw_cpu_* parameters.
+* Add Libvirt driver support for choosing a CPU topology solution based on
+  the given hw_cpu_* parameters.
+
+Dependencies
+============
+
+No external dependencies
+
+Testing
+=======
+
+No tempest changes.
+
+The mechanisms for the cloud administrator and end user to set parameters
+against the flavour and/or image are already well tested. The new
+functionality focuses on interpreting the parameters and setting corresponding
+libvirt XML parameters. This is something that is effectively covered by the
+unit testing framework.
+
+Documentation Impact
+====================
+
+The new flavour extra specs and image properties will need to be documented.
+Guidance should be given to cloud administrators on how to make most
+effective use of the new features. Guidance should be given to the end user
+on how to use the new features to address their use cases.
+
+References
+==========
+
+Current "big picture" research and design for the topic of CPU and memory
+resource utilization and placement. vCPU topology is a subset of this
+work
+
+* https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement
diff --git a/specs/juno/implemented/virt-objects-juno.rst b/specs/juno/implemented/virt-objects-juno.rst
new file mode 100644
index 0000000..dcdfe18
--- /dev/null
+++ b/specs/juno/implemented/virt-objects-juno.rst
@@ -0,0 +1,174 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Virt Driver Objects Support (Juno Work)
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/virt-objects-juno
+
+This blueprint represents the remaining work to be done in Juno around
+moving the virt drivers to using objects instead of raw conductor
+methods. This is important because objects provide versioning of the
+actual data, which supports our upgrade goals.
+
+Problem description
+===================
+
+Nova virt drivers still send and receive unversioned bundles of data
+using conductor methods, which is problematic during an upgrade where
+the format of the data has changed across releases.
+
+Proposed change
+===============
+
+Migrate uses of raw conductor methods in the virt drivers to
+objects. For example, consider this::
+
+  instance = conductor.instance_get_by_uuid(context, uuid)
+  conductor.instance_update(context, instance['uuid'],
+                            host='foo')
+
+would become::
+
+  instance = instance_obj.Instance.get_by_uuid(context, uuid)
+  instance.host = 'foo'
+  instance.save()
+
+Using the objects mechanism allows older code to interact with newer
+code, backleveling the format of the instance object as necessary.
+
+Alternatives
+------------
+
+This is the accepted direction of the project to solve this
+problem. However, alternatives would be:
+
+1. Don't solve the problem and continue using unversioned data
+2. Attempt to enforce version bumps of individual methods when any
+   data (including nested downstream data) has changed
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+In general, conversion of code to use objects does not affect
+notifications. However, at times, emission of notifications is
+embedded into an object method to achieve higher consistency about
+when and how the notifications are sent. No such changes are
+antitipated in this work, but it's always a possibility.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None.
+
+Other deployer impact
+---------------------
+
+Moving to objects enhances the ability for deployers to incrementally
+roll out new code. It is, however, largely transparent for them.
+
+Developer impact
+----------------
+
+This is normal refactoring, so the impact is minimal. In general,
+objects-based code is easier to work with, so long-term it is a win
+for the developers.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  danms
+
+Work Items
+----------
+
+The following virt driver methods still need attention:
+
+* attach_volume
+* check_can_live_migrate_destination
+* check_can_live_migrate_source
+* check_instance_shared_storage_local
+* cleanup
+* default_device_names_for_instance
+* default_root_device_name
+* destroy
+* detach_volume
+* dhcp_options_for_instance
+* ensure_filtering_rules_for_instance
+* get_diagnostics
+* get_info
+* get_volume_connector
+* inject_file
+* inject_network_info
+* live_migration
+* macs_for_instance
+* post_live_migration
+* pre_live_migration
+* refresh_instance_security_rules
+* reset_network
+* rollback_live_migration_at_destination
+* unfilter_instance
+* unplug_vifs
+
+
+Dependencies
+============
+
+There is a cross-dependency between this blueprint and the following:
+
+  https://blueprints.launchpad.net/nova/+spec/compute-manager-objects-juno
+
+At times, a virt driver will need to be passed an object by the
+compute manager, and thus finishing the conversion of a virt driver
+method requires the calling compute manager method to be converted as
+well.
+
+
+Testing
+=======
+
+In general, unit tests require minimal change when this happens,
+depending on how the tests are structured. Ideally, they are already
+mocking out database calls, which means the change to objects is a
+transparent one. In reality, this usually means minor tweaking to the
+tests to return whole data models, etc.
+
+Documentation Impact
+====================
+
+None.
+
+References
+==========
+
+* https://blueprints.launchpad.net/nova/+spec/virt-objects
+* https://blueprints.launchpad.net/nova/+spec/compute-manager-objects
+* https://blueprints.launchpad.net/nova/+spec/unified-object-model
diff --git a/specs/juno/implemented/vmware-hot-plug.rst b/specs/juno/implemented/vmware-hot-plug.rst
new file mode 100644
index 0000000..203d1ac
--- /dev/null
+++ b/specs/juno/implemented/vmware-hot-plug.rst
@@ -0,0 +1,111 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===============================
+VMware: support for vif hotplug
+===============================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-hot-plug
+
+Support for hotpluging virtual network cards into instances.
+
+Problem description
+===================
+
+Support for hotpluging virtual network cards into instances has already
+been implemented in the libvirt driver:
+https://blueprints.launchpad.net/nova/+spec/network-adapter-hotplug
+
+The plan is to add the same support into the VMware driver.
+
+Proposed change
+===============
+
+Implement the methods attach_interface and detach_interface in the VMware
+driver.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+A user will now be able to add or remove interfaces from an instance that is
+run by the VMware driver. The new nic will be added ore removed when the action
+takes place and does not require rebooting the guest.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+Feature parity.
+
+Developer impact
+----------------
+
+None
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+
+Primary assignee:
+  Gary Kotton<gkotton@vmware.com>
+
+Work Items
+----------
+
+Code was posted in Icehouse - https://review.openstack.org/#/c/59365/
+
+Dependencies
+============
+
+Common VIF parameters were added - https://review.openstack.org/#/c/72292/
+
+Testing
+=======
+
+Unit tests and 3rd party testing. Note that the feature is only supported with
+Neutron at the moment.
+
+Documentation Impact
+====================
+
+Remove limitation that this is only supported with libvirt.
+
+References
+==========
+
+None
diff --git a/specs/juno/implemented/vmware-spawn-refactor.rst b/specs/juno/implemented/vmware-spawn-refactor.rst
new file mode 100644
index 0000000..37d2f74
--- /dev/null
+++ b/specs/juno/implemented/vmware-spawn-refactor.rst
@@ -0,0 +1,175 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=====================
+VMware Spawn Refactor
+=====================
+
+https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
+
+A structured refactor of the VMware driver spawn utility code to create a more
+cohesive and coherent whole.
+
+
+Problem description
+===================
+
+The VMware driver's spawn utility method is over 500 lines of code and very
+difficult to follow. It features redundant logic in several places as well as
+a general lack of cohesive constructs for a programmer to follow. Tests of the
+spawn method involve complicated test frameworks that require a developer or
+reviewer to hold important context between different seemingly unrelated
+modules in their heads. While test coverage is actually quite good on the
+spawn method, it can be very difficult to comprehend how a test functions and
+comprehending this complexity slows reviews.
+
+* create a spawn method that composes utility methods
+
+* improve readability
+
+* provide encapsulation
+
+* separate model code from action code for easier maintenance
+
+* make tests more understandable to reviewers and test coverage easier to see
+
+
+Proposed change
+===============
+
+* Extract inner methods and create reusable and testable methods
+
+  * allow for simple mocking in tests to easily cover all paths
+
+  * create easier to follow test cases with shallower call depths
+
+* Consolidate vSphere image properties for easier use and testing
+
+  * NOTE: for the scope of this blueprint we examine only existing configs
+
+  * include checks for valid values for use in vSphere API before transmiting
+    to vSphere where possible. Pre-checking values will make it easier to
+    diagnose a driver fault.
+
+* Identify and extract additional utilities and methods hidden in spawn
+
+  * large sections of spawn are repeated in other utilities (stop that)
+
+  * identify image actions and create utilities for those
+
+Alternatives
+------------
+
+* continue to add to the existing method
+
+* expand fake.py into a full blown vCenter simulator
+
+* only change code as it pertains to new features or bugs
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None. This is a zero new feature blueprint.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+
+Performance Impact
+------------------
+
+None or negligible. Some early work has determined that there are multiple
+network round trips to glance that do not need to occur, but performance
+changes will be an expected side-effect of the refactoring work.
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+- Sanity preservation: consolidation and refactoring of driver logic will make
+  an easier to follow driver that will make addition of new features easier.
+
+- Simplified testing, smaller units of code means more granular tests and
+  easier to follow test structure.
+
+- Introduction of better practice, this code will serve as a positive example
+  for future contributions.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  hartsock
+
+Other contributors:
+  vui
+  rhsu
+  tjones-i
+  garyk
+  maithem
+
+Work Items
+----------
+
+* extract inner methods from spawn
+
+* consolidate VMware specific image configurations
+
+  * identify parameters set in image metadata and formalize them
+
+  * identify values that control current behavior and isolate them
+
+* refactor image file manipulation into a set of re-usable utilities
+
+
+Dependencies
+============
+
+None.
+
+
+Testing
+=======
+
+Standard Minesweeper testing should reveal if this refactor has not regressed
+any features and will cover all cases this code will refactor.
+
+
+Documentation Impact
+====================
+
+None. Internal developer documentation will be greatly improved.
+
+
+References
+==========
+
+* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
\ No newline at end of file
diff --git a/specs/juno/input-output-based-numa-scheduling.rst b/specs/juno/input-output-based-numa-scheduling.rst
deleted file mode 100644
index 6c9ce77..0000000
--- a/specs/juno/input-output-based-numa-scheduling.rst
+++ /dev/null
@@ -1,183 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-================================
-I/O (PCIe) based NUMA scheduling
-================================
-
-https://blueprints.launchpad.net/nova/+spec/input-output-based-numa-scheduling
-
-I/O based NUMA scheduling will add support for intelligent NUMA node placement
-for guests that have been assigned a host PCI device, avoiding unnecessary
-memory transactions
-
-Problem description
-===================
-
-Currently it is common for virtualisation host platforms to exhibit multi NUMA
-node characteristics.
-
-An optimal configuration would be where the guests assigned PCI device and RAM
-allocation are associated with the same NUMA node. This will ensure there is
-no cross NUMA node memory traffic.
-
-To reach a remote NUMA node the memory request must traverse the inter CPU
-link and use the remote NUMA nodes associated memory controller to access the
-remote node. This incurs a latency penalty on remote NUMA node memory access
-which is not desirable for NFV workloads.
-
-Openstack needs to offer more fine grained control of NUMA configuration in
-order to deliver higher performing, lower latency guest applications. The
-default guest placement policy is to use any available pCPU or NUMA node.
-
-Proposed change
-===============
-
-Libvirt now provides the numa node a PCI device is associated with, we will
-use this information to populate the nova DB. For versions of libvirt that do
-not provide this information we will add a fall back mechanism to query the
-host for this info.
-
-Logic will be added to nova scheduler to allow it decide on which host is best
-able satisfy a guests PCI NUMA node requirements.
-
-Logic, similar to what will be implemented in the nova scheduler will be added
-to the libvirt driver to allow it decide on which NUMA node to place the guest.
-
-Alternatives
-------------
-
-Libvirt supports integration with a NUMA daemon (numad) that monitors NUMA
-topology and usage. It attempts to locate guests for optimum NUMA locality,
-dynamically adjusting to changing system conditions.
-
-This is insufficient because we need this intelligence in nova for host
-selection and node deployment.
-
-Data model impact
------------------
-
-The PciDevice model will be extended to add a field identifying the NUMA node
-that PCI device is associated with.
-
-numa_node = Column(Integer, nullable=False, default="-1")
-
-A DB migration script will use ALTER_TABLE to add a new column to the
-pci_devices table in nova DB.
-
-REST API impact
----------------
-
-There will be no change to the REST API.
-
-Security impact
----------------
-
-This blueprint does not introduce any new security issues.
-
-Notifications impact
---------------------
-
-This blueprint does not introduce new notifications.
-
-Other end user impact
----------------------
-
-This blueprint adds no other end user impact.
-
-Performance Impact
-------------------
-
-The benefits of associating a guests PCI device and RAM allocation with the
-same NUMA node will provides an optimal configuration that will give improved
-I/O throughput and reduced memory latencies, compared with the default libvirt
-guest placement policy.
-
-This feature will add some scheduling overhead, but this overhead will deliver
-improved performance on the host.
-
-The optimisation described here is dependent on the guest CPU and RAM
-allocation being associated with the same NUMA node. This feature is described
-in the "Virt driver guest NUMA node placement & topology" blueprint referenced
-in the dependency section.
-
-Other deployer impact
----------------------
-
-To use this feature the deployer must use HW that is capable of reporting
-numa related info to the OS.
-
-Developer impact
-----------------
-
-This blueprint will have no developer impact.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-    James Chapman
-
-Other contributors:
-    Przemyslaw Czesnowicz
-    Sean Mooney
-    Adrian Hoban
-
-Work Items
-----------
-
-* Add a NUMA node attribute to the pci_device object
-* Use libvirt to discover hosts PCI device NUMA node association
-* Enable nova compute synchronise PCI device NUMA node associations with nova
-  DB
-* Enable libvirt driver configure guests with requested PCI device NUMA node
-  associations
-* Enable the nova scheduler decide on which host is best able to support
-  a guest
-* Enable libvirt driver decide on which NUMA node to place a guest
-
-Dependencies
-============
-
-The blueprint listed below will define a policy used by the scheduler to decide
-on which host to place a guest. We plan to respect this policy while extending
-it to add support for a PCI devices NUMA node association.
-
-Virt driver guest NUMA node placement & topology
-* https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
-
-The blueprint listed below will support use cases requiring SR-IOV NICs to
-participate in neutron managed networks.
-
-Enable a nova instance to be booted up with neutron SRIOV ports
-* https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
-
-Testing
-=======
-
-Scenario tests will be added to validate these modifications.
-
-Documentation Impact
-====================
-
-This feature will not add a new scheduling filter, but as it depends on the bp
-mentioned in the dependency section we will need to extend their filter. We
-will add documentation as required.
-
-References
-==========
-
-Support for NUMA and VCPU topology configuration
-* https://blueprints.launchpad.net/nova/+spec/virt-driver-guest-cpu-memory-placement
-
-Virt driver guest NUMA node placement & topology
-* https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
-
-Enable a nova instance to be booted up with neutron SRIOV ports
-* https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
diff --git a/specs/juno/instance-network-info-hook.rst b/specs/juno/instance-network-info-hook.rst
deleted file mode 100644
index c640a59..0000000
--- a/specs/juno/instance-network-info-hook.rst
+++ /dev/null
@@ -1,126 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===============================================
-Add hook for update_instance_cache_with_nw_info
-===============================================
-
-https://blueprints.launchpad.net/nova/+spec/instance-network-info-hook
-
-A hook of the update_instance_cache_with_nw_info call will allow hooks access
-to valuable network information as soon as it becomes available. This will be
-useful for sending this data to scripts that can make informed tweaks to the
-networking on hosts.
-
-Problem description
-===================
-
-Right now there is no way to hook into the updating of network info.
-
-Usecase:
-* Deployer would be able to register a hook to send networking information
-to a script that could make informed tweaks to networking on hosts. This
-might include flows or QoS.
-
-
-Proposed change
-===============
-
-Add a hook to the update_instance_cache_with_nw_info call to allow hooks
-access to this information.
-
-Alternatives
-------------
-
-This information is stored in the database, and could be accessed from there.
-But, this would require giving access to the database to outside applications
-and could potentitally increase load on the database.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-The new code itself will not introduce any performance impact, but due to the
-nature of hooks, any deployer introduced hooks could have a performance impact.
-It will be up to the deployer to test their hooks for performance impact.
-
-Other deployer impact
----------------------
-
-This change will introduce a new location for hooks. It will not immediately
-effect a deployment, as new hooks would need to be introduced that would
-take advantage of this new hook location.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  andrew-melton
-
-Other contributors:
-  None
-
-Work Items
-----------
-
-* Register a new hook for update_instance_cache_with_nw_info
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Unit testing to verify that you can register a hook for
-update_instance_cache_with_nw_info should be sufficient. The functional
-testing of the actual hooks should be left to the deployer.
-
-
-Documentation Impact
-====================
-
-If there is a list of hook locations, it will need to be updated to include
-this new location.
-
-References
-==========
-
-* Dev docs on nova hooks: http://docs.openstack.org/developer/nova/devref/hooks.html
-
diff --git a/specs/juno/io-ops-weight.rst b/specs/juno/io-ops-weight.rst
deleted file mode 100644
index 849897f..0000000
--- a/specs/juno/io-ops-weight.rst
+++ /dev/null
@@ -1,132 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=====================================
-Create Nova Scheduler IO Ops Weighter
-=====================================
-
-https://blueprints.launchpad.net/nova/+spec/io-ops-weight
-
-Add a new nova scheduler weighter, sort the filter hosts according to host io
-ops number, aims to booting instances on light workload hosts.
-
-
-Problem description
-===================
-
-Currently, Nova scheduler can use host ram or metrics as hosts weight to choice
-host to booting instance, but have a large free ram host maybe have this many
-or more instances currently in build, resize, snapshot, migrate, rescue or
-unshelve task states, especially in some cases of the ram resource of compute
-hosts is very uneven. For example, We had two compute hosts, they had large
-enough free ram(hostA:64G and hostB:10G) to booting instances, by default Nova
-scheduler always choose hostA to booting instance and don't consider the
-concurrent instance task. The io_ops_filter can filter out the heavy workload
-hosts, but it can't help us to choose a most free compute host to booting.
-Using CONF.scheduler_host_subset_size can spread instances on suitable randomly
-compute hosts, but we think it's better that consider instance io ops as weight
-value.
-
-
-Proposed change
-===============
-
-Create a new scheduler weighter class 'IoOpsWeigher', use host_state.num_io_ops
-as weigh_object. Add a new CONF.io_ops_weight_multiplier, default value is
--1.0.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-The new code itself will introduce some performance impact, new scheduler
-weighter 'IoOpsWeigher' add new calculation logic about hosts weight value.
-Direct use of the attribute 'num_io_ops' of HostState will not bring a big
-performance impact.
-
-Other deployer impact
----------------------
-
-* Add a new weighter class 'IoOpsWeighter', it takes effect by default.
-* Add a new config option CONF.io_ops_weight_multiplier in nova.conf, default
-  value is -1.0, positive numbers mean to prior choose heavy workload compute
-  hosts.
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  <kiwik-chenrui>
-
-Work Items
-----------
-
-* Add new weighter class 'IoOpsWeighter'.
-* Add some unit tests and tempest.
-
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-New unit tests and tempest about 'IoOpsWeighter' will be added.
-
-
-Documentation Impact
-====================
-
-The docs about 'IoOpsWeighter' need to be drafted and new config option
-'io_ops_weight_multiplier' in nova.conf should be introduced, default value is
--1.0, negative numbers mean to preference choose light workload compute hosts,
-positive numbers mean to the opposite thing.
-
-
-References
-==========
-
-None
diff --git a/specs/juno/juno-slaveification.rst b/specs/juno/juno-slaveification.rst
deleted file mode 100644
index 1cab37c..0000000
--- a/specs/juno/juno-slaveification.rst
+++ /dev/null
@@ -1,157 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-More periodic tasks to slave for Juno
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/juno-slaveification
-
-In the Icehouse development cycle we gave deployers the option to offload
-most reads from nova-compute periodic tasks to a DB replication slave.
-We will continue this work in Juno by "slaveifying" the rest of the
-periodic tasks where appropriate.
-
-Problem description
-===================
-
-Currently the accepted way to scale the database for reads and writes in Nova
-is to do a multi-master setup or use some sort of database clustering. The
-problem with this approach is that while read scalability is potentially
-increased by making more hardware resources available (CPU, RAM, iops, etc).
-Write scalability is decreased and more operational complexity is inherited.
-
-Proposed change
-===============
-
-I would like to continue the work done in Icehouse by completing the
-"slaveification" of periodic tasks.
-
-Alternatives
-------------
-
-There are alternative ways to scale reads and writes both:
-
--Handling scaling within the application through some sort of sharding scheme.
--Handle scaling at the DB level.
-
-We have a sharding model, cells, in Nova currently. It could be argued that
-time would be better spent improving this approach rather than spending time
-trying to scale it using available DB technologies.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-No negative changes, hopefully this allows us to take some load off of
-a "write master" and offload them to a slave or slaves.
-
-Other deployer impact
----------------------
-
-If a deployer changes the slave_connection configuration parameter in the
-database section it is assumed that they are accepting the behavior of
-having all reads from periodic tasks be sent to that connection. The
-deployer needs to be educated and aware of the implication of running a
-database replication slave and fetching actionable data from said slave.
-These include, but may not be limited to:
-
--Need for monitoring of the slave status
--Operational staff familiar with maintenance of replication slaves
--Possibility to operate on data that is slightly out of date
-
-See https://wiki.openstack.org/wiki/Slave_usage
-
-
-Developer impact
-----------------
-
-Developers should consider which reads might benefit from optionally using
-a slave handle. When new reads are introduced, consider the context in which
-the code is called. Will it matter if this code operates on possibly out of
-date data? Is the benefit of offloading reads greater than an inconvenience
-caused by acting on old data?
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  <geekinutah>
-
-Other contributors:
-  <None>
-
-Work Items
-----------
-
-Slaveify the following periodic tasks in nova/compute/manager.py
-
-update_available_resource
-_run_pending_deletes
-_instance_usage_audit
-_poll_bandwidth_usage
-_poll_volume_usage
-
-Dependencies
-============
-
-We will need to have an object for bw_usage, this is covered by
-https://blueprints.launchpad.net/nova/+spec/compute-manager-objects-juno
-
-Testing
-=======
-
-Currently there is no testing in Tempest for reads going to the alternate
-slave handle. We should add a replication slave to our test runs and test
-the periodic tasks with and without slave_connection enabled.
-
-Documentation Impact
-====================
-
-The operations guide should be updated and provide instructions with references
-to MySQL and Postgres documentation on setting up and maintaining slaves. We
-should also talk about HA possibilities with asynchronous slaves and various
-automation frameworks that deal with this problem. It would also be good to
-explain that while being able to specify a slave_connection is primarily a
-scaling feature, the ability to use it for availability purposes is there.
-
-References
-==========
-
-https://wiki.openstack.org/wiki/Slave_usage
-
-The original blueprint with code history and discussion:
-https://blueprints.launchpad.net/nova/+spec/db-slave-handle
-
-The Icehouse blueprint:
-https://blueprints.launchpad.net/nova/+spec/periodic-tasks-to-db-slave
diff --git a/specs/juno/libvirt-disk-discard-option.rst b/specs/juno/libvirt-disk-discard-option.rst
deleted file mode 100644
index 30faea9..0000000
--- a/specs/juno/libvirt-disk-discard-option.rst
+++ /dev/null
@@ -1,147 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-======================================================
-Libvirt-Enable support discard option for disk device
-======================================================
-
-https://blueprints.launchpad.net/nova/+spec/libvirt-disk-discard-option
-
-Most SCSI devices have supported UNMAP command which is used to return unused
-or freed blocks back to the storage. And SSD drive have a similar command
-called "Trim" command.
-
-This blueprint aims to support setting discard option for instance's disk.
-If discard option is enabled, unused/freed blocks can be return back to the
-storage device.
-
-Qemu1.5 has supported setting discard option for the raw disk. In Qemu1.6,
-qcow2 file supported discard option too.
-
-Cinder support is out of scope for this spec. This spec only covers disks
-managed by nova.
-
-Problem description
-===================
-
-Thin provision volume
----------------------
-When we write data to a thin provision volume, storage device will allocate
-blocks to it and it will grow.
-But without discard option supported, the blocks will not be freed to the
-storage device even if we deleted the data in the volume. The result is the
-thin volume can grow, but can't shrink.
-With discard option supported, when user deleted the data in the volume,
-the blocks will be freed to the storage device. The volume will shrink.
-
-It's useful for both ephemeral volume and cinder volume.
-
-SSD backed volume
---------------------
-Freeing the unused blocks to the SSD storage is useful to improving the
-performance and prolonging the lifetime of SSD.
-
-Proposed change
-===============
-
-Add support for the deployer to specify the discard option in the nova.conf by
-"hw_disk_discard".
-
-There are two available values for "hw_disk_discard" now::
-
-  "unmap" : Discard requests("trim" or "unmap") are passed to the filesystem.
-  "ignore": Discard requests("trim" or "unmap") are ignored and aren't passed
-  to the filesystem.
-
-For example::
-
-  hw_disk_discard=unmap    #enable discard
-  hw_disk_discard=ignore   #disable discard, by default
-
-For an instance running on a host which has the discard option in nova.conf,
-nova will produce the XML with a discard option when the nova managed disk
-is attached.
-
-Alternatives
-------------
-None
-
-Data model impact
------------------
-None
-
-REST API impact
----------------
-None
-
-Security impact
----------------
-None
-
-Notifications impact
---------------------
-None
-
-Other end user impact
----------------------
-None
-
-Performance Impact
-------------------
-Discard option maybe cause performance degradation and fragmentation.
-But for the storage based on SSD, discard option is good for the performance.
-
-The users have control over this behaviour in the guest os if they don't want
-it. They can use the mount parameter or the command tools to control the
-discard behaviour.
-
-Other deployer impact
----------------------
-Initially, only the libvirt driver will support this function, and
-only with qemu/kvm as the hypervisor.
-
-A serious consideration is needed before enabling discard option.
-With discard option enabled, the freed blocks of the thin provision volume
-will be return to the storage and can be reused. But it also maybe cause
-performance degradation and fragments.
-So it's reasonable to enable the discard option only when you use the thin
-provision volume and the storage are UNMAP/TRIM-capable. For example the SSD,
-the disk-arrays or other distributed storage which supports the UNMAP/TRIM.
-
-Developer impact
-----------------
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-boh.ricky
-
-Work Items
-----------
-
-* Libvirt driver will create a discard option for a disk device which the
-  instance flavor has discard option.
-
-Dependencies
-============
-Libvirt(1.0.6) Qemu1.5(raw format) Qemu1.6(qcow2 format)
-
-Testing
-=======
-None
-
-Documentation Impact
-====================
-
-The document should be modified to reflect this new feature.
-
-References
-==========
-None
diff --git a/specs/juno/libvirt-domain-listing-speedup.rst b/specs/juno/libvirt-domain-listing-speedup.rst
deleted file mode 100644
index a73a149..0000000
--- a/specs/juno/libvirt-domain-listing-speedup.rst
+++ /dev/null
@@ -1,139 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-============================================
-Speedup listing of domains in libvirt driver
-============================================
-
-https://blueprints.launchpad.net/nova/+spec/libvirt-domain-listing-speedup
-
-The libvirt driver currently uses the legacy libvirt APIs for getting
-lists of domains. These are inefficient and prone to race conditions,
-so have been replaced by much better designed APIs.
-
-Problem description
-===================
-
-The libvirt driver in Nova currently uses a combination of calls to
-numOfDomains, listDomainsID, numOfDefinedDomains, listDefinedDomains,
-lookupByID and lookupByName to list domains on a host. This is very
-inefficient as it requires O(N) libvirt API calls to list 'N' guests.
-It also has designed in race condition where Nova can loose a guest
-if it transitions from shutoff to running while the list of domains
-is being fetched.
-
-The 0.9.13 version of libvirt introduced a new method listAllDomains
-which can be used to replace all those calls with a single API call,
-thus providing a way to get a list of domains which has constant
-execution time regardless of how many domains there are.
-
-Proposed change
-===============
-
-A new method 'list_instance_domains' will be introduced that will
-attempt to use the listAllDomains method to fetch the list of domains,
-and fallback to using the old method if it is not supported by the
-libvirt version or the hypervisor driver in use.
-
-Rather than just returning a list of domain IDs, names or UUIDs,
-it will return a list of libvirt.virDomain object instances avoiding
-the need todo separate lookups.
-
-By default it will only return running instances which were originally
-launched by Nova. It can be optionally told to include inactive
-instances, instances launched by other systems (eg libguestfs) or
-the Xen Domain-0 instance.
-
-Everywhere in the libvirt driver which calls list_instances or
-list_instance_ids will then be changed to use this new method, thus
-significantly improving their scalability.
-
-Alternatives
-------------
-
-Continue to use current APIs, but this is inefficient and prone to
-race conditions, so not at all desirable
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-It will improve the performance of the libvirt driver when used against
-libvirt >= 0.9.13, particularly when there are lots of instances of the
-host.
-
-Other deployer impact
----------------------
-
-Deployers are strongly recommended to use libvirt >= 0.9.13 to take
-advantage of the performance improvements.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  berrange
-
-Work Items
-----------
-
- - Implement new list_instance_domains method
- - Write test cases for list_instance_domains
- - Convert libvirt driver to use list_instance_domains
-
-Dependencies
-============
-
-* Current min libvirt is 0.9.6, but this requires 0.9.13. Fallback
-  code will be provided for use with 0.9.6 versions.
-
-Testing
-=======
-
-The current tempest gate tests should fully exercise the new code
-paths.
-
-Documentation Impact
-====================
-
-Recommend deployment of libvirt >= 0.9.13 for best scalability.
-
-References
-==========
-
-None
diff --git a/specs/juno/libvirt-driver-class-refactor.rst b/specs/juno/libvirt-driver-class-refactor.rst
deleted file mode 100644
index d626a33..0000000
--- a/specs/juno/libvirt-driver-class-refactor.rst
+++ /dev/null
@@ -1,222 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-================================
-Libvirt driver class refactoring
-================================
-
-https://blueprints.launchpad.net/nova/+spec/libvirt-driver-class-refactor
-
-The libvirt driver.py class is growing ever larger and more complicated.
-There are circular dependencies between this class and other libvirt
-classes. This work aims to split some of the functionality out into
-new classes
-
-Problem description
-===================
-
-The libvirt driver.py class is growing ever larger over time. This is
-increasing the complexity of the code and also resulting in larger
-test suites.
-
-The driver.py class is serving what are really a number of distinct
-use cases. Primarily it is the interface for the compute manager
-class to consume. It also, however, has alot of helper APIs for
-dealing with the libvirt connection and the host operating system,
-as well as helpers for dealing with guest instance configuration.
-A number of these helpers are required by other libvirt modules
-such as the vif, volume and image backend drivers. This has resulted
-in circular dependancies between the driver.py and the other libvirt
-modules. For example, LibvirtDriver uses NWFilterFirewall, but also
-has to pass a 'get_connection' callback so that NWFilterFirewall can
-obtain the libvirt connection from the LibvirtDriver class. There are
-a number of other similar deps.
-
-Proposed change
-===============
-
-The intention is to introduce two new modules to the codebase
-
-* host.py - this will encapsulate access to libvirt and the host
-  operating system state. It will contain a 'Host' class, which
-  manages a single libvirt connection. It will contain the methods
-  for connecting to libvirt, getting lists of domains, querying
-  host performance metrics and so on (see the work-items section
-  for specifics). This is not to be confused with the existing
-  HostState class which is just a trivial helper for the driver
-  'host_state' method.
-
-* guest.py - this will encapsulate interaction with libvirt guest
-  domain instances. It will contain a 'Guest' class, which manages
-  a single libvirt guest domain. It will contain all methods used
-  to construct the guest XML configuration during instance startup
-  that currently live in driver.py
-
-The code for host.py and guest.py will be pulled out of the
-existing driver.py class. Other libvirt modules will be updated
-as needed to access the new APIs. To minimize the risk of creating
-regressions changes to the methods being moved will be minimized,
-to just minor renames & fixups where appropriate.
-
-The intended end result is that none of the modules in the libvirt
-driver directory should need to access the driver.py file. They
-should be able to consume the host.py and guest.py APIs instead,
-thus breaking the circular dependancies. For example the
-NWFilterFirewall class can be given an instance of the Host class
-instead of a callback to LibvirtDriver.
-
-The new structure should also reduce the size of the test_driver.py
-file and make it possible to create simpler, self contained tests
-for the functionality that's in host.py and guest.py, since it will
-be isolated from the overall virt driver API.
-
-It is not anticipated that any configuration parameters will move.
-The high level desire is that the new APIs will not directly use
-any Nova configuration parameters. Instead the LibvirtDriver would
-be responsible for reading the config parameters and then setting
-attributes on the new class or passing method parameters where
-appropriate.
-
-At the end of the work there should be absolutely no functional
-change on the libvirt driver. This is intended to be purely
-refactoring work that is invisible to anyone except the people
-writing libvirt driver code.
-
-Alternatives
-------------
-
-Doing nothing is always an option, but it isn't very appealing
-because it leaves us with an ever growing monster ready to
-devour us at any moment.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-There are liable to be conflicts with any developers who have patches
-touching driver.py or test_driver.py
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  berrange
-
-Work Items
-----------
-
-* Create a host.py module and move the basic connection handling code
-  out of driver.py into the new Host class. This will cover the following
-  methods:
-
-  * _conn_has_min_version
-  * _has_min_version
-  * _native_thread
-  * _dispatch_thread
-  * _event_lifecycle_callback
-  * _queue_Event
-  * _dispatch_events
-  * _init_events_pipe
-  * _init_events
-  * _get_new_connection
-  * _close_callback
-  * _test_connection
-  * _connect
-
-* Move helpers used by HostState out into the Host class. This will
-  cover the following methods
-
-  * _get_vcpu_total
-  * _get_memory_mb_total
-  * _get_vcpu_used
-  * _get_memory_mb_used
-  * _get_hypervisor_type
-  * _get_hypervisor_version
-  * _get_hypervisor_hostname
-  * _get_cpu_info
-  * _get_disk_available_least
-
-* Create a guest.py module and move the code for creating the guest XML
-  configuration out of driver.py into the new Guest class. This will cover
-  the following methods
-
-  * _get_guest_cpu_model_config
-  * _get_guest_cpu_config
-  * _get_guest_disk_config
-  * _get_guest_storage_config
-  * _get_guest_config_sysinfo
-  * _get_guest_pci_device
-  * _get_guest_config
-  * _get_guest_xml
-
-* Move the code for listing domains into the new Host class. This
-  will cover the '_list_instance_domains' method.
-
-* Change NWFilterFirewall and LibvirtBaseVIFDriver so that they
-  accept a 'Host' object instance, instead of requiring a callback
-  to the LibvirtDriver class.
-
-* Anything else that appears relevant to move :-)
-
-Dependencies
-============
-
-* None
-
-Testing
-=======
-
-Since it is intended that there is no functional change in this work,
-the existing test coverage should be sufficient. The existing unit
-tests will need some refactoring as code is moved, and some more unit
-tests will be written where appropriate.
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-None
diff --git a/specs/juno/libvirt-driver-domain-metadata.rst b/specs/juno/libvirt-driver-domain-metadata.rst
deleted file mode 100644
index 45e7c64..0000000
--- a/specs/juno/libvirt-driver-domain-metadata.rst
+++ /dev/null
@@ -1,158 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==============================
-Libvirt driver domain metadata
-==============================
-
-https://blueprints.launchpad.net/nova/+spec/libvirt-driver-domain-metadata
-
-Metadata will be recorded in the libvirt domain XML configuration to provide
-information about the Nova instance that the domain corresponds to. The aim
-is to provide information that can be useful to administrators troubleshooting
-compute hosts.
-
-Problem description
-===================
-
-When troubleshooting a compute node there will be a number of running libvirt
-domains which correspond to Nova instances. There may also be other running
-domains which were not launched by Nova, for example, utility guests run by
-libguestfs for file injection. The libvirt domain uuid will match that of the
-Nova instance, but there is more information about a Nova instance that could
-usefully be provided to administrators. For example, the identity of the
-tenant who launched it, the original flavour name and/or settings, the time at
-which the domain was launched, and the version number of the Nova instance that
-launched it (can be relevant if Nova is upgraded while a VM is running).
-
-Proposed change
-===============
-
-The Libvirt domain XML configuration schema allows for applications to insert
-arbitrary metadata under a private XML namespace. The proposal is to make use
-of this to define some metadata that is relevant to Nova, specifically it will
-record
-
- - The nova package version
- - The display name of the instance (as matching 'nova list')
- - The name of the flavor
- - The creation time of the instance
- - The user and project ID/name of owner
- - The root disk glance image or cinder volume UUID
-
-This would correspond to the following XML blob
-
-::
-
-  <domain type='kvm'>
-    ...rest of domain XML config...
-    <metadata>
-      <nova:instance xmlns:nova="http://openstack.org/nova/instance/1">
-        <nova:package version="2014.2.3"/>
-        <nova:flavor name="m1.small">
-          <nova:memory>512</nova:memory>
-          <nova:disk>10</nova:disk>
-          ....
-        </nova:flavor>
-        <nova:name>demo1vm</nova:name>
-        <nova:creationTime>2014-12-25 12:03:20</nova:creationTime>
-        <nova:owner>
-          <nova:user uuid="85bd45c0...213684">joe</nova:user>
-          <nova:project uuid="d33b8c0e...342d69">acmecorp</nova:project>
-        </nova:owner>
-        <nova:root type="image|volume" uuid="69f2991b...f29a8bc"/>
-      </nova:instance>
-    </metadata>
-  </domain>
-
-Alternatives
-------------
-
-Administrators can ask libvirt for the UUID of the running instance and then
-attempt to trace all the information back via Nova APIs. If Nova itself is in
-some failure scenario though, this would not be possible. It also places more
-burden on the administrator to trace the info which could be provided directly
-in the Libvirt XML.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-The compute host administrator will be able to ask libvirt to provide the XML
-config for the running instance and from there find out various useful pieces
-of metadata about the instance.
-
-Developer impact
-----------------
-
-None, this is entirely within the libvirt driver impl
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  berrange
-
-Work Items
-----------
-
-* Extend the nova/virt/libvirt/config.py object model to represent the
-  proposed metadata schema for Nova
-* Extend the nova/virt/libvirt/driver.py get_guest_config() method to fill
-  in the metadata when generating guest XML config
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-None required beyond unit tests
-
-Documentation Impact
-====================
-
-Document that the libvirt XML config contains this metadata as an aid
-for administrators debugging compute nodes.
-
-References
-==========
-
-* Libvirt XML format docs http://libvirt.org/formatdomain.html#elementsMetadata
diff --git a/specs/juno/libvirt-lxc-user-namespaces.rst b/specs/juno/libvirt-lxc-user-namespaces.rst
deleted file mode 100644
index b671209..0000000
--- a/specs/juno/libvirt-lxc-user-namespaces.rst
+++ /dev/null
@@ -1,184 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==================================
-Libvirt-lxc User Namespace Support
-==================================
-
-https://blueprints.launchpad.net/nova/+spec/libvirt-lxc-user-namespaces
-
-User namespaces provide a way for a process running in a container to appear to
-be running as root, but are in fact running as a different user on the host.
-The objective of this feature is to allow deployers to enable and configure
-which users and groups are mapped between container and host.
-
-Problem description
-===================
-
-It is a security risk to allow user processes to run as root on container
-hosts. In order to mitigate this risk, it is a good idea to run processes in
-those containers as non-root users. The problem with this is some processes
-may like to run (or at least appear to run as root inside the container).
-For example, running an init system as the init process of the container.
-
-Further, to boot an image in a user namespaced environment, the contents of
-it's filesystem must be owned by the target user for root on the host.
-
-Proposed change
-===============
-
-User namespaces allow processes inside a container to appear to be run as root,
-but are in fact running as another user. Libvirt exposes this feature through
-idmaps. This change would introduce a set of elements on the instance's domain
-xml to indicate which user and group ids should map between container and host.
-
-To address the owning of the filesystem by the targeted root user, the image
-will be chowned by Nova at boot time.
-
-Config for this feature will be disabled by default. It will be up to the
-deployer to enable and configure it.
-
-New config options in libvirt group:
-
-* uid_maps: comma separated list of mappings, maximum of 5
-
-* gid_maps: comma separated list of mappings, maximum of 5
-
-Format for mappings is "guest-id:host-id:count,guest-id:host-id_count,..."
-
-Alternatives
-------------
-
-Alternative image chown points, with performance impact:
-
-* Chown by image creator: No performance impact
-
-  * Rejected as the end user shouldn't have to worry about it
-
-* Chown by Glance on import: Image will take longer to become active
-
-  * Not ideal is it introduces a dependency on import being configured properly
-    in glance.
-
-
-* Chown by Nova when cached: Initial boot on all hosts will take longer
-
-  * Rejected initially as there are too many changes going on around image
-    caching. Once activity around iamge caching slows down, this may be the
-    ideal option.
-
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-This change will improve the security of containers in Nova significantly.
-Before this change, processes running in containers built by Nova will be run
-as the host's root user. After this change, a deployer can restrict which
-user(s) processes will be run as.
-
-It should be noted that this change is not meant to provide isolation between
-guests, but instead isolation between host and guest. It is out of the scope
-of this change, but is reasonable to assume that if a mechanism was created
-to ensure that containers all used different UID/GIDs, user namespacing could
-be used to provide further guest-guest separation. This change provides a base
-that could be extended in the future for that use case.
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-Images need to be deliberately created to be run in a user namespaced
-environment. The contents of an image's filesystem need to be owned by the
-target uid/gid. In this iteration of this feature, Nova will chown the
-image on boot.
-
-Performance Impact
-------------------
-
-Due to the chowning of the image's filesystem on boot by Nova, there will
-be a performance hit on boot depending on how many files are on the image's
-filesystem.
-
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  andrew-melton
-
-Other contributors:
-  rconradharris
-  thomas-maddox
-
-Work Items
-----------
-
-* Modify libvirt config.py to include new idmap xml
-
-* Create util function to chown rootfs
-
-* Actual setup of new instance
-
-Dependencies
-============
-
-* Linux 3.8+ kernel
-
-  * Early 3.8 kernels may be buggy. If user needs minimum kernel, user
-    should use latest 3.8 kernel possible.
-
-
-* Libvirt 1.1.1
-
-Testing
-=======
-
-Making sure that the nova config options are properly mapped to libvirt domain
-objects can easily be handled by unit testing. Functional testing for this will
-not be possible until libvirt-lxc is included in the CI environment. Depending
-on how chowning is implemented, functional testing could be a bit tricky.
-
-Documentation Impact
-====================
-
-New config options.
-
-References
-==========
-
-* http://libvirt.org/formatdomain.html#elementsOSContainer
-
-* http://libvirt.org/drvlxc.html#secureusers
-
-* https://lwn.net/Articles/532593/
diff --git a/specs/juno/libvirt-sheepdog-backed-instances.rst b/specs/juno/libvirt-sheepdog-backed-instances.rst
deleted file mode 100644
index 92d3b89..0000000
--- a/specs/juno/libvirt-sheepdog-backed-instances.rst
+++ /dev/null
@@ -1,181 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-====================================================
-Support Sheepdog ephemeral disks for libvirt driver
-====================================================
-
-https://blueprints.launchpad.net/nova/+spec/libvirt-sheepdog-backed-instances
-
-Add support for Sheepdog instance ephemeral disks.
-
-Problem description
-===================
-
-Sheepdog block devices can already be attached to QEMU and KVM
-virtual machines.  Nova's libvirt driver supports most of the
-functionality. The only additional changes are to the image-backend
-drivers. Both Glance and Cinder support sheepdog backends, so this
-would complement the efforts made in those projects.
-
-
-Proposed change
-===============
-
-This change would extend several parts of the libvirt driver. In
-general, these changes are very similar to the changes required for
-the RBD driver. These changes would bring Sheepdog support into
-feature-parity with RBD, QEMU and other libvirt image drivers.
-
-- nova.virt.libvirt.driver would be extended with cleanup functions
-  for Sheepdog, in the same way that ``cleanup_rbd_instance`` does
-  for the RBD backend.
-
-- A new ``Image`` subclass class would be added to
-  ``nova.virt.libvirt.imagebackend`` for Sheepdog.
-
-- Helper functions would be added to ``nova.virt.libvirt.utils`` where needed.
-
-- /etc/nova/rootwrap.d/filters would be extended to support rootwrap
-  on the ``dog`` command used to interact with Sheepdog.
-
-- See Deployer impact for configuration changes.
-
-Alternatives
-------------
-
-Cinder has existing support for Sheepdog volumes. One alternative
-is to use that driver and only launch instances from volumes. There
-are two problems with this option. First, it would not support
-instance disks that are deleted after the instance is destroyed.
-Second, for the end-user it requires additional steps to provision
-the volume before the instance is booted.
-
-Data model impact
------------------
-None.
-
-REST API impact
----------------
-
-None. This blueprint makes no REST API changes.
-
-Security impact
----------------
-Rootwrap of 'dog' command on nova-compute machines.
-
-Notifications impact
---------------------
-None. No plans for new notifications.
-
-Performance Impact
-------------------
-None.
-
-Other end user impact
----------------------
-
-None. This blueprint has no impact on python-novaclient or any other
-end-user interface.
-
-Other deployer impact
----------------------
-
-To use this feature, a deployer must first set up a sheepdog cluster and
-then make several configuration changes to nova on machines running the
-nova-compute process. If Sheepdog is to be used with Glance and Cinder,
-the deployer must also make the appropriate configuration changes for those
-services.
-
-To setup a Sheepdog cluster, follow the install guide provided by
-Sheepdog [1]_. After setting up a Sheepdog cluster, each nova-compute
-process must be configured. The following options must be set under
-the ``[libvirt]`` section of ``nova.conf`` or ``nova-compute.conf``:
-
-- ``images_type=sheepdog`` This must be set to indicate that sheepdog should
-  be used.
-
-- ``images_sheepdog_host=locahost`` Change this if the machine running the
-  nova-compute process is not a member of the sheepdog cluster, or is not
-  acting as a gateway node within the cluster.
-
-- ``images_sheepdog_port=7000``` Change this if the sheep process is listening
-  on a different port than the default.
-
-For sites doing continuous deployment, this change will have no impact until
-the deployer changes the ``images_type`` setting to deliberately switch a
-nova-compute machine to use Sheepdog.
-
-There are no database migrations required for this change.
-
-Developer impact
-----------------
-
-This change would add an additional disk-backend to the libvirt driver,
-slightly increasing code support costs.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  <scott-devoid> Scott Devoid
-
-Other contributors:
-  <None>
-
-
-Work Items
-----------
-
-- Implement basic support for Sheepdog images booted from a Glance image.
-
-- Implement snapshot support.
-
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Devstack integration is required before tempest can run functional
-tests against the Sheepdog drivers for Nova, Glance and Cinder. A patch
-has been proposed which would use Sheepdog for each service. [2]
-
-This, I think, would result in many functional tests of the Sheepdog
-drivers via the Tempest tests. However, a Jenkins job would need
-to be defined and VMs would need to be provisioned to run the jobs.
-It is not clear if openstack-infra is willing or capable of committing
-to a proliferation of CI test runs. There is a Juno Summit scheduled for
-this. [3]
-
-
-Documentation Impact
-====================
-
-- Configuration reference would need to be updated with the new configuration
-  options. See the Deployer Impact section.
-
-- Cloud Administer or Operations guide should be updated with a section which
-  describes in detail how to configure Sheepdog for nova and what sort of
-  considerations should be taken into account, e.g. cluster size, Zookeeper vs
-  Corosync, the use of gateway nodes.
-
-These documentation changes would happen as part of this blueprint.
-
-
-References
-==========
-
-.. [1] https://github.com/sheepdog/sheepdog/wiki
-.. [2] https://review.openstack.org/#/c/90244/
-.. [3] http://summit.openstack.org/cfp/details/198
diff --git a/specs/juno/libvirt-start-lxc-from-block-devices.rst b/specs/juno/libvirt-start-lxc-from-block-devices.rst
deleted file mode 100644
index f73495d..0000000
--- a/specs/juno/libvirt-start-lxc-from-block-devices.rst
+++ /dev/null
@@ -1,128 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Libvirt - Start LXC from a block device
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/libvirt-start-lxc-from-block-devices
-
-The purpose of this blueprint is to enable the LXC containers
-to be started from a block device volumes.
-
-Problem description
-===================
-
-Currently, LXC containers can only be started from a Glance image.
-However, a minor adjustment is needed to support it's being booted
-using a block volume as its root OS filesystem.
-
-Proposed change
-===============
-
-Separate the lxc disk handling code from _create_domain() to
-_lxc_disk_handler context manager. It will use block_device_mapping
-to map the device that instance has been started from, otherwise,
-an image will be used.
-
-The _lxc_disk_handler will handle the "pre" and "post" lxc start actions
-on the disk, to mount it and clean the lxc namespace, after it starts.
-These actions are specific to LXC, both for images and volumes.
-
-The following layout of the volumes will be supported.
-
- - Unpartitioned, filesystem across entire content.
- - Partitioned. Only mount the filesystem in the first partition.
-   In case there are more than one partition present, only the first one
-   will be considered, while others will be ignored.
-
-The user may create a volume from and existing Glance image and boot
-LXC container in one command:
-
-    nova boot --flavor FLAVOR --block-device source=image,id=ID,dest=volume,\
-              size=SIZE,shutdown=PRESERVE,bootindex=0 NAME
-
-or booting the LXC container from an existing volume
-
-    nova boot --flavor FLAVOR --block-device source=volume,id=ID,dest=volume,\
-              size=SIZE,shutdown=PRESERVE,bootindex=0 NAME
-
-
-Alternatives
-------------
-None
-
-Data model impact
------------------
-None
-
-REST API impact
----------------
-None
-
-Security impact
----------------
-As LXC will always share the host's kernel, between all instanances,
-any vulnerability in the kernel, maybe used to harm the host.
-In general, the kernel's filesystem drivers should be trusted to
-free of vulnerabilities that the user filesystem image may exploit.
-
-Notifications impact
---------------------
-None
-
-Other end user impact
----------------------
-None
-
-Performance Impact
-------------------
-None
-
-Other deployer impact
----------------------
-None
-
-Developer impact
-----------------
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-Vladik Romanovsky <vladik.romanovsky@enovance.com>
-
-Work Items
-----------
- - Introduce a _lxc_disk_handler context manager method and
-   separate all lxc disk handling code from _create_domain()
-   to it.
- - Add logic to the _lxc_disk_handler to mount the volumes,
-   using the provided block_device_mapping
- - Remove the lxc specific mapping creation in blockinfo.py
-
-Dependencies
-============
-None
-
-Testing
-=======
-
-None
-
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-[1] https://review.openstack.org/#/c/74537
diff --git a/specs/juno/libvirt-volume-snap-network-disk.rst b/specs/juno/libvirt-volume-snap-network-disk.rst
deleted file mode 100644
index 1ae8790..0000000
--- a/specs/juno/libvirt-volume-snap-network-disk.rst
+++ /dev/null
@@ -1,160 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Volume Snapshots for Network-Backed Disks
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/libvirt-volume-snap-network-disk
-
-Nova currently supports creating and deleting snapshots of file-backed
-Cinder volumes via libvirt's snapshot mechanism.  This work extends
-that capability to create and delete snapshots for network-backed disks
-in a similar fashion.
-
-This enables more complete Cinder volume functionality for deployments using
-qemu network-backed volumes through a mechanism like libgfapi.
-
-
-Problem description
-===================
-
-Nova does not support creating a snapshot via libvirt for a network-backed
-Cinder volume that is attached to an instance.  Currently, attempting to
-snapshot a Cinder volume configured this way will result in a failed snapshot
-operation.
-
-This is important for deployers who use qemu network-backed storage for Cinder
-volumes.  (Typically for performance reasons.)
-
-
-Proposed change
-===============
-
-Nova needs to be able to construct a <domainsnapshot> XML entity with
-the required fields to snapshot a network-backed disk via libvirt.
-
-Nova similarly needs to be able to pass in arguments for libvirt's
-blockCommit and blockRebase operations to delete snapshots for network-backed
-disks.  libvirt is adding support for a different style of parameters to the
-blockjob APIs to support this, which allows referencing an existing item in
-the disk snapshot change by index rather than by path name.
-
-Alternatives
-------------
-
-There is no alternative for deployers wishing to use Nova-assisted snapshots
-of Cinder-backed storage.  Nova must be able to interact with libvirt to
-enable this functionality.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-This work is used by the os-assisted-volume-snapshots extension APIs with no
-API-level changes.
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-End-user impact is that Cinder volume snapshots now work when Nova
-is configured to use libgfapi for the GlusterFS Cinder driver.
-(qemu_allowed_storage_drivers=['gluster'])
-
-Performance Impact
-------------------
-
-Deleting (merging) a GlusterFS volume snapshot may be more efficient,
-particularly for simultaneous snapshot deletes for different volumes, as
-this work uses qemu direct storage access (via libgfapi) rather than a
-FUSE-mounted file system.
-
-No direct performance impact within Nova itself.
-
-Other deployer impact
----------------------
-
-This change is relevant when using the Cinder GlusterFS driver and Nova
-is configured with qemu_allowed_storage_drivers=['gluster'].
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  eharney
-
-Work Items
-----------
-
-* Support for creating a volume snapshot of a network-backed disk
-* Support for deleting a volume snapshot of a network-backed disk
-  - Parse backing chain information from libvirt's domain XML
-  - Pass new-style arguments to blockCommit and blockRebase
-
-Dependencies
-============
-
-* This functionality depends on libvirt changes which are currently targeted
-  for libvirt 1.2.6.
-  - The libvirt capability is detected without using the libvirt version.
-
-* The libvirt changes also require fixes within qemu (targeting 2.1).
-
-* Currently only relevant for GlusterFS Cinder deployments.
-
-
-Testing
-=======
-
-This should be tested via Tempest volume snapshot test cases.  Since it is
-dependent on having a GlusterFS deployment this is not currently tested in
-the gate.
-
-When third-party CI is enabled for the GlusterFS driver within Cinder, it
-should cover this.
-
-
-Documentation Impact
-====================
-
-None
-
-
-References
-==========
-
-* Required libvirt changes:
-  - https://www.redhat.com/archives/libvir-list/2014-June/msg00492.html
-
-* Required QEMU changes:
-  - https://lists.gnu.org/archive/html/qemu-devel/2014-06/msg04058.html
-
-* Based on work done in
-  - https://blueprints.launchpad.net/nova/+spec/qemu-assisted-snapshots
-
-* Patch series: https://review.openstack.org/#/c/78748/
diff --git a/specs/juno/log-request-id-mappings.rst b/specs/juno/log-request-id-mappings.rst
deleted file mode 100644
index bbdc9b3..0000000
--- a/specs/juno/log-request-id-mappings.rst
+++ /dev/null
@@ -1,167 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Log Request ID Mappings
-==========================================
-
-
-https://blueprints.launchpad.net/nova/+spec/log-request-id-mappings
-
-Tracking requests across multiple OpenStack services is difficult.
-
-
-Problem description
-===================
-
-Each OpenStack service sends a request ID header with HTTP responses. This
-value can be useful for tracking down problems in the logs. However, when
-operations cross service boundaries, this tracking becomes difficult, as
-services generate a new ID for each inbound request; a nova request ID cannot
-help users find debug info for other services that were called by nova while
-a request to nova was being fulfilled. This becomes especially problematic when
-many requests are coming at once, especially at the gate, where tempest is now
-running tests in parallel. Simply matching timestamps between service logs is
-no longer a feasible solution. Therefore, another method of request tracing
-is needed to aid debugging.
-
-
-Proposed change
-===============
-
-A request ID is generated at the start of request processing. This is the ID
-that a user will see when their request returns. Other OpenStack services
-that nova calls out to (such as glance) will send responses with their request
-ID as a header. By logging the mapping of the two request IDs (nova->glance), a
-user will be  able to easily lookup the request ID returned by glance in the
-n-api log. With the glance request ID in hand, a user can then check the glance
-logs for debug info which corresponds to the request made to nova.
-
-There are two request IDs required to form the log message: one generated by
-nova, and one included in the response from another service. The request ID
-generated by nova is in the context that is passed in to the python client
-wrappers. The request ID of the other service does not yet reach nova's
-client wrappers; this value being returned depends on some client blueprints
-being implemented (see Dependencies). Once both request IDs are available, the
-logging will be done using link_request_ids() from request_utils.
-
-Alternatives
-------------
-
-This idea surfaced in previous cycles[1], with the proposed solution being a
-unified request ID that would be passed between services. While work was
-started on this, the approach was eventually deemed unsatisfactory, as it
-would allow for meddling with the request ID value on the client side. A client
-reusing the same request ID would eliminate the benefit of request tracing.
-With a carefully chosen request ID, one user could even interfere with the
-debugging effort of another user. So having the request ID be generated by the
-server is essential.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-The request_utils module used will generate an INFO notification for each
-request ID mapping that is logged.
-
-Other end user impact
----------------------
-
-The only places an end user will see this is in the nova logs or in the
-generated notifications.
-
-Performance Impact
-------------------
-
-None.
-
-Other deployer impact
----------------------
-
-In order for nova to log the request ID value of the other service, the python
-client for that service needs to be passing back the request ID it gets from
-the HTTP response from the service. New releases of python-glanceclient,
-python-cinderclient, etc. will be necessary for this to occur. As such,
-deployers will need to be running newer versions of the client in order for
-the request ID mappings to be logged. If the client is not returning the
-request ID, no logging will occur.
-
-Developer impact
-----------------
-
-None.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  chris-buccella
-
-Work Items
-----------
-
-1. Sync request_utils module from oslo
-2. As various service's python clients are updated to return the request ID,
-   log the request ID mapping using request_utils.link_request_ids():
-   1. python-glanceclient
-   2. python-cinderclient
-   3. python-neutronclient
-3. Update requirements.txt with the new required versions of the clients
-
-Dependencies
-============
-
-* New versions of some python clients. The following blueprints need to be
-  completed first:
-
-  * https://blueprints.launchpad.net/python-glanceclient/+spec/return-req-id
-  * https://blueprints.launchpad.net/python-cinderclient/+spec/return-req-id
-  * https://blueprints.launchpad.net/python-neutronclient/+spec/return-req-id
-
-Testing
-=======
-
-A tempest test could be added to ensure that notifications are being generated
-in the correct format.
-
-
-Documentation Impact
-====================
-
-The Operations Guide should be updated particularly:
-
-* Chapter 11, under "Determining Which Component Is Broken"
-* Chapter 13, under "Tracing Instance Requests"
-
-
-References
-==========
-
-[0] Proposed change for nova<->glance logging from Icehouse cycle:
-https://review.openstack.org/#/c/68518
-
-[1] Discussion from the HK Summit:
-https://etherpad.openstack.org/p/icehouse-summit-nova-cross-project-request-ids
-
-[2] Refinements from the ML:
-http://lists.openstack.org/pipermail/openstack-dev/2013-December/020774.html
diff --git a/specs/juno/lvm-ephemeral-storage-encryption.rst b/specs/juno/lvm-ephemeral-storage-encryption.rst
deleted file mode 100644
index bdd8db0..0000000
--- a/specs/juno/lvm-ephemeral-storage-encryption.rst
+++ /dev/null
@@ -1,204 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-============================================
-Ephemeral storage encryption for LVM backend
-============================================
-
-https://blueprints.launchpad.net/nova/+spec/lvm-ephemeral-storage-encryption
-
-The proposed feature will provide data-at-rest security for LVM backed,
-libvirt managed ephemeral storage devices attached to a VM instance.
-
-
-Problem description
-===================
-
-The current implementation of LVM ephemeral storage leaves user data vulnerable
-following instance shutdown, through disk block reuse (if data is not
-securely erased), improper storage media disposal and physical facility
-compromise.
-
-For example, if a compute node goes down without properly disposing of the
-active instances, when it is restarted, the disk blocks that held pre-reboot
-instances' data will be reallocated to new instances.  Since LVM storage is not
-sanitized before allocation this, in principle, permits recovery of other
-users' data.
-
-
-Proposed change
-===============
-
-User data can be protected against inadvertant disclosure by encrypting
-ephemeral storage disks with a unique key, accessible only via a secure key
-manager (most likely Barbican, currently in incubation) with proper
-credentials. The feature will be labeled optional until the status of Barbican
-key manager is finalized.
-
-This feature is part of a larger effort to add ephemeral storage encryption to
-OpenStack.
-
-
-Alternatives
-------------
-
-It is unlikely there is another solution that would cover all the cases such
-unexpected compute node events, preventing proper instance shutdown,
-improper storage media disposal, etc., covered by ephemeral storage encryption.
-
-For example, ephemeral disks could be sanitized before being attached to
-instances to prevent disclosure due to block storage reuse.  However, this
-would not protect users' data against improper storage media disposal.
-Moreover, data sanitization is expensive since the entire ephemeral disk,
-which can be sizeable, must be wiped.
-
-Data model impact
------------------
-
-All necessary data objects and database changes have already been made. See
-
-* https://blueprints.launchpad.net/nova/+spec/encrypt-ephemeral-storage
-
-* https://review.openstack.org/#/c/61544/
-
-* https://review.openstack.org/#/c/60621/
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-This feature will make LVM ephemeral storage more secure by providing
-data-at-rest security for user data.
-
-* A uniqe encryption key is created for each instance (or batch of
-  instances in case of a batch launch) in
-  compute.API._populate_instance_for_create() and stored securely using key
-  manager (e.g., Barbican).
-
-* The key is retrieved, using its uuid and the user's context, immediately
-  before the ephemeral disk is created to minimize the exposure.
-
-Potential security concerns:
-
-* Command cryptsetup will be added to the rootwrap filter.
-
-* User context will be passed to imagebackend.Lvm.create_image(),
-  LibvirtDriver.create_swap() and LibvirtDriver.create_ephemeral() from
-  LibvirtDriver._create_image()
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-Certain instance operations:
-
-* instance rescue
-
-may not be immediately supported.
-
-Performance Impact
-------------------
-
-The optional encryption layer imposes a roughly 10% performance penalty
-on ephemeral storage I/O performance, according to measurements performed
-with the Phoronix test suite on a single-node DevStack cloud.
-
-Other deployer impact
----------------------
-
-* LVM ephemeral storage encryption is controlled by 3 options collected in the
-  ephemeral_storage_encryption options group.  The name is deliberately generic
-  since the same options could be used to control encryption for other
-  backends.
-
-  ephemeral_storage_encryption options group
-
-  * enabled:Boolean -- enables/disables LVM ephemeral storage encryption;
-                       default is False
-
-  * cipher:String -- cipher-mode string to be passed to cryptsetup; the set of
-                     cipher-mode combinations available depends on kernel
-                     support; default is aes-xts-plain64
-
-  * key_size:Integer -- encryption key length in bits; default is 512
-
-  The default values have been chosen to provide a high level of
-  confidentiality.  (Note that in XTS mode only half of the key bits are
-  used for encryption key.)
-
-* Encryption is implemented using the cryptsetup utility, which is available
-  as a package on most Linux distributions.
-
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  Dan Genin <daniel.genin@jhuapl.edu>
-
-Other contributors:
-  None
-
-Work Items
-----------
-
-Two of the three components comprising the feature:
-
-* adding ephemeral storage encryption key uuid to Nova DB
-  (https://review.openstack.org/#/c/61544/)
-
-* dmcrypt module for interacting with cryptsetup
-  (https://review.openstack.org/#/c/60621/)
-
-have already merged in Icehouse.
-
-The final remaining item is to actually add encryption to imagebackend.Lvm.
-
-Dependencies
-============
-
-Depends on Barbican (https://review.openstack.org/#/c/94918/) for key
-management.
-
-Depends on cryptsetup being installed.
-
-
-Testing
-=======
-
-We will work to implement Tempest tests for the feature. However, Tempest
-testing will require Tempest support for LVM backed ephemeral storage as
-well as Barbican for key management. These changes may take some time to
-implement.
-
-Documentation Impact
-====================
-
-Ephemeral storage encryption configuration options and its dependencies,
-namely dmcrypt/cryptsetup and Barbican, will have to be documented.
-
-
-References
-==========
-
-None
diff --git a/specs/juno/make-resource-tracker-use-objects.rst b/specs/juno/make-resource-tracker-use-objects.rst
deleted file mode 100644
index 79ff269..0000000
--- a/specs/juno/make-resource-tracker-use-objects.rst
+++ /dev/null
@@ -1,183 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Make Resource Tracker Use Objects
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/make-resource-tracker-use-objects
-
-This blueprint was approved for Icehouse but missed feature freeze.
-
-Nova is converting data structures it uses to communicate via RPC and through
-the database to use an object encapsulation called Nova Objects. This supports
-of multi-versioning for live-upgrade and database schema independence. This
-blueprint covers the conversion of the resource tracker to use Nova Objects.
-
-Problem description
-===================
-
-Conversion to Nova Objects will replace dict data structures that are currently
-communicated via the conductor API with Nova Object versions. Where necessary
-the Nova Objects will be extended to cover parameters that are not already
-implemented.
-
-Proposed change
-===============
-
-The Nova Object classes that will be used include:
-
-- ComputeNode
-- Instance
-- Migrations
-- Flavor
-- Service
-
-The ComputeNode object is currently missing some parameters that exist
-in the compute_nodes table and are used in the resource tracker. The
-following parameters will be added to the ComputeNode:
-
-- host_ip
-- pci_stats
-
-In addition, the following fields exist in the compute_nodes table but
-are not currently used by the resource tracker. We propose not to add fields
-to the ComputeNode object unless they are used, so these fields will not
-be added as part of this blueprint.
-
-- extra_resources
-- supported_instances
-
-When complete there will be direct calls to conductor in the resource tracker.
-
-Alternatives
-------------
-
-There is another effort to split the scheduler from Nova. When the split is
-complete the resource tracker may no longer interact with the scheduler via
-the database.  Initially, the scheduler-lib blueprint (see references) will
-make all compute node interaction with the scheduler go through a new
-scheduler library in the Juno-1 timeframe in preparation for the split.
-
-This suggests that it might be unnecessary to use the ComputeNode object at
-least. However, it is reasonable to continue using the ComputeNode object
-even if it is not used to persist data in the database, so we will go ahead
-with the existing plan to implement it.
-
-Data model impact
------------------
-
-The objects isolate the code from the database schema. They are written to
-operate with existing data model versions. At present the scheduler does not
-the ComputeNode object, so code there will need to tolerate changes in
-database schema or the format of data stored in fields.
-
-The fields that need to be added to the ComputeNode object are as follows:
-
-**host_ip**
-
-Database field type: varchar(39)
-
-Object field type: fields.IPAddressField()
-
-**pci_stats**
-
-Database field type: text
-
-Object field type: fields.ObjectField('PciDeviceList', nullable=True)
-
-The pci_stats field is currently populated with a PciDeviceList serialized
-as an object primitive. This is already the correct form for an object field.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-None.
-
-Other deployer impact
----------------------
-
-The objects are written to be compatible with the database schema used in
-Icehouse. There is no database migration associated with this blueprint and
-the format of data stored in the fields of the database will not change. This
-means that a combination of Icehouse and Juno versions of the compute nodes
-will be able to coexist and interact with the scheduler.
-
-Developer impact
-----------------
-
-Developers working on the resource tracker will be required to use the new
-objects instead of directly making database calls to conductor.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  pmurray
-
-Other contributors:
-  alexisl
-
-Work Items
-----------
-
-* Use flavor object in resource tracker - (merged in Icehouse)
-
-* Use Service object in resource tracker
-
-* Use Instance object in resource tracker
-
-* Use Migrations object in resource tracker
-
-* Use ComputeNode object in resource tracker
-
-Several of these work items are currently ready for review:
-https://review.openstack.org/#/q/topic:bp/make-resource-tracker-use-objects,n,z
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-This does not affect existing tempest tests. Unit tests will be
-added for each object and existing tests will be modified to deal
-with the new data structure.
-
-Documentation Impact
-====================
-
-No new features or API changes so no document impact.
-
-References
-==========
-
-https://blueprints.launchpad.net/nova/+spec/scheduler-lib
diff --git a/specs/juno/migrate-libvirt-volumes.rst b/specs/juno/migrate-libvirt-volumes.rst
deleted file mode 100644
index ac028d2..0000000
--- a/specs/juno/migrate-libvirt-volumes.rst
+++ /dev/null
@@ -1,204 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===========================================================
-Use Libvirt Storage Pool Methods to Migrate Libvirt Volumes
-===========================================================
-
-Include the URL of your launchpad blueprint:
-
-https://blueprints.launchpad.net/nova/+spec/migrate-libvirt-volumes
-
-Currently, the libvirt driver uses SSH (rsync or scp) to do cold migrations
-and resizes on non-shared storage.  This requires SSH permissions between
-compute nodes, which is problematic for a number of reasons.  Instead we can
-use the methods built in to libvirt's storage pool API to do migrations.
-
-NOTE: this proposal requires
-`Use libvirt storage pools`_
-(`Gerrit Spec Review <https://review.openstack.org/#/c/86947>`_).
-
-Problem description
-===================
-
-The primary issue is that, currently, the Nova libvirt driver requires
-SSH access between compute nodes to perform cold migrations and resizes on
-non-shared storage.  This presents several issues:
-
-* From a security perspective, providing SSH access between compute nodes
-  is sub-optimal.  Giving compute nodes SSH access could allow a compromised
-  node to compromise other nodes and potentially inflict harm on a cloud.
-
-* From a setup perspective, it adds several extra steps to a setup:
-  System administrators, or their setup tools, must generate a keypair
-  for each compute node, and upload the public key to each of the other
-  compute nodes.
-
-Proposed change
-===============
-
-As specified in blueprint mentioned above, Nova's disk images would be placed
-in a libvirt storage pool.  At migration time, a new volume would be created in
-the destination node's storage pool, and the methods virStorageVolDowload and
-virStorageVolUpload would be used to stream the contents of the disk between
-compute nodes
-(http://libvirt.org/html/libvirt-libvirt.html#virStorageVolUpload).
-
-In order to ensure secure migrations, libvirt should be configured to use one
-of the various forms of authentication and encryption that it supports, such as
-Kerberos (via SASL -- http://libvirt.org/auth.html) or TLS client certificates
-(http://libvirt.org/remote.html#Remote_libvirtd_configuration).
-
-Note that this would only apply to setups using the new image backend
-described in the previous blueprint; setups using the "legacy" image
-backends would continue to use the SSH method until the "legacy" image
-backends are removed.
-
-Alternatives
-------------
-
-* Requiring shared storage for cold migrations and resizes: there are many
-  OpenStack users who would like to be able to perform cold migrations and
-  resizes without having shared storage.
-
-* Just setting up SSH keys between compute nodes: see the problem description
-
-* Temporarily provisioning SSH keys for the duration of the migration:
-  While this somewhat mitigates the security issue and remove the extra setup
-  steps, it still provides a window where the compute nodes are vulnerable.
-  It would be much harder to secure, and would require having Nova be able
-  to securely generate SSH keys.
-
-* Using an rsync daemon: People seemed to be averse to requiring an rsync
-  daemon.  Additionally, rsync daemon connections are not encryptable out
-  of the box, although they can be run over stunnel.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-While this change does require two compute nodes' libvirt daemons to connect
-to each other, such a process is already done by live migration.  While the
-disks would no longer be encrypted by SSH while transfering, system
-administrators could simply secure the libvirt connections instead
-(http://libvirt.org/auth.html).  Libvirt supports TLS for encryption with x509
-client certificates for authentication, as well as SASL for GSSAPI/Kerberos
-encryption and authentication.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-There are a couple potential performance issues:
-
-* Rsync compresses compress the contents to be transfered, but AFAIK libvirt
-  does not (although this is being worked on in conjunction with the libvirt
-  developers).  This could result in more data being transfered over
-  the network.
-
-* The actually streaming process would be using python as an intermediary
-  (e.g. :code:`data = stream1.recv(1024*64); stream2.send(data)`, although
-  the actual code would properly support async IO, detection of partial sends,
-  etc).  While this would be less performant than having C code which would do
-  the transfer, I suspect there are ways in which we could optimize the code.
-
-Other deployer impact
----------------------
-
-In order for the new method to work, deployers would have to enable the libvirt
-daemon on each compute node to listen for remote libvirt connections (if live
-migrations are enabled, this has already been done).  Such connections must be
-secured as noted in `Security Impact`_.
-
-Developer impact
-----------------
-
-None.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-    sross-7
-
-Other contributors:
-    None
-
-Work Items
-----------
-
-1. Implement the virStorageVolUpload/virStorageVolDownload code in the
-   :code:`migrate_disk_and_power_off` method, replacing the existing calls
-   to :code:`libvirt_utils.copy_image`.
-
-2. Follow Up: remove the instances of SSH that create the instance directory
-   and detect shared storage.  These could easily be done in a pre-migration
-   method, similarly to how live-migration works currently.
-
-
-Dependencies
-============
-
-`Use libvirt storage pools`_
-(`Gerrit spec review <https://review.openstack.org/#/c/86947>`_)
-
-.. _Use libvirt storage pools:
-   https://blueprints.launchpad.net/nova/+spec/use-libvirt-storage-pools
-
-Testing
-=======
-
-Since this only changes how migration works under the hood, existing migration
-tests should be sufficient for the most part.
-
-
-Documentation Impact
-====================
-
-For the OpenStack Security Guide, we should document that SSH keys are no
-longer required between compute nodes, as well as provide instructions for
-securing remove libvirtd connections.
-
-In the Compute Admin Guide, we should provide instructions for how to enable
-remote libvirtd connections (as required for libvirt live migration), as well
-as noting that these connections need to be secured, as per the Security Guide.
-
-Since much of this documentation also applies to libvirt live migrations, it
-may be beneficial to place the instructions in a "general" section and link
-to it from both the libvirt cold migrations and libvirt live migrations
-documentation.
-
-
-References
-==========
-
-* http://libvirt.org/html/libvirt-libvirt.html#virStorageVolUpload
-
-* http://libvirt.org/auth.html)
-
-* http://libvirt.org/remote.html#Remote_libvirtd_configuration
diff --git a/specs/juno/move-prep-resize-to-conductor.rst b/specs/juno/move-prep-resize-to-conductor.rst
deleted file mode 100644
index e488d67..0000000
--- a/specs/juno/move-prep-resize-to-conductor.rst
+++ /dev/null
@@ -1,125 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===============================
- Move prep_resize to Conductor
-===============================
-
-https://blueprints.launchpad.net/nova/+spec/move-prep-resize-to-conductor
-
-So as to prepare the scheduler to be a separate project, we need to remove
-all proxy calls from the scheduler to compute nodes.
-prep_resize() is still in Scheduler V3 API, so we need to modify how cold
-migrations are retried.
-
-Problem description
-===================
-
-When a cold migration is requested, there is a direct call from conductor to
-compute.prep_resize() which is OK. The problem is when the cold migration is
-failing, where compute node is asking Scheduler to reschedule a new migration
-by calling scheduler.prep_resize(), which itself calls compute.prep_resize()
-after issuing a select_destinations().
-
-Proposed change
-===============
-
-The idea is to replace the call back by conductor.migrate_server instead of
-scheduler.prep_resize in the compute prep_resize method.
-
-
-Alternatives
-------------
-
-All prep_resize logic should be left to the conductor, but that's a bigger step
-than just moving the scheduler logic to conductor. With regards to small
-iterations, that blueprint is quicker to implement and less risky, so that
-another blueprint for placing cold and hot migrations to conductor [1] could
-use it as dependency.
-
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  sylvain-bauza
-
-Other contributors:
-  None
-
-Work Items
-----------
-
-- Replace call to scheduler.prep_resize by call to conductor.migrate_server
-  in compute.prep_resize
-- Remove prep_resize in Scheduler RPC API and note it to be removed in manager
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-Covered by existing tempest tests and CIs.
-
-
-Documentation Impact
-====================
-
-None
-
-
-References
-==========
-
-* [1] https://blueprints.launchpad.net/nova/+spec/cold-migrations-to-conductor-final
diff --git a/specs/juno/nfv-multiple-if-1-net.rst b/specs/juno/nfv-multiple-if-1-net.rst
deleted file mode 100644
index e05ccca..0000000
--- a/specs/juno/nfv-multiple-if-1-net.rst
+++ /dev/null
@@ -1,184 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-====================================================================
-Support multiple interfaces from one VM attached to the same network
-====================================================================
-
-https://blueprints.launchpad.net/nova/+spec/multiple-if-1-net
-
-Permit VMs to attach multiple interfaces to one network to facilitate use
-of common NFV network function VMs that require this form of attachment.
-
-Problem description
-===================
-
-At present, Nova only permits a single VIF from each VM to be attached
-to a given Neutron network.  If you attempt to attach multiple VIFs to
-the same network, an error is issued, meaning that the second network
-is not found from the list of networks remaining after the first
-network is not used.
-
-NFV functions occasionally require multiple interfaces to be attached
-to a single network from the same VM, for reasons described below in
-the 'use cases' section.  When this is required, the VNF generally
-cannot be used under Openstack.
-
-VNFs are often large, complex pieces of code, and may be supplied by third
-parties.  For various reasons, it is not uncommon that it is necessary to
-feed traffic out of an interface and into another interface (when the VNF
-implements multiple functions and the functions cannot be chained internally)
-or to feed traffic from e.g. the internet into multiple interfaces to run
-them through separate processing functions internally.
-
-The limitation can be seen as one of the VNF.  Clearly, the VNF could be
-changed to put multiple addresses or functions on a single port (to fix the
-incoming traffic issue) or to connect functions internally (to fix the
-passthrough problem.
-
-The problem with this solution is that the timescale for getting such a fix
-is often prohibitive.  VNFs are large, complex pieces of code, and often the
-supplier of the VNF is not the same organisation as that trying to use
-the VNF within Openstack, necessitating a feature change request which may
-well not be possible within reasonable timescales.
-
-We propose changing the code within Nova to remove this limitation.
-
-Proposed change
-===============
-
-We propose removing the limitation, which exists in Nova (Neutron has no such
-limitation), allowing any number of VIFs to be attached to the same network.
-
-The ordering in the nova 'boot' command or POST should be respected, so if
-multiple interfaces are in use on the same network they are attached to the VM
-in the order in which they are provided, as with other NICs.
-
-API changes
------------
-
-When the attempt is made to attach multiple interfaces to a single
-network, Nova will, instead of returning the error, attach multiple
-interfaces to the same network and return a normal success code to the
-'nova boot' attempt.
-
-('nova interface-attach' already permits a second attachment to the same
-network and needs no change.)
-
-Alternatives
-------------
-
-It may be possible to work around this limitation by using multiple
-ports on the same network and attach the VM to the ports, rather than
-the same network twice.  This has not been tested.  On the other hand,
-this indicates that the limitation is highly artificial and should, in
-any case, be removed.  (In any case we should confirm this is possible
-after the change and fix it if not.)
-
-It is possible to boot the VM and use 'nova interface-attach' to get
-multiple interfaces on the same network, but this requires the VM to support
-PCI hotplug.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-When the attempt is made to attach multiple interfaces to a single
-network, Nova will, instead of returning the error, attach all
-interfaces to the same network and return a normal success code to the
-'nova boot' attempt.
-
-Security impact
----------------
-
-It is now going to be possible to bridge multiple interfaces together
-within a VM and cause a broadcast storm.  It was always possible to
-flood a Neutron network from a VM; this makes it easier.  It doesn't
-make a security issue in and of itself but it certainly does make it a
-little more straightforward to trigger one that arguably already
-exists.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.  An end user using API calls that currently succeed will see no change
-in behaviour in those APIs.  This only changes a case where an API currently
-fails.
-
-Performance Impact
-------------------
-
-None.
-
-Other deployer impact
----------------------
-
-None.
-
-Developer impact
-----------------
-
-None.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  ijw-ubuntu
-
-Work Items
-----------
-
-* Change the Nova code, per the existing abandoned patch in
-  https://review.openstack.org/#/c/26370 - which requires porting
-  forward from the code in question to the current trunk.
-
-* Add unit tests, which are missing from the abandoned patch.
-
-Dependencies
-============
-
-None.
-
-Testing
-=======
-
-Independently of this spec, tests should be added to Tempest:
-
-* minimally, to ensure that traffic can be passed between the two
-  interfaces on a VM created in this fashion
-
-* optionally, traffic flow should be tested from another VM or
-  external packet supplier to either of the interfaces.
-
-Testing should be conducted with both the nova boot and nova
-interface-attach methods.
-
-Documentation Impact
-====================
-
-The change should be documented. No documentation exists for the
-current behaviour.  Documentation exists for nova-network multinic
-saying that VIFs are attached to separate networks but this is specific
-to nova-network.
-
-References
-==========
-
-* https://review.openstack.org/#/c/26370
-* https://bugs.launchpad.net/nova/+bug/1166110
diff --git a/specs/juno/nova-pagination.rst b/specs/juno/nova-pagination.rst
deleted file mode 100644
index b18e509..0000000
--- a/specs/juno/nova-pagination.rst
+++ /dev/null
@@ -1,333 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-====================================
-Nova REST API Sorting Enhancements
-====================================
-
-https://blueprints.launchpad.net/nova/+spec/nova-pagination
-
-Currently, the pagination support for Nova does not allow the caller to
-specify the sort order and direction of the data set. This blueprint
-enhances the pagination support for the /servers and /servers/detail
-APIs so that multiple sort keys and sort directions can be supplied on
-the request.
-
-
-Problem description
-===================
-
-There is no support for retrieving server data in a specific order, it is
-defaulted to descending sort order by the "created date" and "id" keys. In
-order to retrieve data in any sort order and direction, the REST APIs need
-to accept multiple sort keys and directions.
-
-Use Case: A UI that displays a table with only the page of data that it
-has retrieved from the server. The items in this table need to be sorted
-by status first and by display name second. In order to retrieve data in
-this order, the APIs must accept multiple sort keys/directions.
-
-
-Proposed change
-===============
-
-The /servers and /servers/detail APIs will support the following parameters
-being repeated on the request:
-
-* sort_key: Key used to determine sort order
-* sort_dir: Direction for with the associated sort key ("asc" or "desc")
-
-The caller can specify these parameters multiple times in order to generate
-a list of sort keys and sort directions. The first key listed is the primary
-key, the next key is the secondary key, etc.
-
-For example: /servers?sort_key=status&sort_dir=desc&sort_key=display_name&
-&sort_dir=desc&sort_key=created_at&sort_dir=desc
-
-Note: The "created_at" and "id" sort keys are always appended at the end of
-the key list if they are not already specified on the request.
-
-The database layer already supports multiple sort keys and directions. This
-blueprint will update the API layer to retrieve the sort information from
-the API request and pass that information down to the database layer.
-
-All sorting is handled in the sqlalchemy.utils.paginate_query function.  This
-function accepts an ORM model class as an argument and the only valid sort
-keys are attributes on the given model class.  Therefore, the valid sort
-keys are limited to the model attributes on the models.Instance class.
-
-For the v2 API a new 'os-server-sort-keys' API extension will be created to
-signify that the new sorting parameters proposed in this blueprint should be
-honored. The v3 API will support the new sorting parameters by default.
-
-Alternatives
-------------
-
-Repeating parameter key/values was chosen because glance already did it:
-
-https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L526
-
-However, the list of sort keys and directions could be built by splitting the
-associated parameter values.
-
-For example:
-/servers?sort_key=status,display_name,created_at&sort_dir=desc,desc,desc
-
-The downside of this approach is that it would require pre-defined token
-characters. I'm open to this solution if it is deemed better.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-A new v2 API extension will be created to signify that the new sorting
-parameters should be honored. Extension details:
-
-* Name: ServerSortKeys
-* Alias: os-server-sort-keys
-
-Note that this API extension will behave in the same manner as the current
-'os-user-data' extension. For example, the extension is defined as:
-
-http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute
-/contrib/user_data.py
-
-And it is checked in the v2 server API here:
-
-http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute/
-servers.py#n850
-
-The following existing v2 GET APIs will support the new sorting parameters
-if the 'os-server-sort-keys' API extenstion is loaded:
-
-* /v2/{tenant_id}/servers
-* /v2/{tenant_id}/servers/detail
-
-The following existing v3 GET APIs will support the new sorting parameters
-by default:
-
-* /v3/servers
-* /v3/servers/details
-
-Note that the design described in this blueprint could be applied to other GET
-REST APIs but this blueprint is scoped to only those listed above. Once this
-design is finalized, then the same approach could be applied to other APIs.
-
-The existing API documentation needs to be updated to include the following
-new Request Parameters:
-
-+-----------+-------+--------+---------------------------------------------+
-| Parameter | Style | Type   | Description                                 |
-+===========+=======+========+=============================================+
-| sort_key  | query | string | Sort key (repeated for multiple), keys      |
-|           |       |        | default to 'created_at' and 'id'            |
-+-----------+-------+--------+---------------------------------------------+
-| sort_dir  | query | string | Sort direction, either 'asc' or 'desc'      |
-|           |       |        | (repeated for multiple), defaults to 'desc' |
-+-----------+-------+--------+---------------------------------------------+
-
-Neither the API response format nor the return codes will be modified, only
-the order of the servers that are returned.
-
-In the event that an invalid sort key is specifed then a "badRequest" error
-response (code 400) will be returned with a message like "Invalid input
-received: Invalid sort key".
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-The novaclient should be updated to accept sort keys and sort directions, new
-parameters:
-
-+-------------+----------------------------------------------------------+
-| Parameter   | Description                                              |
-+=============+==========================================================+
-| --sort-keys | Comma-separated list of sort keys used to specify server |
-|             | ordering. Each key must be paired with a sort direction  |
-|             | value.                                                   |
-+-------------+----------------------------------------------------------+
-| --sort-dirs | Comma-separated list of sort directions used to specify  |
-|             | server ordering. Each key Must be paired with a sort key |
-|             | value. Valid values are 'asc' and 'desc'.                |
-+-------------+----------------------------------------------------------+
-
-Performance Impact
-------------------
-
-All sorting will be done in the database. The choice of sort keys is limited
-to attributes on the models.Instance ORM class -- not every attribute key
-returned from a detailed query is a valid sort key.
-
-Performance data was gathered by running on a simple devstack VM with 2GB
-memory. 5000 instances were inserted into the DB. The data shows that the
-sort time on the main data table is dwarfed (see first table below) when
-running a detailed query -- most of the time is spent querying the the other
-tables for each item; therefore, the impact of the sort key on a detailed
-query is minimal.
-
-For example, the data below compares the processing time of a GET request for
-a non-detailed query to a detailed query with various limits using the default
-sort keys. The purpose of this table is to show the the processing time for a
-detailed query is dominated by getting the additional details for each item.
-
-+-------+--------------------+----------------+---------------------------+
-| Limit | Non-Detailed (sec) | Detailed (sec) | Non-Detailed / Detailed % |
-+=======+====================+================+===========================+
-| 50    | 0.0560             | 0.8621         | 6.5%                      |
-+-------+--------------------+----------------+---------------------------+
-| 100   | 0.0813             | 1.6839         | 4.8%                      |
-+-------+--------------------+----------------+---------------------------+
-| 150   | 0.0848             | 2.4705         | 3.4%                      |
-+-------+--------------------+----------------+---------------------------+
-| 200   | 0.0874             | 3.2502         | 2.7%                      |
-+-------+--------------------+----------------+---------------------------+
-| 250   | 0.0985             | 4.1237         | 2.4%                      |
-+-------+--------------------+----------------+---------------------------+
-| 300   | 0.1229             | 4.8731         | 2.5%                      |
-+-------+--------------------+----------------+---------------------------+
-| 350   | 0.1262             | 5.6366         | 2.2%                      |
-+-------+--------------------+----------------+---------------------------+
-| 400   | 0.1282             | 6.5573         | 2.0%                      |
-+-------+--------------------+----------------+---------------------------+
-| 450   | 0.1458             | 7.2921         | 2.0%                      |
-+-------+--------------------+----------------+---------------------------+
-| 500   | 0.1770             | 8.1126         | 2.2%                      |
-+-------+--------------------+----------------+---------------------------+
-| 1000  | 0.2589             | 16.0844        | 1.6%                      |
-+-------+--------------------+----------------+---------------------------+
-
-Non-detailed query data was also gathered. The table below compares the
-processing time using default sort keys to the processing using display_name
-as the sort key. Items were added with a 40 character display_name that was
-generated in an out-of-alphabetical sort order.
-
-+-------+--------------------+------------------------+------------+
-| Limit | Default keys (sec) | display_name key (sec) | Slowdown % |
-+=======+====================+========================+============+
-| 50    | 0.0560             | 0.0600                 | 7.1%       |
-+-------+--------------------+------------------------+------------+
-| 100   | 0.0813             | 0.0832                 | 2.3%       |
-+-------+--------------------+------------------------+------------+
-| 150   | 0.0848             | 0.0879                 | 3.7%       |
-+-------+--------------------+------------------------+------------+
-| 200   | 0.0874             | 0.0906                 | 3.7%       |
-+-------+--------------------+------------------------+------------+
-| 250   | 0.0985             | 0.1031                 | 4.7%       |
-+-------+--------------------+------------------------+------------+
-| 300   | 0.1229             | 0.1198                 | -2.5%      |
-+-------+--------------------+------------------------+------------+
-| 350   | 0.1262             | 0.1319                 | 4.5%       |
-+-------+--------------------+------------------------+------------+
-| 400   | 0.1282             | 0.1368                 | 6.7%       |
-+-------+--------------------+------------------------+------------+
-| 450   | 0.1458             | 0.1458                 | 0.0%       |
-+-------+--------------------+------------------------+------------+
-| 500   | 0.1770             | 0.1619                 | -8.5%      |
-+-------+--------------------+------------------------+------------+
-| 1000  | 0.2589             | 0.2659                 | 2.7%       |
-+-------+--------------------+------------------------+------------+
-
-In conclusion, the sort processing on the main data table has minimal impact
-on the overall processing time. For a detailed query, the sort time is dwarfed
-by other processing -- even if the sort time when up 3x it would only
-represent 4.8% of the total processing time for a detailed query with a limit
-of 1000 (and only increase the processing time by .11 sec with a limit of 50).
-
-Other deployer impact
----------------------
-
-The choice of sort keys has a minimal impact on data retrieval performance
-(see performance data above). Therefore, the user should be allowed to
-retrieve data in whatever order they need to for creating their views (see
-use case in the Problem Description).
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  Steven Kaufer
-
-Other contributors:
-  None
-
-Work Items
-----------
-
-Ideally the logic for processing the sort parameters would be common to all
-components and would be done in oslo; a similar blueprint is also being
-proposed in cinder:
-https://blueprints.launchpad.net/cinder/+spec/cinder-pagination
-
-Therefore, I see the following work items:
-
-* Create common functions to process the API parameters and create a list of
-  sort keys and directions
-* Update v2 and v3 APIs to retrieve the sort information and pass down to the
-  DB layer (requires changes to compute/api.py, objects/instance.py,
-  db/api.py, and db\sqlalchemy\api.py)
-* Update the novaclient to accept and process multiple sort keys and sort
-  directions
-
-
-Dependencies
-============
-
-* Related (but independent) change being proposed in cinder:
-  https://blueprints.launchpad.net/cinder/+spec/cinder-pagination
-
-
-Testing
-=======
-
-Both unit and Tempest tests need to be created to ensure that the data is
-retrieved in the specified sort order. Tests should also verify that the
-default sort keys ("created_at" and "id") are always appended to the user
-supplied keys (if the user did not already specify them).
-
-Testing should be done against multiple backend database types.
-
-
-Documentation Impact
-====================
-
-The /servers and /servers/detail API documentation will need to be updated to:
-
-- Reflect the new sorting parameters and explain that these parameters will
-  affect the order in which the data is returned.
-- Explain how the default sort keys will always be added at the end of the
-  sort key list
-
-The documentation could also note that query performance will be affected by
-the choice of the sort key, noting which keys are indexed.
-
-
-References
-==========
-
-None
diff --git a/specs/juno/object-subclassing.rst b/specs/juno/object-subclassing.rst
deleted file mode 100644
index 559380b..0000000
--- a/specs/juno/object-subclassing.rst
+++ /dev/null
@@ -1,146 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Support subclassing objects
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/object-subclassing
-
-Implement support for subclassing objects properly. If some hook, extension,
-or alternative DB API backend subclasses one of the base objects, the new
-object should be registered and all code should end up using this new class.
-
-Problem description
-===================
-
-Subclassing objects may be necessary to implement alternative DB API backends.
-There are probably some other use cases where it may be necessary to override
-some default object behavior. There was a rough plan to support subclassing
-objects in trunk. However, it wasn't fully thought through before we started
-landing all of the current object code. All objects do get registered right
-now, however there is no checking of the versions the objects advertise.
-Additionally, all code directly references the base object classes under
-nova/objects right now.
-
-Proposed change
-===============
-
-As objects are registered, check the version to see if it already exists. If
-so, replace the original in the tracked object list with the new one. As
-objects are registered, set an attribute on the nova.objects module to point
-to the newest class for latest version of the object. Replace all code that
-directly references object classes in modules under nova/objects with code
-that uses the nova.objects attribute. This has a side-effect of cleaning up
-imports. Instead of importing a ton of nova.object modules, only nova.objects
-will be imported.
-
-NOTE: This spec does not cover adding a hook/entrypoint for allowing
-alternative object implementations. That will be proposed at some point as
-a separate spec. At the moment, someone could specify an alternative db_backend
-and register alternative object implementations that way, but I don't see that
-as being the correct way to do that in the longer term.
-
-Alternatives
-------------
-
-There are probably some alternatives to setting attributes on the nova.objects
-module, like creating a method that returns the newest object and calling to
-that method everywhere. That would result in slightly lower performance. I
-suppose another solution to avoid having to change code everywhere is to
-rename all object classes to Base<Object> and then somehow setattr the latest
-version to be <Object> on the current modules. But, I rather like how using
-objects.<Object> everywhere will look.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-None.
-
-Other deployer impact
----------------------
-
-None.
-
-Developer impact
-----------------
-
-The changes will touch a lot of files. Anywhere there is a reference to
-something like instance_obj.Instance, it will change to objects.Instance.
-There's high chance of conflicts to resolve in either the object-subclassing
-patches or in other patches up for review.
-
-New patchsets should never import the module defining an object to reference
-the object class in it, directly. One should always import nova.objects and use
-nova.objects.<Object>.
-
-Objects register themselves when the module that defines them is imported.
-With this change, since there's likely no need to import object modules in
-code that uses them (you'll import nova.objects, instead), you must make sure
-that object modules are imported within nova/objects/__init__.py's
-register_all() method.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  cbehrens
-
-Work Items
-----------
-
-* Fix object registration to track object classes properly and set attributes
-  on the nova.objects module
-* Switch code to use the nova.objects module. This will be broken up into
-  areas of nova like nova/api and nova/compute, etc.
-
-Dependencies
-============
-
-None.
-
-Testing
-=======
-
-Tests will be modified to use nova.objects as well.
-
-Documentation Impact
-====================
-
-None.
-
-References
-==========
-
-None.
diff --git a/specs/juno/on-demand-compute-update.rst b/specs/juno/on-demand-compute-update.rst
deleted file mode 100644
index 1d40519..0000000
--- a/specs/juno/on-demand-compute-update.rst
+++ /dev/null
@@ -1,184 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=================================================
-Change compute updates from periodic to on demand
-=================================================
-
-Include the URL of your launchpad blueprint:
-
-https://blueprints.launchpad.net/nova/+spec/on-demand-compute-update
-
-Currently, all compute nodes update status info in the DB on a periodic
-basis (the period is currently 60 seconds). Given that the status of
-the node only changes at specific points (mainly image
-creation/destruction) this leads to significant DB overhead on a large
-system. This BP changes the update mechanism to only update the DB when
-a node state changes, specifically at node startup, instance creation
-and instance destruction.
-
-Problem description
-===================
-
-The status information about compute nodes is updated into the DB on
-a periodic basis.  This means that every compute node in the system
-updates a row in the DB once every 60 seconds (the default period for
-this update).  This is unnecessary and a scalability problem given that
-the status info is mostly static and doesn't change very often, mainly
-it changes when an instance is created or destroyed.
-
-Proposed change
-===============
-
-Compute node only sends an update if its status changes.  On a periodic
-interval (using the current default period of 60 seconds) the compute
-node will compare its status with the status saved from the last update.
-Only if the state or claims have changed will the DB be updated.
-
-The advantage to this method is that it should significantly cut down
-on the number of DB updates while changing almost nothing about the way
-the system currently works, compute node changes will still take 60
-seconds before they are updated but any change, for any reason, will
-ultimately be reported.
-
-One potential issue with this is a possible sensitivity concern, updates
-shouldn't be constantly sent if, for example, steady state system activity
-causes something like RAM usage to change slightly.  Some experiments will
-have to be run to decide if adding in a sensitivity control for certain
-status metrics is needed.  This can be a follow on optimization, if it's
-necessary, since this design is no worse than the current mechanism.
-
-Alternatives
-------------
-
-Another idea would be to only update the DB at certain well defined events.
-The update code would only be called at system start up, instance creation
-and instance deletion.  This would reduce the latency for status updates
-(the DB is modified as soon as the system state changes) but it suffers
-from some disadvantages:
-
-1)  Finding all of the appropriate events to record a status change.  Are
-statup/creation/destruction the only places where the system state changes,
-maybe there are other events that should be tracked.
-
-2)  Future changes could add new events that change status and recognizing
-that an update is needed is an easy mistake to miss.
-
-3)  Status could change unknown to the nova code, imagine something like a
-hot plug add of memory.
-
-Data model impact
------------------
-
-There is no change to the data that is being stored in the DB, all of the
-current fields are stored exactly as before, this BP is just changing the
-frequency at which those fields are updated.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-As measured by DB updates this change will clearly cause no more DB updates
-than the current technique and, assuming instances are created on a node
-at a rate of less then one every 60 seconds, should cause much fewer DB
-updates.
-
-Other deployer impact
----------------------
-
-None.
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  Don Dugger <donald.d.dugger@intel.com>
-
-Other contributors:
-
-Work Items
-----------
-
-Change should be fairly simple, change the current update code to only
-update the DB if a 'state_modified' routine returns true.  The
-'state_modified' routine returns false if the current state matches the
-last recorded state.  Otherwise, it saves the current state as the last
-recorded state and returns true.
-
-The 'state_modified' routine maintains an in memory copy of the current
-status of the compute node.  This copy of the state is compared with the
-current state and the update to the DB only happens if the copy and the
-current state differ.  Note that the in memory copy is initialized to
-zero values on node startup so that the first periodic update call will
-find a miss match between the two states and the DB will be updated.
-
-Based upon experiments it has been determined that a simple comparison
-of the entire current vs. the saved state is sufficient.  If, in the
-future, more rapidly changing data that shouldn't be stored in the DB
-is added to the compute node state then 'state_modified' can be easily
-changed to ignore such data that isn't needed in the DB.
-
-Dependencies
-============
-
-None.
-
-
-Testing
-=======
-
-A unit test will be created to make sure that the DB is updated when the
-compute node status changes and is not updated when the status doesn't
-change.
-
-
-Documentation Impact
-====================
-
-Section 5 of the Associate Training Guide
-
-http://docs.openstack.org/training-guides/content/associate-computer-node.html
-
-is slightly incorrect and should be fixed.  It currently says "All compute
-nodes (also known as hosts in terms of OpenStack) periodically publish their
-status, resources available and hardware capabilities to nova-scheduler
-through the queue."  This should be modified to reflect that the status is
-updated in the DB which is then queried by the scheduler.  (Note this is a
-generic fix that is really unrelated to this blueprint.)
-
-
-References
-==========
-
-None.
diff --git a/specs/juno/pci-passthrough-sriov.rst b/specs/juno/pci-passthrough-sriov.rst
deleted file mode 100644
index 46dba41..0000000
--- a/specs/juno/pci-passthrough-sriov.rst
+++ /dev/null
@@ -1,414 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-PCI SR-IOV passthrough to nova instance
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov
-
-Enable nova instance to be booted up with SR-IOV neutron ports.
-
-Problem description
-===================
-Right now it is possible to boot VM with general purpose PCI device passthrough
-by means of libvirt's managed hostdev device definition in the domain XML. A
-guide to use it can be found in [GPP_WIKI]_. However, it's not possible to
-request access to virtual network via SR-IOV NICs. Nova enhancments are
-required to support SR-IOV networking with Neutron.
-
-Traditionally, a neutron port is a virtual port that is either attached to a
-linux bridge or an openvswitch bridge on a compute node. With the introduction
-of SR-IOV based NIC (called vNIC), the virtual bridge is no longer required.
-Each SR-IOV port is associated with a virtual function (VF) that logically
-resides on a vNIC.  There exists two variants for SR-IOV networking. SR-IOV
-ports may be provided by Hardware-based Virtual Eithernet Bridging (HW VEB); or
-they may be extended to an upstream physical switch (IEEE 802.1br). In the
-latter case, port's configuration is enforced in the switch.  There are also
-two variants in connecting a SR-IOV port to its corresponding VF. A SR-IOV port
-may be directly connected to its VF. Or it may be connected with a macvtap
-device that resides on the host, which is then connected to the corresponding
-VF. Using a macvtap device makes live migration with SR-IOV possible.
-
-In the Icehouse release, a couple of blueprints from neutron side were approved
-and their associated patches were committed that enable the interactions
-between nova and neutron for SR-IOV networking. Refer to [VIF_DETA]_ and
-[BIND_PRF]_ for details about them.
-
-Another blueprint [VNIC_TYP]_ added the support in the neutron port API to
-allow users to specify vnic-type when creating a neutron port. The currently
-supported vnic-types are:
-
-* normal: a traditional virtual port that is either attached to a linux bridge
-  or an openvswitch bridge on a compute node.
-* direct: an SR-IOV port that is directly attached to a VM
-* macvtap: an SR-IOV port that is attached to a VM via a macvtap device.
-
-This specification attempts to build up on top of the above-mentioned neutron
-changes and address the following functionalities in Nova so that SR-IOV
-networking in openstack is fully functional end-to-end:
-
-1. Generating libvirt domain XML and network XML that enables SR-IOV for
-   networking.
-2. Scheduling based on SR-IOV port's network connectivity.
-
-The initial use case that is targeted in this specification and therefore for
-Juno is to boot a VM with one or more vNICs that may use different vnic-types.
-Particularly a user would do the following to boot a VM with SR-IOV vnics:
-
-* create one or more neutron ports. For example:
-
-::
-
-  neutron port-create <net-id> --binding:vnic-type direct
-
-* boot a VM with one or more neutron ports. For example:
-
-::
-
-  nova boot --flavor m1.large --image <image>
-            --nic port-id=<port1> --nic port-id=<port2>
-
-Note that in the nova boot API, users can specify either a port-id or a net-id.
-If it's the latter case, it's assumed that the user is requesting a normal
-virtual port (which is not a SR-IOV port).
-
-This specification will make use of the existing PCI passthrough
-implementation, and make a few enhancements to enable the above use cases.
-Therefore, the existing PCI passthrough support as documented by [GPP_WIKI]_
-works as it is for general-purpose PCI passthrough.
-
-Proposed change
-===============
-
-To schedule an instance with SR-IOV ports based on their network connectivity,
-the neutron ports' associated physical networks have to be used in making the
-scheduling decision. A VF has to be selected for each of the neutron port.
-Therefore, the VF's associated physical network has to be known to the system,
-and the selected VF's associated physical network has to match that from the
-neutron port. To make the above happen, this specification proposes associating
-an extra tag called *physical_network* to each networking VF. In addition, nova
-currently has no knowledge of a neutron port's associated physical network.
-Therefore, nova needs to make extra calls to neutron in order to retrieve this
-information from neutron. In the following, detailed changes in nova will be
-described on how to achieve that.
-
-Note that this specification only supports libvirt driver.
-
-PCI Whitelist
--------------
-
-This specification introduces a few enhancements to the existing PCI whitelist:
-
-* allows aggregated declaration of PCI devices by using '*' and '.'
-* allows tags to be associated with PCI devices.
-
-Note that it's compatible with the previous PCI whitelist definition. And
-therefore, the existing functionalities associated with the PCI whitelist work
-as is.
-
-with '[' to indicate 0 or one time occurrence, '{' 0 or multiple occurrences,
-'|' mutually exclusive choice, a whitelist entry is defined as:
-
-::
-
-      ["device_id": "<id>",] ["product_id": "<id>",]
-      ["address": "[[[[<domain>]:]<bus>]:][<slot>][.[<function>]]" |
-       "devname": "PCI Device Name",]
-      {"tag":"<tag_value>",}
-
-*<id>* can be a '*' or a valid *device/product id* as displayed by the linux
-utility lspci. The *address* uses the same syntax as it's in lspci. Refer to
-lspci's manual for its description about the '-s' switch. The *devname* can be
-a valid PCI device name. The only device names that are supported in this
-specification are those that are displayed by the linux utility *ifconfig -a*
-and correspond to either a PF or a VF on a vNIC. There may be 0 or more tags
-associated with an entry.
-
-If the device defined by the *address* or *devname* corresponds to a SR-IOV PF,
-all the VFs under the PF will match the entry.
-
-For SR-IOV networking, a pre-defined tag "physical_network" is used to define
-the physical network that the devices are attached to. A whitelist entry is
-defined as:
-
-::
-
-      ["device_id": "<id>",] ["product_id": "<id>",]
-      ["address": "[[[[<domain>]:]<bus>]:][<slot>][.[<function>]]" |
-       "devname": "Ethernet Interface Name",]
-      "physical_network":"name string of the physical network"
-
-Multiple whitelist entries per host are supported as they already are. The
-fields *device_id*, *product_id*, and *address* or *devname* will be matched
-against PCI devices that are returned as a result of querying libvirt.
-
-Whitelist entries are defined in nova.conf in the format:
-
-::
-
-    pci_passthrough_whitelist = {<entry>}
-
-{<entry>} is a json dictionary and is defined as in above.
-*pci_passthrough_whitelist* is a plural configuration, and therefore can appear
-multiple times in nova.conf.
-
-Some examples are:
-
-::
-
-    pci_passthrough_whitelist = {"devname":"eth0",
-                                 "physical_network":"physnet"}
-
-    pci_passthrough_whitelist = {"address":"*:0a:00.*",
-                                 "physical_network":"physnet1"}
-
-    pci_passthrough_whitelist = {"address":":0a:00.",
-                                 "physical_network":"physnet1"}
-
-    pci_passthrough_whitelist = {"vendor_id":"1137","product_id":"0071"}
-
-    pci_passthrough_whitelist = {"vendor_id":"1137","product_id":"0071",
-                                 "address": "0000:0a:00.1",
-                                 "physical_network":"physnet1"}
-
-PCI stats
----------
-
-On the compute node, PCI devices are matched against the PCI whitelist entries
-in the order as they are defined in the nova.conf file. Once a match is found,
-the device is placed in the corresponding PCI stats entry.
-
-If a device matches a PCI whitelist entry, and if the PCI whitelist entry is
-tagged, the tags together with *product_id* and *vendor_id* will be used as
-stats keys; otherwise, the existing predefined keys will be used.
-
-A PCI whitelist entry for SR-IOV networking will be tagged with a physical
-network name. Therefore, the physical network name is used as the stats key for
-SR-IOV networking devices. Conceptually speaking for SR-IOV networking, a PCI
-stats entry keeps track of the number of SR-IOV ports that are attached to a
-physical network on a compute node. And for scheduling purpose, it can be
-considered as a tuple of
-
-::
-
-    <host_name> <physical_network_name> <count>
-
-When a port is requested from a physical network, the compute nodes that host
-the physical network can be found from the stats entries. The existing PCI
-passthrough filter in nova scheduler works without requiring any change in
-support of SR-IOV networking.
-
-There is no change in how the stats entries are updated and persisted into the
-compute_nodes database table with the use of nova resource tracker.  Currently,
-a collumn called *pci_stats* in the compute_nodes database table is used to
-store the PCI stats as a JSON document. The PCI stats JSON document is
-basically a list of stats entries in the format of *<key1> <key2> ....<keyn>* :
-*<count>*. This will not be changed for SR-IOV networking. Specifically for
-SR-IOV networking, however, PCI stats records are keyed off with the tag
-*physical_network_name*, plus *product_id* and *vendor_id*. a stats entry for
-SR-IOV networking will look like:
-
-::
-
-   <physical_network_name>, <product_id>, <vendor_id> : <count>.
-
-requested_networks (NICs)
--------------------------
-
-Currently, each requested network is a tuple of
-
-::
-
-    <neutron-net-id> <v4-fixed-ip> <neutron-port-id>
-
-Either neutron-net-id or neutron-port-id must have a valid value, and
-v4-fixed-ip can be None. For each --nic option specified in the *nova boot*
-command, a requested_network tuple is created. All the requested_network tuples
-are passed to the compute node, and the compute service running on the node
-uses the information to request neutron services. This specification proposes
-one additional field in the tuple: *pci-request-id*.
-
-Corresponding to each requested_network tuple, there is a neutron port with a
-valid vnic-type. If the vnic-type is direct or macvtap, a valid
-*pci_request_id* must be populated into the tuple (see below for details). The
-*pci-request-id* is later used to locate the PCI device from PCI manager that
-is allocated for the requested_network tuple (therefore the NIC).
-
-PCI Requests
-------------
-
-Currently, pci_requests as key and a JSON doc string as associated value are
-stored in the instance's system metadata. In addition, all the PCI devices
-allocated for PCI passthrough are treated the same in terms of generating
-libvirt xml. However, for SR-IOV networking, special libvirt xml is required.
-Further, we need a way to correlate the allocated device with the requested
-network (NIC) later on during the instance boot process. In this specification,
-we propose the use of *pci_request_id* for that purpose.
-
-Each PCI request is associated with a *pci_request_id* that is generated while
-creating/saving the PCI request to the instance's system metadata. The
-*pci_request_id* is used on the compute node to retrieve the allocated PCI
-device. Particularly for SR-IOV networking, a PCI request is expressed as
-
-::
-
-   "physical_network" : <name>
-   "count" : 1
-   "pci_request_id" : <request-uuid>
-
-For each --nic specified in the 'nova boot', nova-api creates a requested
-network tuple. For a SR-IOV NIC, it creates a PCI request and as a
-result a *pci_request_id* is generated and saved in the PCI request spec. The
-same *pci_request_id* is also saved in the requested_network (Refer to the last
-section).
-
-nova neutronv2 and VIF
--------------------------------------
-
-Note that Nova network will not be enhanced to support SR-IOV. However, Nova
-modules that are responsible for interacting with neutron need to be enhanced.
-
-Refer to [BIND_PRF]_, [VIF_DETA]_, [VNIC_TYP]_ that has added the
-functionalities required to support SR-IOV ports in neutron. Accordingly, nova
-neutronv2 will be enhanced to work with them in support of SR-IOV ports.
-Particularly:
-
-* When nova processes the --nic options, physical network names will be
-  retrieved from neutron. This needs to be done by using neutron provider
-  extension with admin access. As a result, additional neutron calls will be
-  made to retrieve the physical network name.
-* When nova updates neutron ports, binding:profile needs to be populated with
-  pci information that includes pci_vendor_info, pci_slot, physical_network.
-* After nova successfully updates the neutron ports, it retrieves the ports'
-  information from neutron that are used to populate VIF objects. New
-  properties will be added in the VIF class in support of binding:profile,
-  binding:vif_details and binding:vnic_type.
-
-nova VIF driver
----------------
-
-Each neutron port is associated with a vif-type. The following VIF types are
-related to SR-IOV support:
-
-* VIF_TYPE_802_QBH: corresponds to IEEE 802.1BR (used to be IEEE 802.1Qbh)
-* VIF_TYPE_HW_VEB: for vNIC adapters that supports virtual embedded bridging
-* VIF_TYPE_802_QBG: corresponds to IEEE 802.1QBG. However, this existing vif
-  type may not be useful now because the libvirt parameters for 1QBG
-  (managerid, typeidversion and instanceid) are not supported by known neutron
-  plugins that support SR-IOV.
-
-The nova generic libvirt VIF driver will be enhanced to support the first two
-VIF types. This includes populating the VIF config objects and generating the
-interface XMLs.
-
-Alternatives
-------------
-
-N/A
-
-Data model impact
------------------
-
-Currently, a nova object *PciDevice* is created for each PCI passthrough
-device. The database table *pci_devices* is used to persist the *PciDevice*
-nova objects. A new field *request_id* will be added in the *PciDevice* nova
-object. Correspondingly, a new column *request_id* is added in the database
-table *pci_devices*. Database migration script will be incorporated
-accordingly.
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-The physical network to which a port is connected needs to be retrieved from
-neutron, which requires additional calls to neutron. Particularly, nova will
-call neutron *show_port* to check the port's *vnic_type*. If the *vnic_type* is
-either *direct* or *macvtap*, it will call neutron *show_network* to retrieve
-the associated physical network. As a consequence, the number of calls to
-neutron will be slightly increased when *port-id* is specified in the --nic
-option in nova boot.
-
-Other deployer impact
----------------------
-
-No known deployer impact other than configuring the PCI whitelist for SR-IOV
-networking devices.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  baoli
-
-Other contributors:
-  TBD
-
-Work Items
-----------
-
-* PCI whitelist
-* PCI request
-* PCI stats
-* DB change and the required migration script, PCI device object change
-* neutronv2
-* VIF
-* libvirt generic VIF driver and instance configuration
-* nova compute api retrieving physical network, change of requested_networks
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Both unit and tempest tests need to be created to ensure proper functioning of
-SR-IOV networking. For tempest testing, given the nature of SR-IOV depending on
-hardware, it may require vendor support and use of proper neutron ML2 mechanism
-drivers. Cisco Neutron CI and Mellanox External Testing need to be enhanced in
-support of SR-IOV tempest testing.
-
-Documentation Impact
-====================
-
-* document new whitelist configuration changes
-* a user guide/wiki on how to use SR-IOV networking in openstack
-
-References
-==========
-.. [GPP_WIKI] `Generic PCI Passthrough WIKI <https://wiki.openstack.org/wiki/Pci_passthrough>`_
-.. [VIF_DETA] `Extensible port attribute for plugin to provide details to VIF driver  <https://blueprints.launchpad.net/neutron/+spec/vif-details>`_
-.. [BIND_PRF] `Implement the binding:profile port attribute in ML2 <https://blueprints.launchpad.net/neutron/+spec/ml2-binding-profile>`_
-.. [VNIC_TYP] `Add support for vnic type request to be managed by ML3 mechanism drivers <https://blueprints.launchpad.net/neutron/+spec/ml2-request-vnic-type>`_
diff --git a/specs/juno/per-aggregate-filters.rst b/specs/juno/per-aggregate-filters.rst
deleted file mode 100644
index 2fa1051..0000000
--- a/specs/juno/per-aggregate-filters.rst
+++ /dev/null
@@ -1,145 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=====================================
-Scheduler: Adds per-aggregate filters
-=====================================
-
-https://blueprints.launchpad.net/nova/+spec/per-aggregate-disk-allocation-ratio
-https://blueprints.launchpad.net/nova/+spec/per-aggregate-max-instances-per-host
-https://blueprints.launchpad.net/nova/+spec/per-aggregate-max-io-ops-per-host
-
-The aim of this bp is to add the ability to the filters DiskFilter,
-NumInstancesFilter and IoOpsFilter to set our options by aggregates.
-
-Problem description
-===================
-
-Operator wants to define different filtering options (disk_allocation_ratio,
-max_instances_per_host, max_io_ops_per_host) for a subset of compute hosts.
-
-Proposed change
-===============
-
-Create new filters that extend DiskFilter, NumInstancesFilter and
-IoOpsAggregateFilter to provide the ability to read the metadata from
-aggregates. If no valid values found fall back to the global default
-configurations set in nova.conf.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-Not related with this bp but a performance impact has been reported
-to the bug 1300775_
-about using aggregate with the scheduler on large
-cluster.
-
-.. _1300775: https://bugs.launchpad.net/nova/+bug/1300775
-
-Other deployer impact
----------------------
-
-The operator needs to update the scheduler's nova.conf to activate filters,
-also he has to set metadata of the aggregates with the configurations options
-disk_allocation_ratio, max_instances_per_host, max_io_ops_per_host.
-
-::
-
-  $ # This one provides for hosts in the aggregate 'agr1' the possibility to
-  $ # host 60 instances.
-  $ nova aggregate-set-metadata agr1 set metadata max_instances_per_host=60
-
-  $ # This one provides for hosts in the aggregate 'agr2' the possibility to
-  $ # oversubscribe disk allocation and configures the scheduler to ignore
-  $ # hosts that have currently more than 3 heavy operations.
-  $ nova aggregate-set-metadata agr2 set\
-  $   metadata max_io_ops_per_host=3
-  $   disk_allocation_ratio=3
-
-The operator also needs to reload the scheduler service to activate this new
-filter.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  sahid-ferdjaoui
-
-Other contributors:
-  <None>
-
-Work Items
-----------
-
- * New filter AggregateNumInstancesFilter
- * New filter AggregateDiskFilter
- * New filter AggregateIoOpsFilter
-
-Dependencies
-============
-
- * A bug has been open to factory per-aggregate logic.
-   https://bugs.launchpad.net/nova/+bug/1301340
-
-Testing
-=======
-
-We need to add unit tests in test_host_filters.py also we probably need to
-think about adding functional tests in tempest.
-
-Documentation Impact
-====================
-
-We need to refer these new filters in the documentation, also
-'doc/source/devref/filter_scheduler.rst' needs to be updated.
-
-References
-==========
-
-These blueprints was accepted for icehouse but because of a work started to add
-helper that provides utility methods to get metadata from aggregates and so
-remove duplicate code between filters (bug 1301340_). The blueprints was
-deferred.
-
-.. _1301340: https://bugs.launchpad.net/nova/+bug/1301340
diff --git a/specs/juno/persistent-resource-claim.rst b/specs/juno/persistent-resource-claim.rst
deleted file mode 100644
index ff702cd..0000000
--- a/specs/juno/persistent-resource-claim.rst
+++ /dev/null
@@ -1,201 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Persistent resource claims
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/persistent-resource-claim
-
-This blueprint plans to enhance the compute resource tracker to keep resource
-claim as persistent to across nova-compute restart. This will be helpful
-to move the resource claim process to the conductor.
-
-Problem description
-===================
-
-The resource tracker provides an interface to claim resources for an instance.
-However, the claim result is only kept in memory instead of kept persistently
-and a context manager is returned to the caller.
-
-There are several potential issue with this implementation. Firstly, it is
-not easy to support two-phases resources claim, because it return the claim
-as a context manager. Secondly without the persistent claim, the resource
-tracker has to recalculate the claims from instances and migration object,
-which requires more DB access and also requires locks to create the migration
-object and to set the instance's host/node. Thirdaly, it's not easy to move
-the _prep_resize() to the conductor because the claim is not persistent and
-can't be invoked remotely.
-
-Proposed change
-===============
-
-We suggest to change the resource tracker to track the resources claim and
-persist the resources claim.
-
-* When resources claim, the resource tracker will track the result claim. Each
-  claim will be identified by compute node and a unique ID in the node.
-
-* The resources claim is kept persistently by compute manager.
-
-  There are several solutions to persist the claim like keeping it in
-  central DB or in a local sqlite. In this spec, we will persistent the claim
-  in local sqlite only, and a claim table is created to track the resources
-  claim. We will enhance it later to be configurable as local sqlite or
-  central DB. A mechanism like service_group is used to make the future
-  extension easier.
-
-  See "Alternative" for more info.
-
-* The claim persistent code defines claim format version and upgrades the claim
-  table if new version is required, so that we can enhance the resources claim,
-  like support for extra_info, easily. A separate sqlite table is created to
-  save the current claim table's version information.
-
-  When a compute service is restarted after upgrade, it will check the claim
-  table version. If the claim table version is lower than the latest version
-  in the claim persistent code, it will upgrade the claim table to
-  latest version and then update the version table. The upgrade code knows
-  about the schema for each version so that we don't need keep schema
-  information in the sqlite.
-
-* When the compute node upgrades from non-persistent claim to persist
-  claim, the upgrade code will find the table does not exist and thus
-  will create the table from scratch based on the instance/migration
-  information.
-
-* The compute manager's periodic task will clean up any orphan claims. If
-  a resources claim has no corresponding instance or migration object in the
-  node, and it has been created for a specified period, it's an orphan claim
-  and will be cleaned. The 'specified' time is a configuration item.
-
-  Also if a server is evacuated when host is shutdown, the corresponding
-  resources claim will be released when the compute service is restarted.
-
-  In future, such clean up should happen in conductor which will take response
-  of garbage collector.
-
-Alternatives
-------------
-
-We had some discussions on how to keep the claims persistent. Originally it's
-proposed to keep the claims in central DB. central DB will provide a global
-view and will be more robust, but it will impact performance for each periodic
-task. Later, it's suggested to keep in sqlite first which will provide much
-better performance and can be extended to central DB in future.
-
-Data model impact
------------------
-
-A sqlite table (claim table) is created to keep the claim. Below is
-the data model to be used in sqlite. Although defined with sqlite
-type, but it should similar to central DB also:
-
-id: INTEGER
-host: TEXT
-node: TEXT
-instance_uuid: TEXT
-vcpus: INTEGER
-memory_mb: INTEGER
-disk_gb: INTEGER
-pci: TEXT
-resize_target: INTEGER
-created_at: TEXT
-
-The resize_target is to distinguish the resources claims in the same host for
-the same instance, when resize in the same host. It is in fact a boolean value
-stored as integers 0 (false) and 1 (true).
-
-The created_at is the timestamp when the claim is created. It's a ISO 8601
-format string.
-
-Another table is created to keep the claim format version information.
-table_name: TEXT
-version: INTEGER
-
-This table has only one entry, with the table_name as "claims" and the version
-is the version of the claims in the claim table. As stated above, the upgrade
-code knows about the schema of each version and knows how to upgrade between
-versions.
-
-REST API impact
----------------
-
-No.
-
-Security impact
----------------
-
-No.
-
-Notifications impact
---------------------
-
-No.
-
-Other end user impact
----------------------
-
-No.
-
-Performance Impact
-------------------
-
-No for this BP since we will not cover the DB solution.
-
-Other deployer impact
----------------------
-
-A new configuration 'claim_db' will be added to define how the sqlite
-database is stored on disk. It will be a relative path to state_path
-configuration item. The default value is claim.sqlite, which means
-$state_path/claim.sqlite.
-
-A new configuration 'claim_expiry_time'. A claim that has been created for
-'claim_expiry_time' seconds and not is not associated with a instance or
-migration object is an orphan claim and will be released.
-The default value is 300 seconds.
-
-Developer impact
-----------------
-
-No
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  yunhong-jiang
-
-Work Items
-----------
-
-* Claims persistent code.
-* Update the resource tracker.
-
-Dependencies
-============
-No
-
-Testing
-=======
-
-We should have test code to check the sqlite is really populated correctly.
-
-Documentation Impact
-====================
-
-The documentation should be updated to describe the 'claim_db' configuration,
-where is the sqlite db lives now. Also the documents should describe how the
-upgrade works according to the "Proposed change" section.
-
-References
-==========
-
-https://wiki.openstack.org/wiki/Persistent_resource_claim
diff --git a/specs/juno/quiesced-image-snapshots-with-qemu-guest-agent.rst b/specs/juno/quiesced-image-snapshots-with-qemu-guest-agent.rst
deleted file mode 100644
index 3189326..0000000
--- a/specs/juno/quiesced-image-snapshots-with-qemu-guest-agent.rst
+++ /dev/null
@@ -1,131 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=====================================================================
-Quiescing filesystems with QEMU guest agent during image snapshotting
-=====================================================================
-
-https://blueprints.launchpad.net/nova/+spec/quiesced-image-snapshots-with-qemu-guest-agent
-
-When QEMU Guest Agent is installed in a kvm instance, we can request the
-instance to freeze filesystems via libvirt during snapshotting to make the
-snapshot consistent.
-
-Problem description
-===================
-
-Currently we need to quiesce filesystems (fsfreeze) manually before
-snapshotting an image of active instances to create consistent backups.
-This should be automated when QEMU Guest Agent is enabled.
-
-Proposed change
-===============
-
-When QEMU Guest Agent is enabled in an instance, Nova-compute libvirt driver
-will request the agent to freeze the filesystems (and applications if
-fsfreeze-hook is installed) before taking snapshot of the image.
-After taking snapshot, the driver will request the agent to thaw the
-filesystems.
-
-The prerequisites of this feature are:
-
-1. the hypervisor is 'qemu' or 'kvm'
-
-2. libvirt >= 1.2.5 (which has fsFreeze/fsThaw API) is installed in the
-   hypervisor
-
-3. 'hw_qemu_guest_agent=yes' property in the image metadata is set to 'yes'
-   and QEMU Guest Agent is installed and enabled in the instance
-
-When quiesce is failed even though these conditions are satisfied
-(e.g. the agent is not responding), snapshotting may fail by exception
-not to get inconsistent snapshots.
-
-Alternatives
-------------
-
-Rewrite nova's snapshotting with libvirt's domain.reateSnapshot API with
-VIR_DOMAIN_SNAPSHOT_CREATE_QUIESCE flag, although it will change the current
-naming scheme of disk images. In addition, it cannot be leveraged to implement
-live snapshot of cinder volumes.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-While taking snapshots, disk writes from the instance are blocked.
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  tsekiyama
-
-Work Items
-----------
-
-Implement the automatic quiesce during snapshotting when it is available.
-Now the code is ready to  review: https://review.openstack.org/#/c/72038/
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Live snapshotting with an image with qemu-guest-agent should be added to
-scenario tests.
-Note that it requires environment with libvirt >= 1.2.5.
-
-Documentation Impact
-====================
-
-Need to document how to use this feature in the operation guide (which
-currently recommends you use the fsfreze tool manually).
-
-References
-==========
-
-None
diff --git a/specs/juno/rbd-clone-image-handler.rst b/specs/juno/rbd-clone-image-handler.rst
deleted file mode 100644
index 66c7186..0000000
--- a/specs/juno/rbd-clone-image-handler.rst
+++ /dev/null
@@ -1,222 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===================================================
-Storage: Copy-on-write cloning for RBD-backed disks
-===================================================
-
-https://blueprints.launchpad.net/nova/+spec/rbd-clone-image-handler
-
-Currently RBD-backed ephemeral disks are created by downloading an image from
-Glance to a local file, then uploading that file into RBD. Even if the file is
-cached, uploading may take a long time, since 'rbd import' is synchronous and
-slow. If the image is already stored in RBD by Glance, there's no need for any
-local copies - it can be cloned to a new image for a new disk without copying
-the data at all.
-
-
-Problem description
-===================
-
-The primary use case that benefits from this change is launching an instance
-from a Glance image where Ceph RBD backend is enabled for both Glance and Nova,
-and Glance images are stored in RBD in RAW format.
-
-Following problems are addressed:
-
-* Disk space on compute nodes is wasted by caching an additional copy of the
-  image on each compute node that runs instances from that image.
-
-* Disk space in Ceph is wasted by uploading a full copy of an image instead of
-  creating a copy-on-write clone.
-
-* Network capacity is wasted by downloading the image from RBD to a compute
-  node the first time that node launches an instance from that image, and by
-  uploading the image to RBD every time a new instance is launched from the
-  same image.
-
-* Increased time required to launch an instance reduces elasticity of the cloud
-  environment and increases the number of in-flight operations that have to be
-  maintained by Nova.
-
-
-Proposed change
-===============
-
-Extract RBD specific utility code into a new file, align its structure and
-provided functionality in line with similar code in Cinder. This includes the
-volume cleanup code that should be converted from rbd CLI to using the RBD
-library.
-
-Add utility functions to support cloning, including checks whether image exists
-and whether it can be cloned.
-
-Add direct_fetch() method to nova.virt.libvirt.imagebackend, make its
-implementation in the Rbd subclass try to clone the image when possible.
-Following criteria are used to determine that the image can be cloned:
-
-* Image location uses the rbd:// schema and contains a valid reference to an
-  RBD snapshot;
-
-* Image location references the same Ceph cluster as Nova configuration;
-
-* Image disk format is 'raw';
-
-* RBD snapshot referenced by image location is accessible by Nova.
-
-Extend fetch_to_raw() in nova.virt.images to try direct_fetch() when a new
-optional backend parameter is passed. Make the libvirt driver pass the backend
-parameter.
-
-Instead of calling disk.get_disk_size() directly from verify_base_size(), which
-assumes the disk is stored locally, add a new method that is overridden by the
-Rbd subclass to get the disk size.
-
-Alternatives
-------------
-
-An alternative implementation based on the image-multiple-location blueprint
-(https://blueprints.launchpad.net/glance/+spec/multiple-image-locations) was
-tried in Icehouse. It was ultimately reverted, which can be attributed to a sum
-of multiple reasons:
-
-* The implementation in https://review.openstack.org/33409 took a long time to
-  stabilize, and didn't land until hours before Icehouse feature freeze.
-
-* The impact of https://review.openstack.org/33409 was significantly larger
-  than that of the ephemeral RBD clone change that was built on top of it.
-
-* The impact included exposing nova.image.glance._get_locations() method that
-  relies on Glance API v2 to code paths that assume Glance API v1, which caused
-  LP bug #1291014 (https://bugs.launchpad.net/nova/+bug/1291014).
-
-This design has a significantly smaller footprint, and is mostly isolated to
-the RBD image backend in the libvirt driver.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-When Ceph RBD backend is enabled for Glance and Nova, there will be a
-noticeable difference in time and resource consumption when launching instances
-from Glance images in RAW and non-RAW formats.
-
-Performance Impact
-------------------
-
-In the primary use case defined in the `Problem description`_ section above,
-there will be a significant performance improvement.
-
-In other use cases, libvirt driver will introduce one more API call to Glance
-to retrieve a list of image locations when RBD backend is enabled. The
-performance impact of that call is insignificant compared to the time and
-resources it takes to fetch a full image from Glance.
-
-Other deployer impact
----------------------
-
-None.
-
-Developer impact
-----------------
-
-None.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  jdurgin
-
-Other contributors:
-  angdraug
-
-Work Items
-----------
-
-Current implementation (see `References`_) consists of following changes:
-
-* Move libvirt RBD utilities to a new file
-
-* Use library instead of CLI to cleanup RBD volumes
-
-* Enable cloning for rbd-backed ephemeral disks
-
-
-Dependencies
-============
-
-None.
-
-
-Testing
-=======
-
-This is a non-functional change with no impact on the test cases that need to
-be covered.
-
-There is work currently going on to get all of tempest running against an
-environment using Ceph in the OpenStack CI environment.  The first step is ceph
-support for devstack, which you can see here:
-
-    https://review.openstack.org/#/c/65113
-
-There's also a test devstack patch with forces ceph to be enabled, which
-results in all of the devstack jobs being run with ceph enabled.  You can find
-that here:
-
-    https://review.openstack.org/#/c/107472/
-
-There are some tests failing (14 and 15 the first couple of runs).  However,
-that also means that the vast majority of tests that cover this code (anything
-that spawns an instance) are passing.  So, we at least have a way to run these
-tests on demand against master.  Once the devstack patch merges, we will enable
-a job that can run against patches in all projects (perhaps experimental to
-start with).
-
-Fuel CI also includes a suite of tests for OpenStack deployments with Ceph:
-https://github.com/stackforge/fuel-main/blob/master/fuelweb_test/tests/test_ceph.py
-
-
-Documentation Impact
-====================
-
-None.
-
-
-References
-==========
-
-Mailing list discussions:
-http://lists.openstack.org/pipermail/openstack-dev/2014-March/029127.html
-http://lists.ceph.com/pipermail/ceph-users-ceph.com/2014-March/008659.html
-
-Current implementation:
-https://github.com/angdraug/nova/tree/rbd-ephemeral-clone
-https://review.openstack.org/#/q/status:open+topic:bp/rbd-clone-image-handler,n,z
diff --git a/specs/juno/redirects b/specs/juno/redirects
new file mode 100644
index 0000000..49db867
--- /dev/null
+++ b/specs/juno/redirects
@@ -0,0 +1,88 @@
+add-differencing-vhdx-resize-support.rst implemented/add-differencing-vhdx-resize-support.rst
+add-ironic-driver.rst implemented/add-ironic-driver.rst
+allow-image-to-be-specified-during-rescue.rst implemented/allow-image-to-be-specified-during-rescue.rst
+backportable-db-migrations-juno.rst implemented/backportable-db-migrations-juno.rst
+better-support-for-multiple-networks.rst implemented/better-support-for-multiple-networks.rst
+compute-manager-objects-juno.rst implemented/compute-manager-objects-juno.rst
+config-drive-image-property.rst implemented/config-drive-image-property.rst
+convert_ec2_api_to_use_nova_objects.rst implemented/convert_ec2_api_to_use_nova_objects.rst
+cross-service-request-id.rst implemented/cross-service-request-id.rst
+enabled-qemu-memballoon-stats.rst implemented/enabled-qemu-memballoon-stats.rst
+extensible-resource-tracking.rst implemented/extensible-resource-tracking.rst
+find-host-and-evacuate-instance.rst implemented/find-host-and-evacuate-instance.rst
+hyper-v-console-log.rst implemented/hyper-v-console-log.rst
+hyper-v-soft-reboot.rst implemented/hyper-v-soft-reboot.rst
+i18n-enablement.rst implemented/i18n-enablement.rst
+instance-network-info-hook.rst implemented/instance-network-info-hook.rst
+juno-slaveification.rst implemented/juno-slaveification.rst
+libvirt-disk-discard-option.rst implemented/libvirt-disk-discard-option.rst
+libvirt-domain-listing-speedup.rst implemented/libvirt-domain-listing-speedup.rst
+libvirt-driver-domain-metadata.rst implemented/libvirt-driver-domain-metadata.rst
+libvirt-lxc-user-namespaces.rst implemented/libvirt-lxc-user-namespaces.rst
+libvirt-volume-snap-network-disk.rst implemented/libvirt-volume-snap-network-disk.rst
+move-prep-resize-to-conductor.rst implemented/move-prep-resize-to-conductor.rst
+nfv-multiple-if-1-net.rst implemented/nfv-multiple-if-1-net.rst
+object-subclassing.rst implemented/object-subclassing.rst
+on-demand-compute-update.rst implemented/on-demand-compute-update.rst
+pci-passthrough-sriov.rst implemented/pci-passthrough-sriov.rst
+per-aggregate-filters.rst implemented/per-aggregate-filters.rst
+rbd-clone-image-handler.rst implemented/rbd-clone-image-handler.rst
+refactor-network-api.rst implemented/refactor-network-api.rst
+remove-cast-to-schedule-run-instance.rst implemented/remove-cast-to-schedule-run-instance.rst
+rescue-attach-all-disks.rst implemented/rescue-attach-all-disks.rst
+return-status-for-hypervisor-node.rst implemented/return-status-for-hypervisor-node.rst
+scheduler-lib.rst implemented/scheduler-lib.rst
+serial-ports.rst implemented/serial-ports.rst
+server-group-quotas.rst implemented/server-group-quotas.rst
+servers-list-support-multi-status.rst implemented/servers-list-support-multi-status.rst
+support-cinderclient-v2.rst implemented/support-cinderclient-v2.rst
+use-oslo-vmware.rst implemented/use-oslo-vmware.rst
+user-defined-shutdown.rst implemented/user-defined-shutdown.rst
+v2-on-v3-api.rst implemented/v2-on-v3-api.rst
+v3-api-schema.rst implemented/v3-api-schema.rst
+v3-diagnostics.rst implemented/v3-diagnostics.rst
+virt-driver-numa-placement.rst implemented/virt-driver-numa-placement.rst
+virt-driver-vcpu-topology.rst implemented/virt-driver-vcpu-topology.rst
+virt-objects-juno.rst implemented/virt-objects-juno.rst
+vmware-hot-plug.rst implemented/vmware-hot-plug.rst
+vmware-spawn-refactor.rst implemented/vmware-spawn-refactor.rst
+add-all-in-list-operator-to-extra-spec-ops.rst approved/add-all-in-list-operator-to-extra-spec-ops.rst
+add-virtio-scsi-bus-for-bdm.rst approved/add-virtio-scsi-bus-for-bdm.rst
+allocation-ratio-to-resource-tracker.rst approved/allocation-ratio-to-resource-tracker.rst
+clean-logs.rst approved/clean-logs.rst
+cold-migration-with-target.rst approved/cold-migration-with-target.rst
+db2-database.rst approved/db2-database.rst
+ec2-volume-and-snapshot-tags.rst approved/ec2-volume-and-snapshot-tags.rst
+encryption-with-barbican.rst approved/encryption-with-barbican.rst
+enforce-unique-instance-uuid-in-db.rst approved/enforce-unique-instance-uuid-in-db.rst
+input-output-based-numa-scheduling.rst approved/input-output-based-numa-scheduling.rst
+io-ops-weight.rst approved/io-ops-weight.rst
+libvirt-driver-class-refactor.rst approved/libvirt-driver-class-refactor.rst
+libvirt-sheepdog-backed-instances.rst approved/libvirt-sheepdog-backed-instances.rst
+libvirt-start-lxc-from-block-devices.rst approved/libvirt-start-lxc-from-block-devices.rst
+log-request-id-mappings.rst approved/log-request-id-mappings.rst
+lvm-ephemeral-storage-encryption.rst approved/lvm-ephemeral-storage-encryption.rst
+make-resource-tracker-use-objects.rst approved/make-resource-tracker-use-objects.rst
+migrate-libvirt-volumes.rst approved/migrate-libvirt-volumes.rst
+nova-pagination.rst approved/nova-pagination.rst
+persistent-resource-claim.rst approved/persistent-resource-claim.rst
+quiesced-image-snapshots-with-qemu-guest-agent.rst approved/quiesced-image-snapshots-with-qemu-guest-agent.rst
+restrict-image-isolation-with-defined-keys.rst approved/restrict-image-isolation-with-defined-keys.rst
+return-all-servers-during-multiple-create.rst approved/return-all-servers-during-multiple-create.rst
+selecting-subnet-when-creating-vm.rst approved/selecting-subnet-when-creating-vm.rst
+server-count-api.rst approved/server-count-api.rst
+standardize-nova-image.rst approved/standardize-nova-image.rst
+string-field-max-length.rst approved/string-field-max-length.rst
+support-console-log-migration.rst approved/support-console-log-migration.rst
+tag-instances.rst approved/tag-instances.rst
+use-libvirt-storage-pools.rst approved/use-libvirt-storage-pools.rst
+vif-vhostuser.rst approved/vif-vhostuser.rst
+virt-driver-cpu-pinning.rst approved/virt-driver-cpu-pinning.rst
+virt-driver-large-pages.rst approved/virt-driver-large-pages.rst
+vmware-driver-ova-support.rst approved/vmware-driver-ova-support.rst
+vmware-ephemeral-disk-support.rst approved/vmware-ephemeral-disk-support.rst
+vmware-spbm-support.rst approved/vmware-spbm-support.rst
+vmware-vsan-support.rst approved/vmware-vsan-support.rst
+websocket-proxy-to-host-security.rst approved/websocket-proxy-to-host-security.rst
+xenapi-set-ipxe-url-as-img-metadata.rst approved/xenapi-set-ipxe-url-as-img-metadata.rst
+xenapi-vcpu-topology.rst approved/xenapi-vcpu-topology.rst
diff --git a/specs/juno/refactor-network-api.rst b/specs/juno/refactor-network-api.rst
deleted file mode 100644
index 04f824b..0000000
--- a/specs/juno/refactor-network-api.rst
+++ /dev/null
@@ -1,109 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-======================
-Refactor network API
-======================
-
-https://blueprints.launchpad.net/nova/+spec/refactor-network-api
-
-To have a common API network base with all required methods so
-neutron / nova network api can inherit from.
-
-
-Problem description
-===================
-
-Right now network api's do not inherit from a common base, and if the
-functionality is not implemented developers may forget to add the
-method.
-The situation is that every time that functionality want to be accessed
-from the API an exception is thrown due to missing methods and not clear
-error is returned.
-
-Proposed change
-===============
-
-The idea is to create a network_base API that define all the possible
-methods and just throw NotImplementedError, so next time the user will
-see the proper error message.
-
-Also fields like sentinel object could be directly inherited in the base
-api.
-
-Alternatives
-------------
-
-The current way to do this is to manually add the missing methods to
-neutronv2 api for instance. Every time someone add a new method to one
-api has to do the same for the others and raise NotImplementedError if
-not supported.
-
-Data model impact
------------------
-None
-
-REST API impact
----------------
-None
-
-Security impact
----------------
-None
-
-Notifications impact
---------------------
-None
-
-Other end user impact
----------------------
-None
-
-Performance Impact
-------------------
-None
-
-Other deployer impact
----------------------
-None
-
-Developer impact
-----------------
-
-If developers add new methods to neutronv2 or nova-network api,
-they must define it first on the new network base api.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  leandro-i-costantino
-
-Work Items
-----------
-
- * Create a base network api files that has all the public methods
-   from current network api
-
-
-Dependencies
-============
-None
-
-Testing
-=======
-None
-
-Documentation Impact
-====================
-None
-
-References
-==========
-None
diff --git a/specs/juno/remove-cast-to-schedule-run-instance.rst b/specs/juno/remove-cast-to-schedule-run-instance.rst
deleted file mode 100644
index 1b7c51a..0000000
--- a/specs/juno/remove-cast-to-schedule-run-instance.rst
+++ /dev/null
@@ -1,153 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-============================================
-Stop using the scheduler run_instance method
-============================================
-
-https://blueprints.launchpad.net/nova/+spec/remove-cast-to-schedule-run-instance
-
-Currently the scheduler is used to both pick a host for an instance to be built
-on and to handle some setup and failure conditions for booting an instance.
-The scheduler should be responsible for placement logic and everything else
-should be moved elsewhere.  This will make efforts to introduce new scheduler
-drivers or split the scheduler out of Nova easier to tackle by keeping a clean
-interface with a clear responsibility.
-
-
-Problem description
-===================
-
-The flow of execution for spawning an instance is complicated and highly
-distributed.  Some amount of distribution is necessary but there is work
-happening and decisions being made in unexpected parts of the code.  This makes
-it very difficult to look at separating the scheduler out, and means that it
-will need intimate integration with Nova that should be unnecessary.  It is
-also unecessarily difficult to reason about what is happening at which point in
-the code which makes it challenging to improve those parts of the code.
-
-
-Proposed change
-===============
-
-In Havana it became possible to query the scheduler for a list of hosts to
-provision an instance to.  The conductor service also emerged as a place to
-help orchestrate tasks that don't logically belong in either the api or compute
-nodes.  There has already been work to move some of the spawn instance workflow
-into the conductor and the final part of that effort is to have the conductor
-communicate with compute nodes rather than the scheduler.
-
-There is a new, currently unused, build_and_run_instance method in the compute
-manager which mimics the currently used run_instance method, but handles a
-failed build by sending an RPC cast to the conductor service rather than the
-scheduler.  The proposed change is to have the conductor query the scheduler
-and send a message to a compute which invokes the new build_and_run_instance
-method.  Because the new method is unused and therefore untested by Tempest
-there will likely be some work required to achieve full compatibility with the
-current run_instance method.
-
-Alternatives
-------------
-
-An alternative would be to rework the run_instance method to cast back to
-conductor rather than use the new build_and_run_instance method.  This was
-decided against because the amount of refactoring that would need to happen
-there meant it was easier to rebuild that method from scratch.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None.  The notifications being sent by the scheduler will be ported over to
-conductor to maintain the same behaviour.
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-Some database updates that were occuring within the scheduler will be moved out
-to less performance critical sections of code.  This should speed up the
-scheduler.
-
-There may be a decrease in the amount of time to boot an instance if it needs
-to be rescheduled.  The new build_and_run_instance performs some pre-build
-checks earlier and doesn't generally deallocate and reallocate networks for a
-rescheduled instance.  It will deallocate/reallocate if the baremetal driver is
-in use as the networking there is host specific.
-
-Other deployer impact
----------------------
-
-None.
-
-Developer impact
-----------------
-
-Developers will need to be aware of the new code path being used.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  alaski
-
-Other contributors:
-  None
-
-Work Items
-----------
-
-  * Change the conductor to query the scheduler and cast to a compute.
-
-    * Move notifications from the scheduler into the conductor.
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-This is essentially a refactoring of the current spawn process.  So the current
-Tempest tests will act as good integration tests for this change since the new
-method will be used on every instance boot.
-
-
-Documentation Impact
-====================
-
-None
-
-
-References
-==========
-
-None
diff --git a/specs/juno/rescue-attach-all-disks.rst b/specs/juno/rescue-attach-all-disks.rst
deleted file mode 100644
index 5436556..0000000
--- a/specs/juno/rescue-attach-all-disks.rst
+++ /dev/null
@@ -1,135 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-====================================
-Attach All Local Disks During Rescue
-====================================
-
-https://blueprints.launchpad.net/nova/+spec/rescue-attach-all-disks
-
-Attach all local disks during rescue to allow users access to all of
-their data.
-
-
-Problem description
-===================
-
-Currently only the root disk of the original instance is attached to the
-rescue instance. If an instance is unbootable, then there is no way to
-salvage data off ephemeral or other local disks.
-
-
-Proposed change
-===============
-
-When an instance is placed into rescue, attach all local disks in addition
-to the root disk already attached.
-
-This explicitly does not attach any non-local disks, such as volumes. Any
-attempt to rescue a volume-backed instance will continue being
-rejected.
-
-
-Alternatives
-------------
-
-None
-
-
-Data model impact
------------------
-
-None
-
-
-REST API impact
----------------
-
-None
-
-
-Security impact
----------------
-
-None
-
-
-Notifications impact
---------------------
-
-None
-
-
-Other end user impact
----------------------
-
-None
-
-
-Performance Impact
-------------------
-
-None
-
-
-Other deployer impact
----------------------
-
-None
-
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  johannes.erdfelt
-
-Other contributors:
-  None
-
-
-Work Items
-----------
-
-Implement feature for each virt driver.
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-Each virt driver will be expected to test that all disks are attached
-during rescue as part of the existing Nova tests.
-
-Tempest will be updated to assert that the original disks are attached
-during rescue.
-
-
-Documentation Impact
-====================
-
-It should be documented that this is a behavior change when rescuing
-instances.
-
-
-References
-==========
-
-https://bugs.launchpad.net/nova/+bug/1223396
diff --git a/specs/juno/restrict-image-isolation-with-defined-keys.rst b/specs/juno/restrict-image-isolation-with-defined-keys.rst
deleted file mode 100644
index d44cadd..0000000
--- a/specs/juno/restrict-image-isolation-with-defined-keys.rst
+++ /dev/null
@@ -1,177 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
-
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-============================================
-Strictly isolate group of hosts for an image
-============================================
-
-https://blueprints.launchpad.net/nova/+spec/restrict-image-isolation-with-defined-keys
-
-The aim of this blueprint is to improve the filter
-`AggregateImagePropertiesIsolation`
-
-An operator wants to schedule instances for a specific image on a
-pre-defined group of hosts. In addition, he wants to strictly isolate this
-group of hosts for the image only and accept images without key scheduled
-to other hosts.
-
-Problem description
-===================
-
-Currently with the filter `AggregateImagePropertiesIsolation` we have the
-possibility to define images that will be scheduled on a specific aggregate
-following this matrix:
-
-+--------------+------------+----------+----------+
-| img \\ aggr  | key=foo    | key=xxx  | <empty>  |
-+==============+============+==========+==========+
-| key=foo      | True       | False    | True     |
-+--------------+------------+----------+----------+
-| key=bar      | False      | False    | True     |
-+--------------+------------+----------+----------+
-| <empty>      | True       | True     | True     |
-+--------------+------------+----------+----------+
-
-*Table 1: row are image properties, col are aggregate metadata.*
-
-The problem is:
- * An image without key can still be scheduled in a tagged aggregate
- * The hosts outside aggregates or in a no-tagged aggregate can still accept a
-   tagged image
-
-Proposed change
-===============
-
-We would like to add an option to:
- * Make tagged aggregate refuse not-tagged images
- * Make not-tagged aggregate accept ONLY not-tagged images
-
-+--------------+------------+----------+----------+
-| img \\ aggr  | key=foo    | key=xxx  | <empty>  |
-+==============+============+==========+==========+
-| key=foo      | True       | False    | False    |
-+--------------+------------+----------+----------+
-| key=bar      | False      | False    | False    |
-+--------------+------------+----------+----------+
-| <empty>      | False      | False    | True     |
-+--------------+------------+----------+----------+
-
-*Table 2: row are image properties, col are aggregate metadata*
-
-We propose to add global option `aggregate_image_filter_strict_isolation` in
-the filter which dictates strictness level of the isolation:
-
- * aggregate_image_filter_strict_isolation = False:
-   the filter functions as before (Tab. 1)
- * aggregate_image_filter_strict_isolation = True:
-   the filter functions as proposed decision (Tab. 2)
-
-*For backward compatibility this option will be set by default to False.*
-
-We also propose to add this option configurable in per-aggregate.
-
-
-Alternatives
-------------
-
-* An alternative solution would be to create a new filter that inherits from
-  `AggregateImagePropertiesIsolation`.
-* A more configurable solution could be to use two config options
-  `allow_untagged_images_in_tagged_aggregate=True` and
-  `allow_tagged_images_in_untagged_aggregate=True` but currently we cannot
-  find any cases of using this alternative.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-* Operator needs to update the scheduler's `nova.conf` to set the option
-  `aggregate_image_filter_strict_isolation`.
-
-::
-  aggregate_image_filter_strict_isolation=True
-
-* To configure per-aggregate Operator needs to set the metadata.
-
-::
-  nova aggregate-set-metadata aggrA
-  aggregate_image_filter_strict_isolation=True
-
-*Note: For existing system, instances will be not re-scheduled. The operator
-always have the possibility to do migration.*
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  sahid-ferdjaoui
-
-Work Items
-----------
-
-* Updating `AggregateImagePropertiesIsolation` to accept the new global option.
-* Updating `AggregateImagePropertiesIsolation` to accept per-aggregate
-  configuration.
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-* Unit tests can validate the expected behavior.
-
-Documentation Impact
-====================
-
-We need to update the documentation:
-  'doc/source/devref/filter_scheduler.rst'
-
-References
-==========
-
-* http://docs.openstack.org/developer/nova/devref/filter_scheduler.html#filtering
-* https://review.openstack.org/#/c/80940/
diff --git a/specs/juno/return-all-servers-during-multiple-create.rst b/specs/juno/return-all-servers-during-multiple-create.rst
deleted file mode 100644
index 980950d..0000000
--- a/specs/juno/return-all-servers-during-multiple-create.rst
+++ /dev/null
@@ -1,225 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=========================================
-Return all servers during multiple create
-=========================================
-
-https://blueprints.launchpad.net/nova/+spec/return-all-servers-during-multiple-create
-
-In this blueprint, we propose to improve the server create API in V3 by
-including all of the created servers in the response instead of only the first
-server. In V2, servers[0] is returned to the caller in response to a create
-request that has specified min/max_count.
-
-Problem description
-===================
-
-End users of the server create API have the ability to create multiple
-instances in one batch by specifying min/max_count in the request. One reason
-to use this ability is to scale up a cloud-hosted application quickly or in
-response to increased load. Upon requesting multiple servers, the user needs to
-know the list of servers that have been created in order to work with them. It
-would be ideal to receive the list in the response for the create request.
-
-Proposed change
-===============
-
-In order to provide the end user with the list of created servers most
-efficiently, we propose to change the server create API response in V3 from
-returning only one server to returning a list of servers. In the case when the
-end user has requested creation of just one server, a list containing one
-server will be returned.
-
-Alternatives
-------------
-
-Callers can work around in V2 by specifying return_reservation_id=True in the
-request to receive a reservation ID which they can use to obtain the list of
-servers. This is also currently possible in V3, but it requires an additional
-API call to get the server list.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-V3 API specification:
-
- * Description: Create one or more servers
-
- * Method type: POST
-
- * Normal http response code: 202
-
- * Expected error http response codes:
-
-  * 400: Invalid request parameter, image/flavor not found
-
-  * 409: Port in use, no unique match
-
-  * 413: Quota or port limit exceeded
-
- * URL: v3/servers
-
- * Example JSON request (no change)::
-
-    {
-        "server": {
-            "name": "server-test-1",
-            "image_ref": "b5660a6e-4b46-4be3-9707-6b47221b454f",
-            "flavor_ref": "2",
-            "max_count": 2,
-            "min_count": 2
-    }
-
- * Example JSON response (new format)::
-
-    {
-        "servers": [
-            {
-                "admin_password": "qpYU66rKxmnK",
-                "id": "215d1109-216d-48c3-af8e-998bb9bc3ca0",
-                "links": [
-                    {
-                        "href": "http://openstack.example.com/v3/servers/<id>",
-                        "rel": "self"
-                    },
-                    {
-                        "href": "http://openstack.example.com/servers/<id>",
-                        "rel": "bookmark"
-                    }
-                ]
-            },
-            {
-                "admin_password": "wfksH3GTTseP",
-                "id": "440cf918-3ee0-4143-b289-f63e1d2000e6",
-                "links": [
-                    {
-                        "href": "http://openstack.example.com/v3/servers/<id>",
-                        "rel": "self"
-                    },
-                    {
-                        "href": "http://openstack.example.com/servers/<id>",
-                        "rel": "bookmark"
-                    }
-                ]
-            }
-        ]
-    }
-
- * Partial JSON response schema definition to show change::
-
-    create = {
-        'type': 'object',
-        'properties': {
-            'servers': {
-                'type': 'array',
-                'items': {
-                    'type': 'object',
-                    'properties: {
-                        'admin_password': {type': 'string'},
-                        'id': {'type': 'string'},
-                        'links': {
-                            'type': 'array',
-                            'items': {
-                                'type': 'object',
-                                'properties': {
-                                    'href': {'type': 'string'},
-                                    'rel': {'type': 'string'}
-                                }
-                            }
-                        }
-                    }
-                }
-            }
-        }
-    }
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-The python-novaclient will have to be changed to handle the list of servers in
-the V3 API server create response and show the list to the user.
-
-Performance Impact
-------------------
-
-For a server create API request for multiple servers, instead of returning only
-the first server, all of the created server objects must be serialized and
-returned in the response instead of just the first one.
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  melwitt
-
-Other contributors:
-  None
-
-Work Items
-----------
-
- * Change the V3 API response for server create to return a list of instances.
-
- * Update tests in tempest to handle the changed response.
-
-Dependencies
-============
-
-This blueprint is related to the tasks API blueprint [1] because it needs to
-interact with how tasks will work in V3. Initial comments on this interaction
-are available in the original review [2].
-
-[1] https://blueprints.launchpad.net/nova/+spec/instance-tasks-api
-
-[2] https://review.openstack.org/#/c/54214/
-
-Testing
-=======
-
-Tempest tests must be updated to accept the changed server create API
-response format. Tempest tests already exercise the various server creation
-scenarios, but the response format has changed for V3.
-
-Documentation Impact
-====================
-
-The changed REST API response for server create, as represented by the
-jsonschema definition above, will need to be documented. The changed API
-response will be available as API samples generated from testing.
-
-References
-==========
-
-None
diff --git a/specs/juno/return-status-for-hypervisor-node.rst b/specs/juno/return-status-for-hypervisor-node.rst
deleted file mode 100644
index 98689e4..0000000
--- a/specs/juno/return-status-for-hypervisor-node.rst
+++ /dev/null
@@ -1,191 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Return hypervisor node status
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/return-status-for-hypervisor-node
-
-Problem description
-===================
-
-Currently when user show or list the hypervisor, it will have no idea of the
-status, possibly it's down or disabled already. Sometimes it will cause
-confusion like in bug https://bugs.launchpad.net/nova/+bug/1285259 .
-
-Proposed change
-===============
-
-Propose to return the service state/status when showing the hypervisor node.
-For v2 api, an extra extension is added. When the extension is loaded, we will
-return the service state/status. For a later microversion of v2.1 api, we will
-always return the state/status.
-
-When the service is disabled, add the disabled reason in the service
-information in the details/show endpoint.
-
-Alternatives
-------------
-
-There are several other options:
-
-* User first get the service information from hypervisor
-  node and then show the service status. But I think showing the hypervisor
-  status directly will be more straight forward. For example, like in
-  https://bugs.launchpad.net/nova/+bug/1285259 , user may trying to figure
-  out the instances in a compute node and didn't realize the node is disabled
-  already and the information is useless.
-
-* Currently the os-hypervisors extension already returns the service
-  information like host and service id. We can extend that field to include
-  all service state/status/disabled_reason information. However, it may be
-  better to  add the state/status to the list endpoint and only
-  disabled_reason to the service information.
-
-Data model impact
------------------
-
-No change on data model.
-
-REST API impact
----------------
-
-* For V2 API, a new extension will be added as:
-  alias: os-hypervisor-status
-  name: HypervisorStatus
-  namespace: http://docs.openstack.org/compute/ext/hypervisor_status/api/v1.1
-
-  When the new extension "os-hypervisor-status" is loaded, a new field 'status'
-  will be added to the os-hypervisor API.
-
-* For a later microversion of v2.1 API, no new extension needed, the
-  existing hypervisor REST API will be updated to return the status.
-
-
-* URL: existed hypervisors extension as:
-       * /v2/{tenant_id}/os-hypervisors:
-       * /v2.1/os-hypervisors:
-
-  JSON response body::
-
-    {
-        "hypervisor": [
-        {
-            "state": "enabled",
-            "status": "up",
-            "id": 1,
-            "hypervisor_hostname": "otccloud06"
-         }]
-     }
-
-  The 'status' and 'state' are the new added fields, and are same as
-  service API.
-
-* URL: existed hypervisors extension as:
-       * /v2/{tenant_id}/os-hypervisors/{id}
-       * /v2.1/os-hypervisors/{id}
-
-  JSON response body::
-
-    {"hypervisor": {
-            "state": "enabled",
-            "status": "up",
-            "os-pci:pci_stats": [],
-            "service":
-            {
-                "host": "otccloud06",
-                "id": 3,
-                "disabled_reason": ""
-            },
-            "vcpus_used": 0,
-            "hypervisor_type": "QEMU",
-            "local_gb_used": 0,
-            "host_ip": "172.25.110.34",
-            "hypervisor_hostname": "otccloud06",
-            "memory_mb_used": 512,
-            "memory_mb": 128956,
-            "current_workload": 0,
-            "vcpus": 32,
-            "cpu_info": {"vendor": "Intel}
-            "running_vms": 0,
-            "free_disk_gb": 469,
-            "hypervisor_version": 1000000,
-            "disk_available_least": 408,
-            "local_gb": 469,
-            "free_ram_mb": 128444,
-            "id": 1}
-    }
-
-  The 'status', 'disabled_reason' and 'state' are the new added fields, and
-  are same as service API.
-
-Security impact
----------------
-
-No
-
-Notifications impact
---------------------
-
-No
-
-Other end user impact
----------------------
-
-Yes, this will impact the python-novaclient. novaclient should show the status
-on the 'nova hypervisor list'.
-
-Performance Impact
-------------------
-
-No
-
-Other deployer impact
----------------------
-
-For V2 api, the extension should be added.
-
-Developer impact
-----------------
-
-No
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-    yunhong-jiang
-
-Work Items
-----------
-
-* Changes to V2 API
-* Changes to V3 API
-
-
-Dependencies
-============
-
-No
-
-Testing
-=======
-
-Both unit and Tempest tests will be created to ensure the correct
-implementation.
-
-Documentation Impact
-====================
-
-Document the change to the REST API.
-
-References
-==========
-No
diff --git a/specs/juno/scheduler-lib.rst b/specs/juno/scheduler-lib.rst
deleted file mode 100644
index 563abf4..0000000
--- a/specs/juno/scheduler-lib.rst
+++ /dev/null
@@ -1,221 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=================================
- Create Scheduler Python Library
-=================================
-
-https://blueprints.launchpad.net/nova/+spec/scheduler-lib
-
-We want to split out nova-scheduler into gantt. To do this, we need to
-isolate the scheduler from the rest of nova.
-
-In this blueprint, we need to define in a clear library all accesses to the
-Scheduler code or data (compute_nodes DB table) from other Nova bits (conductor
-and ResourceTracker).
-
-No scheduler bits of code will be impacted by this blueprint, the change is
-only affecting other Nova components and provides a new module for Scheduler.
-
-
-Problem description
-===================
-
-To create the gantt project we need to introduce a much cleaner "seam" between
-nova-scheduler and the rest of Nova. This will allow the existing
-nova-scheduler code to remain in Nova, while at the same time giving us a clean
-way to test the new gantt scheduler.
-
-This split will also be useful to allow efforts such as the no-db-scheduler
-to evolve in a way that allows multiple patterns to co-exist, thus encouraging
-more innovation, while keeping the existing stable and pluggable solution.
-
-This change in approach for the gantt project was agreed at the Nova
-Icehouse mid-cycle meetup:
-https://etherpad.openstack.org/p/icehouse-external-scheduler
-
-
-Proposed change
-===============
-
-The basic points to note about this change are:
-
-* No change in behaviour. This is just a refactor.
-
-* Produce a scheduler lib, a prototype interface for python-ganttclient
-
-* Assume select_destinations will be the single call to the scheduler from nova
-  by the end of Juno. This is the first bit of the interface.
-
-* Move all accesses to the compute_nodes table behind the new scheduler lib.
-  This is the second part of the interface.
-
-Here we need to define a line in the sand by exposing a Scheduler interface
-that Nova can use (mostly the ResourceTracker) for updating stats to the
-Scheduler instead of directly calling DB for updating compute_nodes table.
-
-In addition, calls to the Scheduler RPC API will now go through the scheduler
-lib, so as to have all current interfaces going to the same module .But given
-the above assumptions, we need only do this for select_destinations.
-
-As said, all interfaces will go into a single module (nova.scheduler.client).
-
-The current interfaces we identify are ::
-
-    select_destinations(context, request_spec, filter_properties)
-        """Returns a list of resources based on request criterias.
-        """
-        :param context: security context
-        :param request_spec: specification of requested resources
-        :type requested_resources: dict
-        :param filter_properties: scheduler hints and instance spec
-
-    update_resource_stats(context, name, stats)
-        """Update Scheduler state for a set of resources."""
-        :param context: context
-        :param name: name, as returned by select_destinations
-        :type name: tuple or string
-        :param stats: dict of stats to send to scheduler
-
-If we still need to support the node and host distinction in nova, this can be
-done by passing a tuple (host, node) as the resource name, instead of a string.
-
-In a similar way, resource_request, will, for now, contain both
-request_spec and filter_properties in a generic dict.
-
-The stats parameter is planned to be 1:1 matched with conductor/DB
-compute_node_update() (or create()) values parameter, ie. a dict matching
-compute_nodes fields in a JSON way.
-
-
-This proposal is just drawing a line in the sand. In the future we will need to
-make more invasive changes that are not triggered for this blueprint, such as:
-
-* Adding more data into compute_nodes, so the scheduler doesn't need access to
-  any other Nova objects. For example, filters that need to know about the AZ,
-  that could be included in the stats that are added into compute_nodes
-
-* Having a data collection plugin system, so data is extracted and sent from
-  the resource tracker to the scheduler in a format that the matches the
-  filters and/or weights on the receiving end. Also ensuring, only the data
-  that is required for your particular set of filters and/or weights are sent.
-  This is very similar to the extensible resource tracker blueprint or could
-  leverage it.
-
-* Proxying select_destinations by another method for having it less Nova
-  specific and allowing in the future a python-ganttclient client to use it.
-
-
-Alternatives
-------------
-
-The other alternative would be to fork the scheduler code at a point in time to
-a separate Git repository, do the necessary changes within the code (unittests,
-imports). However neither syncing changes or having a code freeze on
-nova-scheduler seem like the best approach.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None. This effort is just refactoring, not splitting now into a separate
-repository.
-
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-Ideally:
-
-* All new operations will be scheduled using select_destinations.
-
-* ResourceTracker will only take use of update_resource_stats.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  sylvain-bauza
-
-Other contributors:
-  None
-
-Work Items
-----------
-
-* Create scheduler lib for calls to select_resources
-
-* Add update_resource_stats to lib
-
-
-Dependencies
-============
-
-* https://review.openstack.org/#/c/86988/
-  (bp/remove-cast-to-schedule-run-instance)
-
-
-Testing
-=======
-
-Covered by existing tempest tests and CIs.
-
-
-Documentation Impact
-====================
-
-None
-
-
-References
-==========
-
-* Other effort related to RT using objects is not mandatory for this blueprint
-  but both efforts can mutally benefit
-  https://blueprints.launchpad.net/nova/+spec/make-resource-tracker-use-objects
-  (pmurray)
-
-* Cast to scheduler for running instances is mandatory for the Gantt forklift
-  but not for this blueprint
-  https://blueprints.launchpad.net/nova/+spec/remove-cast-to-schedule-run-instance
-  (alaski)
-
-* https://etherpad.openstack.org/p/icehouse-external-scheduler
-
-* http://eavesdrop.openstack.org/meetings/gantt/2014/gantt.2014-03-18-15.00.html
diff --git a/specs/juno/selecting-subnet-when-creating-vm.rst b/specs/juno/selecting-subnet-when-creating-vm.rst
deleted file mode 100644
index 98c5c27..0000000
--- a/specs/juno/selecting-subnet-when-creating-vm.rst
+++ /dev/null
@@ -1,214 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==================================
-CreateVM supports subnet specified
-==================================
-
-https://blueprints.launchpad.net/nova/+spec/selecting-subnet-when-creating-vm
-
-Currently the network info specified as part of server creation is limited to
-network-id, port-id, and ip address. When a network has multiple subnets
-then we need to also be able to specify a subnet-id.
-
-
-Problem description
-===================
-
-Currently the network info specified as part of server creation is limited to
-network-id, port-id, and ip address.
-
-So if an network has multiple subnets in it, it's impossible to select
-which of the possible subnets a VM should be created in.
-You only could choose an ip address in one subnet and then create an instance.
-But this is not a convenient way. Moreover, this method is also not available
-for bulk instances creation.
-
-
-Proposed change
-===============
-
-1. Add one optional param 'subnet-id' in networking structure of 'spawn'.
-
-2. This parameter will affect in 'allocate_for_instance()'
-   in nova/network/neutronv2/api.py.
-
-3. Bulk instances creation with 'subnet-id' will be supported,
-   as the 'net-uuid' is specified.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-The new 'spawn' rest API in v2::
- /v2/{project_id}/servers
-
-    {
-        'server':{
-        ...
-        'networks': [
-        {
-        'subnet-id': '892b9731-044a-4c87-b003-1e75869028c0'
-        }
-        ...
-        ]
-        ...
-        }
-
-    }
-
-and in v3 it is like::
- /v3/servers
-
-{
-    'server':{
-    ...
-    'networks': [
-    {
-    'subnet-id': '892b9731-044a-4c87-b003-1e75869028c0'
-    }
-    ...
-    ]
-    ...
-    }
-
-}
-
-* Here, the <string> 'subnet-id' means the subnet your instances
-  want to be created in. No default value.
-
-* If 'subnet-id' is not a string or uuid-like, a BadRequest exception
-  will be raised.(HTTP 400)
-
-* The status code will be HTTP 202 when the request succeeded as usual,
-  and the response body won't be changed.
-
-In the current implement in Nova, the network info specified is limited to
-network-id, port-id, and ip address, and port-id has the highest priority.
-So, we also need to point the priority during server creation.
-
-* The 'port' parameter still has the highest priority here.
-
-  That means, if both 'port' & 'subnet-id' are specified, 'port' will be used
-  and the 'subnet-id' won't effect here.
-
-* The 'subnet-id' has the same priority with 'v4-fixed-ip'/'v6-fixed-ip'.
-
-  That means, if both 'subnet-id' & 'v4-fixed-ip'/'v6-fixed-ip' are specified,
-  compatibility validation of these two arguments will be executed.
-  * If it passed, the ip address you assigned will be used as usual.
-  * If not, a BadRequest exception will be raised.(HTTP 400)
-
-* The 'net-uuid' parameter still has the lowest priority like before.
-
-  That means, if both 'subnet-id' & 'net-id' are specified, 'subnet-id'
-  will effect here and 'net-uuid' will be ignored like port specified.
-
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-The related works in python-novaclient will also be added.
-After this modification, user could create instances with 'subnet-id' specified
-like 'net-uuid' does via CLI.
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Assignee: wingwj <wingwj@gmail.com>
-
-Work Items
-----------
-
-In nova:
-
-  * Add 'subnet-id' to 'create' in API layer
-
-  * Use 'subnet-id' for 'allocate_for_instance()'
-    in nova/network/neutronv2/api.py
-
-  * Add related tests both API & nova-compute
-
-In python-novaclient:
-
-  * Add 'subnet-id' support in python-novaclient
-
-  * Add related tests in python-novaclient
-
-In tempest:
-
-  * Related test-cases will definitely be added here
-
-In doc:
-
-  * The API modification will also be registered in openstack-doc
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-The unit tests need to be added in each related projects like I described
-in <Work Items> part. After the modifications, all changed methods above
-will be verified together.
-
-
-Documentation Impact
-====================
-
-The 'server creation' in API & CLI documentations will need to be updated to:
-
-* Reflect the new 'subnet-id' parameter and explain its usage
-* Explain the priority of network info during server creation
-
-
-References
-==========
-
-None
\ No newline at end of file
diff --git a/specs/juno/serial-ports.rst b/specs/juno/serial-ports.rst
deleted file mode 100644
index f94d00c..0000000
--- a/specs/juno/serial-ports.rst
+++ /dev/null
@@ -1,258 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-======================================
-Interactive web-based serial consoles
-======================================
-
-https://blueprints.launchpad.net/nova/+spec/serial-ports
-
-This blueprint is about exposing interactive web-based serial consoles to
-openstack VMs through a websocket proxy. It is mainly raised because of the
-problems openstack is facing with the serial console logs that are hard to
-maintain, grow indefinitely, etc. The point is not to eliminate the serial
-console logs, but to give the users another option besides logging to a file
-and to expose an interactive serial console.
-
-Problem description
-===================
-
-Right now the serial console has unsolved issues with the logging that have
-bounced from one release to another and no suitable solution was developed for
-them. Most of the issues are nicely summed up in the serial console log
-blueprint for juno https://review.openstack.org/#/c/80865/ however, this
-proposal doesn't deal with exposing an interactive serial console to the end
-user.
-
-Proposed change
-===============
-
-This blueprint proposes the addition of a new service - serialproxy (a
-websocket proxy) that would handle websocket connections to the serial
-consoles. The websocket proxy can be deployed on a machine other from the
-hypervisor, so unix domain sockets wouldn't do the trick. The best way to
-expose them would be by opening a TCP socket for every serial console.
-http://libvirt.org/formatdomain.html#elementsCharTCP
-This service would act similarly to the novncproxy and scale in more or less
-the same way.
-
-One serial port can be accessed only by one user at a time, i.e. it can't
-be muxed since none of the hypervisors have a 'clear this line' command
-separate from the 'connect' command (or a flag to integrate that with the
-original 'connect' call).
-The proposed scenario for multiple users accessing the same serial port is the
-following:
-If a user is already connected, then reject the attempt of a second user to
-access the console, but have an API to forceably disconnect an existing
-session. This would be particularly important to cope with hung sessions where
-the client network went away before the console was cleanly closed.
-
-To allow multiple clients to connect to serial ports we'd need to create the
-ports when the instance is booted, but we'd need to know the number of ports
-that would need to be created in advance. That number can be passed through a
-property in the image metadata, e.g. "serial_ports".
-Since the serial ports are exposed through TCP sockets we would also need a
-module that tests for free TCP ports and allocates them so that the libvirt
-driver can use them when creating the serial ports. This should be persistent,
-so that the ports that are already tested won't be tested again for a new
-serial port.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-The REST API would have one additional method to obtain the serial console URL
-for the end user or for displaying in the dashboard.
-
-V2 API specification:
-POST: v2/{tenant_id}/servers/{server_id}/get-serial-console
-
-V3 API specification:
-POST: v3/servers/{server_id}/get-serial-console
-
-Request parameters:
-
-* tenant_id: The ID for the tenant or account in a multi-tenancy cloud.
-* server_id: The UUID for the server to get the serial console for.
-
-JSON response
-::
-
-    {
-        "serial_console":
-        {
-            "url": "http://example.com:6083/serial.html?token=b40ac1c3-b640-4a6a-ae34-bf347ef089d6"
-        }
-    }
-
-JSON schema definition
-::
-
-    serial_console = {
-        'type': 'object',
-        'properties': {
-            'serial_console': {
-                'type': ['object', 'null'],
-                'properties': {},
-                'additionalProperties': False,
-            },
-        },
-        'additionalProperties': False,
-    }
-
-
-HTTP response codes:
-v2:
-
-* Normal HTTP Response Code: 200 on success
-
-v3:
-
-* Normal HTTP Response Code: 202 on success
-
-Security impact
----------------
-
-The opening of TCP ports in the hypervisor node can enable anyone to gain
-access to any of the serial consoles by scanning for open ports if the ports
-specified in port_range config param are visible to the public.
-Usually the hypervisor ports aren't externally exposed, so this wouldn't be any
-better or worse than VNC.
-The insecurity of VNC is being tackled by a blueprint that will add strong auth
-to VNC on the internal network. That's not a reason to block this serial
-console feature though. We can work with the QEMU community at a later date to
-get SSL support for the character device sockets it exposes.
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-The python-novaclient will have to implement a new command.
-
-Command:
-get-serial-console <server> <console-type>
-
-* param server: The name or Id of the server.
-
-
-Performance Impact
-------------------
-
-Using the serial consoles instead of a graphical console would be more optimal
-since it interacts with the instance through a text stream.
-
-Other deployer impact
----------------------
-
-Config options that are being added in the serial_console group:
-[serial_console]
-- enabled (type=BoolOpt, default=False)
-- base_url (type=StrOpt, default='http://127.0.0.1:6083/serial.html')
-- listen (type=StrOpt, default='0.0.0.0')
-- proxyclient_address (type=StrOpt, default='127.0.0.1')
-- port_range (type=StrOpt, default='10000:20000')
-- record (type=BoolOpt, default=False)
-- daemon (type=BoolOpt, default=False)
-- ssl_only (type=BoolOpt, default=False)
-- source_is_ipv6 (type=BoolOpt, default=False)
-- cert (type=StrOpt, default='self.pem')
-- key (type=StrOpt)
-- web (type=StrOpt, default='/usr/share/serialproxy-static')
-
-The default value of the "enabled" confing param is False so there's no need
-to take something into account after this change gets merged.
-
-A new service - serialproxy is introduced which will need to be deployed
-separately in order for this feature to work with websockets.
-The command line params would be no different from novnc's which would override
-some of the config params specified in the config file).
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  Vladan Popovic
-
-Other contributors:
-  Ian Wells
-  Sushma Korati
-
-Work Items
-----------
-
-**Websocket proxy**
-
-* Add a config param in nova that would enable the web-based serial console,
-  e.g. enabled=True|False where False would be the default.
-* Configure libvirt to open TCP channels on the ports
-  http://libvirt.org/formatdomain.html#elementsCharTCP
-* Add a port allocator module that would generate/test TCP ports and assign
-  them to the instance's libvirt config when it finds a free one.
-  This would require another config param in nova, e.g. port_range=10000:20000
-* Implement the serial console config generation and retreival in the libvirt
-  driver.
-* Add a method for obtaining the serial console in the compute manager.
-* Add methods in the consoleauth that would authorize the tokens.
-* Add API calls that would obtain the serial console URL with the generated
-  consoleauth token.
-* Add a serialproxy service that will serve as a wesocket proxy for serial
-  consoles
-* Add static files that will be serverd from the proxy, including a terminal
-  emulator, probably https://github.com/chjj/term.js/
-
-
-Dependencies
-============
-
-May require packaging of the static files for the websocket proxy and the
-terminal emulator.
-
-Testing
-=======
-
-Unit tests should be sufficient to cover libvirt and the API part.
-
-
-Documentation Impact
-====================
-
-Since tihs proposal introduces a new console and service the following things
-should be documented at least:
-
-* Deploying the serialproxy (with SSL/TLS support if possible)
-* Changes in the image metadata (if that solution fits the needs for multiuser
-  serial consoles)
-* Now to obtain a serial console URL from the API or from python-novaclient
-* Examples of managing the ports specified in the port_range so that they are
-  only accessible from the node where the serialproxy is deployed and not from
-  the outside.
-
-References
-==========
-
-None
diff --git a/specs/juno/server-count-api.rst b/specs/juno/server-count-api.rst
deleted file mode 100644
index 1038da6..0000000
--- a/specs/juno/server-count-api.rst
+++ /dev/null
@@ -1,315 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===============================
-Nova Server Count API Extension
-===============================
-
-https://blueprints.launchpad.net/nova/+spec/server-count-api
-
-This blueprint proposes a new REST API extension that returns the number of
-servers that match the specified search criteria.
-
-
-Problem description
-===================
-
-There is no current API that can retrieve summary count data for servers that
-match a variety of search filters. For example, getting the total number of
-servers in a given state.
-
-Retrieving all servers and then manually determining the count data does not
-scale because pagination queries must be implemented (see Alternatives section
-for a detailed explanation).
-
-The use cases that are driving this API extension are derived from a user's
-experience in a GUI.
-
-Use Case 1: A UI dashboard that contains servers in various states for a cloud
-administrator. A new API extension is needed to retrieve the server count data
-associated with various filters (ie, servers in active state, servers in
-building state, servers in error state, etc.) for the entire cloud.
-
-Assume that you have 5k instances in your cloud. The admin wants to see a
-summary of instances in each state -- this API extension will help them
-quickly determine if there is an issue that need attention; for example, if
-there are many instances in 'error'. It is likely that once the admin sees
-this count that they will then drill down into the data. However, without
-this new API extension, the admin will not know if there are unacceptable
-number of systems in a given state without drilling down into each set.
-
-From a deployer's perspective, creating this dashboard with the existing APIs
-is very painful since pagination is required (assume more then the default of
-1k items). Also, processing time to get this data using the existing APIs
-(even the non-detailed) is slow (and possibly inaccurate -- see #3) compared
-to the processing time to get and return a single number.
-
-Use Case 2: Showing filtered data in a table in the UI. Assume that the UI
-supports tables that show filtered data (ie, table just showing instances in
-'error' state) and uses pagination to get the data. Many users do not like
-"infinite scrolling" where they have no idea how many items really are in the
-list (more just show up as you scroll down or navigate to the next page).
-Using this new count API, the UI table can indicate how many total items are
-in the list (ie, showing 1-20 of 1000).
-
-Assume that you have 500 instances in error state and that you can open a UI
-table showing their details -- when creating the table, assume that the UI
-uses a page size of 100 and assume that there is no dashboard showing the
-'error' count. In this case, the admin logs into the UI and wants to know
-how many servers are in error state. In order to do this, the admin navigates
-to the 'servers in error state' table -- the UI only retrieves the first 100
-items so it impossible to know if there are 101 total items or 500 total
-items. As an admin, I would like to know what the total number of items in the
-table is.
-
-Use Case 3:  Inherent timing window when adding a new item with limit/marker
-processing. Assume that you are using pagination to iterate over the data to
-get a count. When you are getting page n, it is possible that page n-1 has a
-new item x that was just added. Due to the sorting of the data, limit/marker
-will not detect that this new item was added.
-
-While this timing window is small, it does exist so getting an accurate count
-using this method is not guaranteed to be accurate.
-
-I realize that you can argue that the count API may not handle this UI use case
-either. However, the count will always be accurate from the DB at the time that
-the .count() function was processed -- the same claim cannot be made about
-getting the count using limit/marker since multiple DB calls are being invoked
-to calculate the number.
-
-
-Proposed change
-===============
-
-The new count API extension must accept that same filter values as the
-existing /servers and /servers/details APIs and re-use the existing filter
-processing (once the common parts are refactored into utility methods that
-can be utilized by both paths). Once the filters are processed to create the
-query object, then the number of matching servers will be retrieved and
-returned from the database.
-
-The count API extension will be both per tenant and global (admin-only),
-similar to the existing /servers APIs. An admin can supply the 'all_tenants'
-parameter to signify that server count data should be retrieved globally.
-
-This new flow requires new functions to retrieve the count value in the
-compute API layer, in the instance layer, and in the database layers; all
-functions return an integer value. The naming conventions for the functions
-will follow the existing functions used for retrieving server instances, for
-example:
-
-* Compute API: get_count function
-
-* Instance layer (InstanceList class): get_count_by_filters function
-
-* DB layer: instance_count_by_filters function
-
-* Sqlalchemy layer: instance_count_by_filters function
-
-In the sqlalchemy DB layer, the filter processing (for processing exact name
-filters, regex filters, and tag filters) needs to be moved into a common
-function so that both the new count API extension and the existing get servers
-APIs can utilize it. Once the query object is created, then the count()
-function is invoked to retrieve the total number of matching servers for the
-given query.
-
-For the v2 API extension, the existing filtering pre-processing done in
-nova.api.openstack.compute.servers.Controller._get_servers needs to be moved
-into a static utility method so that the new count API extension can utilize
-it; this is critical so that the filtering support for the count API matches
-the filtering support for the /servers API.
-
-For the v3 API, a new count function (similiar to 'index' and 'detail') needs
-to be added to nova.api.openstack.compute.plugins.v3.servers directly. Common
-filter processing needs to broken out into utility functions (same idea as the
-v2 API). For v3, the 'count' GET API can be registered with the Servers
-extensions.V3APIExtensionBase directly.
-
-Alternatives
-------------
-
-Other APIs exist that return count data (quotas and limit) but they do not
-accept filter values.
-
-A user could accomplish the same result (less the timing window noted in Use
-Case #3) using the existing non-detailed /servers API with a filter and then
-count up the results. However, the primary use case for this blueprint is
-getting summary count data at scale.  For example, if the total cloud has 5k
-VMs then doing paginated queries to iterate over the non-detailed '/servers'
-API with a filter and limit/marker is really inefficient -- the API is going
-to return more data then the user cares about (and do a lot of processing to
-get it).  Assume that there are 2,500 instances in an active state; if the
-non-detailed query (and the default limit of 1k) is used then the application
-would have to make 3 separate REST API calls to get the all of the VMs and,
-at the DB layer, the marker processing would be used to find the correct page
-of data to return.  Since the user only cares about a summary count, then the
-most efficient mechanism to retrieve that data would be a single DB query
-using the count() function.
-
-Note that the default maximum page set is set on the server (default of 1k);
-therefore, a user MUST HANDLE pagination since the number of items being
-queried may be greater then the default.
-
-There are other options for how the v2 and v3 APIs can be registered. For v2,
-the new count API could be registered by modifying the API routing in
-nova.api.openstack.compute.__init__.APIRouter directly (to create the
-/servers/count API just like /server/detail). Since v3 is still experimental,
-this blueprint is proposing that the count API is baked into
-nova.api.openstack.compute.plugins.v3.servers directly.
-
-I cannot think of alternative implementations. The new API needs to utilitize
-the existing filter processing as the current /servers APIs in order to ensure
-consistency and prevent dual maintenance.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-The response for the existing /servers and /servers/detail REST APIs will
-not be affected.
-
-* New v2 API extension:
-
-  * Name: ServerCounts
-  * Alias: os-server-counts
-
-* NEW v2 URL: v2/{tenant_id}/servers/count
-
-* NEW v3 URL: v3/servers/count
-
-* Description: Get number of servers
-
-* Method type: GET
-
-* Normal Response Codes: Same as the 'v2/{tenant_id}/servers/detail' API):
-
-  * 200
-  * 203
-
-* Error Response Codes (same as the 'v2/{tenant_id}/servers/detail' API):
-
-  * computeFault (400, 500, ...)
-  * serviceUnavailable (503)
-  * badRequest (400)
-  * unauthorized (401)
-  * forbidden (403)
-  * badMethod (405)
-
-* Parameters (same as the 'v2/{tenant_id}/servers' API except the 'limit' and
-  'marker' parameters):
-
-+---------------+-------+--------------+--------------------------------------+
-| Parameter     | Style | Type         | Description                          |
-+===============+=======+==============+======================================+
-| all_tenants   | query | xsd:boolean  | Display server count information     |
-| (optional)    |       |              | from all tenants (Admin only).       |
-+---------------+-------+--------------+--------------------------------------+
-| changes-since | query | xsd:dateTime | A time/date stamp for when the       |
-| (optional)    |       |              | serverlast changed status.           |
-+---------------+-------+--------------+--------------------------------------+
-| image         | query | xsd:anyURI   | Name of the image in URL format.     |
-| (optional)    |       |              |                                      |
-+---------------+-------+--------------+--------------------------------------+
-| flavor        | query | xsd:anyURI   | Name of the flavor in URL format.    |
-| (optional)    |       |              |                                      |
-+---------------+-------+--------------+--------------------------------------+
-| name          | query | xsd:string   | Name of the server as a string.      |
-| (optional)    |       |              |                                      |
-+---------------+-------+--------------+--------------------------------------+
-| status        | query | csapi:Server | Value of the status of the server so |
-| (optional)    |       | Status       | that you can filter on "ACTIVE" for  |
-|               |       |              | example.                             |
-+---------------+-------+--------------+--------------------------------------+
-
-  * JSON schema definition for the body data: N/A
-
-  * JSON schema definition for the response data: {"count": <int>}
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None -- This new API is not introducing any new DB joins that would affect
-performance.
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  Steven Kaufer
-
-Other contributors:
-  <launchpad-id or None>
-
-Work Items
-----------
-
-* Move filter processing code into utility functions at the API layer and at
-  the DB sqlalchemy layer.
-* Create new API functions in the various layers to get the count data.
-* v2 API extension and v3 API updates to expose the new count API function.
-
-
-Dependencies
-============
-
-Related (but independent) change being proposed in cinder:
-https://blueprints.launchpad.net/cinder/+spec/volume-count-api
-
-
-Testing
-=======
-
-Both unit and Tempest tests need to be created to ensure that the count data
-is accurate for various filters.
-
-Testing should be done against multiple backend database types.
-
-
-Documentation Impact
-====================
-
-Document the new v2 API extension and v3 API updates (see "REST API impact"
-section for details).
-
-
-References
-==========
-
-None
-
diff --git a/specs/juno/server-group-quotas.rst b/specs/juno/server-group-quotas.rst
deleted file mode 100644
index d8b2aa8..0000000
--- a/specs/juno/server-group-quotas.rst
+++ /dev/null
@@ -1,330 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===================
-Server Group Quotas
-===================
-
-https://blueprints.launchpad.net/nova/+spec/server-group-quotas
-
-Add quota values to constrain the number and size of server groups a
-users can create.
-
-Problem description
-===================
-
-Server groups can be used to control the affinity and anti-affinity scheduling
-policy for a group of servers (instances).  Whilst this is a useful mechanism
-for users such scheduling decisions need to be balanced by a deployers
-requirements to make effective use of the available capacity.
-
-For example it may be considered reasonable for a user to be able to request
-anti-affinity between a set of 10 servers to support a particular
-availability schematic.   However a user creating anti-affinity between 100
-servers would be in direct conflict with a stacking policy intended to
-avoid fragmentation of the overall cloud capacity.
-
-Unlimited anti-affinity could allow a user to derive information about the
-overall size of the cloud, which is generally considered private information
-of the cloud provider.
-
-Unlimited server groups could in themselves be used as a DoS attack against
-systems not protected by an API rate limiter, a user creating groups until
-the DB fills up.
-
-Proposed change
-===============
-
-Two new quota values will be introduced to limit the number of sever groups
-and the number of servers in a server group.
-
-These will follow the existing pattern for quotas (for example security
-groups and rules per security group) in that:
-
-* They are defined by config values, which also include the default value
-
-* They can be defined per project or per user within a project
-
-* A value of -1 for either quota will be treated as unlimited.
-
-* Defaults can be set via the quota groups API
-
-* Values may be changed at any time but will only take effect at the next
-  server group or server create.   Reducing the quota will not affect any
-  existing groups, but new servers will not be allowed into groups
-  that have become over quota.
-
-The new options will be defined as follows:
-
-cfg.IntOpt('quota_server_groups',
-           default=10,
-           help='Number of server groups per project')
-
-cfg.IntOpt('quota_server_group_members',
-           default=10,
-           help='Number of servers per server group')
-
-
-Alternatives
-------------
-
-None.
-
-Data model impact
------------------
-
-None.  The quota values will be simply checked at the point when a server
-group is created or a server is created.
-
-REST API impact
----------------
-
-Because this change introduces additional fields to existing API methods
-it will be controlled in V2 by the presence of a new api extension.
-
-Name = "ServerGroupQuotas"
-Alias = "os-server-group-quotas"
-
-
-Change in the response when getting the quotas for a user/tenant.
-* Method: GET
-* Path: /os-quota-sets/{tenant_id}
-* Resp: Normal Response Codes 200
-
-JSON response
-
-{
- "quota_set": {
-  "cores": 20,
-  "fixed_ips": -1,
-  "floating_ips": 10,
-  "id": "fake_tenant",
-  "injected_file_content_bytes": 10240,
-  "injected_file_path_bytes": 255,
-  "injected_files": 5,
-  "instances": 10,
-  "key_pairs": 100,
-  "metadata_items": 128,
-  "ram": 51200,
-  "security_group_rules": 20,
-  "security_groups": 10,
-  "server_groups": 10,
-  "server_group_members": 10,
-
- }
-
-}
-
-Change in the response when getting the default quotas.
-* Method: GET
-* Path: /os-quota-sets/defaults
-* Resp: Normal Response Codes 200
-
-JSON response
-
-{
- "quota_set": {
-  "cores": 20,
-  "fixed_ips": -1,
-  "floating_ips": 10,
-  "id": "fake_tenant",
-  "injected_file_content_bytes": 10240,
-  "injected_file_path_bytes": 255,
-  "injected_files": 5,
-  "instances": 10,
-  "key_pairs": 100,
-  "metadata_items": 128,
-  "ram": 51200,
-  "security_group_rules": 20,
-  "security_groups": 10,
-  "server_groups": 10,
-  "server_group_members": 10,
-
- }
-
-}
-
-Change in the request when updating the quotas for a user/tenant.
-* Method: POST
-* Path: /os-quota-sets/{tenant_id}/{user_id}
-* Resp: Normal Response Codes 200
-
-JSON response:
-
-{
- "quota_set": {
-  "force": "True",
-  "instances": 9,
-  "server_groups": 10,
-  "server_group_members": 10,
-
- }
-
-}
-
-JSON Schema:
-
-common_quota = {
-    'type': ['integer', 'string'],
-    'pattern': '^-?[0-9]+$',
-    'minimum': -1
-
-}
-
-update = {
-    'properties': {
-        'type': 'object',
-         'quota_set': {
-            'properties': {
-                'instances': common_quota,
-                'cores': common_quota,
-                'ram': common_quota,
-                'floating_ips': common_quota,
-                'fixed_ips': common_quota,
-                'metadata_items': common_quota,
-                'key_pairs': common_quota,
-                'security_groups': common_quota,
-                'security_group_rules': common_quota,
-                'server_groups': common_quota,
-                'server_group_members': common_quota,
-                'force': parameter_types.boolean,
-
-            },
-            'additionalProperties': False,
-
-        },
-
-    },
-    'required': ['quota_set'],
-    'additionalProperties': False,
-
-}
-
-Change in the response of the of limits request:
-
-
-JSON response:
-
-{
-    "limits": {
-        "rate": [
-
-        ],
-    "absolute": {
-        "maxServerMeta": 128,
-        "maxPersonality": 5,
-        "maxImageMeta": 128,
-        "maxPersonalitySize": 10240,
-        "maxSecurityGroupRules": 20,
-        "maxTotalKeypairs": 100,
-        "totalRAMUsed": 2048,
-        "totalInstancesUsed": 4,
-        "maxSecurityGroups": 10,
-        "totalFloatingIpsUsed": 0,
-        "maxTotalCores": 20,
-        "totalSecurityGroupsUsed": 1,
-        "maxTotalFloatingIps": 10,
-        "maxTotalInstances": 10,
-        "totalCoresUsed": 4,
-        "maxTotalRAMSize": 51200,
-        "maxServerGroups": 10,
-        "totalServerGroupsUsed": 2,
-        "maxServersPerServerGroups": 10,
-
-    }
-
-  }
-
-}
-
-Change in the response of ServerGroup API:
-
-Create can now return 413 "Quota Exceeded for server groups"
-
-
-
-Security impact
----------------
-
-Improves the security of systems with the Server Groups API enabled
-by limiting the resources each project can consume.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-python-novaclient will be updated to support the new quota values.
-
-If the new values are not returned by the API (i.e the system has not yet
-been updated to include this change) then the client will return a value
-of -1 (unlimited)
-
-Performance Impact
-------------------
-
-None - the quota validation will be a minor additional step in the  API.
-
-Other deployer impact
----------------------
-
-Quotas will only be validated for new requests, so it is possible (as with
-any default quota change) that some existing projects may already be over
-quota.  No existing groups will be affected, but users will be unable to
-create new groups and/or add servers to groups until they drop below their
-quota allowances.
-
-Deployers will have to consider what default quota values they want to
-configure, and if they want to configure any project specific quotas.
-
-The new quota checks will only be effective and vakues reported via the API
-when the new extension is loaded.
-
-Developer impact
-----------------
-
-None.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  philip-day
-
-Work Items
-----------
-
-The change will be submitted as a single patch set.
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-Existing Tempest quota tests will be extended to cover the new values.
-
-
-Documentation Impact
-====================
-
-The new values will need to be included in the documentation.
-
-
-References
-==========
-
-None.
diff --git a/specs/juno/servers-list-support-multi-status.rst b/specs/juno/servers-list-support-multi-status.rst
deleted file mode 100644
index 193741d..0000000
--- a/specs/juno/servers-list-support-multi-status.rst
+++ /dev/null
@@ -1,133 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==============================================
-servers list API support specify multi-status
-==============================================
-
-https://blueprints.launchpad.net/nova/+spec/servers-list-support-multi-status
-
-Allow to specify multiple status value concurrently in the servers list API.
-
-Problem description
-===================
-
-Currently the service list API allows the user to specify an optional status
-value to use as a filter - for example to limit the list to only servers with
-a status of Active.
-
-However often the user wants to filter the list by a set of status values,
-for example list servers with a status of Active or Error,
-which requires two separate API calls.
-
-Allowing the API to accept a list of status values would reduce this to a
-single API call.
-
-Proposed change
-===============
-
-Enable servers list API to support to specify multiple status values
-concurrently.
-
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-Allow to specify status value for many times in a request.
-
-For example::
-
-    GET /v2/{tenant_id}/servers?status=ACTIVE&status=ERROR
-    GET /v3/servers?status=ACTIVE&status=ERROR
-
-V2 API extension::
-
-    {
-        "alias": "os-server-list-multi-status",
-        "description": "Allow to filter the
-            servers by a set of status values.",
-        "links": [],
-        "name": "ServerListMultiStatus",
-        "namespace": "http://docs.openstack.org/compute/ext/
-            os-server-list-multi-status/api/v2",
-        "updated": "2014-05-11T00:00:00Z"
-    }
-
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  boh.ricky
-
-Work Items
-----------
-
-Implement the support for servers list API to specify multiple status values
-concurrently.
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-None
-
-Documentation Impact
-====================
-
-Need to document in the API document.
-
-References
-==========
-
-None
diff --git a/specs/juno/standardize-nova-image.rst b/specs/juno/standardize-nova-image.rst
deleted file mode 100644
index 41ea3e8..0000000
--- a/specs/juno/standardize-nova-image.rst
+++ /dev/null
@@ -1,125 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-======================
-Standardize Nova Image
-======================
-
-https://blueprints.launchpad.net/nova/+spec/standardize-nova-image
-
-Standardize Nova's nova.image module to work like nova.network.api
-and nova.volume.cinder.
-
-Problem description
-===================
-
-For some reason, nova.image does things differently than nova.volume and
-nova.network. Instead of nova.compute.manager instantiating a
-self.image_api object, like it does for self.network_api and
-self.volume_api, the compute manager calls an obtuse collection of
-nova.image.glance module calls.
-
-This blueprint is around the work to make a new nova.image.api module
-and have it called like other submodule "internal APIs" in Nova.
-
-Proposed change
-===============
-
-A new nova.image.api module shall be created, following in the style
-of nova.network.api and nova.volume.cinder. There will be an API class
-in the nova.image.api module that follows identical conventions as the
-nova.volume.cinder.API class, with methods for listing (get_all), showing
-(get), creating (create), updating (update), and removing (delete)
-images from the backend image store. There will be a nova.image.driver
-module with a base driver class.
-
-The nova.image.glance module will be updated to subclass the base driver
-class.
-
-Alternatives
-------------
-
-There is a series of patches in review up for Nova that tries to add
-support for Glance's V2 API operations:
-
-https://review.openstack.org/#/q/status:open+project:openstack/nova+branch:master+topic:bp/use-glance-v2-api,n,z
-
-Unfortunately, I believe this patch series further muddies the image
-service inside Nova instead of making it cleaner and standardized with
-the rest of Nova's external API interfaces.
-
-The idea of this blueprint is to lay a good foundation for future V2
-Glance API work by first standardizing the image API inside of Nova.
-
-Data model impact
------------------
-None
-
-REST API impact
----------------
-None
-
-Security impact
----------------
-None
-
-Notifications impact
---------------------
-None
-
-Other end user impact
----------------------
-None
-
-Performance Impact
-------------------
-None
-
-Other deployer impact
----------------------
-None
-
-Developer impact
-----------------
-
-See above link to Eddie Sheffield's patch series that would be affected by the
-code in this blueprint. Hopefully, however, once the image API is brought in
-line with the other internal-to-external Nova APIs, the work on V2 Glance API
-should be quite a bit easier.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  jaypipes
-
-Work Items
-----------
-
- * Create the nova.image.api module that instantiates a driver
- * Create the base image driver class, modeling after the new
-   nova.network.driver class created by the refactor-network-api blueprint
-   code.
- * Move the existing glance code into a subclassed driver
-
-Dependencies
-============
-None
-
-Testing
-=======
-None
-
-Documentation Impact
-====================
-None
-
-References
-==========
-None
diff --git a/specs/juno/string-field-max-length.rst b/specs/juno/string-field-max-length.rst
deleted file mode 100644
index 3aeee19..0000000
--- a/specs/juno/string-field-max-length.rst
+++ /dev/null
@@ -1,174 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=======================================
-Allow StringField to enforce max length
-=======================================
-
-https://blueprints.launchpad.net/nova/+spec/string-field-max-length
-
-This blueprint aims to add a max length constraint to the
-`nova.objects.fields.StringField` class.
-
-Problem description
-===================
-
-Currently, the nova object framework revolves around the use of field type
-classes that describe the schema of an object. Each object model is simply
-a collection of fields, each of which have a particular type, such as
-IntegerField or StringField.
-
-In much the same way that a SQL database schema describes the constraints
-that a given column in a table must adhere to -- e.g. the length of characters
-possible in a CHAR field, or a valid DATETIME string -- the nova objects
-should be self-validating.
-
-Proposed change
-===============
-
-This specification proposes to change the `coerce` method of the
-`String` class to validate on the number of characters in the
-field's string value.
-
-The `StringField` concrete field class shall have a new `max_length` kwarg
-added to its constructor that will control the validation. The default
-value will be None, and no `StringField` objects defined in the schemas of
-any of the nova object models shall be changed in this spec.
-
-Alternatives
-------------
-
-None (keep things the way they are now)
-
-Data model impact
------------------
-
-None (the existing models themselves won't be changed in this specification
-at all)
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Roughly, the code the `String` field type class would change from this:
-
-.. code:: python
-
-    class String(FieldType):
-        @staticmethod
-        def coerce(obj, attr, value):
-            # FIXME(danms): We should really try to avoid the need to do this
-            if isinstance(value, (six.string_types, int, long, float,
-                                  datetime.datetime)):
-                return unicode(value)
-            else:
-                raise ValueError(_('A string is required here, not %s') %
-                                 value.__class__.__name__)
-
-to this:
-
-.. code:: python
-
-    class String(FieldType):
-
-        def __init__(self, max_length=None):
-            """
-            :param max_length: Optional constraint on the number of Unicode
-                               characters the string value can be.
-            """
-            self._max_length = max_length
-
-        @staticmethod
-        def coerce(self, obj, attr, value):
-            # FIXME(danms): We should really try to avoid the need to do this
-            if isinstance(value, (six.string_types, int, long, float,
-                                  datetime.datetime)):
-                result = unicode(value)
-            else:
-                raise ValueError(_('A string is required here, not %s') %
-                                 value.__class__.__name__)
-            if self._max_length is not None:
-                if len(value) > self._max_length):
-                    msg = _("String %(result)s is longer than maximum allowed "
-                            "length of %(max_length)d.")
-                    msg = msg % dict(result=result,
-                                     max_length=self._max_length)
-                    raise ValueError(msg)
-            return result
-
-The StringField class would then need to be modified to allow passing the
-max_length parameter along to its type class.
-
-Work Items
-----------
-
-N/A
-
-Assignee(s)
------------
-
-Primary assignee:
-  jaypipes
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Would need new unit tests. No need for any integration test changes.
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-The server-instance-tagging work will likely be the first work to use
-this functionality, as the tag string has a max length associated with
-it and we need to be very careful about changing existing model fields' string
-length validation code, so a new field like the tag field is an ideal place to
-begin with this implementation.
-
-http://git.openstack.org/cgit/openstack/nova-specs/tree/specs/juno/tag-instances.rst
diff --git a/specs/juno/support-cinderclient-v2.rst b/specs/juno/support-cinderclient-v2.rst
deleted file mode 100644
index 394684d..0000000
--- a/specs/juno/support-cinderclient-v2.rst
+++ /dev/null
@@ -1,155 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Cinder Client V2 Support
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/support-cinderclient-v2
-
-Cinder has a new API version 2 [1]. This version has existed since Grizzly [2]
-and has been available in devstack since Havana [3].
-
-The API provides:
-
-* More consistent responses like name, description instead of 'display_name',
-  etc.
-
-* Caching data between controllers instead of multiple database hits.
-
-* Filtering when listing information on volumes, snapshots and backups. This
-  would be great support to have in Nova so the full listing of resources
-  doesn't have to be given over the network for Nova to sort through. [4]
-
-Cinder is also deprecating version 1 in favor of 2, so it would be great to
-give users a transition period in other projects.
-
-Problem description
-===================
-
-Nova currently has a wrapper to the Cinder client in nova.volumes.cinder which
-supports version 1 and expects a variety of response keys like 'display_name'
-and 'display_description' which aren't available in version 2. These were
-changed to be consistent with other projects that just use 'name' and
-'description'.
-
-Proposed change
-===============
-
-Nova should use Cinder v2 client [5] which understands how to talk to the
-Cinder v2 API. Since v1 is deprecated, we can leave Cinder client v1 support
-in.
-
-`cinder_catalog_info` option in nova.conf should also be set to
-`volumev2:cinder:publicURL` which would default new users the v2 API which is
-on by default in Cinder since Grizzly.
-
-Making these changes to the wrapper won't require any change to its interface
-or changes to how it returns information. This is done by the wrapper doing the
-translation and still giving back the expected data structure as it would with
-v1.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-Existing deployments will not need to make any changes to nova.conf in the Juno
-release. Cinder will just be deprecating v1 support, so they'll receive
-a warning on start up in the cinder-api service. If the deployer wants Nova to
-use Cinder v2, they'll need to change `cinder_catalog_info` to use the
-appropriate service_type they have Cinder v2 endpoint setup in the service
-catalog. It is acceptable to have a mix of Nova hosts talking to different
-versions of the Cinder API, assuming both v1 and v2 are enabled in Cinder.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  thingee
-
-Other contributors:
-  dzyu
-
-Work Items
-----------
-
-* Write changes in nova.volumes.cinder to support Cinder client v2, while
-  keeping support for v1. [6]
-* Add Cinder filtering support in Nova.
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Tempest gate tests for compute will test against Cinder v2. Tempest has both
-versions available, so Nova's config option of cinder_catalog_info will be
-updated to the appropriate service_type of v2. If resources allow, we can also
-test against v1.
-
-Unit tests will test against Nova's wrapper which talks to Cinder client. This
-will specifically verify usage between v1 and v2 is handled on this layer and
-is transparent to the rest of Nova.
-
-Documentation Impact
-====================
-
-None
-
-References
-==========
-
-[1] - http://docs.openstack.org/api/openstack-block-storage/2.0/content/
-[2] - https://review.openstack.org/#/q/status:merged+project:openstack/cinder+branch:master+topic:bp/bp,n,z
-[3] - https://review.openstack.org/#/c/22489/
-[4] - https://github.com/openstack/cinder/commit/88e688317dc4066f2f0b4dfc454a3f049da4d0e3
-[5] - https://github.com/openstack/python-cinderclient/tree/master/cinderclient/v2
-[6] - https://review.openstack.org/#/c/43986/
diff --git a/specs/juno/support-console-log-migration.rst b/specs/juno/support-console-log-migration.rst
deleted file mode 100644
index 867cd12..0000000
--- a/specs/juno/support-console-log-migration.rst
+++ /dev/null
@@ -1,140 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===================================================
-Support Console Log migration during Live-migration
-===================================================
-
-https://blueprints.launchpad.net/nova/+spec/support-console-log-migration
-
-Implement console log migration during live-migration in the libvirt driver
-
-
-Problem description
-===================
-
-Currently, in libvirt driver with a kvm hypervisor, console output is written
-to console.log. Nova responds to a get-console-log request with the contents
-of this file and this information is useful for debugging issues during boot
-process. However, during a live-migration the contents of the file in the
-source node is discarded.
-
-There are two issues which play a role in this.
-
-* The new kvm process in the destination would have already started using
-  an empty console log.
-
-* While the migration progresses the VM in the source node will continue
-  to write to the console log.
-
-Proposed change
-===============
-
-We propose the following in this blueprint to solve this issue without
-depending on kvm.
-
-* Require that VIR_MIGRATE_UNDEFINE_SOURCE is not set. Instead wait for the
-  condition that the instance is shutoff at the source.
-
-* During post-live-migration copy the console log be from source node and save
-  in the destination node as console.log.1. If log rotation is implemented,
-  all the rotated files need to be rotated once.
-
-* Change get-console-log function such that console.log and console.log.1 are
-  merged in the response (within the MAX_CONSOLE_BYTES limit). It log rotation
-  is implemented then the function needs to read as many files as it takes to
-  fill up the MAX_CONSOLE_BYTES limit.
-
-* The source VM would get undefined by the periodic task once the database is
-  updated with the new hostname.
-
-Alternatives
-------------
-
-* Change qemu to move the file content
-* Stream console output to a shared location
-* If spec/libvirt-serial-console is implemented we can leverage on that
-  mechanism and trigger a rotation and move it to destination.
-
-
-Data model impact
------------------
-None
-
-REST API impact
----------------
-None
-
-Security impact
----------------
-None
-
-Notifications impact
---------------------
-None
-
-Other end user impact
----------------------
-None
-
-Performance Impact
-------------------
-
-* There's a brief window between the time the VM is activated in the
-  destination and before post-live-migration is completed. Any nova
-  console-log requests will return almost empty content during this window.
-
-Other deployer impact
----------------------
-
-* If people are using VIR_MIGRATE_UNDEFINE_SOURCE then they need to remove this
-  option to get this feature. If this flag exists we will fallback to not
-  having the console log migrated.
-
-Developer impact
-----------------
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  parthipan
-
-Work Items
-----------
-
-* Change live-migration to wait for shutoff state if flag
-  VIR_MIGRATE_UNDEFINE_SOURCE is not set.
-* Change get_console_log to handle rotated log files
-* Implement console log migration during post-live-migration
-
-Dependencies
-============
-None
-
-Testing
-=======
-
-Tempest tests should be added to test that the console logs are merged in the
-response and catch other corner-cases.
-
-Documentation Impact
-====================
-
-We expect to have the following documentation changes:
-
-* The migration flag changes to get console logs migrated
-* Expected empty console log during the VM offline period in the final stages
-  of the migration
-
-References
-==========
-
-* https://bugs.launchpad.net/nova/+bug/1203193
diff --git a/specs/juno/tag-instances.rst b/specs/juno/tag-instances.rst
deleted file mode 100644
index debc962..0000000
--- a/specs/juno/tag-instances.rst
+++ /dev/null
@@ -1,318 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-========================================
-Allow simple string tagging of instances
-========================================
-
-https://blueprints.launchpad.net/nova/+spec/tag-instances
-
-This blueprint aims to add support for a simple string tagging mechanism
-for the instance object in the Nova domain model.
-
-Problem description
-===================
-
-In most popular REST API interfaces, objects in the domain model can be
-"tagged" with zero or more simple strings. These strings may then be used
-to group and categorize objects in the domain model.
-
-In order to align Nova's REST API with the Internet's common understanding
-of `resource tagging`_, we can add an API extension that allows normal users
-to add, remove and list tags for an instance.
-
-.. _resource tagging: http://en.wikipedia.org/wiki/Tag_(metadata)
-
-Proposed change
-===============
-
-No changes to existing metadata, system_metadata or extra_specs functionality
-are being proposed. This is *specfically* for adding a new API for *normal
-users* to be able to tag their instances with simple strings.
-
-Add a v2[.1] API extension that allows a user to add, remove, and list tags
-for an instance.
-
-Add a v2[.1] API extension to allow searching for instances based on one
-or more string tags.
-
-Alternatives
-------------
-
-Alternatives to simple string tagging are already available in Nova through
-the instance metadata key/value pairs API extension. However, these existing
-approaches suffer from a few issues:
-
-* The key/value pairs in the existing server metadata API extension are
-  all exposed via the nova-metadata endpoint, and therefore some people
-  think they are limited to being queried only from the 169.254.169.254
-  address.
-* It is not clear in the API that some metadata key/value pairs are added by
-  the user and some are added by Nova, Glance, or some external system. Part
-  of the idea behind this simple string tagging proposal is to have a way
-  to tag instances that is *only* for normal users.
-* Finally, and *most importantly*, the direction that the Glance program is
-  taking is to use simple string tagging for **user-side categorization of
-  resources**, and to use key/value pairs, hierarchical metadata, and property
-  bags for describing system-side metadata about resources. Property bags are
-  basically enumerated types for metadata, with a key and a constrained list of
-  value choices. The proposed Catalog program will be following a strategy
-  used by the Graffiti project that is designed to handle metadata/catalog data
-  of various formats in a structured way, and leave user-focused taxonomy as
-  simple-string tags only. This blueprint aligns with that direction.
-
-Data model impact
------------------
-
-The `nova.objects.instance.Instance` object would have a new `tags` field
-of type `nova.objects.fields.ListOfStrings` that would be populated on-demand
-(i.e. not eager-loaded).
-
-A tag shall be defined as a Unicode bytestring no longer than 60 bytes in
-length. (This length is entirely arbitrary and could be reduced or expanded
-depending on review discussion...)
-
-The tag is an opaque string and is not intended to be interpreted or even
-read by the virt drivers. In the REST API changes below, non-URL-safe
-characters in tags will need to be urlencoded if referred in the URI (for
-example, doing a DELETE /servers/{server}/tags/{tag}, the {tag} would need
-to be urlencoded.
-
-.. note::
-
-    Glance already has object tagging functionality, and the database schema
-    in that project uses a VARCHAR(255) length for the tag value. I would
-    greatly prefer to keep a shorter-than-255 length. There
-    are a number of performance reasons (including the fact that MySQL
-    converts all varchar columns to fixed-width columns when doing aggregation
-    and temporary tables containing the varchar columns). In addition, if the
-    tags are UTF-8 (as proposed above), the 255 width will actually be 765
-    bytes wide (which exacerbates the fixed-width problems on MySQL).
-
-For the database schema, the following table constructs would suffice ::
-
-    CREATE TABLE tags (
-        resource_id CHAR(32) NOT NULL PRIMARY KEY,
-        tag VARCHAR(80) NOT NULL CHARACTER SET utf8
-         COLLATION utf8_ci PRIMARY KEY
-    );
-
-There shall be a new hard-coded limit of 50 for the number of tags a user can
-use on a server. No need to make this configurable or use the quota system at
-this point.
-
-REST API impact
----------------
-
-This proposal would add a v2[.1] API extension for retrieving and setting tags
-against an instance. In addition, it would add an API extension to allow the
-searching/listing of instances based on one or more string tags.
-
-The tag CRUD operations API extension would look like the following:
-
-Return list of tags for a server ::
-
-    GET /v2/{project_id}/servers/{server_id}/tags
-
-returns ::
-
-    [
-        'tag-one',
-        'tag-two'
-    ]
-
-JSONSchema document for response ::
-
-    {
-        "title": "Server tags",
-        "type": "array",
-        "items": {
-            "type": "string"
-        },
-    }
-
-Replace set of tags on a server ::
-
-    POST /v2/{project_id}/servers/{server_id}/tags
-
-with request payload ::
-
-    [
-        'tag-one',
-        'tag-three'
-    ]
-
-JSONSchema document for request ::
-
-    {
-        "title": "Server tags",
-        "type": "array",
-        "items": {
-            "$ref": "#/definitions/tag"
-        },
-        "maxItems": 50,
-        "definitions": {
-            "tag": {
-                "type": "string",
-                "maxLength": 60
-            }
-        }
-    }
-
-Returns a `200 OK`. If the number of tags exceeds the limit of tags per
-server, shall return a `403 Forbidden`
-
-Add a single tag on a server ::
-
-    PUT /v2/{project_id}/servers/{server_id}/tags/{tag}
-
-Returns `204 No Content`.
-
-If the tag already exists, no error is raised, it just returns the
-`204 No Content`
-
-If the number of tags would exceed the per-server limit, shall return a
-`403 Forbidden`
-
-Remove a single tag on a server ::
-
-    DELETE /v2/{project_id}/servers/{server_id}/tags/{tag}
-
-Returns `204 No Content` upon success. Returns a `404 Not Found` if you
-attempt to delete a tag that does not exist.
-
-Remove all tags on a server ::
-
-    DELETE /v2/{project_id}/servers/{server_id}/tags
-
-Returns `204 No Content`.
-
-The API extension that would allow searching/filtering of the `GET /servers`
-REST API call would add the following query parameters:
-
-* `tag` -- One or more strings that will be used to filter results in an
-  AND expression.
-* `tag-any` -- One or more strings that will be used to filter results in
-  an OR expression.
-
-Get all servers having a single tag ::
-
-    GET /v2/{project_id}/servers?tag={tag}
-
-Would return the servers having the `{tag}` tag. No change is needed to the
-JSON response for the `GET /v2/{project_id}/servers/` call.
-
-Get all servers having either of two tags ::
-
-    GET /v2/{project_id}/servers?tag-any={tag_a}&tag-any={tag_b}
-
-Would return the servers having either the `{tag_a}` or the `{tag_b}` tag.
-No change is needed to the JSON response for the
-`GET /v2/{project_id}/servers/` call.
-
-Get all servers having *both* tag A and tag B::
-
-    GET /v2/{project_id}/servers?tag={tag_a}&tag={tag_b}
-
-Would return the servers having both the `{tag_a}` AND the `{tag_b}` tag.
-No change is needed to the JSON response for the
-`GET /v2/{project_id}/servers/` call.
-
-Mixing of `tag` and `tag-any` is perfectly fine. All `tag-any` tags will
-be grouped into a single OR'd expression that is AND'd to the expression
-built from all of the `tag` tags. For example::
-
-    GET /v2/{project_id}/servers?tag=A&tag=B&tag-any=C&tag-any=D
-
-Would yield servers that were tagged with "A", "B", and either "C" or "D".
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None, though REGEXP-based querying on some fields might be modified to
-use a faster tag-list filtering query.
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-See `Work Items`_ section below.
-
-Assignee(s)
------------
-
-Primary assignee:
-  jaypipes
-
-Other contributors:
-  snikitin
-
-Work Items
-----------
-
-Changes would be made, in order, to:
-
-1. the database API layer to add support for CRUD operations on instance tags
-2. the database API layer to add tag-list filtering support to
-   `instance_get_all_by_filters`
-3. the nova.objects layer to add support for a tags field of the Instance
-   object
-4. the API extension for CRUD operations on the tag list
-
-Dependencies
-============
-
-Soft dependency on specification for adding field type validation to nova
-objects. I say soft because technically this blueprint can be implemented
-with the tag string length validation done at the database schema level:
-
-https://blueprints.launchpad.net/nova/+spec/field-type-validation
-
-Note that the above is NOT a hard dependency and the work for this blueprint
-should not be held up for it. Hard-coded database schema string size limits
-are usable in this blueprint for the tag string length constraint.
-
-Testing
-=======
-
-Would need new Tempest and unit tests.
-
-Documentation Impact
-====================
-
-Docs needed for new API extension and usage.
-
-References
-==========
-
-Mailing list discussions:
-
-http://lists.openstack.org/pipermail/openstack-dev/2014-April/033222.html
-http://www.mail-archive.com/openstack-dev@lists.openstack.org/msg23310.html
diff --git a/specs/juno/use-libvirt-storage-pools.rst b/specs/juno/use-libvirt-storage-pools.rst
deleted file mode 100644
index 8c54b78..0000000
--- a/specs/juno/use-libvirt-storage-pools.rst
+++ /dev/null
@@ -1,291 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=========================
-Use libvirt Storage Pools
-=========================
-
-https://blueprints.launchpad.net/nova/+spec/use-libvirt-storage-pools
-
-Currently, the libvirt driver does not make use of libvirt's storage pools
-and volumes.  Using libvirt storage pools would simplify adding support for
-new image backends, as well as facilitating cold migrations (see follow up
-blueprint).
-
-
-Problem description
-===================
-
-Currently, Nova's libvirt driver does not make any use of libvirt volumes
-and storage pools.
-
-This means that, for the image backends, we have a lot
-of code that deals directly with various images backend formats, and we have
-to manually deal with a variety of different situations via various command
-line tools and libraries.
-
-However, much of this functionality is already present in libvirt, in the form
-of libvirt storage pools, so the libvirt driver duplicates functionality
-already present in libvirt itself.
-
-Proposed change
-===============
-
-The cache of images downloaded from Glance would be placed into a volume pool
-(:code:`nova-base-images-pool`).  This is done simply by instructing libvirt
-that Nova's image cache directory (e.g. :code:`/var/lib/nova/_base`) is a
-volume pool, and as such does not affect directory layout (and is thus
-compatible with both the legacy image backends and the new image backend
-proposed below).
-
-A new image backend, :code:`LibvirtStorage`, would be introduced.  This would
-support being used in place of all of the current types (with the exeception of
-RBD support, which for the time being would need a subclass [1]_).
-
-If we are not using COW, the libvirt :code:`pool.createXMLFrom` method
-could be used to appropriately copy the template image from the source pool,
-:code:`nova-base-images-pool`, into the target image in the target pool
-`nova-disks-pool`.
-
-If we are using COW, the libvirt :code:`pool.createXML` method could be used
-with a :code:`backingStore` element, which will appropriately create the new
-QCOW2 file with the backing file as the file in the image cache.
-
-This has the additional benefit of paving the way for the simplification of the
-image cache manager -- instead of having to run an external executable to check
-if an image is in the qcow2 format and has a backing store, we can simply check
-the :code:`backingStore` element's :code:`path` subelement for each
-libvirt volume (this also makes the code less brittle, should we decide to
-support other formats with backing stores) [2]_.
-
-A similar approach could be used with :code:`extract_snapshot` -- use
-:code:`createXMLFrom` to duplicate the libvirt volume (the new XML we pass
-in can handle compression, etc).
-
-In order to associate images with instances, the volumes in `nova-disks-pool`
-would have a name of the form `{instance-uuid}_{name}` (with :code:`name` being
-"disk", "kernel", etc, depending on the name passed to the image creation
-method).  This way, it still remains easy to find the disk image associated
-with a particular instance.
-
-The use of this new backend would become the default for new installations.
-However, the legacy backends would be left in place to maintain the live
-upgrade functionality (e.g. Icehouse->Juno). See the `Other deployer impact`_
-section below for more information.
-
-For the :code:`disk` XML element in the :code:`domain` element supplied to
-libvirt on instance creation, a type of :code:`volume` can be supplied, with
-the :code:`<source>` element specifying the pool name and volume name [3]_.
-
-.. [1] Currently, libvirt does not have support for the createXMLFrom operation
-   for RBD-backed pools, so for RDB support, we would have to subclass the new
-   backend and add in code to manually upload the template image.  This
-   functionality should be present in a future version of libvirt. See
-   `Red Hat BZ 1089079 <https://bugzilla.redhat.com/show_bug.cgi?id=1089079>`_.
-
-.. [2] Note that this functionality will most likely have to wait until the
-   OpenStack K release to be enabled by default, since such functionality would
-   be difficult to implement while supporting instances using both the legacy
-   and new backend -- see the `Other deployer impact`_ section below.  It could
-   be enabled in Juno by setting the :code:`images_type` configuration option
-   to 'libvirt-storage', which would imply that the deployer didn't want the
-   transitional functionality described in the aforementioned section.
-
-.. [3] Note that this XML is only available in libvirt version 1.0.5 and up,
-   so if we wish to support a version less than that for Juno, we
-   would simply have to rely on the current code (with some slight tweaks -- we
-   no longer have to try to detect the format, etc ourselves, as libvirt will
-   give it to us via the libvirt volume XML specification).
-
-Alternatives
-------------
-
-The setup described in this document calls for using a single storage pool
-for all VMs on a system.
-
-When using a file-based backend, this would require storing disk images in a
-single directory (such as :code:`/var/lib/nova/instance/disks`) instead of the
-current setup, where the disk images are stored in the instance directory
-(:code:`/var/lib/nova/instances/{instance-id}`).  This is due to the way that
-the libvirt :code:`dir` storage pool works.
-
-While it would be possible to create a new storage pool for each instance,
-this would only be applicable for file-based backends.  Having different
-functionality between file-based backends and other backends would complicate
-the code and reduce the abstraction introduced by this blueprint.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-Since the :code:`createXMLFrom` is actually intelligent about creating and
-copying image files (for instance, it calls :code:`qemu-img` under the hood
-when appropriate), there should be no performance impact.  As per what is
-mentioned in the `Proposed change`_ section, we would maintain current image
-cache functionality, including support for COW (via QCOW2), while paving the
-road for other file formats that libvirt supports as well.
-
-Other deployer impact
----------------------
-
-For live migration/upgrade from OpenStack Icehouse to OpenStack Juno, the
-legacy image backends (and support for them in Nova's image cache) will be left
-in place for the next release (Juno), but will be marked as deprecated.  In
-the K release, the legacy backends will be removed (as well as support for
-them in the image cache manager).
-
-To allow existing installations to easily transition to the new backend,
-existing instances would be left on the legacy backend, while all new instances
-would be created to use the new backend.  Whether or not an instance was using
-a legacy backend could be determined by checking the instance directory for
-images (if they are present, the instance is using a legacy backend, if not the
-instance is using the new backend).
-
-During operations which allow the changing of libvirt XML, such as cold
-migrations, resizes, reboots, and live migrations, instances would be
-automatically transitioned to using the new system [5]_.  This would allow
-deployers to move to the new system at their leisure, since they could either
-choose to bulk-restart the VMs themselves, or simply ask the VMs owners to do
-so when convinient.  For instances still on the legacy system, a warning would
-be issued on compute node startup.
-
-.. [5] This would entail telling libvirt to use the volume as the disk source.
-   In the case of live migrations with shared storage, resizes to the same
-   host, and reboots, a couple extra steps would be taken for deployments using
-   the local-file-based legacy backends.  For reboots and resizes, we can
-   simply move the disk image file to the directory pool location while the VM
-   is shut off.  In the case of shared storage which supports hard-linking, a
-   hard link pointing to the disk image file would be placed into the storage
-   pool directory.  Once the live migration finishes, the original location
-   would be deleted, leaving the new hard link as the only remaining reference
-   to the disk image file.  For filesystems where hard linking isn't supported,
-   a block live migration would be necessary to migrate the VM to the new image
-   backend.
-
-Developer impact
-----------------
-
-Currently, file-based images for a particular instance are stored in the
-instance directory (:code:`/var/lib/nova/instances/{instance-id}`).  In order
-to have one storage pool per compute node, libvirt's directory-based storage
-pool would require all of the disk images to be stored in one directory, so
-the images themselves would no longer be in
-:code:`/var/lib/nova/instances/{instance-id}`, but instead in something
-to the effect of :code:`/var/lib/nova/instance/disks`.
-
-Should it be desired to have different disk types (e.g. main disk vs swap)
-stored differently [6]_, we could simply create a pool for each type, and place
-the images into the appropriate pool based on their name.  An advantage to
-using pools is that Nova doesn't actually need to know the underlying details
-about the pool, only its name.  Thus, if a deployer wanted to move a particular
-pool to a different location, device, etc, no XML changes would be needed,
-assuming the same pool name was kept.
-
-.. [6] As suggested in
-   `this blueprint <https://review.openstack.org/#/c/83727>`_, for instance
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-    sross-7
-
-Other contributors:
-    None
-
-Work Items
-----------
-
-1. Modify the code which downloads images from Glance into a cache to
-   create a storage pool in the cache directory and refresh the cache
-   when a new image is downloaded.
-
-2. Implement the new image backend (and subclass it for RBD as long as it's not
-   supported natively as per [1]_) and sections in the XML config builder to
-   accept the :code:`volume` type for disk elements.
-
-3. Implement the functionality required to support transitional installations
-   (detecting legacy backend use, adding code to migration and reboots to
-   transition into new backend use).
-
-4. Implement functionality in the image cache manager to take advantage of the
-   new data about backing files stored in libvirt's volume information XML
-   (this would be disabled in Juno unless :code:`images_type` was set to
-   'libvirt-storage', implying the deployer didn't want the transitional
-   functionality mentioned above).
-
-
-Dependencies
-============
-
-No new libraries are required for this change.  However, the XML changes
-discussed above require a libvirt version > 1.0.5 (the actual storage pools do
-not, however).  While this is not strictly needed (as we can simply use the
-existing code for determining the correct XML for a given image), it does
-simplify the section of the code responsible for XML generation.  Since we
-will most likely be increasing the minimum libvirt version for Juno, however,
-this should not be problematic.
-
-Testing
-=======
-
-We will want to duplicate the existing tests for the various image backends to
-ensure that the new backend covers all of the existing functionality.
-Additionally, new tests should be introduced for:
-
-* the XML changes
-
-* storage pool management
-
-* migrating existing instances to the new backend and the supporting
-  transitional functionality
-
-Documentation Impact
-====================
-
-We should warn about the deprecation of the legacy image backends,
-and note the change to the new backend.  It should also be noted that
-migrations and cold resizes are the preferred method to transition existing
-instances to the new backend.
-
-
-References
-==========
-
-* http://libvirt.org/formatdomain.html#elementsDisks
-
-* http://libvirt.org/formatstorage.html
-
-* http://libvirt.org/storage.html
-
-* http://libvirt.org/html/libvirt-libvirt.html#virStorageVolCreateXMLFrom
diff --git a/specs/juno/use-oslo-vmware.rst b/specs/juno/use-oslo-vmware.rst
deleted file mode 100644
index 748d5a6..0000000
--- a/specs/juno/use-oslo-vmware.rst
+++ /dev/null
@@ -1,152 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-========================================================
-Integrate the vmware driver with the oslo.vmware library
-========================================================
-
-https://blueprints.launchpad.net/nova/+spec/use-oslo-vmware
-
-Now that the oslo.vmware library has been released, the vmware driver should be
-updated to use it.
-
-
-Problem description
-===================
-
-Too much code duplication of vmware-related projects led to the creation of the
-oslo.vmware project (https://github.com/openstack/oslo.vmware). Now that it is
-released, and already started to be used by Glance and Ceilometer, it's time
-the nova driver does the same.
-
-
-Proposed change
-===============
-
-This means mostly adding new import lines, mechanical conversion of call sites
-and deleting existing code obsoleted by the library.  Most of the work has
-already be done and proposed in the icehouse cycle
-(https://review.openstack.org/#/c/70175/) so that can be used as the starting
-point of the patch.
-The changes are pure code reorganization, and has no externally visible impact.
-
-Alternatives
-------------
-
-None, unless we consider the undesirable option of keeping status quo as one.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-Some version of the oslo.vmware library as eventually dictated by the
-the project requirements will have to be installed for the updated vmware
-driver to function.
-
-Developer impact
-----------------
-
-While the changes are mechanical, it touches many places in the vmwareapi
-driver code base, so it can cause a lot of conflict with other driver work.
-Once merged, it is likely all vmware driver related patches under review will
-have to be updated to account for it.
-
-On the flip side, there is developer impact of this change not being merged as
-well:
-
-Until this change is merged, driver changes/fixes to areas of functionality
-that oslo.vmware also provides means that a developer should almost always have
-to update both nova and oslo.vmware with similar patches.
-
-To migitate this issue of conflicts and code duplication, it is recommended
-that patches related to the vmware driver should be made dependent on this
-work.
-
-Changes to the nova driver may now require a change/release to oslo.vmware
-as a pre-requisite.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  vui
-
-Work Items
-----------
-
-Mostly https://review.openstack.org/#/c/70175/ plus some additional updates to
-account for recent code additions to the vmware driver code.
-
-
-Dependencies
-============
-
-Changes pertaining to
-https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
-will cause significant code churn, but given the mostly mechanical nature of
-the changes to this blueprint, reacting to the former should be fairly
-straightforward.
-
-Given that this work and that for the vmware-spawn-refactor blueprint are
-fairly orthogonal, and both necessary to facilitate additional changes to the
-driver, it is proposed that they be considered the highest-priority items for
-the vmware driver to be included in Juno-1.
-
-
-Testing
-=======
-
-Unit tests exercising the obsoleted code will be removed. Updating existing
-tests that currently mocks the obsoleted code to use use.vmware accordingly
-so that they pass should be sufficient to validate the change.
-
-No externally visible changes means no additional Tempest tests are needed.
-
-
-Documentation Impact
-====================
-
-None
-
-
-References
-==========
-
-* https://github.com/openstack/oslo.vmware
-* https://review.openstack.org/#/c/70175/
-* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
diff --git a/specs/juno/user-defined-shutdown.rst b/specs/juno/user-defined-shutdown.rst
deleted file mode 100644
index b9ac215..0000000
--- a/specs/juno/user-defined-shutdown.rst
+++ /dev/null
@@ -1,259 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================================================
-Allow controlled shutdown of GuestOS for operations which power off the VM
-==========================================================================
-
-https://blueprints.launchpad.net/nova/+spec/user-defined-shutdown
-
-The current behavior of powering off a VM without giving the Guest Operating
-system a chance to perform a controlled shutdown can lead to data corruption.
-
-
-Problem description
-===================
-
-Currently in libvirt operations which power off the VM (stop, rescue, shelve,
-resize) do so without giving the GuestOS a chance to shutdown gracefully.
-Some GuestOS's (for example Windows) do not react well to this type of virtual
-power failure, and so it would be better if these operations follow the
-same approach as soft_reboot and give the GuestOS a chance to shutdown
-gracefully.
-
-
-Proposed change
-===============
-
-The proposed changes will make the default behavior for stop, rescue, resize,
-and shelve to give the GuestOS a chance to perform a controlled shutdown
-before the VM is powered off.
-
-The change will encapsulate the complexity of signaling to and waiting for
-the GuestOS in the hypervisor, and allow image owners the ability to tune
-the associated timing via image metadata to take account of GuestOSs that
-require an extended period to shutdown (such as Windows).
-
-Users will be able to specify the shutdown behavior on a per operation basis
-via a new shutdown_type parameter where, in keeping with the current reboot
-operation, a "soft" shutdown will give the GuestOS a chance to perform a
-clean shutdown, and a "hard" shutdown will cause an immediate power off.  The
-default behavior will be a "soft" shutdown.
-
-An example of a user wanting to override the default behavior is Tempest
-which does not generally care if a GuestOS becomes corrupted and may
-prefer speed of execution over data integrity.
-
-At the hypervisor layer the shutdown behavior will be controlled by two
-values:
-
-* A timeout value specifying in seconds how long the hypervisor should
-  wait for the GuestOS to shutdown. If the GuestOS does not shutdown
-  within this period then the VM will be powered off anyway. A value of 0
-  will power off the VM without signaling the Guest to shutdown.
-
-* A retry interval specifying in seconds how frequently within that period the
-  hypervisor should signal the guest to shutdown.  This is a protection
-  against guests that may not be ready to process the shutdown signal
-  when it is first issued - a common problem if an instance is deleted
-  just after it has been created and the GuestOS is booting.
-
-For example if the overall timeout is set to 60 seconds and the retry interval
-is set to 10 seconds then the guest will be signaled up to six times before
-being powered off.
-
-These values will be passed into the virt driver by the compute manager,
-allowing the same values to be used for all hypervisors.
-
-The timeout value will be a Nova configuration parameter as different
-operators may want a different default.  The retry value will be implemented
-as a constent in the Nova code.  The timeout value can be overridden
-on a per image basis via image metadata settings.
-
-Alternatives
-------------
-
-An alternative approach would be to expose a new operation that only shuts
-down the GuestOS (with used defined timing parameters), expose the status of
-that operation via the API, and rely on the client for all retry logic.
-However we believe that a clean shutdown should be the default behavior in
-Nova and not have to be managed as a separate activity (which would have to
-be replicated in all API bindings).
-
-An alternative using a simpler single parameter to specify how long the
-hypervisor should wait was previously merged but had to be reverted
-because it added around 25 minutes to the tempest runs:
-https://review.openstack.org/#/c/35303/
-
-This was due to Tempest frequently stopping an instance immediately after
-it is created, in which case the ACPI signal is delivered before the GuestOS
-is in a state to process it.  This results in the shutdown waiting for the
-full duration of the timeout.
-
-The revised approach described above avoids this issue by periodically
-resending the shutdown signal to the GuestOS.
-
-Once this change has been merged Tempest could be optimized to avoid this delay
-(for example by setting the timeout to zero via image metadata or nova.conf).
-
-It could be argued that the delete operation should allow the same
-controlled shutdown schematics so that instances using and/or booting
-from volumes can also leave those file systems in a safe state.  However
-if the stop operation is modified to provide a controlled shutdown then
-users can achieve the required sequence by performing a stop prior to
-the delete.  This also avoids an issue of the http delete request not
-normally accepting a body.
-
-
-Data model impact
------------------
-
-None, the change is contained mainly within the interaction between the compute
-manager and the virt driver.
-
-REST API impact
----------------
-
-The following API methods will be extended to accept an optional shutdown_type
-parameter:
-
-* Stop       POST servers/{server_id}/action
-                        {"os-stop": {"shutdown_type": "HARD|SOFT"}}
-
-* Rescue     POST servers/{server_id}/action
-                        {"rescue": {"shutdown_type": "HARD|SOFT"}}
-
-* Resize     POST servers/{server_id}/action
-                        {"resize": {"shutdown_type": "HARD|SOFT",
-                                    "flavor_id": <id>}}
-
-* Shelve     POST servers/{server_id}/action
-                        {"shelve": {"shutdown_type: "HARD|SOFT"}}
-
-* Migrate    POST servers/{server_id}/action
-                        {"migrate": {"shutdown_type: "HARD|SOFT"}}
-
-
-Security impact
----------------
-
-None, the change doesn't change the set of operations that a user can perform.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-Users will be able to provide additional options to the stop, rescue, and
-delete.  These will be exposed in the python-novaclient:
-
-nova stop [--hard-shutdown]
-nova rescue [--hard-shutdown]
-nova resize [--hard-shutdown]
-nova shelve [--hard-shutdown]
-
-Note that "--hard-shutdown" is preferred here over the "--hard" option used
-for reboot since a "soft resize" might be interpreted to mean a soft change
-in allocated resources (such as disabling a cpu).
-
-To make the novaclient CLI reboot command consistent it will be also modified
-to accept --hard-shutdown as an alias for --hard.
-
-Performance Impact
-------------------
-
-The performance impact is limited to the changes in the processing path of the
-stop, rescue, and delete operations. When performing a clean shutdown
-these will take longer than before as the system waits for the GuestOS to
-shutdown. The overhead of polling to observe this change in state is
-negligible and the calling thread will sleep (yield) between each poll.
-
-Other deployer impact
----------------------
-
-Once this set of changes has been merged the system will by default be
-configured to wait for instances to shutdown gracefully for stop, shelve,
-rescue, and resize operations.
-
-Deployers will need to consider if they want to modify the default timeout
-parameters, and/or to add override values to the metadata of existing images.
-
-The configuration parameters will be common to all hypervisors, but this
-BP will only deliver a libvirt implementation.
-
-
-Developer impact
-----------------
-
-Only the first stage of the implementation is hypervisor dependent, once
-that has merged other hypervisor implementations can be added.
-
-The remaining stages will apply to any hypervisor that implements the revised
-power_off options.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  philip-day
-
-Work Items
-----------
-
-* Add timeout parameters to virt power_off method of virt driver and provide
-  the libvirt implementation.   Implement clean_shutdown for stop() within
-  the compute manager as an initial example.
-* Add clean_shutdown option to compute manager Rescue, Resize, and Shelve
-  operations
-* Use image properties to override the timeout values
-* Expose clean shutdown via rpcapi
-* Expose clean shutdown via API
-
-
-Dependencies
-============
-
-None
-
-
-Testing
-=======
-
-The methods that are being modified are already extensively tested by Tempest
-which will ensure no functional regression.
-
-The default behavior will be to perform a clean shutdown, although it's not
-easy to see how this can be verified by Tempest, since it needs specific
-support within the Guest, and the behavior of any GuestOS is generally
-considered outside the scope of Nova.  Likewise the ability to stop without a
-clean shutdown could be exercised from Tempest (it's possible that Tempest
-would want to make this its normal case), but its hard to see how that could
-be verified.  Input will be sought from the Tempest community to see what can
-be done to address these issues.
-
-
-Documentation Impact
-====================
-
-* The API specs will need to be updated.
-* The change in default behavior for stop, rescue, resize, and shelve (to wait
-  for the GuestOS to shutdown) will need to be documented.
-* The ability to override the shutdown timeouts on a per image basis will need
-  to be documented.
-
-References
-==========
-
-The code for the first work item is available for review
-https://review.openstack.org/#q,I432b0b0c09db82797f28deb5617f02ee45a4278c,n,z
-
diff --git a/specs/juno/v2-on-v3-api.rst b/specs/juno/v2-on-v3-api.rst
deleted file mode 100644
index 1663226..0000000
--- a/specs/juno/v2-on-v3-api.rst
+++ /dev/null
@@ -1,262 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=============================================
-Implement the v2.1 API on the V3 API codebase
-=============================================
-
-https://blueprints.launchpad.net/nova/+spec/v2-on-v3-api
-
-Implement v2 compatible API based on v3 API infrastructure.
-
-Problem description
-===================
-
-On v3 API development, we have improved API infrastructure such as API
-plugin loading, input validation, policy check, etc. In addition, to fix
-inconsistent interfaces of v2 API, we have made a significant number of
-backwards incompatible changes of the Nova API (Change success status
-codes, API attribute names, and API URLs). There is a comprehensive
-description of the problems with the v2 API for users, operators and
-developers here:
-http://ozlabs.org/~cyeoh/V3_API.html
-
-However, there have been intensive discussions over the future of Nova
-and the maintenance overhead implications from having to support two
-APIs such as v2 and v3 simultaneously for a long period of time.
-
-Proposed change
-===============
-
-Through a lot of discussions, we have understood the advantages of v3 API
-infrastructure (API plugin loading, input validation, policy check, etc).
-However, their backwards incompatible interfaces seem a little premature at
-this time, because now we aren't sure that current v3 API is the best.
-That means we cannot be sure that any more backwards incompatible changes
-are unnecessary even if switching to current v3 API.
-
-This spec proposes the removal of backwards incompatible changes from v3 code.
-That means current v3 consistent interfaces would go back to v2 inconsistent
-ones like::
-
-  --- a/nova/api/openstack/compute/plugins/v3/servers.py
-  +++ b/nova/api/openstack/compute/plugins/v3/servers.py
-  @@ -752,7 +752,7 @@ class ServersController(wsgi.Controller):
-  The field image_ref is mandatory when no block devices have been
-  defined and must be a proper uuid when present.
-  """
-  - image_href = server_dict.get('image_ref')
-  + image_href = server_dict.get('imageRef')
-
-This proposal is painful for v3 API developers because they have worked hard
-to make consistent interfaces over a year and v3 interfaces are exactly better
-than v2 ones. However, through the discussions, we have known that backwards
-incompatible changes are very painful for users and we must pay attention to
-these changes.
-
-On this spec, we would provide v2 compatible API with the other v3 advantages
-as the first step. After this spec, we will provide consistent interfaces by
-defining API rules step by step. These rules will prevent the same backwards
-incompatible changes and keep consistent interfaces even if adding a lot of
-new APIs in the future. However, that is out of scope from this spec now.
-
-It is also agreed that we wont implement proxies for other OpenStack APIs such
-as glance, cinder or neutron as part of the initial v2.1 implementation. These
-will instead be added later, but before the removal of the original v2 code.
-
-Alternatives
-------------
-
-Through these discussions, we got an idea that we could support both v2 API
-and v3 API on the top of the v3 API codebase. On this idea, nova translates a
-v2 request to v3 request and passes it to v3 API method. After v3 API method
-operation, nova translates its v3 response to v2 response again and returns
-it to a client.
-However, there was an intensive discussion against this idea also because it
-would be difficult to debug API problems due to many translations when we have
-a lot of backwards incompatible changes in the long term.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-The V2.1 REST API presented will be identical to the V2 API except as
-noted above.
-
-Note however that V2.1 will not support the XML version of the V2 API,
-only the JSON one. However the XML version of the V2 API is currently
-marked as deprecated.
-
-Security impact
----------------
-
-Better up front input validation will reduce the ability for malicious
-user input to exploit security bugs.
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-Potentially it may be advantageous if python novaclient could talk to
-/v2.1 instead of /v2 but code changes may not be required to change
-this. It may be simpler just to do this through keystone configuration.
-The API itself remains identical.
-
-Performance Impact
-------------------
-
-More stringent input validation also means more work that is needed to
-be done in the API layer but overall this is a good thing.
-
-Other deployer impact
----------------------
-
-If the deployer wanted to export the API as /v2 rather than /v2.1 then
-they would need to modify the api-paste.ini file (a couple of line
-change to disable the original V2 API and use the APIRouterV21 as
-the /v2 API.
-
-The long term goal would be to deprecate and eventually remove the
-original V2 API code when deployers and users are satisfied that v2.1
-satisfies their requirements.
-
-The process which we would use is:
-
-* V2.1 fully implemented with Tempest verification (including extra
-  verification that is being added in terms of response data)
-* Verification from multiple sources (cloud providers, users etc) that
-  V2.1 is compatible with V2
-
-  * This could be done in various ways
-
-    * Keystone changes so /v2.1 is advertised instead of /v2
-    * Exporting the V2.1 as /v2
-    * Combined with the possibility of putting V2.1 input validation into
-      a log rather than reject mode.
-
-* V2.1 is in an openstack release for N versions
-* After widespread confirmation that the V2.1 API is compatible, V2
-  would be marked as deprecated
-
-Developer impact
-----------------
-
-Long term advantages for developers are:
-
-* All the API implementations are on the new API framework
-
-* Reduction in maintenance overhead of supporting two major API
-  versions
-
-* Have a better framework for handling future backwards incompatible
-  changes.
-
-In the short term while the old V2 API code exists there will still be
-a dual maintenance overhead.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  cyeoh-0
-
-Other contributors:
-  oomichi
-  Alex Xu
-
-Work Items
-----------
-
-* Change v3 success status codes to v2 ones.
-
-* Change v3 API routings to v2 ones.
-
-  * Handle API URLs include a project id.
-  * Change the API resource paths. (e.g: /keypairs(v3) -> /os-keypairs(v2))
-  * Change action names. (e.g: migrate_live(v3) -> os-migrateLive(v2))
-
-* Change v3 API attribute names to v2 ones.
-
-  * Change the API parsers of v3 code.
-  * Change the API schemas of input validation.
-
-* Change v3 API behaviors to v2 ones.
-  On some APIs, there are different behaviors.
-  For example, v3 "create a private flavor" API adds a flavor access for its
-  own project automatically, but v2 one doesn't.
-
-The following work item is not mandatory and it is one of wishlist.
-
-* Change v3 plugin code path.
-  e.g::
-
-    nova/api/openstack/compute/plugins/v3/servers.py
-    -> nova/api/openstack/compute/plugins/servers.py
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Tempest has already contained a lot of v2 API tests, and that is a good test
-coverage now. For this v2.1 API, we need to run v2 API tests for both current
-v2 and v2.1 in parallel. As an idea, we will add v2.1 API tests by inheriting
-from the existing v2 API test classes and executing them against /v2.1.
-A spec for this idea has been already proposed:
-
-https://review.openstack.org/#/c/96661/
-
-Documentation Impact
-====================
-
-The documentation for the v2 API will essentially remain the same as the API
-will not change except for improvements in input validation. There will need
-to be some updates on possible error status codes.
-
-Longer term the improved infrastructure for input validation and the
-development of JSON schema for response validation will make it much
-easier to automate the generation of documentation for v2 rather relying
-on the current mostly manual process.
-
-References
-==========
-
-* Juno Mid-Cycle meetup https://etherpad.openstack.org/p/juno-nova-mid-cycle-meetup
-
-* Juno design summit discussion https://etherpad.openstack.org/p/juno-nova-v2-on-v3-api-poc
-
-* Mailing list discussions about the Nova V3 API and the maintenance
-  overhead
-
-  * http://lists.openstack.org/pipermail/openstack-dev/2014-March/028724.html
-  * http://lists.openstack.org/pipermail/openstack-dev/2014-February/027896.html
-
-* Etherpad page which discusses the V2 on V3 Proof of Concept and
-  keeps track of the ongoing work.
-
-  * https://etherpad.openstack.org/p/NovaV2OnV3POC
-
-* Document about the problems with the V2 API
-
-  * http://ozlabs.org/~cyeoh/V3_API.html
-
-* Document describing the current differences between the V2 and V3 API
-
-  * https://wiki.openstack.org/wiki/NovaAPIv2tov3
diff --git a/specs/juno/v3-api-schema.rst b/specs/juno/v3-api-schema.rst
deleted file mode 100644
index d2873b1..0000000
--- a/specs/juno/v3-api-schema.rst
+++ /dev/null
@@ -1,223 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==============================================
-Create JSON Schema definitions for Nova v3 API
-==============================================
-
-https://blueprints.launchpad.net/nova/+spec/v3-api-schema
-
-Complete JSON Schema definitions for Nova v3 API request bodies.
-
-Problem description
-===================
-
-Nova contains a lot of RESTful API, but not all API parameters of a request
-body are completely validated. To validate all parameters, an API validation
-framework has been implemented with JSON Schema library.
-After that, we needed to add JSON Schema definitions for each API but we could
-not complete them in Icehouse cycle.
-In Juno cycle, we need to implemented strong validation for v2.1 API as the
-design summit discussion. That means we need to implement strong validation
-for v3 API because v2.1 API is implemented on the top of v3 API implementation.
-
-Proposed change
-===============
-
-Each API definition should be added with the following ways:
-
-* Create definition files under ./nova/api/openstack/compute/schemas/v3/.
-* Each definition should be described with JSON Schema.
-* Each parameter of definitions(type, minLength, etc.) can be defined from
-  current validation code, DB schema, unit tests, Tempest code, or so on.
-* Reuse the existing predefined parameter types(name, hostname, boolean, etc.)
-  in nova/api/validation/parameter_types.py as possible.
-
-Alternatives
-------------
-
-Before the API validation framework, we needed to add the validation code into
-each API method in ad-hoc. These changes would make the API method code dirty
-and we needed to create multiple patches due to incomplete validation.
-For example, "create a flavor extraspec" API has been changed twice in Icehouse
-for its validation:
-
-* Enforce FlavorExtraSpecs Key format.
-  http://git.openstack.org/cgit/openstack/nova/commit/?id=050ce0e5891ba816baaef
-
-* Fix the validation of flavor_extraspecs v2 API
-  http://git.openstack.org/cgit/openstack/nova/commit/?id=8010c8faf9f030d2c0264
-
-If using JSON Schema definitions instead, acceptable request formats are clear
-and we don't need to do this ad-hoc works in the future.
-
-* Why not Pecan
-
-  Some projects(Ironic, Ceilometer, etc.) are implemented with Pecan/WSME
-  frameworks and we can get API documents automatically from the frameworks.
-  In WSME implementation, the developers should define API parameters for
-  each API. Pecan would make the implementations of API routes(URL, METHOD)
-  easy. And API documentation is generated from the combinations of these
-  definitions.
-  In Icehouse summit, Nova team decided to pick Pecan as Nova v3 API framework
-  with JSONSchema instead of WSME. because Nova contains complex APIs (API
-  extensions) and WSME could not cover them. In addition, Pecan implementation
-  (https://blueprints.launchpad.net/nova/+spec/v3-api-pecan) also was difficult
-  in the development and not completed. So now, Nova v3 API is implemented with
-  Nova's original WSGI framework and JSONSchema, we cannot use Pecan.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-By applying strict validation to every APIs, some values which are accepted
-in v2 API will be denied in v3 API. For example, here picks the server name
-of "create a server" API up.
-The string pattern of the server name is not validated in v2 API at all. We
-can specify UTF-8(non-ascii) characters as a server name through v2 API now.
-For strong/comprehensive validation, we will apply the predefined parameter
-type "name" to the server name also. The types allows "a-zA-Z0-9. _-" only as
-the string pattern and denies UTF-8 characters. In the worst cases we could
-relax input validation for names.
-
-Security impact
----------------
-
-Better up front input validation will reduce the ability for malicious user
-input to exploit security bugs.
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-Nova will need some performance cost for this comprehensive validation, because
-the checks will be increased for API parameters which are not validated now.
-However, I believe this is necessary cost for public REST APIs and we need to
-pay it.
-
-Other deployer impact
----------------------
-
-None
-
-Developer impact
-----------------
-
-Developers, who implement a new REST API, need to add JSON Schema definitions
-as the part of an API implementation.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  oomichi
-
-Other contributors:
-  aikawa
-  takada-yuiko
-  xu-haiwei
-
-Work Items
-----------
-
-This task requires a lot of patches, and the progress management is the key.
-They are tracked on https://etherpad.openstack.org/p/nova-v3-api-validation
-Now the implementations of most APIs have been done except some new APIs (
-instance-group and server-external-events) and we need to review them.
-
-
-Dependencies
-============
-
-* If porting nova-network to v3 API, we need to create some JSON Schema patces
-  for it.
-
-
-Testing
-=======
-
-Through this implementation, we need to improve the unit test coverage from
-the viewpoint of negative request cases. Current unit tests don't cover every
-negative cases and we will be able to add them because of making valid request
-format clear.
-In addition, we will be able to find original unit test bugs through this work.
-We have fixed some bugs of unit tets in Icehouse:
-
-* Fix the sample and unittest params of v3 scheduler-hints
-  http://git.openstack.org/cgit/openstack/nova/commit/?id=b699c703e00eda1c8368b
-
-* Fix the flavor_ref type of unit tests
-  http://git.openstack.org/cgit/openstack/nova/commit/?id=5191576c279dc9905e881
-
-* Change evacuate test hostnames to preferable ones
-  http://git.openstack.org/cgit/openstack/nova/commit/?id=9888f61128ed82d15d074
-
-Now Tempest contains the negative test generator. The generator operates the
-negative tests automatically based on the API definitions which are described
-with JSON Schema. By porting the API definitions of this blueprint from Nova
-to Tempest, we can improve the test coverage of Tempest also.
-
-
-Documentation Impact
-====================
-
-In long term, I hope this API definitions are used for API specification
-document auto-genaration also. We can get the trustable API document and
-it would be good for users and developers.
-As the first step, I have submitted the blueprint for generating API sample
-files from the API definitions. This is out of the scope of this description
-but I pick it up as a useful sample:
-https://blueprints.launchpad.net/nova/+spec/generate-api-sample-from-api-schema
-
-* Why not current template files
-
-  API samples are generated from template files which are fixed format like::
-
-    {
-        "evacuate": {
-            "host": "%(host)s",
-            "admin_password": "%(adminPass)s",
-            "on_shared_storage": "%(onSharedStorage)s"
-        }
-    }
-
-  API developers should write this kind of template file for API implementation
-  and they should generate API sample files from them.
-  As the result, API implementation review has many files and sometime these
-  files were wrong at broken indents, non-existent parameters(typo, etc.).
-  To improve this situation, I proposed to use JSONSchema definitions instead
-  of the template files. After that, we can remove the template files and
-  reviews will be more easy.
-
-References
-==========
-
-* Links to mailing list
-
-  * [Nova] What validation feature is necessary for Nova v3 API
-    http://lists.openstack.org/pipermail/openstack-dev/2013-October/016649.html
-
-* Links to notes from a summit session
-
-  * API Validation for the Nova V3 API
-    https://etherpad.openstack.org/p/icehouse-summit-nova-pecan-wsme
diff --git a/specs/juno/v3-diagnostics.rst b/specs/juno/v3-diagnostics.rst
deleted file mode 100644
index cfa4699..0000000
--- a/specs/juno/v3-diagnostics.rst
+++ /dev/null
@@ -1,339 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==============================
-V3 Diagnostics - common output
-==============================
-
-https://blueprints.launchpad.net/nova/+spec/v3-diagnostics
-
-Currently there is no defined format for VM diagnostics. This BP will ensure
-that all of the drivers that provide VM diagnostics will have a consistent
-format.
-
-**NOTE:** this cannot be used for V2 as there may be existing deployments that
-parse the current output of the V2 diagnostics.
-
-Problem description
-===================
-
-In V2 the VM diagnostics are a 'blob' of data returned by each hypervisor. The
-goal here is to have a formal definition of what output should be returned, if
-possible, by the drivers supporting the API. In additition to this a driver
-will be able to return additional data if they choose.
-
-Proposed change
-===============
-
-Introduce a new driver method that will return a predefined structure:
-get_instance_diagnostics(self, context, instance)
-
-This is a new driver method. The reason for this is that it is much cleaner
-to have a new method instead of having if's which indicate if it is new or
-legacy. We should also consider deprecating get_diagnostics. This should be
-documented in the virt driver API.
-
-The proposal is to have the drivers return the following information in a
-object class. A diagnostics Model() class will be introduced. This will
-be instantiated and populated by the virt drivers. The class will have a
-method to serialize to JSON so that the API interface can return a JSON
-format to the user. A field that is not populated by the driver will return
-a default value set in the aforementioned class.
-
-The table below has the key and a description of the value returned:
-
-+------------------------+---------------------------------------------------+
-| Key                    | Description                                       |
-+========================+===================================================+
-| state                  | The current state of the VM. Example values       |
-|                        | are: 'pending', 'running', 'paused', 'shutdown',  |
-|                        | 'crashed', 'suspended' and 'building' (String)    |
-+------------------------+---------------------------------------------------+
-| driver                 | A string denoting the driver on which the VM is   |
-|                        | running. Examples may be: 'libvirt', 'xenapi',    |
-|                        | 'hyperv' and 'vmwareapi' (String) [Admin only -   |
-|                        | key will not appear if non admin]                 |
-+------------------------+---------------------------------------------------+
-| hypervisor_os          | A string denoting the hypervisor OS (String)      |
-|                        | [Admin only - key will not appear if non admin]   |
-+------------------------+---------------------------------------------------+
-| uptime                 | The amount of time in seconds that the VM has     |
-|                        | been running (Integer)                            |
-+------------------------+---------------------------------------------------+
-| num_cpus               | The number of vCPUs (Integer)                     |
-+------------------------+---------------------------------------------------+
-| num_nics               | The number of vNICS (Integer)                     |
-+------------------------+---------------------------------------------------+
-| num_disks              | The number of disks (Integer)                     |
-+------------------------+---------------------------------------------------+
-| cpu_details            | An array of details (a dictionary) per vCPU (see  |
-|                        | below)                                            |
-+------------------------+---------------------------------------------------+
-| nic_details            | An array of details (a dictionary) per vNIC (see  |
-|                        | below)                                            |
-+------------------------+---------------------------------------------------+
-| disk_details           | An array of details (a dictionary) per disk (see  |
-|                        | below)                                            |
-+------------------------+---------------------------------------------------+
-| memory_details         | A dictionary of memory details (see below)        |
-+------------------------+---------------------------------------------------+
-| config_drive           | Indicates if the config drive is supported on     |
-|                        | the instance (Boolean)                            |
-+------------------------+---------------------------------------------------+
-| driver_private_data    | A dictionary of private data from the driver.     |
-|                        | This is driver specific and each driver can       |
-|                        | return information valuable for diagnosing VM     |
-|                        | issues. The raw data should versioned.            |
-+------------------------+---------------------------------------------------+
-
-Note: A number of the above details are common to all drivers. These values
-will be filled in by the Nova compute manager prior to invoking the driver
-call. The ones that are virt driver specific will be filled, if possible, by
-the virt driver. If the virt driver is unable to provide a spcific field
-then that field will not be reported in the diagnostics.
-
-For example::
-
-    def get_instance_diagnostics(self, context, instance):
-        """Retrieve diagnostics for an instance on this host."""
-        current_power_state = self._get_power_state(context, instance)
-        if current_power_state == power_state.RUNNING:
-            LOG.audit(_("Retrieving diagnostics"), context=context,
-                      instance=instance)
-            diagnostics = {}
-            diagnostics['state'] = instance.vm_state
-            ...
-            driver_diags = self.driver.get_instance_diagnostics(instance)
-            diagnostics.update(driver_diags)
-            return diagnostics
-
-The cpu details will be an array of dictionaries per each virtual CPU.
-
-+------------------------+---------------------------------------------------+
-| Key                    | Description                                       |
-+========================+===================================================+
-| time                   | CPU Time in nano seconds (Integer)                |
-+------------------------+---------------------------------------------------+
-
-The network details will be an array of dictionaries per each virtual NIC.
-
-+------------------------+---------------------------------------------------+
-| Key                    | Description                                       |
-+========================+===================================================+
-| mac_address            | Mac address of the interface (String)             |
-+------------------------+---------------------------------------------------+
-| rx_octets              | Received octets (Integer)                         |
-+------------------------+---------------------------------------------------+
-| rx_errors              | Received errors (Integer)                         |
-+------------------------+---------------------------------------------------+
-| rx_drop                | Received packets dropped (Integer)                |
-+------------------------+---------------------------------------------------+
-| rx_packets             | Received packets (Integer)                        |
-+------------------------+---------------------------------------------------+
-| tx_octets              | Transmitted Octets (Integer)                      |
-+------------------------+---------------------------------------------------+
-| tx_errors              | Transmit errors (Integer)                         |
-+------------------------+---------------------------------------------------+
-| tx_drop                | Transmit dropped packets (Integer)                |
-+------------------------+---------------------------------------------------+
-| tx_packets             | Transmit packets (Integer)                        |
-+------------------------+---------------------------------------------------+
-
-The disk details will be an array of dictionaries per each virtual disk.
-
-+------------------------+---------------------------------------------------+
-| Key                    | Description                                       |
-+========================+===================================================+
-| id                     | Disk ID (String)                                  |
-+------------------------+---------------------------------------------------+
-| read_bytes             | Disk reads in bytes(Integer)                      |
-+------------------------+---------------------------------------------------+
-| read_requests          | Read requests (Integer)                           |
-+------------------------+---------------------------------------------------+
-| write_bytes            | Disk writes in bytes (Integer)                    |
-+------------------------+---------------------------------------------------+
-| write_requests         | Write requests (Integer)                          |
-+------------------------+---------------------------------------------------+
-| errors_count           | Disk errors (Integer)                             |
-+------------------------+---------------------------------------------------+
-
-The memory details is a dictionary.
-
-+------------------------+---------------------------------------------------+
-| Key                    | Description                                       |
-+========================+===================================================+
-| maximum                | Amount of memory provisioned for the VM in MB     |
-|                        | (Integer)                                         |
-+------------------------+---------------------------------------------------+
-| used                   | Amount of memory used by the VM in MB (Integer)   |
-+------------------------+---------------------------------------------------+
-
-Below is an example of the dictionary data returned by the fake driver::
-
-           {'state': 'running',
-            'driver': 'fake-driver',
-            'hypervisor_os': 'fake-os',
-            'uptime': 7,
-            'num_cpus': 1,
-            'num_vnics': 1,
-            'num_disks': 1,
-            'cpu_details': [{'time': 1024}]
-            'nic_details': [{'rx_octets': 0,
-                             'rx_errors': 0,
-                             'rx_drop': 0,
-                             'rx_packets': 0,
-                             'tx_octets': 0,
-                             'tx_errors': 0,
-                             'tx_drop': 0,
-                             'tx_packets': 0}],
-            'disk_details': [{'read_bytes':0,
-                              'read_requests': 0,
-                              'write_bytes': 0,
-                              'write_requests': 0,
-                              'errors_count': 0}],
-            'memory_details': {'maximum': 512, 'used': 256},
-            'driver_private_data': {'version': 1,
-                                    'memory': {'actual': 220160,
-                                               'rss': 200164}}
-
-Alternatives
-------------
-
-Continue with the same format that the V2 has. This is problematic as
-we are unable to build common user interface that can query VM states,
-for example in tempest.
-
-We can add an extension to the V2 API that will enable us to return
-the information defined in this spec.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-The V3 diagnostics API will no longer return data defined by the
-driver but it will return common data defined in this spec.
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-It will make life easier - deployers will be able to get better insight into
-the state of VM and be able to troubleshoot.
-
-We should consider adding this support for V2. In order to support backward
-compatibility we can add a configuration flag. That is, we can
-introduce a flag for the legacy format.
-
-Developer impact
-----------------
-
-None
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  Gary Kotton - garyk
-
-Other contributors:
-  Bob Ball - bob-ball
-
-Work Items
-----------
-
-All work items were in review Icehouse. They were broken up as
-follows:
-
-* VM diagnostics (v3 API only)
-
-* XenAPI
-
-* libvirt
-
-* VMware
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Once the code is approved we will add tests to Tempest that will do the
-following for the V3 API (assuming that the underlying driver does
-not return NotImplemented (501), which may be the case if the driver
-does not support the method):
-
-* Check that the returned driver is one of the supported ones in tree (at
-  the moment only libvirt, vmware and xenapi support the v3 method).
-
-* Check that the number of CPU's matches the flavor.
-
-* Check that the disk data matches the flavor.
-
-* Check that the memory matches the flavor.
-
-* If a cinder volume has been attached then we check that there is the
-  correct amount of disks attached.
-
-* Check that the number of vNics matches the instance running.
-
-* If the private data is present then check that this is a dictionary and
-  has a key 'version'.
-
-In addition to this, if there are tests that fail then we can use the V3
-diagnostics to help debug. That is, we can get the diagnostics which may help
-isolate problems.
-
-Documentation Impact
-====================
-
-We can now at least document the fields that are returned and their meaning.
-
-If we do decide to update the v2 support we will need to update:
-
-Please also update:
-http://docs.openstack.org/admin-guide-cloud/content/instance_usage_statistics.html
-http://docs.openstack.org/user-guide/content/usage_statistics.html
-http://docs.openstack.org/user-guide/content/novaclient_commands.html
-http://docs.openstack.org/trunk/openstack-ops/content/lay_of_the_land.html#diagnose-compute
-
-We will need to make sure that we update all of the equivalent v3 docs.
-The information in the tables above will be what we add to the documentation.
-
-References
-==========
-
-https://wiki.openstack.org/wiki/Nova_VM_Diagnostics
-https://bugs.launchpad.net/nova/+bug/1240043
diff --git a/specs/juno/vif-vhostuser.rst b/specs/juno/vif-vhostuser.rst
deleted file mode 100644
index 5e3164b..0000000
--- a/specs/juno/vif-vhostuser.rst
+++ /dev/null
@@ -1,163 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-====================
-Create VIF_VHOSTUSER
-====================
-
-https://blueprints.launchpad.net/nova/+spec/vif-vhostuser
-
-We propose to add a new VIF type to support the new QEMU vhost-user
-feature. vhost-user is a new QEMU feature that supports efficient
-Virtio-net I/O between a guest and a user-space vswitch. vhost-user is
-the userspace equivalent to /dev/vhost-net and is based on a Unix
-socket for communication instead of a kernel device file.
-
-
-Problem description
-===================
-
-QEMU has a new type of network interface, vhost-user, and we want to
-make this available to Neutron drivers. This will support deploying
-high-throughput userspace vswitches for OpenStack-based NFV
-applications. (This is the reason that vhost-user was developed.)
-
-
-Proposed change
-===============
-
-This change defines nova.network.model.VIF_TYPE_VHOSTUSER.
-
-We propose to add VIF_VHOSTUSER to Nova for creating network
-interfaces based on vhost-user. This VIF type would be enabled by
-Neutron drivers that want to assign certain ports to a userspace agent
-(vswitch) that is based on vhost-user.
-
-VIF_VHOSTUSER is to be implemented by extending the Libvirt driver.
-Libvirt support for vhost-user is currently under review and we expect
-it to be merged in time for Juno. We see that upstream Libvirt support
-for vhost-user is a dependency for merging the VIF_VHOSTUSER
-implementation into Nova.
-
-
-Alternatives
-------------
-
-Intel DPDK has a separate mechanism for accessing vhost from
-userspace, based on replacing /dev/vhost-net with a FUSE-based device
-file that traps ioctls into userspace. However, vhost-user is the new
-standard way to achieve this and is upstream in QEMU.
-
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-vhost-user will make OpenStack compatible with vswitches supporting N
-x 10G Virtio-net workloads.
-
-
-Other deployer impact
----------------------
-
-VIF_VHOSTUSER does not have to be enabled by the deployer. Neutron
-drivers will automatically enable VIF_VHOSTUSER via port binding if
-this is the appropriate choice for the agent on the compute host.
-
-VIF_VHOSTUSER will require a version of QEMU with vhost-user support,
-which is currently upstream and will be released in QEMU 2.1.
-
-VIF_VHOSTUSER will also require a version of Libvirt with vhost-user
-support.
-
-Developer impact
-----------------
-
-None.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  Luke Gorrie <lukego>
-
-Other contributors:
-  m.paolino
-
-Work Items
-----------
-
-* Add vhost-user support to the Libvirt driver.
-* Add VIF_VHOSTUSER support to Nova.
-
-Dependencies
-============
-
-* Libvirt must add support for vhost-user. Current patch under review:
-  http://www.redhat.com/archives/libvir-list/2014-July/msg00111.html
-
-* VIF_VHOSTUSER will enable the Neutron driver for Snabb NFV:
-  https://blueprints.launchpad.net/neutron/+spec/snabb-nfv-mech-driver
-  http://snabb.co/nfv.html
-  http://github.com/SnabbCo/snabbswitch
-
-
-Testing
-=======
-
-VIF_VHOSTUSER will be Tempest-tested by the planned 3rd party CI
-integration for the Snabb NFV mech driver.
-
-
-Documentation Impact
-====================
-
-No documentation changes for Nova are anticipated. VIF_VHOSTUSER will
-be automatically enabled by Neutron where appropriate.
-
-
-References
-==========
-
-* vhost-user:
-  http://www.virtualopensystems.com/en/solutions/guides/snabbswitch-qemu/
-
-* Snabb NFV (initial vswitch supporting vhost-user): http://snabb.co/nfv.html
-
-* Deutsche Telekom TeraStream project (initial user of VIF_VHOSTUSER):
-  http://blog.ipspace.net/2013/11/deutsche-telekom-terastream-designed.html
-
-* Discussion from NFV BoF (Atlanta) etherpad:
-  https://etherpad.openstack.org/p/juno-nfv-bof
-
diff --git a/specs/juno/virt-driver-cpu-pinning.rst b/specs/juno/virt-driver-cpu-pinning.rst
deleted file mode 100644
index 24890f0..0000000
--- a/specs/juno/virt-driver-cpu-pinning.rst
+++ /dev/null
@@ -1,224 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=============================================
-Virt driver pinning guest vCPUs to host pCPUs
-=============================================
-
-https://blueprints.launchpad.net/nova/+spec/virt-driver-cpu-pinning
-
-This feature aims to improve the libvirt driver so that it is able to strictly
-pin guest vCPUS to host pCPUs. This provides the concept of "dedicated CPU"
-guest instances.
-
-Problem description
-===================
-
-If a host is permitting overcommit of CPUs, there can be prolonged time
-periods where a guest vCPU is not scheduled by the host, if another guest is
-competing for the CPU time. This means that workloads executing in a guest can
-have unpredictable latency, which may be unacceptable for the type of
-application being run.
-
-Depending on the workload being executed the end user or admin may wish to
-have control over how the guest used hyperthreads. To maximise cache
-efficiency, the guest may wish to be pinned to thread siblings. Conversely
-the guest may wish to avoid thread siblings (ie only pin to 1 sibling)
-or even avoid hosts with threads entirely.
-
-Proposed change
-===============
-
-The flavour extra specs will be enhanced to support two new parameters
-
-* hw:cpu_policy=shared|dedicated
-* hw:cpu_threads_policy=avoid|separate|isolate|prefer
-
-If the policy is set to 'shared' no change will be made compared to the current
-default guest CPU placement policy. The guest vCPUs will be allowed to freely
-float across host pCPUs, albeit potentially constrained by NUMA policy. If the
-policy is set to 'dedicated' then the guest vCPUs will be strictly pinned to a
-set of host pCPUs. In the absence of an explicit vCPU topology request, the
-virt drivers typically expose all vCPUs as sockets with 1 core and 1 thread.
-When strict CPU pinning is in effect the guest CPU topology will be setup to
-match the topology of the CPUs to which it is pinned. ie if a 2 vCPU guest is
-pinned to a single host core with 2 threads, then the guest will get a topology
-of 1 socket, 1 core, 2 threads.
-
-The threads policy will control how the scheduler / virt driver place guests
-wrt CPU threads. It will only apply if the sheduler policy is 'dedicated'
-
- - avoid: the scheduler will not place the guest on a host which has
-   hyperthreads.
- - separate: if the host has threads, each vCPU will be placed on a
-   different core. ie no two vCPUs will be placed on thread siblings
- - isolate: if the host has threads, each vCPU will be placed on a
-   different core and no vCPUs from other guests will be able to be
-   placed on the same core. ie one thread sibling is guaranteed to
-   always be unused,
- - prefer: if the host has threads, vCPU will be placed on the same
-   core, so they are thread siblings.
-
-The image metadata properties will also allow specification of the
-threads policy
-
-* hw_cpu_threads_policy=avoid|separate|isolate|prefer
-
-This will only be honoured if the flavour does not already have a threads
-policy set. This ensures the cloud administrator can have absolute control
-over threads policy if desired.
-
-The schedular will have to be enhanced so that it considers the usage of CPUs
-by existing guests. Use of a dedicated CPU policy will have to be accompanied
-by the setup of aggregates to split the hosts into two groups, one allowing
-overcommit of shared pCPUs and the other only allowing dedicated CPU guests.
-ie we do not want a situation with dedicated CPU and shared CPU guests on the
-same host. It is likely that the administrator will already need to setup host
-aggregates for the purpose of using huge pages for guest RAM. The same grouping
-will be usable for both dedicated RAM (via huge pages) and dedicated CPUs (via
-pinning).
-
-The compute host already has a notion of CPU sockets which are reserved for
-execution of base operating system services. This facility will be preserved
-unchanged. ie dedicated CPU guests will only be placed on CPUs which are not
-marked as reserved for the base OS.
-
-Alternatives
-------------
-
-There is no alternative way to ensure that a guest has predictable execution
-latency free of cache effects from other guests working on the host, that does
-not involve CPU pinning.
-
-The proposed solution is to use host aggregates for grouping compute hosts into
-those for dedicated vs overcommit CPU policy. An alternative would be to allow
-compute hosts to have both dedicated and overcommit guests, splitting them onto
-separate sockets. ie if there were for sockets, two sockets could be used for
-dedicated CPU guests while two sockets could be used for overcommit guests,
-with usage determined on a first-come, first-served basis. A problem with this
-approach is that there is not strict workload isolation even if separate
-sockets are used. Cached effects can be observed, and they will also contend
-for memory access, so the overcommit guests can negatively impact performance
-of the dedicated CPU guests even if on separate sockets. So while this would
-be simpler from an administrative POV, it would not give the same performance
-guarantees that are important for NFV use cases. It would none the less be
-possible to enhance the design in the future, so that overcommit & dedicated
-CPU guests could co-exist on the same host for those use cases where admin
-simplicity is more important than perfect performance isolation. It is believed
-that it is better to start off with the simpler to implement design based on
-host aggregates for the first iteration of this feature.
-
-Data model impact
------------------
-
-No impact.
-
-The new data items are stored in the existing flavour extra specs data model
-and in the host state metadata model.
-
-REST API impact
----------------
-
-No impact.
-
-The existing APIs already support arbitrary data in the flavour extra specs.
-
-Security impact
----------------
-
-No impact.
-
-Notifications impact
---------------------
-
-No impact.
-
-The notifications system is not used by this change.
-
-Other end user impact
----------------------
-
-There are no changes that directly impact the end user, other than the fact
-that their guest should have more predictable CPU execution latency.
-
-Performance Impact
-------------------
-
-The schedular will incur small further overhead if a threads policy is set
-on the image or flavour. This overhead will be negligible compared to that
-implied by the enhancements to support NUMA policy and huge pages. It is
-anticipated that dedicated CPU guests will typically be used in conjunction
-with huge pages.
-
-Other deployer impact
----------------------
-
-The cloud administrator will gain the ability to define flavours which offer
-dedicated CPU resources. The administrator will have to place hosts into groups
-using aggregates such that the schedular can separate placement of guests with
-dedicated vs shared CPUs. Although not required by this design, it is expected
-that the administrator will commonly use the same host aggregates to group
-hosts for both CPU pinning and large page usage, since these concepts are
-complementary and expected to be used together. This will minimise the
-administrative burden of configuring host aggregates.
-
-Developer impact
-----------------
-
-It is expected that most hypervisors will have the ability to setup dedicated
-pCPUs for guests vs shared pCPUs. The flavour parameter is simple enough that
-any Nova driver would be able to support it.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  berrange
-
-Other contributors:
-  ndipanov
-
-Work Items
-----------
-
-* Enhance libvirt to support setup of strict CPU pinning for guests when the
-  appropriate policy is set in the flavour
-
-* Enhance the schedular to take account of threads policy when choosing
-  which host to place the guest on.
-
-Dependencies
-============
-
-* Virt driver guest NUMA node placement & topology
-
-   https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
-
-Testing
-=======
-
-It is unknown at this time if the gate hosts have sufficient pCPUs available
-to allow this feature to be effectively tested by tempest.
-
-Documentation Impact
-====================
-
-The new flavour parameter available to the cloud administrator needs to be
-documented along with recommendations about effective usage. The docs will
-also need to mention the compute host deployment pre-requisites such as the
-need to setup aggregates.
-
-References
-==========
-
-Current "big picture" research and design for the topic of CPU and memory
-resource utilization and placement. vCPU topology is a subset of this
-work
-
-* https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement
diff --git a/specs/juno/virt-driver-large-pages.rst b/specs/juno/virt-driver-large-pages.rst
deleted file mode 100644
index f549067..0000000
--- a/specs/juno/virt-driver-large-pages.rst
+++ /dev/null
@@ -1,340 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===============================================
-Virt driver large page allocation for guest RAM
-===============================================
-
-https://blueprints.launchpad.net/nova/+spec/virt-driver-large-pages
-
-This feature aims to improve the libvirt driver so that it can use large pages
-for backing the guest RAM allocation. This will improve the performance of
-guest workloads by increasing TLB cache efficiency. It will ensure that the
-guest has 100% dedicated RAM that will never be swapped out.
-
-Problem description
-===================
-
-Most modern virtualization hosts support a variety of memory page sizes. On
-x86 the smallest, used by the kernel by default, is 4kb, while large sizes
-include 2MB and 1GB. The CPU TLB cache has a limited size, so when there is a
-very large amount of RAM present and utilized, the cache efficiency can be
-fairly low which in turn increases memory access latency. By using larger page
-sizes, there are fewer entries needed in the TLB and thus its efficiency goes
-up.
-
-The use of huge pages for backing guests implies that the guest is running with
-a dedicated resource allocation. ie the concept of memory overcommit is no
-longer possible to provide. This is a tradeoff that cloud administrators may
-be willing to make to support workloads that require predictable memory access
-times, such as NFV.
-
-While large pages are better than small pages, it can't be assumed that the
-benefit increases as the page size increases. In some workloads, a 2 MB page
-size can be better overall than 1 GB page sizes. Also the choice of page size
-affects the granularity of guest RAM size. ie a 1.5 GB guest would not be able
-to use 1 GB pages since RAM is not a multiple of the page size.
-
-Although it is theoretically possible to reserve large pages on the fly, after
-a host has been booted for a period of time, physical memory will have become
-very fragmented. This means that even if the host has lots of free memory, it
-may be unable to find contiguous chunks required to provide large pages. This
-is a particular problem for 1 GB sized pages. To deal with this problem, it is
-usual practice to reserve all required large pages upfront at host boot time,
-by specifying a reservation count on the kernel command line of the host. This
-would be a one-time setup task done when deploying new compute node hosts.
-
-Proposed change
-===============
-
-The flavour extra specs will be enhanced to support a new parameter
-
-* hw:mem_page_size=large|small|any|2MB|1GB
-
-In absence of any page size setting in the flavour, the current behaviour of
-using the small, default, page size will continue. A setting of 'large' says
-to only use larger page sizes for guest RAM, eg either 2MB or 1GB on x86;
-'small' says to only use the small page sizes, eg 4k on x86, and is the
-default; 'any' means to leave policy upto the compute driver implementation to
-decide. When seeing 'any' the libvirt driver might try to find large pages,
-but fallback to small pages, but other drivers may choose alternate policies
-for 'any'. Finally an explicit page size can be set if the workload has very
-precise requirements for a specific large page size. It is expected that the
-common case would be to use page_size=large or page_size=any. The
-specification of explicit page sizes would be something that NFV workloads
-would require.
-
-The property defined for the flavour can also be set against the image, but
-the use of large pages would only be honoured if the flavour already had a
-policy or 'large' or 'any'. ie if the flavour said 'small', or a specific
-numeric page size, the image would not be permitted to override this to access
-other large page sizes. Such invalid override in the image would result in
-an exception being raised and the attempt to boot the instance resulting in
-an error. While ultimate validation is done in the virt driver, this can also
-be caught and reported at the at the API layer.
-
-If the flavor memory size is not a multiple of the specified huge page size
-this would be considered an error which would cause the instance to fail to
-boot. If the page size is 'large' or 'any', then the compute driver would
-obviously attempt to pick a page size which was a multiple of the RAM size
-rather than erroring. This is only likely to be a significant problem when
-when using 1 GB page sizes, which imply that ram size must be in 1 GB
-increments.
-
-The libvirt driver will be enhanced to honour this parameter when configuring
-the guest RAM allocation policy. This will effectively introduce the concept
-of a "dedicated memory" guest, since large pages must be 1-to-1 associated with
-guests - there's not facility to over commit by allowing one large page to be
-used with multiple guests or to swap large pages.
-
-The libvirt driver will be enhanced to report on large page availability per
-NUMA node, building on previously added NUMA topology reporting.
-
-The scheduler will be enhanced to take account of the page size setting on the
-flavour and pick hosts which have sufficient large pages available when
-scheduling the instance. Conversely if large pages are not requested, then the
-scheduler needs to avoid placing the instance on a host which has pre-reserved
-large pages. The enhancements for the scheduler will be done as part of the
-new filter that is implemented as part of the NUMA topology blueprint. This
-involves altering the logic done in that blueprint, so that instead of just
-looking at free memory in each NUMA node, it instead looks at the free page
-count for the desired page size.
-
-As illustrated later in this document each host will be reporting on all
-page sizes available and this information will be available to the scheduler.
-So when it interprets 'small', it will consider the smallest page size
-reported by the compute node. Conversely when intepreting 'large' it will
-consider any page size except the smallest one. This obviously implies that
-there is potential for 'large' and 'small' to have different meanings
-depending on the host being considered. For the use cases where this would
-be a problem, an explicit page size would be requested instead of using
-these symbolic named sizes. It will also have to consider whether the page
-size is a multiple of the flavor memory size. If the instance is using
-multiple NUMA nodes, it will have to consider whether the RAM in each
-guest node is a multiple of the page size, rather than the total memory
-size.
-
-Alternatives
-------------
-
-Recent Linux hosts have a concept of "transparent huge pages" where the kernel
-will opportunistically allocate large pages for guest VMs. The problem with
-this is that over time, the kernel's memory allocations get very fragmented
-making it increasingly hard to find contiguous blocks of RAM to use for large
-pages. This makes transparent large pages impractical for use with 1 GB page
-sizes. The opportunistic approach also means that users do not have any hard
-guarantee that their instance will be backed by large pages. This makes it an
-unusable approach for NFV workloads which require hard guarantees.
-
-Data model impact
------------------
-
-The previously added data in the host state structure for reporting NUMA
-topology would be enhanced to further include information on page size
-availability per node. So it would then look like
-
-::
-
-  hw_numa = {
-     nodes = [
-         {
-            id = 0
-            cpus = 0, 2, 4, 6
-            mem = {
-               total = 10737418240
-               free = 3221225472
-            },
-            mempages = {
-               4096 = {
-                  total = 262144
-                  free = 262144
-               }
-               2097152 = {
-                  total = 1024
-                  free = 1024
-               }
-               1073741824 = {
-                  total = 7
-                  free = 0
-               }
-            }
-            distances = [ 10, 20],
-         },
-         {
-            id = 1
-            cpus = 1, 3, 5, 7
-            mem = {
-               total = 10737418240
-               free = 5368709120
-            },
-            mempages = {
-               4096 = {
-                  total = 262144
-                  free = 262144
-               }
-               2097152 = {
-                  total = 1024
-                  free = 1024
-               }
-               1073741824 = {
-                  total = 7
-                  free = 2
-               }
-            }
-            distances = [ 20, 10],
-         }
-     ],
-  }
-
-The data provided to the extensible resource tracker would be similarly
-enhanced to include this page info in a flattened format, which can be
-efficiently queried based on the key name:
-
-* hw_numa_nodes=2
-* hw_numa_node0_cpus=4
-* hw_numa_node0_mem_total=10737418240
-* hw_numa_node0_mem_avail=3221225472
-* hw_numa_node0_mem_page_total_4=262144
-* hw_numa_node0_mem_page_avail_4=262144
-* hw_numa_node0_mem_page_total_2048=1024
-* hw_numa_node0_mem_page_avail_2048=1024
-* hw_numa_node0_mem_page_total_1048576=7
-* hw_numa_node0_mem_page_avail_1048576=0
-* hw_numa_node0_distance_node0=10
-* hw_numa_node0_distance_node1=20
-* hw_numa_node1_cpus=4
-* hw_numa_node1_mem_total=10737418240
-* hw_numa_node1_mem_avail=5368709120
-* hw_numa_node1_mem_page_total_4=262144
-* hw_numa_node1_mem_page_avail_4=262144
-* hw_numa_node1_mem_page_total_2048=1024
-* hw_numa_node1_mem_page_avail_2048=1024
-* hw_numa_node1_mem_page_total_1048576=7
-* hw_numa_node1_mem_page_avail_1048576=2
-* hw_numa_node1_distance_node0=20
-* hw_numa_node1_distance_node1=10
-
-REST API impact
----------------
-
-No impact.
-
-The existing APIs already support arbitrary data in the flavour extra specs.
-
-Security impact
----------------
-
-No impact.
-
-Notifications impact
---------------------
-
-No impact.
-
-The notifications system is not used by this change.
-
-Other end user impact
----------------------
-
-There are no changes that directly impact the end user, other than the fact
-that their guest should have more predictable memory access latency.
-
-Performance Impact
-------------------
-
-The scheduler will have more logic added to take into account large page
-availability per NUMA node when placing guests. Most of this impact will have
-already been incurred when initial NUMA support was added to the scheduler.
-This change is merely altering the NUMA support such that it considers the
-free large pages instead of overall RAM size.
-
-Other deployer impact
----------------------
-
-The cloud administrator will gain the ability to set large page policy on the
-flavours they configured. The administrator will also have to configure their
-compute hosts to reserve large pages at boot time, and place those hosts into a
-group using aggregates.
-
-It is possible that there might be a need to expose information on the page
-counts to host administrators via the Nova API. Such a need can be considered
-for followup work once the work refernced in this basic spec is completed
-
-Developer impact
-----------------
-
-If other hypervisors allow the control over large page usage, they could be
-enhanced to support the same flavour extra specs settings. If the hypervisor
-has self-determined control over large page usage, then it is valid to simply
-ignore this new flavour setting. ie do nothing.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  berrange
-
-Other contributors:
-  ndipanov
-
-Work Items
-----------
-
-* Enhance libvirt driver to report available large pages per NUMA node in the
-  host state data
-* Enhance libvirt driver to configure guests based on the flavour parameter
-  for page sizes
-* Add support to scheduler to place instances on hosts according to the
-  availability of required large pages
-
-Dependencies
-============
-
-* Virt driver guest NUMA node placement & topology. This blueprint is going
-  to be an extension of the work done in the compute driver and scheduler
-  for NUMA placement, since large pages must be allocated from matching
-  guest & host NUMA node to avoid cross-node memory access
-
-   https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
-
-* Libvirt / KVM need to be enhanced to allow Nova to indicate that large
-  pages should be allocated from specific NUMA nodes on the host. This is not
-  a blocker to supporting large pages in Nova, since it can use the more
-  general large page support in libvirt, however, the performance benefits
-  won't be fully realized until per-NUMA node large page allocation can be
-  done.
-
-* Extensible resource tracker
-
-  https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
-
-Testing
-=======
-
-Testing this in the gate would be difficult since the hosts which run the
-gate tests would have to be pre-configured with large pages allocated at
-initial OS boot time. This in turn would preclude running gate tests with
-guests that do not want to use large pages.
-
-Documentation Impact
-====================
-
-The new flavour parameter available to the cloud administrator needs to be
-documented along with recommendations about effective usage. The docs will
-also need to mention the compute host deployment pre-requisites such as the
-need to pre-allocate large pages at boot time and setup aggregates.
-
-References
-==========
-
-Current "big picture" research and design for the topic of CPU and memory
-resource utilization and placement. vCPU topology is a subset of this
-work
-
-* https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement
diff --git a/specs/juno/virt-driver-numa-placement.rst b/specs/juno/virt-driver-numa-placement.rst
deleted file mode 100644
index 6ba3239..0000000
--- a/specs/juno/virt-driver-numa-placement.rst
+++ /dev/null
@@ -1,361 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-================================================
-Virt driver guest NUMA node placement & topology
-================================================
-
-https://blueprints.launchpad.net/nova/+spec/virt-driver-numa-placement
-
-This feature aims to enhance the libvirt driver to be able to do intelligent
-NUMA node placement for guests. This will increase the effective utilization
-of compute resources and decrease latency by avoiding cross-node memory
-accesses by guests.
-
-Problem description
-===================
-
-The vast majority of hardware used for virtualization compute nodes will
-exhibit NUMA characteristics. When running workloads on NUMA hosts it is
-important that the CPUs executing the processes are on the same node as the
-memory used. This ensures that all memory accesses are local to the NUMA node
-and thus not consumed the very limited cross-node memory bandwidth, which adds
-latency to memory accesses. PCI devices are directly associated with specific
-NUMA nodes for the purposes of DMA, so when using PCI device assignment it is
-also desirable that the guest be placed on the same NUMA node as any PCI device
-that is assigned to it.
-
-The libvirt driver does not currently attempt any NUMA placement, the guests
-are free to float across any host pCPUs and their RAM is allocated from any
-NUMA node. This is very wasteful of compute resources and increases memory
-access latency which is harmful for NFV use cases.
-
-If the RAM/vCPUs associated with a flavour are larger than any single NUMA
-node, it is important to expose NUMA topology to the guest so that the OS in
-the guest can intelligently schedule workloads it runs. For this to work the
-guest NUMA nodes must be directly associated with host NUMA nodes.
-
-Some guest workloads have very demanding requirements for memory access
-latency and/or bandwidth, which exceed that which is available from a
-single NUMA node. For such workloads, it will be beneficial to spread
-the guest across multiple host NUMA nodes, even if the guest RAM/vCPUs
-could theoretically fit in a single NUMA node.
-
-Forward planning to maximise the choice of target hosts for use with live
-migration may also cause an administrator to prefer splitting a guest
-across multiple nodes, even if it could potentially fit in a single node
-on some hosts.
-
-For these two reasons it is desirable to be able to explicitly indicate
-how many NUMA nodes to setup in a guest, and to specify how much RAM or
-how many vCPUs to place in each node.
-
-Proposed change
-===============
-
-The libvirt driver will be enhanced so that it looks at the resources available
-in each NUMA node and decides which is best able to run the guest. When
-launching the guest, it will tell libvirt to confine the guest to the chosen
-NUMA node.
-
-The compute driver host stats data will be extended to include information
-about the NUMA topology of the host and the availability of resources in the
-nodes.
-
-The scheduler will be enhanced such that it can consider the availability of
-NUMA resources when choosing the host to schedule on. The algorithm that the
-scheduler uses to decide if the host can run will need to be closely matched,
-if not identical to, the algorithm used by the libvirt driver itself. This
-will involve the creation of a new scheduler filter to match the flavor/image
-config specification against the NUMA resource availability reported by the
-compute hosts.
-
-The flavour extra specs will support the specification of guest NUMA topology.
-This is important when the RAM / vCPU count associated with a flavour is larger
-than any single NUMA node in compute hosts, by making it possible to have guest
-instances that span NUMA nodes. The compute driver will ensure that guest NUMA
-nodes are directly mapped to host NUMA nodes. It is expected that the default
-setup would be to not list any NUMA properties and just let the compute host
-and scheduler apply a sensible default placement logic. These properties would
-only need to be set in the sub-set of scenarios which require more precise
-control over the NUMA topology / fit characteristics.
-
-* hw:numa_nodes=NN - numa of NUMA nodes to expose to the guest.
-* hw:numa_mempolicy=preferred|strict - memory allocation policy
-* hw:numa_cpus.0=<cpu-list> - mapping of vCPUS N-M to NUMA node 0
-* hw:numa_cpus.1=<cpu-list> - mapping of vCPUS N-M to NUMA node 1
-* hw:numa_mem.0=<ram-size> - mapping N GB of RAM to NUMA node 0
-* hw:numa_mem.1=<ram-size> - mapping N GB of RAM to NUMA node 1
-
-The most common case will be that the admin only sets 'hw:numa_nodes' and then
-the flavour vCPUs and RAM will be divided equally across the NUMA nodes.
-
-The 'hw:numa_mempolicy' option allows specification of whether it is mandatory
-for the instance's RAM allocations to come from the NUMA nodes to which it is
-bound, or whether the kernel is free to fallback to using an alternative node.
-If 'hw:numa_nodes' is specified, then 'hw:numa_mempolicy' is assumed to default
-to 'strict'. It is useful to change it to 'preferred' when the 'hw:numa_nodes'
-parameter is being set to '1' to force disable use of NUMA by image property
-overrides.
-
-It should only be required to use the 'hw:numa_cpu.N' and 'hw:numa_mem.N'
-settings if the guest NUMA nodes should have asymetrical allocation of CPUs
-and RAM. This is important for some NFV workloads, but in general these will
-be rarely used tunables. If the 'hw:numa_cpu' or 'hw:numa_mem' settings are
-provided and their values do not sum to the total vcpu count / memory size,
-this is considered to be a configuration error. An exception will be raised
-by the compute driver when attempting to boot the instance. As an enhancement
-it might be possible to validate some of the data at the API level to allow
-for earlier error reporting to the user. Such checking is not a functional
-prerequisite for this work though so such work can be done out-of-band to
-the main development effort.
-
-When scheduling, if only the hw:numa_nodes=NNN property is set the scheduler
-will synthesize hw:numa_cpus.NN and hw:numa_mem.NN properties such that the
-flavour allocation is equally spread across the desired number of NUMA nodes.
-It will then look consider the available NUMA resources on hosts to find one
-that exactly matches the requirements of the guest. So, given an example
-config:
-
-* vcpus=8
-* mem=4
-* hw:numa_nodes=2 - numa of NUMA nodes to expose to the guest.
-* hw:numa_cpus.0=0,1,2,3,4,5
-* hw:numa_cpus.1=6,7
-* hw:numa_mem.0=3
-* hw:numa_mem.1=1
-
-The scheduler will look for a host with 2 NUMA nodes with the ability to run
-6 CPUs + 3 GB of RAM on one node, and 2 CPUS + 1 GB of RAM on another node.
-If a host has a single NUMA node with capability to run 8 CPUs and 4 GB of
-RAM it will not be considered a valid match. The same logic will be applied
-in the scheduler regardless of the hw:numa_mempolicy option setting.
-
-All of the properties described against the flavour could also be set against
-the image, with the leading ':' replaced by '_', as is normal for image
-property naming conventions:
-
-* hw_numa_nodes=NN - numa of NUMA nodes to expose to the guest.
-* hw_numa_mempolicy=strict|prefered - memory allocation policy
-* hw_numa_cpus.0=<cpu-list> - mapping of vCPUS N-M to NUMA node 0
-* hw_numa_cpus.1=<cpu-list> - mapping of vCPUS N-M to NUMA node 1
-* hw_numa_mem.0=<ram-size> - mapping N GB of RAM to NUMA node 0
-* hw_numa_mem.1=<ram-size> - mapping N GB of RAM to NUMA node 1
-
-This is useful if the application in the image requires very specific NUMA
-topology characteristics, which is expected to be used frequently with NFV
-images. The properties can only be set against the image, however, if they
-are not already set against the flavor. So for example, if the flavor sets
-'hw:numa_nodes=2' but does not set any 'hw:numa_cpus' / 'hw:numa_mem' values
-then the image can optionally set those. If the flavour has, however, set a
-specific property the image cannot override that. This allows the flavor
-admin to strictly lock down what is permitted if desired. They can force a
-non-NUMA topology by setting hw:numa_nodes=1 against the flavor.
-
-Alternatives
-------------
-
-Libvirt supports integration with a daemon called numad. This daemon can be
-given a RAM size + vCPU count and tells libvirt what NUMA node to place a
-guest on. It is also capable of shifting running guests between NUMA nodes to
-rebalance utilization. This is insufficient for Nova since it needs to have
-intelligence in the scheduler to pick hosts. The compute drivers then needs to
-be able to use the same logic when actually launching the guests. The numad
-system is not portable to other compute hypervisors. It does not deal with the
-problem of placing guests which span across NUMA nodes. Finally, it does not
-address the needs for NFV workloads which require guaranteed NUMA topology
-and placement policies, not merely dynamic best effort.
-
-Another alternative is to just do nothing, as we do today, and rely on the
-Linux kernel scheduler being enhanced to automatically place guests on
-appropriate NUMA nodes and rebalance them on demand. This shares most of the
-problems seen with using numad.
-
-Data model impact
------------------
-
-No impact.
-
-The reporting of NUMA topology will be integrated in the existing data
-structure used for host state reporting. This already supports arbitrary
-fields so no data model changes are anticipated for this part. This would
-appear as structured data
-
-::
-
-  hw_numa = {
-     nodes = [
-         {
-            id = 0
-            cpus = 0, 2, 4, 6
-            mem = {
-               total = 10737418240
-               free = 3221225472
-            },
-            distances = [ 10, 20],
-         },
-         {
-            id = 1
-            cpus = 1, 3, 5, 7
-            mem = {
-               total = 10737418240
-               free = 5368709120
-            },
-            distances = [ 20, 10],
-         }
-     ],
-  }
-
-To enable more efficient scheduling though, it would be desirable to also map
-NUMA topology into the extensible resource tracker schema. This would imply
-representing the hierarchical data above in a flattened format as a series of
-key, value pairs.
-
-* hw_numa_nodes=2
-* hw_numa_node0_cpus=4
-* hw_numa_node0_memtotal=10737418240
-* hw_numa_node0_memavail=3221225472
-* hw_numa_node0_distance_node0=10
-* hw_numa_node0_distance_node1=20
-* hw_numa_node1_cpus=4
-* hw_numa_node1_memtotal=10737418240
-* hw_numa_node1_memavail=5368709120
-* hw_numa_node1_distance_node0=20
-* hw_numa_node1_distance_node1=10
-
-
-REST API impact
----------------
-
-No impact.
-
-The API for host state reporting already supports arbitrary data fields, so
-no change is anticipated from that POV. No new API calls will be required.
-
-Security impact
----------------
-
-No impact.
-
-There are no new APIs involved which would imply a new security risk.
-
-Notifications impact
---------------------
-
-No impact.
-
-There is no need for any use fo the notification system.
-
-Other end user impact
----------------------
-
-Depending on the flavour chosen, the guest OS may see NUMA nodes backing its
-RAM allocation.
-
-There is no end user interaction in setting up NUMA policies of usage.
-
-The cloud administrator will gain the ability to set policies on flavours.
-
-Performance Impact
-------------------
-
-The new scheduler features will imply increased performance overhead when
-determining whether a host is able to fit the memory and vCPU needs of the
-flavour. ie the current logic which just checks the vCPU count and RAM
-requirement against the host free memory will need to take account of the
-availability of resources in specific NUMA nodes.
-
-Other deployer impact
----------------------
-
-If the deployment has flavours whose RAM + vCPU allocations are larger than
-the size of the NUMA nodes in the compute hosts, the cloud administrator
-should strongly consider defining guest NUMA nodes in the flavour. This will
-enable the compute hosts to have better NUMA utilization and improve perf of
-the guest OS.
-
-Developer impact
-----------------
-
-The new flavour attributes could be used by any full machine virtualization
-hypervisor, however, it is not mandatory that they do so.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  berrange
-
-Other contributors:
-  ndipanov
-
-Work Items
-----------
-
-* Define a schema for reporting NUMA node resources and availability
-  for the host state API / extensible resource tracker
-* Enhance libvirt driver to report NUMA node resources & availability
-* Enhance libvirt driver to support setup of guest NUMA nodes.
-* Enhance libvirt driver to look at NUMA node availability when launching
-  guest instances and pin all guests to best NUMA node
-* Add support to schedular for picking hosts based on the NUMA availability
-  instead of simply considering the total RAM/vCPU availability.
-
-Dependencies
-============
-
-* The driver vCPU topology feature is a pre-requisite
-
-    https://blueprints.launchpad.net/nova/+spec/virt-driver-vcpu-topology
-
-* Supporting guest NUMA nodes will require completion of work in QEMU and
-  libvirt, to enable guest NUMA nodes to be pinned to specific host NUMA
-  nodes. In absence of libvirt/QEMU support, guest NUMA nodes can still be
-  used but it would not have any performance benefit, and may even hurt
-  performance.
-
-    https://www.redhat.com/archives/libvir-list/2014-June/msg00201.html
-
-* Extensible resource tracker
-
-  https://blueprints.launchpad.net/nova/+spec/extensible-resource-tracking
-
-Testing
-=======
-
-There are various discrete parts of the work that can be tested in isolation
-of each other, fairly effectively using unit tests.
-
-The main area where unit tests might not be sufficient is the scheduler
-integration, where performance/scalability would be a concern. Testing the
-scalability of the scheduler in tempest though is not practical, since the
-issues would only become apparent with many compute hosts and many guests.
-ie a scale beyond that which tempest sets up.
-
-Documentation Impact
-====================
-
-The cloud administrator docs need to describe the new flavour parameters
-and make recommendations on how to effectively use them.
-
-The end user needs to be made aware of the fact that some flavours will cause
-the guest OS to see NUMA topology.
-
-References
-==========
-
-Current "big picture" research and design for the topic of CPU and memory
-resource utilization and placement. vCPU topology is a subset of this
-work
-
-* https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement
-
-OpenStack NFV team:
-
-* https://wiki.openstack.org/wiki/Teams/NFV
diff --git a/specs/juno/virt-driver-vcpu-topology.rst b/specs/juno/virt-driver-vcpu-topology.rst
deleted file mode 100644
index f1f4110..0000000
--- a/specs/juno/virt-driver-vcpu-topology.rst
+++ /dev/null
@@ -1,273 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=============================================
-Virt driver guest vCPU topology configuration
-=============================================
-
-https://blueprints.launchpad.net/nova/+spec/virt-driver-vcpu-topology
-
-This feature aims to give users and administrators the ability to control
-the vCPU topology exposed to guests. This enables them to avoid hitting
-limitations on vCPU topologies that OS vendors place on their products.
-
-Problem description
-===================
-
-When a guest is given multiple vCPUs, these are typically exposed in the
-hardware model as discrete sockets. Some operating system vendors will
-place artificial limits on the topologies that their product will support.
-So for example, a Windows guest may support 8 vCPUs only if it is exposed
-as 2 sockets with 4 cores each. If the vCPUs were exposed as 8 sockets
-with 1 core each, some of the vCPUs will be inaccessible to the guest.
-It is thus desirable to be able to control the mixture of cores and
-sockets exposed to the guest. The cloud administrator needs to be able
-to define topologies for flavours, to override the hypervisor defaults,
-such that commonly used OS' will not encounter their socket count limits.
-The end user also needs to be able to express preferences for topologies
-to use with their images.
-
-While the choice of sockets vs cores does not have a significant impact
-on performance, if a guest is given threads or is running on host OS
-CPUs which are thread siblings, this can have a notable performance impact.
-It only makes sense to expose a value of threads > 1 to a guest if all the
-guest vCPUs are strictly pinned to host pCPUs and some of the host pCPUs
-are thread siblings. While this blueprint will describe how to set the
-threads count, it will only make sense to set this to a value > 1 once
-the CPU pinning feature is integrated in Nova.
-
-If the flavour admin wishes to define flavours which avoid scheduling on
-hosts which have pCPUs with threads > 1, then can use scheduler aggregates
-to setup host groups.
-
-Proposed change
-===============
-
-The proposal is to add support for configuration of aspects of vCPU topology
-at multiple levels.
-
-At the flavour there will be the ability to define default parameters for the
-vCPU topology using flavour extra specs
-
-* hw:cpu_sockets=NN - preferred number of sockets to expose to the guest
-* hw:cpu_cores=NN - preferred number of cores to expose to the guest
-* hw:cpu_threads=NN - preferred number of threads to expose to the guest
-* hw:cpu_max_sockets=NN - maximum number of sockets to expose to the guest
-* hw:cpu_max_cores=NN - maximum number of cores to expose to the guest
-* hw:cpu_max_threads=NN - maximum number of threads to expose to the guest
-
-It is not expected that administrators will set all these parameters against
-every flavour. The simplest expected use case will be for the cloud admin to
-set "hw:cpu_max_sockets=2" to prevent the flavour exceeding 2 sockets. The
-virtualization driver will calculate the exact number of cores/sockets/threads
-based on the flavour vCPU count and this maximum sockets constraint.
-
-For larger vCPU counts there may be many possible configurations, so the
-"hw:cpu_sockets", "hw:cpu_cores", "hw:cpu_threads" parameters enable the
-cloud administrator to express their preferred choice from the large set.
-
-The "hw:max_cores" parameter allows the cloud administrator to place an upper
-limit on the number of cores used, which can be useful to ensure a socket
-count greater than 1 and thus enable a VM to be spread across NUMA nodes.
-
-The "hw:max_sockets", "hw:max_cores" & "hw:max_threads" settings allow the
-cloud admin to set mandatory upper limits on the permitted configurations
-that the user can override with properties against the image.
-
-At the image level the exact same set of parameters will be permitted,
-with the exception that image properties will use underscores throughout
-instead of an initial colon.
-
-* hw_cpu_sockets=NN - preferred number of sockets to expose to the guest
-* hw_cpu_cores=NN - preferred number of cores to expose to the guest
-* hw_cpu_threads=NN - preferred number of threads to expose to the guest
-* hw_cpu_max_sockets=NN - maximum number of sockets to expose to the guest
-* hw_cpu_max_cores=NN - maximum number of cores to expose to the guest
-* hw_cpu_max_threads=NN - maximum number of threads to expose to the guest
-
-If the user sets "hw_cpu_max_sockets", "hw_cpu_max_cores", or
-"hw_cpu_max_threads", these must be strictly lower than the values
-already set against the flavour. The purpose of this is to allow the
-user to further restrict the range of possible topologies that the compute
-host will consider using for the instance.
-
-The "hw_cpu_sockets", "hw_cpu_cores" & "hw_cpu_threads" values
-against the image may not exceed the "hw_cpu_max_sockets", "hw_cpu_max_cores"
-& "hw_cpu_max_threads" values set against the flavour or image. If the
-upper bounds are exceeded, this will be considered a configuration error
-and the instance will go into an error state and not boot.
-
-If there are multiple possible topology solutions implied by the set of
-parameters defined against the flavour or image, then the hypervisor will
-prefer the solution that uses a greater number of sockets. This preference
-will likely be further refined when integrating support for NUMA placement
-in a later blueprint.
-
-If the user wants their settings to be used unchanged by the compute
-host they should set "hw_cpu_sockets" == "hw_cpu_max_sockets",
-"hw_cpu_cores" == "hw_cpu_max_cores", and "hw_cpu_threads" ==
-"hw_cpu_max_threads" on the image. This will force use of the exact
-specified topology.
-
-Note that there is no requirement in this design or implementation for
-the compute host topologies to match what is being exposed to the guest.
-ie this will allow a flavour to be given sockets=2,cores=2 and still
-be used to launch instances on a host with sockets=16,cores=1. If the
-admin wishes to optionally control this, they will be able todo so by
-setting up host aggregates.
-
-The intent is to implement this for the libvirt driver, targeting QEMU /
-KVM hypervisors. Conceptually it is applicable to all other full machine
-virtualization hypervisors such as Xen and VMWare.
-
-Alternatives
-------------
-
-The virtualization driver could hard code a different default topology, so
-that all guest always use
-
-   cores==2, sockets==nvcpus/cores
-
-instead of
-
-   cores==1, sockets==nvcpus
-
-While this would address the immediate need of current Windows OS', this is
-not likely to be sufficiently flexible for the longer term, as it forces all
-OS into using cores, even if they don't have any similar licensing
-restrictions. The over-use of cores will limit the ability to do an effective
-job at NUMA placement, so it is desirable to use cores as little as possible.
-
-The settings could be defined exclusively against the images, and not make
-any use of flavour extra specs. This is undesirable because to have best
-NUMA utilization, the cloud administrator will need to be able to constrain
-what topologies the user is allowed to use. The administrator would also
-like to have the ability to set up define behaviour so that guest can get
-a specific topology without requiring every single image uploaded to glance
-to be tagged with the same repeated set of properties.
-
-A more fine grained configuration option would be to allow the specification
-of the core and thread count for each separate socket. This would allow for
-asymmetrical topologies eg
-
-  socket0:cores=2,threads=2,socket1:cores=4,threads=1
-
-It is noted, however, that at time of writing, no virtualization technology
-provides any way to configure such asymmetrical topologies. Thus Nova is
-better served by ignoring this purely theoretical possibility and keeping
-its syntax simpler to match real-world capabilities that already exist.
-
-Data model impact
------------------
-
-No impact.
-
-The new properties will use the existing flavour extra specs and image
-property storage models.
-
-REST API impact
----------------
-
-No impact.
-
-The new properties will use the existing flavour extra specs and image
-property API facilities.
-
-Security impact
----------------
-
-The choice of sockets vs cores can have an impact on host resource utilization
-when NUMA is involved, since over use of cores will prevent a guest being
-split across multiple NUMA nodes. This feature addresses this by allowing the
-flavour administrator to define hard caps, and ensuring the flavour will
-always take priority over the image settings.
-
-Notifications impact
---------------------
-
-No impact.
-
-There is no need for this feature to integrate with notifications.
-
-Other end user impact
----------------------
-
-The user will gain the ability to control aspects of the vCPU topology used
-by their guest. They will achieve this by setting image properties in glance.
-
-Performance Impact
-------------------
-
-The cores vs sockets vs threads decision does not involve any scheduler
-interaction, since this design is not attempting to match host topology
-to guest topology. A later blueprint on CPU pinning will make it possible
-todo such host to guest topology matching, and its performance impact
-will be considered there.
-
-Other deployer impact
----------------------
-
-The flavour extra specs will gain new parameters in extra specs which a
-cloud administrator can choose to use. If none are set then the default
-behaviour is unchanged from previous releases.
-
-Developer impact
-----------------
-
-The initial implementation will be done for libvirt with QEMU/KVM. It should
-be possible to add support for using the cores/sockets/threads parameters in
-the XenAPI and VMWare drivers.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-
-  berrange
-
-Work Items
-----------
-
-* Provide helper methods against the computer driver base class for
-  calculating valid CPU topology solutions for the given hw_cpu_* parameters.
-* Add Libvirt driver support for choosing a CPU topology solution based on
-  the given hw_cpu_* parameters.
-
-Dependencies
-============
-
-No external dependencies
-
-Testing
-=======
-
-No tempest changes.
-
-The mechanisms for the cloud administrator and end user to set parameters
-against the flavour and/or image are already well tested. The new
-functionality focuses on interpreting the parameters and setting corresponding
-libvirt XML parameters. This is something that is effectively covered by the
-unit testing framework.
-
-Documentation Impact
-====================
-
-The new flavour extra specs and image properties will need to be documented.
-Guidance should be given to cloud administrators on how to make most
-effective use of the new features. Guidance should be given to the end user
-on how to use the new features to address their use cases.
-
-References
-==========
-
-Current "big picture" research and design for the topic of CPU and memory
-resource utilization and placement. vCPU topology is a subset of this
-work
-
-* https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement
diff --git a/specs/juno/virt-objects-juno.rst b/specs/juno/virt-objects-juno.rst
deleted file mode 100644
index dcdfe18..0000000
--- a/specs/juno/virt-objects-juno.rst
+++ /dev/null
@@ -1,174 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-==========================================
-Virt Driver Objects Support (Juno Work)
-==========================================
-
-https://blueprints.launchpad.net/nova/+spec/virt-objects-juno
-
-This blueprint represents the remaining work to be done in Juno around
-moving the virt drivers to using objects instead of raw conductor
-methods. This is important because objects provide versioning of the
-actual data, which supports our upgrade goals.
-
-Problem description
-===================
-
-Nova virt drivers still send and receive unversioned bundles of data
-using conductor methods, which is problematic during an upgrade where
-the format of the data has changed across releases.
-
-Proposed change
-===============
-
-Migrate uses of raw conductor methods in the virt drivers to
-objects. For example, consider this::
-
-  instance = conductor.instance_get_by_uuid(context, uuid)
-  conductor.instance_update(context, instance['uuid'],
-                            host='foo')
-
-would become::
-
-  instance = instance_obj.Instance.get_by_uuid(context, uuid)
-  instance.host = 'foo'
-  instance.save()
-
-Using the objects mechanism allows older code to interact with newer
-code, backleveling the format of the instance object as necessary.
-
-Alternatives
-------------
-
-This is the accepted direction of the project to solve this
-problem. However, alternatives would be:
-
-1. Don't solve the problem and continue using unversioned data
-2. Attempt to enforce version bumps of individual methods when any
-   data (including nested downstream data) has changed
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-In general, conversion of code to use objects does not affect
-notifications. However, at times, emission of notifications is
-embedded into an object method to achieve higher consistency about
-when and how the notifications are sent. No such changes are
-antitipated in this work, but it's always a possibility.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-None.
-
-Other deployer impact
----------------------
-
-Moving to objects enhances the ability for deployers to incrementally
-roll out new code. It is, however, largely transparent for them.
-
-Developer impact
-----------------
-
-This is normal refactoring, so the impact is minimal. In general,
-objects-based code is easier to work with, so long-term it is a win
-for the developers.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  danms
-
-Work Items
-----------
-
-The following virt driver methods still need attention:
-
-* attach_volume
-* check_can_live_migrate_destination
-* check_can_live_migrate_source
-* check_instance_shared_storage_local
-* cleanup
-* default_device_names_for_instance
-* default_root_device_name
-* destroy
-* detach_volume
-* dhcp_options_for_instance
-* ensure_filtering_rules_for_instance
-* get_diagnostics
-* get_info
-* get_volume_connector
-* inject_file
-* inject_network_info
-* live_migration
-* macs_for_instance
-* post_live_migration
-* pre_live_migration
-* refresh_instance_security_rules
-* reset_network
-* rollback_live_migration_at_destination
-* unfilter_instance
-* unplug_vifs
-
-
-Dependencies
-============
-
-There is a cross-dependency between this blueprint and the following:
-
-  https://blueprints.launchpad.net/nova/+spec/compute-manager-objects-juno
-
-At times, a virt driver will need to be passed an object by the
-compute manager, and thus finishing the conversion of a virt driver
-method requires the calling compute manager method to be converted as
-well.
-
-
-Testing
-=======
-
-In general, unit tests require minimal change when this happens,
-depending on how the tests are structured. Ideally, they are already
-mocking out database calls, which means the change to objects is a
-transparent one. In reality, this usually means minor tweaking to the
-tests to return whole data models, etc.
-
-Documentation Impact
-====================
-
-None.
-
-References
-==========
-
-* https://blueprints.launchpad.net/nova/+spec/virt-objects
-* https://blueprints.launchpad.net/nova/+spec/compute-manager-objects
-* https://blueprints.launchpad.net/nova/+spec/unified-object-model
diff --git a/specs/juno/vmware-driver-ova-support.rst b/specs/juno/vmware-driver-ova-support.rst
deleted file mode 100644
index f152c44..0000000
--- a/specs/juno/vmware-driver-ova-support.rst
+++ /dev/null
@@ -1,200 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-========================================
-VMware: Support spawning from OVA images
-========================================
-
-https://blueprints.launchpad.net/nova/+spec/vmware-driver-ova-support
-
-This blueprint proposes to add support of spawning an instance from the disk
-embedded in an OVA (Open Virtualization Application) glance image.
-
-
-Problem description
-===================
-
-Given that the best practice for obtaining a compact, portable template of a
-virtual machine in the vSphere platform is to export it into an OVF folder or
-an OVA file (http://www.dmtf.org/standards/ovf), a frequent customer ask is to
-be able to deploy them in OpenStack as Glance images and spawn new instances
-with them.
-
-In addition, OVF/OVA contains virtual disks that are converted to the
-streamOptimized format, and streamOptimized disks are the only disk type
-deployable on vSAN datastores (see
-https://blueprints.launchpad.net/nova/+spec/vmware-vsan-support)
-Since exporting a virtual machine to OVA/OVF remains one of the most convenient
-means to obtain streamOptimized disks, providing support for spawning using OVA
-glance images will streamline the process of providing images for vSAN use.
-
-
-Proposed change
-===============
-
-An OVF contains additional information about the virtual machine beyond its
-disks - it has an .ovf XML descriptor file that describes the virtual machine
-configuration (memory, CPU settings, virtual devices, etc).  But for the
-purpose of this blueprint, it is treated essential as a container of a root
-disk targetted for the spawn process.
-
-Note: An OVA is essentially a tarball of an OVF bundle.  Given the current
-image-as-a-single-file nature of glance images, it is more straightforward to
-support the uploading/download of OVA as a Glance image.
-
-The blueprint propose to support spawning of an image of container format 'ova'
-and disk format 'vmdk'. The driver expects the image to be an OVA tar bundle.
-
-While much of the information in the XML descriptor file could prove useful in
-the proper configuration of the spawned virtual machine in the future, the
-implementation of this blueprint will only perform minimal processing of the
-XML file solely for the purpose of obtaining the right disk file to use for
-spawn as well as type of the virtual disk adapter that the disk should be
-attached to by default. The disk adapter type used will continue to be
-overridable by specifying the "vmware_adaptertype" property in the spawn
-operation.
-
-
-Alternatives
-------------
-
-* When implemented, the vmware-vsan-support blueprint will allow spawning of
-  streamOptimized disk. An alternative is to force all users to extract the
-  streamOptimized disk from any OVA/OVF they intend to deploy in OpenStack and
-  have the compute driver only support spawning of a streamOptimized disk
-  image. This that puts unnecesary burden on the user.
-
-* Use the Task framework under proposal in Glance to provide on-the-fly
-  conversion of a supplied OVF/OVA into some other appropriate forms. This is
-  closely related to the previous alternative, as it may provide a more
-  streamlined workflow in glance to degenerate an incoming OVF into a single
-  streamOptimized disk.
-
-* Add support for OVF folder as the portable vSphere VM image. Since an OVF is
-  a folder with multiple files, it does not work well with existing the glance
-  model.
-
-* There are other proposals that involves using images that references data in
-  the hypervisor's datastore, or storing images directly on the datastore.
-  These are welcome optimizations that will reduce the amount of glance<->nova
-  nova transfers, but they do not address the issue of providing portable
-  image data that can be deployed in other vCenter installations.
-
-* Continue to force customers to upload images using the flat and sparse disk
-  variants. Because there is no straightforward way of obtaining disk images of
-  these type while still adopting the best practice of exporting virtual
-  machines first, this leads a separate, lengthier and more error-prone
-  workflow for preparing images for OpenStack use.
-
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-OVA and streamOptimized disks are more space efficient and streamable, this
-means less storage use in glance and faster first-time deployment times (as
-compared to a flat or sparse disk image).
-
-Other deployer impact
----------------------
-
-This change will allow deployment of existing libraries of exported OVA images,
-with little or no additional transformations. Existing image using flat/sparse
-disk types may be deprecated/deleted in favor of OVA (or standalone
-streamOptimized disks).
-
-Developer impact
-----------------
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  vui
-
-Other contributors:
-  arnaudleg
-
-Work Items
-----------
-
-* Download OVA, process embedded .ovf descriptor file for the path to the
-  root disk in the OVA, and spawn using data from said disk.
-
-Dependencies
-============
-
-* https://blueprints.launchpad.net/nova/+spec/use-oslo-vmware. The oslo.vmware
-  library provides functionality not available in the current vmware nova
-  driver that is required by this blueprint.
-
-* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor. Work
-  related to this blueprint will likely cause non-trivial changes to the
-  patches for this blueprint since several of them involve
-  the spawn operation.
-
-* https://blueprints.launchpad.net/nova/+spec/vmware-vsan-support. This work
-  introduces support for streamOptimized images, a prerequisite for being able
-  to use OVA as images.
-
-Testing
-=======
-
-Since Tempest in general does not support driver-specific tests, the proposal
-is to update the MineSweeper CI
-(https://wiki.openstack.org/wiki/NovaVMware/Minesweeper) with additional tests
-to verify spawning of instances using OVA images uploaded to glance with the
-'ova' container format.
-
-
-Documentation Impact
-====================
-
-In addition, new information in the vmware driver section of the Nova
-documentation will have to be added to document:
-
-* The parameters to use when uploading an OVA image.
-* The scope of the information contained in the OVA that is used in the spawn
-  process (essentially information pertaining to obtaining the root disk and
-  not much else)
-
-References
-==========
-
-* http://www.dmtf.org/standards/ovf
-* https://blueprints.launchpad.net/nova/+spec/use-oslo-vmware
-* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
-* https://blueprints.launchpad.net/nova/+spec/vmware-vsan-support
-* https://wiki.openstack.org/wiki/NovaVMware/Minesweeper
-* https://bugs.launchpad.net/glance/+bug/1286375
diff --git a/specs/juno/vmware-ephemeral-disk-support.rst b/specs/juno/vmware-ephemeral-disk-support.rst
deleted file mode 100644
index 8601616..0000000
--- a/specs/juno/vmware-ephemeral-disk-support.rst
+++ /dev/null
@@ -1,117 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=============================
-VMware Ephemeral Disk Support
-=============================
-
-https://blueprints.launchpad.net/nova/+spec/improve-vmware-disk-usage
-
-The blueprint adds support for support ephemeral disks to the VMware driver.
-
-Problem description
-===================
-
-The VMware driver does not support ephemeral disks.
-
-Proposed change
-===============
-
-The change will add ephemeral disk support to the VMware driver. The commit
-acec2579b796d101f732916bfab557a66cebe512 added in a method create_virtual_disk.
-This method will be used to create the ephemeral disk for the instance.
-
-The method will create an ephemeral disk for the instance on the datastore.
-This will be done according to the size defined in the instance flavor.
-
-Alternatives
-------------
-
-* Do not implement the feature.
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-* Users will be able to use ephemeral disks for the vCenter driver.
-
-Performance Impact
-------------------
-
-A modest increase in network traffic will slow down spawn operations as we
-create the ephemeral disk, size it, and place it for mounting in the vSphere
-virtual machine.
-
-Other deployer impact
----------------------
-
-None.
-
-Developer impact
-----------------
-
-None.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-This work was completed during IceHouse-1 and merely needs to be ported to
-the Juno release.
-
-Primary assignee:
-  tjones
-  heut2008
-  garyk
-
-Work Items
-----------
-
-* refactor and port https://review.openstack.org/#/c/51793/ for Juno
-
-Dependencies
-============
-
-blueprint vmware-spawn-refactor
-
-Testing
-=======
-
-* Minesweeper tests involving ephemeral disks will be turned on or written
-
-
-Documentation Impact
-====================
-
-After this blueprint the vmware driver will support ephemeral disks. This will
-need some additional documentation and changes to supported feature lists.
-
-References
-==========
-
-None
diff --git a/specs/juno/vmware-hot-plug.rst b/specs/juno/vmware-hot-plug.rst
deleted file mode 100644
index 203d1ac..0000000
--- a/specs/juno/vmware-hot-plug.rst
+++ /dev/null
@@ -1,111 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===============================
-VMware: support for vif hotplug
-===============================
-
-https://blueprints.launchpad.net/nova/+spec/vmware-hot-plug
-
-Support for hotpluging virtual network cards into instances.
-
-Problem description
-===================
-
-Support for hotpluging virtual network cards into instances has already
-been implemented in the libvirt driver:
-https://blueprints.launchpad.net/nova/+spec/network-adapter-hotplug
-
-The plan is to add the same support into the VMware driver.
-
-Proposed change
-===============
-
-Implement the methods attach_interface and detach_interface in the VMware
-driver.
-
-Alternatives
-------------
-
-None
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-A user will now be able to add or remove interfaces from an instance that is
-run by the VMware driver. The new nic will be added ore removed when the action
-takes place and does not require rebooting the guest.
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-Feature parity.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-
-Primary assignee:
-  Gary Kotton<gkotton@vmware.com>
-
-Work Items
-----------
-
-Code was posted in Icehouse - https://review.openstack.org/#/c/59365/
-
-Dependencies
-============
-
-Common VIF parameters were added - https://review.openstack.org/#/c/72292/
-
-Testing
-=======
-
-Unit tests and 3rd party testing. Note that the feature is only supported with
-Neutron at the moment.
-
-Documentation Impact
-====================
-
-Remove limitation that this is only supported with libvirt.
-
-References
-==========
-
-None
diff --git a/specs/juno/vmware-spawn-refactor.rst b/specs/juno/vmware-spawn-refactor.rst
deleted file mode 100644
index 37d2f74..0000000
--- a/specs/juno/vmware-spawn-refactor.rst
+++ /dev/null
@@ -1,175 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=====================
-VMware Spawn Refactor
-=====================
-
-https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
-
-A structured refactor of the VMware driver spawn utility code to create a more
-cohesive and coherent whole.
-
-
-Problem description
-===================
-
-The VMware driver's spawn utility method is over 500 lines of code and very
-difficult to follow. It features redundant logic in several places as well as
-a general lack of cohesive constructs for a programmer to follow. Tests of the
-spawn method involve complicated test frameworks that require a developer or
-reviewer to hold important context between different seemingly unrelated
-modules in their heads. While test coverage is actually quite good on the
-spawn method, it can be very difficult to comprehend how a test functions and
-comprehending this complexity slows reviews.
-
-* create a spawn method that composes utility methods
-
-* improve readability
-
-* provide encapsulation
-
-* separate model code from action code for easier maintenance
-
-* make tests more understandable to reviewers and test coverage easier to see
-
-
-Proposed change
-===============
-
-* Extract inner methods and create reusable and testable methods
-
-  * allow for simple mocking in tests to easily cover all paths
-
-  * create easier to follow test cases with shallower call depths
-
-* Consolidate vSphere image properties for easier use and testing
-
-  * NOTE: for the scope of this blueprint we examine only existing configs
-
-  * include checks for valid values for use in vSphere API before transmiting
-    to vSphere where possible. Pre-checking values will make it easier to
-    diagnose a driver fault.
-
-* Identify and extract additional utilities and methods hidden in spawn
-
-  * large sections of spawn are repeated in other utilities (stop that)
-
-  * identify image actions and create utilities for those
-
-Alternatives
-------------
-
-* continue to add to the existing method
-
-* expand fake.py into a full blown vCenter simulator
-
-* only change code as it pertains to new features or bugs
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None. This is a zero new feature blueprint.
-
-Security impact
----------------
-
-None.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-
-Performance Impact
-------------------
-
-None or negligible. Some early work has determined that there are multiple
-network round trips to glance that do not need to occur, but performance
-changes will be an expected side-effect of the refactoring work.
-
-Other deployer impact
----------------------
-
-None.
-
-Developer impact
-----------------
-
-- Sanity preservation: consolidation and refactoring of driver logic will make
-  an easier to follow driver that will make addition of new features easier.
-
-- Simplified testing, smaller units of code means more granular tests and
-  easier to follow test structure.
-
-- Introduction of better practice, this code will serve as a positive example
-  for future contributions.
-
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  hartsock
-
-Other contributors:
-  vui
-  rhsu
-  tjones-i
-  garyk
-  maithem
-
-Work Items
-----------
-
-* extract inner methods from spawn
-
-* consolidate VMware specific image configurations
-
-  * identify parameters set in image metadata and formalize them
-
-  * identify values that control current behavior and isolate them
-
-* refactor image file manipulation into a set of re-usable utilities
-
-
-Dependencies
-============
-
-None.
-
-
-Testing
-=======
-
-Standard Minesweeper testing should reveal if this refactor has not regressed
-any features and will cover all cases this code will refactor.
-
-
-Documentation Impact
-====================
-
-None. Internal developer documentation will be greatly improved.
-
-
-References
-==========
-
-* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
\ No newline at end of file
diff --git a/specs/juno/vmware-spbm-support.rst b/specs/juno/vmware-spbm-support.rst
deleted file mode 100644
index e9f5030..0000000
--- a/specs/juno/vmware-spbm-support.rst
+++ /dev/null
@@ -1,213 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-======================================
-Storage Policy Based Management (SPBM)
-======================================
-
-https://blueprints.launchpad.net/nova/+spec/vmware-spbm-support
-
-The feature will enable an OpenStack environment to take advantage of
-backend storage policies to provide differential services to tenants.
-
-Problem description
-===================
-
-Enable administrators and tenants to take advantage of backend storage
-policies. The storage admin first creates storage profiles in VC based
-on the storage vendor provided capabilities and/or tag based capabilities
-of the underlying storage infrastructure. Refer to
-http://pubs.vmware.com/vsphere-55/topic/com.vmware.vsphere.storage.doc/GUID-A8BA9141-31F1-4555-A554-4B5B04D75E54.html
-to learn more about storage profiles on VC.
-
-The disk(s) of the virtual machine will be placed on the storage that
-matches the storage policy. This can for example provide preferential
-services to the user. For example the user will have an option of
-selecting 'gold', 'silver' or 'bronze' storage. 'gold' can be for
-applications that require fast and reliable results. 'bronze' can be
-for a background VM running in the evening doing maintenance.
-
-The spawn method currently selects the ‘best’ datastore to use. The
-administrator is able to select one or more datastores for selection
-by configuring a datastore regular expression. This logic will not
-be required if the instance flavor contain extra spec information
-that is relevant to the SPBM. That is, the SPBM information will be
-used for the datastore selection.
-
-Proposed change
-===============
-
-In order for Nova to provide SPBM we will need to address the following:
-
-* Enabling the tenant to make use of storage policies. The goal here
-  will be to provide the administrator with the necessary tools to
-  provide differential storage services to the tenant. More specifically
-  the administrator will be able to leverage capabilities provided by the
-  storage infrastructure. There are two parts:
-
-  * Configuration. The admin will need to do the following:
-
-    * Configure a default SPBM policy
-
-    * Create a flavor(s) for the tenants that will enable them to make use
-      of the various storage policies.
-
-  * Tenant usage. The tenant will be able to select a flavor that has
-    a storage policy.
-
-* Driver support for the storage policies.
-
-  * This entails using the information passed by the tenant to the driver.
-    More specifically the storage policy will be passed as flavor metadata.
-
-The driver will need to make use of a different endpoint to access the storage
-policies on the VC. This will require a new configuration variable, that is,
-the PBM WSDL location will need to be defined.
-
-NOTE: all of the nodes will share the same storage so there will not be any
-issues regarding rescheduling.
-
-The change will not affect the cached images. This is only where the disk
-for the VM will be placed.
-
-The flavor extra spec ‘image:storage_policy’ will drive the datastore
-selection. In the event that this flag is not present and the pbm_enabled
-is set in the configuration file then we will make use of a configured default
-policy. That is, if this is present then it will be used to get the list of
-datastores that can be used for selection. If not then we will use the list of
-datastores that can be accessed by the cluster.
-
-If this exists then we will validate that the policy exists.
-If not then an exception will be thrown. We will then proceed to get the moref
-and datastore of the datastore that is relevant to this policy
-
-pseudo code::
-        profile_ids = pbmServiceContent.profileManager.pbmQueryProfile()
-        profiles = pbmRetrieveContent(profile_ids)
-        profiles.find(name=profile_name)
-
-Query Matching ‘datastore’ entities for the profile. API :-
-pbmQueryMatchingHub
-
-If this does not exist then we will proceed to the select the datastore as
-before.
-
-The list of datastores will be processed by the existing code to select the
-best fit.
-
-Alternatives
-------------
-
-At the moment there is no way that a administrator can provide differential
-storage services to a tenant.
-
-Data model impact
------------------
-
-There are no data model changes. The information is passed from the tenant to
-the driver via flavor metadata (extraspecs). The driver in turn will use this
-information to assign the correct storage.
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-The cloud provider will provide a flavor to the tenant that will enable them
-to have preferential storage capabilities.
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-There are 3 new configuration variables (both in the vmware section):
-* pbm_wsdl_location - PBM service WSDL file location URL. e.g.
-file:///opt/SDK/spbm/wsdl/pbmService.wsdl. This will be optional. This
-value is a string. The default is None (not set).
-* pbm_enabled - status of storage policy based placement of instances.
-This value is a boolean. Default is False.
-* pbm_default_policy - The PBM default policy. If pbm_enabled
-is set and there is no defined storage policy for the specific request
-then this policy will be used. This value is a string. The default policy
-is defined out of band by the administrator on the Virtual Center. The
-default is None (not set).
-
-An admin user will create a new flavor either via the dashboard or the CLI.
-The flavor extra spec will have a key ‘image:storage_policy’. The admin
-will associate this this a predfined storage policy on the VC.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-None
-
-Assignee(s)
------------
-
-Primary assignee:
-    garyk
-    smurugesan
-
-Other contributors:
-    rgerganov
-
-Work Items
-----------
-
-Code was posted in the Icehouse cycle:
-* SPBM support (part of oslo integration)
-* Add support for default pbm policy
-* Get storage policy from flavor
-* Use storage policy in datastore selection
-* Associate instance with storage policy
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-This requires 3rd party testing. It is not possible to be tested by the current
-gate.
-
-
-Documentation Impact
-====================
-
-Configuration variables and their usage need to be documented.
-Flavor creation and management should be discussed too. That is, the flavor
-extra spec will need to contain the policy. The key will be:
-'image:storage_policy' and the values can be for example 'gold', 'silver',
-etc.
-
-References
-==========
-
-https://docs.google.com/document/d/14Fr76WsFxBPfQJHRdy389IxlxZHXq-Kr83PeCXgDP1M/edit
diff --git a/specs/juno/vmware-vsan-support.rst b/specs/juno/vmware-vsan-support.rst
deleted file mode 100644
index ab40aab..0000000
--- a/specs/juno/vmware-vsan-support.rst
+++ /dev/null
@@ -1,193 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===================================
-VMware: Support for vSAN Datastores
-===================================
-
-https://blueprints.launchpad.net/nova/+spec/vmware-vsan-support
-
-Currently the vmwareapi compute driver only supports deploying instances to NFS
-and VMFS datastores. This blueprint proposes to add support for using vSAN
-storage as well.
-
-Explanation of terminology used:
-
-The term "datastore" as referred to in the spec and the driver refers to
-the vSphere concept a logical storage container, a place where VM data (among
-other things) is kept. The purpose of this abstraction is to provide a uniform
-way for vSphere clients to access said VM data regardless of what hardware, I/O
-protocols or transport protocols are used by the underlying storage.
-
-All vSphere datastores until recently has been broadly divided into two types,
-VMFS and NFS. The vmwareapi driver has been supporting the use of both since
-its inception, without having to distinguish between either, largely because of
-this datastore abstraction.
-
-vSAN storage is a third type of datastore introduced in vSphere. It is
-a software-defined distributed storage that aggregates disks (magnetic for
-capacity, SSD for cache/performance) attached to a group of hosts into a
-single storage pool. That pool is once again exposed as a single datastore.
-
-Problem description
-===================
-
-Currently datastores with type "vsan" is ignored by compute driver entirely.
-One obstacle to using this type of datastore is that virtual disk data files
-(the "-flat.vmdk" files) are not directly addressable as datastore paths. Since
-both the spawn and snapshot workflow in the vmware driver addresses the data
-files in some way, they will have to be changed to support vSAN datastores.
-
-Proposed change
-===============
-
-The change is divided into two areas:
-
-* Recognize and use datastores of a new type ("vsan").
-* Update existing code involved in exporting and importing Glance images to
-  use alternate vSphere APIs that does not address disk data files directly.
-
-The second area of change is mainly provided by the image-transfer
-functionality in the oslo.vmware library [*]_. The proposal is to update the
-code to use said library.
-
-However, the only disk format that these alternate APIs support is the
-'streamOptimized' format. (The streamOptimized format is a sparse, compressed,
-and stream-friendly version of the VMDK disk that is well suited for
-import/export use cases, such as the glance<->hypervisor exchanges described
-above). This implies that only streamOptimized disk images are deployable on
-vSAN. The driver will be modified to recognize Glance vmdk images tagged
-with the property vmware_disktype='streamOptimized' as disks of such format,
-and only use the alternate APIs when handling disks of this format.
-
-.. [*] To import a disk image to a vSAN datastore, oslo.vmware uses the
-   ImportVApp vSphere API is used to import the image as a shadow virtual
-   machine (a VM container to hold a reference to the base disk disk, and is
-   not meant to be powered on). Likewise, to export the disk image, the library
-   uses the ExportVM vSphere API.  These APIs do not reference the virtual disk
-   data file paths directly and are hence compatible with vSAN storage.
-
-
-Alternatives
-------------
-
-None.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-None
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-The compute driver will require the oslo.vmware library. (See "Dependencies"
-section).
-
-There is a new configuration option under the [vmware] section,
-"image_transfer_timeout_secs", which configures how long an image transfer can
-proceed before timing out.
-
-In order to deploy existing VMDK images to vSAN, these images will have to be
-converted to streamOptimized and reimported to glance.
-
-
-Developer impact
-----------------
-
-Minimal. The changes related to blueprint are mostly isolated in the areas of
-handling a new vmdk format type and add recognition and use of an additional
-datastore type called "vsan".
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  vui
-
-Work Items
-----------
-
-Much of the work was done and proposed in the Icehouse cycle, but did not make
-the release due to time constraints. That work should should continue to be
-considered for this blueprint. The work is broadly decomposed into:
-
-* use oslo.vmware image_transfer module to handle download of images
-* use oslo.vmware image_transfer module to handle upload of image snapshot
-* update driver to allow the use of datastores of type vSAN.
-* update driver to recognized a new vmdk format (streamOptimized)
-
-
-Dependencies
-============
-
-* https://blueprints.launchpad.net/nova/+spec/use-oslo-vmware. The oslo.vmware
-  library provides functionality not available in the current vmware nova
-  driver that is required by this blueprint.
-
-* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor. Work
-  related to this blueprint will likely cause non-trivial changes to the
-  patches for this blueprint since several of them involve
-  the spawn operation.
-
-Testing
-=======
-
-Since Tempest in general does not support driver-specific tests, the proposal
-is to update the MineSweeper CI
-(https://wiki.openstack.org/wiki/NovaVMware/Minesweeper), to provide
-vCenter with vSAN storage and additional tests to verify existing Tempest
-tests passes when invoked against compute nodes using it.
-
-
-Documentation Impact
-====================
-
-New information in the vmware driver section of the Nova documentation will
-have to be added to document:
-
-* How to configure a compute node for vSAN use.
-* The virtual disk format requirement ("streamOptimized" only) when using vSAN
-  storage.
-* The new "image_transfer_timeout_secs" configuration option.
-* How to obtain a streamOptimized disk from a virtual machine or vmdk disk in a
-  non-streamOptimized format.
-
-
-References
-==========
-
-* https://github.com/openstack/oslo.vmware
-* https://blueprints.launchpad.net/nova/+spec/vmware-spawn-refactor
-* https://wiki.openstack.org/wiki/NovaVMware/Minesweeper
diff --git a/specs/juno/websocket-proxy-to-host-security.rst b/specs/juno/websocket-proxy-to-host-security.rst
deleted file mode 100644
index e21c45f..0000000
--- a/specs/juno/websocket-proxy-to-host-security.rst
+++ /dev/null
@@ -1,215 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-===================================================================
-Support Proxying of Encryption and Authentication in WebSocketProxy
-===================================================================
-
-https://blueprints.launchpad.net/nova/+spec/websocket-proxy-to-host-security
-
-Currently, while the noVNC and HTML5 SPICE clients can use TLS-encrypted
-WebSockets to communicate with Websockify (and authenticate with Nova console
-tokens), the encryption and authentication ends there.  There are neither
-encryption nor authentication between Websockify and the hypervisors'
-VNC and SPICE servers.
-
-This blueprint would propose introducing a generic framework for supporting
-proxying security for Websockify to use between itself and the compute nodes.
-
-Problem description
-===================
-
-Currently, there are neither authentication nor encryption between Websockify
-and the hypervisors' SPICE and VNC servers.  Were a malicious entity to gain
-access to the "internal" network of an OpenStack deployment he or she could:
-
-* "Listen" to VNC and SPICE traffic (lack of encryption)
-
-* Connect freely to the SPICE and VNC servers of VMs (lack of authentication)
-
-For example, suppose Alice starts a VM, which gets placed on "hypervisor-a".
-Carol could then use Wireshark or the like to watch what Alice is doing with
-her VM's console.  Furthermore, Carol could point her VNC client at
-"hypervisor-a:5900" and actually access the VM's console.
-
-Proposed change
-===============
-
-This blueprint would introduce a generic framework performing proxying of
-authentication and encryption.  When establishing a connection, the proxy would
-act as a client to the server and a server to the client, performing different
-steps for each during the security negotiation phase of the respective
-protocols.
-
-The proxy would then wrap the server socket in an encryption layer that
-respected the standard python socket class (much like python's :code:`ssl`
-library does) and pass the resulting wrapped socket off to the normal proxy
-code.
-
-Authentication drivers would have a class for SPICE as well as for VNC
-(since VNC has to do some extra negotiation as part of the RFB protocol).
-Deployers could then point Nova to the appropriate driver and options via
-configuration options.
-
-A base driver for TLS [1]_ (VeNCrypt for VNC, plain TLS for SPICE) would be
-included as an example implementation, although it would be beneficial to
-develop further drivers, such as a SASL driver [2]_.
-
-.. [1] To ensure only the correct clients connect, the proxy would send
-       the hypervisor x509 client certificates, and the server would reject
-       any certificates not signed by the specified CA (authentication).  To
-       prevent evesdroppers, the actual data stream would use TLS encryption.
-       While both of these are supported for VNC by QEMU (and thus KVM, Xen,
-       etc), it would appear that SPICE only supports the encryption.  If a
-       deployment is using SPICE, another driver should be used.
-
-.. [2] Such a driver would most likely use the GSSAPI mechanism, which would
-       provide Kerberos encryption and authentication for the connections.
-       However, SASL supports other mechanisms, so non-GSSAPI drivers could
-       be written.  Some mechanisms do not support encryption ("data-layer
-       security" in SASL terms), so TLS should be used to provide encryption
-       with those.  SASL connections are by both SPICE and VNC on QEMU fully.
-
-Alternatives
-------------
-
-* Doing end-to-end security: this would require supporting more advanced
-  encryption and authentication in the HTML5 clients.  Unfortunately, this
-  requires doing cryptography in the browser, which is not really feasible
-  until more browsers start implementing the HTML5 WebCrypto API.
-
-* Using a tool like stunnel: There are a couple of issues with this.  The first
-  is that it locks us in to a particular authentication mechanism -- stunnel
-  works fine for TLS, but will not work if we want to use SASL instead.
-  The second issue is that it bypasses normal VNC security negotation, which
-  does the initial handshake in the clear, and then moves on to security
-  negotiation later.  It is desired to stay within the confines of the standard
-  RFB (VNC) specification.  The third issue is that this would sidestep the
-  issue of authentication -- a malicous entity could still connect directly to
-  the unauthenticated port, unless you explicitly set up your firewall to block
-  remote connections to the normal VNC ports (which requires more setup on the
-  part of the deployer -- we want to make it fairly easy to use this).
-
-Data model impact
------------------
-
-None.
-
-REST API impact
----------------
-
-None.
-
-Security impact
----------------
-
-The actual crypto done would depend on the driver being used.  It will be
-important to ensure that the libraries used behind any implemented drivers
-are actually secure.
-
-Assuming the driver is secure and implements both authentication and
-encryption, the security of the deployment would be strengthened.
-
-Notifications impact
---------------------
-
-None.
-
-Other end user impact
----------------------
-
-None.
-
-Performance Impact
-------------------
-
-Minimal.  The extra encryption will most likely be performed via a C-based
-python library, so there will be relatively low overhead.
-
-Other deployer impact
----------------------
-
-First, a deployer would have to choose the driver that he or she wished to use:
-:code:`console_proxy_security_driver = driver_name`.  Then, the particular
-driver would be have configuration options under its own section in the
-configuration file.  For instance, the x509/TLS driver would appear as the
-following:
-
-.. code::
-
-   [console_proxy_tls]
-   ca_certificate = /path/to/ca.cert
-   client_certificate = /path/to/client.cert
-
-Finally, most drivers will require extra setup outside of Nova.  For instance,
-the x509/TLS driver will reqiure generating CA, client, and server
-certificates, distributing the CA and client certificates, and configuring
-libvirt to require x509/TLS encryption and authentication when connecting to
-VNC and SPICE consoles (see `References`_).
-
-Developer impact
-----------------
-
-None.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-    sross-7
-
-Other contributors:
-    None
-
-Work Items
-----------
-
-1. Implement the base framework for proxying authentication and
-   encryption.
-
-2. Implement a No-op driver
-
-3. Implement the basic x509/TLS driver
-
-
-Dependencies
-============
-
-While individual drivers might introduce new dependencies,
-the actual framework would not.
-
-
-Testing
-=======
-
-We should test that the framework is callable correctly.  Additionally,
-it will be necessary to work with infra to ensure that we can test the actual
-drivers (for instance, for x509/TLS, we will need to generate certificates,
-etc).
-
-
-Documentation Impact
-====================
-
-We will need to document the new configuration options, as well as how to
-generate certificates for the TLS driver (See `Other deployer impact`_).
-
-
-References
-==========
-
-* The most recent version of the VeNCrypt specification can be found in
-  this thread http://sourceforge.net/p/tigervnc/mailman/message/25748057/ --
-  http://sourceforge.net/p/tigervnc/mailman/attachment/20100720083109.GA3303%40evileye.atkac.brq.redhat.com/1/
-
-* SPICE TLS: http://www.spice-space.org/docs/spice_user_manual.pdf -- page 11
-
-* libvirt TLS setup:
-  VNC: http://wiki.libvirt.org/page/VNCTLSSetup,
-  SPICE: http://people.freedesktop.org/~teuf/spice-doc/html/ch02s08.html
diff --git a/specs/juno/xenapi-set-ipxe-url-as-img-metadata.rst b/specs/juno/xenapi-set-ipxe-url-as-img-metadata.rst
deleted file mode 100644
index 7d2c4b0..0000000
--- a/specs/juno/xenapi-set-ipxe-url-as-img-metadata.rst
+++ /dev/null
@@ -1,117 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-=======================================================
-Set ipxe url as image metadata instead of config option
-=======================================================
-
-https://blueprints.launchpad.net/nova/+spec/xenapi-set-ipxe-url-as-img-metadata
-
-Move xenapi_ipxe_boot_menu_url to a image property so that it is user
-configurable.
-
-Problem description
-===================
-
-Currently the xenapi iPXE URL is specified as a configuration option in Nova.
-Because it is a configuration option, users are unable to specify their own
-iPXE URL on their own images.  The proposal is to allow the iPXE URL to be
-specified as an image property.  By doing this, a customer can upload an iPXE
-ISO, with the iPXE URL specified as a metadata option and boot from their own
-custom configurations.
-
-Proposed change
-===============
-
-Add the ability to specify ipxe_boot_menu_url as an image metadata property
-which can override the nova configuration of xenapi_ipxe_boot_menu_url.
-
-Alternatives
-------------
-
-Remove the main configuration option of xenapi_ipxe_boot_menu_url and rely on
-the image property to populate the configuration.
-
-Data model impact
------------------
-
-None
-
-REST API impact
----------------
-
-None
-
-Security impact
----------------
-
-None
-
-Notifications impact
---------------------
-
-None
-
-Other end user impact
----------------------
-
-Users will need to specify the ipxe_boot_menu_url in order to boot from their
-iPXE configuration.
-
-Performance Impact
-------------------
-
-None
-
-Other deployer impact
----------------------
-
-Because the settings set on the image properties would override the Nova
-configuration settings, an operator could prevent users from overriding the
-ipxe settings by setting policies to restrict usage of the various flags like
-ipxe_boot and ipxe_boot_menu_url.
-
-Developer impact
-----------------
-
-None
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignee:
-  antonym
-
-Work Items
-----------
-
-* Create ipxe_boot_menu_url image metadata configuration to be used when
-  generating iPXE ISO image.
-
-Dependencies
-============
-
-None
-
-Testing
-=======
-
-Testing of this feature will be covered by the XenServer CI.
-
-Documentation Impact
-====================
-
-Change documentation to reflect that ipxe_boot_menu_url can now be specified as
-an image property which will override the default configuration.
-
-References
-==========
-
-* Original iPXE implementation:
-  https://blueprints.launchpad.net/nova/+spec/xenapi-ipxe-iso-boot-support
diff --git a/specs/juno/xenapi-vcpu-topology.rst b/specs/juno/xenapi-vcpu-topology.rst
deleted file mode 100644
index e50a219..0000000
--- a/specs/juno/xenapi-vcpu-topology.rst
+++ /dev/null
@@ -1,152 +0,0 @@
-..
- This work is licensed under a Creative Commons Attribution 3.0 Unported
- License.
-
- http://creativecommons.org/licenses/by/3.0/legalcode
-
-====================
-XenAPI vCPU Topology
-====================
-
-https://blueprints.launchpad.net/nova/+spec/xenapi-vcpu-topology
-
-The proposal is to add support for vCPU topology for XenAPI.  It will utilize
-the work done on the virt-driver-vcpu-toplogy blueprint.  Most of this
-blueprint has been copied from the virt-driver-vcpu-topology blueprint as
-they align well but differ slightly on implementations per hypervisor.
-
-Problem description
-===================
-
-See Virt Driver VCPU Topology spec referenced at the end of blueprint.
-
-Proposed change
-===============
-
-See Virt Driver VCPU Topology spec referenced at the end of blueprint.
-
-For XenServer implementation the following configurations will be used:
-
-* hw:cpu_sockets=NN - preferred number of sockets to expose to the guest
-* hw:cpu_cores=NN - preferred number of cores to expose to the guest
-* hw:cpu_max_sockets=NN - maximum number of sockets to expose to the guest
-* hw:cpu_max_cores=NN - maximum number of cores to expose to the guest
-
-At the image level the exact same set of parameters will be permitted,
-with the exception that image properties will use underscores throughout
-instead of an initial colon.
-
-* hw_cpu_sockets=NN - preferred number of sockets to expose to the guest
-* hw_cpu_cores=NN - preferred number of cores to expose to the guest
-* hw_cpu_max_sockets=NN - maximum number of sockets to expose to the guest
-* hw_cpu_max_cores=NN - maximum number of cores to expose to the guest
-
-Note that XenServer does not have a specific setting for number of threads
-so setting threads will not function on XenServer currently.
-
-Alternatives
-------------
-
-None, will utilize existing work done in virt-driver-vcpu-topology blueprint
-
-Data model impact
------------------
-
-No impact.
-
-The new properties will use the existing flavour extra specs and image
-property storage models.
-
-REST API impact
----------------
-
-No impact.
-
-The new properties will use the existing flavour extra specs and image
-property API facilities.
-
-Security impact
----------------
-
-The choice of sockets vs cores can have an impact on host resource utilization
-when NUMA is involved, since over use of cores will prevent a guest being
-split across multiple NUMA nodes. This feature addresses this by allowing the
-flavour administrator to define hard caps, and ensuring the flavour will
-always take priority over the image settings.
-
-Notifications impact
---------------------
-
-No impact.
-
-There is no need for this feature to integrate with notifications.
-
-Other end user impact
----------------------
-
-The user will gain the ability to control aspects of the vCPU topology used
-by their guest. They will achieve this by setting image properties in glance.
-
-Performance Impact
-------------------
-
-The cores vs sockets vs threads decision does not involve any scheduler
-interaction, since this design is not attempting to match host topology
-to guest topology. A later blueprint on CPU pinning will make it possible
-todo such host to guest topology matching, and its performance impact
-will be considered there.
-
-Other deployer impact
----------------------
-
-The flavour extra specs will gain new parameters in extra specs which a
-cloud administrator can choose to use. If none are set then the default
-behaviour is unchanged from previous releases.
-
-Developer impact
-----------------
-
-Implementation will add support for XenAPI drivers.
-
-Implementation
-==============
-
-Assignee(s)
------------
-
-Primary assignees:
-  antonym
-  johngarbutt
-
-Work Items
-----------
-
-* Add XenAPI driver support for choosing a CPU topology solution based on
-  the given hw_cpu_* parameters.
-
-Dependencies
-============
-
-No external dependencies
-
-Testing
-=======
-
-Testing of this feature will be covered by the XenServer CI.
-
-Documentation Impact
-====================
-
-The new flavour extra specs and image properties will need to be documented.
-Guidance should be given to cloud administrators on how to make most
-effective use of the new features. Guidance should be given to the end user
-on how to use the new features to address their use cases.
-
-References
-==========
-
-* Virt Driver VCPU Topology:
-  https://blueprints.launchpad.net/nova/+spec/virt-driver-vcpu-topology
-
-* Information on cores-per-socket in XenServer:
-  https://support.citrix.com/article/CTX126524
diff --git a/tests/test_titles.py b/tests/test_titles.py
index 2972d11..46f86b9 100644
--- a/tests/test_titles.py
+++ b/tests/test_titles.py
@@ -93,7 +93,7 @@ class TestTitles(testtools.TestCase):
             spec = docutils.core.publish_doctree(template)
             template_titles = self._get_titles(spec)
 
-            files = glob.glob("specs/%s/*" % release)
+            files = glob.glob("specs/%s/*/*" % release)
             for filename in files:
                 self.assertTrue(filename.endswith(".rst"),
                                 "spec's file must uses 'rst' extension.")
-- 
1.9.1

