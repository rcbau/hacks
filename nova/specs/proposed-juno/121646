From cb4c6919ed0312c1d47826dbc15a97d3a699a7b6 Mon Sep 17 00:00:00 2001
From: "Chris St. Pierre" <stpierre@metacloud.com>
Date: Mon, 15 Sep 2014 13:30:05 -0500
Subject: [PATCH] Support a common cache for metadata

Currently, the Nova metadata API stores its cache in memory, which
prevents it from being shared amongst multiple hypervisors and thus
reduces the hit rate. This blueprint proposes to optionally replace
the in-memory cache with dogpile.cache, allowing cache sharing between
hypervisors.

Change-Id: I7b2f6d3fd6408d0220439d8c74f5e1af22bb0583
---
 specs/juno/common-nova-metadata-cache.rst | 164 ++++++++++++++++++++++++++++++
 1 file changed, 164 insertions(+)
 create mode 100644 specs/juno/common-nova-metadata-cache.rst

diff --git a/specs/juno/common-nova-metadata-cache.rst b/specs/juno/common-nova-metadata-cache.rst
new file mode 100644
index 0000000..e1fb58d
--- /dev/null
+++ b/specs/juno/common-nova-metadata-cache.rst
@@ -0,0 +1,164 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+===================================
+Support a common cache for metadata
+===================================
+
+https://blueprints.launchpad.net/nova/+spec/common-nova-metadata-cache
+
+Currently, the Nova metadata API stores its cache in memory, which
+prevents it from being shared amongst multiple hypervisors and thus
+reduces the hit rate. This blueprint proposes to optionally replace
+the in-memory cache with dogpile.cache, allowing cache sharing between
+hypervisors.
+
+Problem description
+===================
+
+Under heavy use, the Nova metadata API can be a significant
+performance bottleneck. This use pattern can be common with
+configuration management systems that draw their own metadata from
+Nova, for instance.
+
+In our experience, this bottleneck can be greatly improved by
+implementing a longer-lived, shared cache. I have already proposed a
+change, https://review.openstack.org/#/c/119421/, that will make the
+cache time configurable, but that is of limited benefit when the cache
+cannot be shared amongst multiple hypervisors.
+
+
+Proposed change
+===============
+
+By using ``dogpile.cache`` to back the metadata cache, we can provide
+the option to share the cache between hypervisors by using memcached
+(for instance) and greatly increase the cache hit rate.
+
+The use of a common cache will be optional; the simple in-memory cache
+will still remain supported in order to keep simple deployments simple.
+
+Alternatives
+------------
+
+The only other way to increase cache hit rates is to increase the TTL
+of cache items, which we have already proposed.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+This actually makes it more difficult to launch a resource exhaustion
+attack, since the cache will no longer be stored in memory local to
+each hypervisor, but instead in a central system that can be protected
+(and set to expire the cache) independently.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+If the central caching feature is enabled, the operator can expect:
+
+* Memory usage on each individual hypervisor to decrease;
+* Cache hits on the metadata API (and thus performance thereof) to
+  increase;
+* A new service (or, if using the same ``dogpile.cache`` backend as
+  Keystone, an existing memcached service) that uses more memory.
+
+Other deployer impact
+---------------------
+
+A new group, ``[metadata_cache]``, should be added to
+``nova.conf``. It will contain the following options, all of which are
+exact analogs to the same options in the ``[cache]`` section of
+``keystone.conf`` (see
+http://docs.openstack.org/developer/keystone/configuration.html#caching-layer):
+
+* ``enabled``
+* ``debug_cache_backend``
+* ``backend``
+* ``expiration_time``
+* ``backend_argument``
+
+The defaults will be::
+
+    enabled = yes
+    debug_cache_backend = no
+    backend = dogpile.cache.memory
+    expiration_time = 15
+    backend_argument =
+
+This will mimic the current state of a completely in-memory cache.
+
+Developer impact
+----------------
+
+When writing unit tests that may use the metadata API, developers will
+need to select the memory cache backend.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  stpierre
+
+Work Items
+----------
+
+#. Write the caching layer module. This can be cribbed substantially
+   from the existing Keystone key-value store, although it will be
+   simpler in many ways because the data it will be storing is more
+   predictable.
+#. Update the metadata request handler to use the new caching layer
+   instead of the in-memory cache.
+
+Dependencies
+============
+
+None. ``dogpile.cache`` is already a global requirement.
+
+Testing
+=======
+
+The change linked above already extends unit tests to actually test
+the metadata request handler, so they should be sufficient to test
+that portion of it. New tests will need to be added to test the
+caching layer, but since that's not exposed functionally no new
+tempest tests should be necessary.
+
+Documentation Impact
+====================
+
+The options listed above will need to be added to the
+documentation. Since the configuration is substantially similar to the
+Keystone caching layer, it is expected that much of the Keystone
+documentation can be borrowed whole cloth.
+
+References
+==========
+
+None.
-- 
1.9.1

