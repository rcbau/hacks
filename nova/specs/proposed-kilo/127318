From 96f4a09e810485a779c82fea0d55f6c7347bbdbe Mon Sep 17 00:00:00 2001
From: Mitsuhiro Tanino <mitsuhiro.tanino@hds.com>
Date: Thu, 9 Oct 2014 11:39:07 -0400
Subject: [PATCH] LVM: Support a volume-group on shared storage

This proposal enables direct I/O path from VMs to storage using LVM driver
with LVM on shared storage volume. By enabling direct I/O path provides
stable I/O path for VMs and improve throughput to VMs.

In this feature, both Nova and Cinder pieces are necessary.

Change-Id: I95fe79432a96496dbcd8b50fe8f906d52784adce
Implements: blueprint lvm-driver-for-shared-storage
---
 .../approved/lvm-driver-for-shared-storage.rst     | 250 +++++++++++++++++++++
 1 file changed, 250 insertions(+)
 create mode 100644 specs/kilo/approved/lvm-driver-for-shared-storage.rst

diff --git a/specs/kilo/approved/lvm-driver-for-shared-storage.rst b/specs/kilo/approved/lvm-driver-for-shared-storage.rst
new file mode 100644
index 0000000..ba17e03
--- /dev/null
+++ b/specs/kilo/approved/lvm-driver-for-shared-storage.rst
@@ -0,0 +1,250 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Support LVM on a shared storage volume
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/lvm-driver-for-shared-storage
+
+Based on the OpenStack-Atlanta-User-Survey_, many users are still using LVM
+driver at their Dev/QA(30%), POC(26%) and Production(22%) environment,
+LVM driver occupies an important place in Cinder driver.
+
+.. _Openstack-Atlanta-User-Survey: http://www.slideshare.net/ryan-lane/openstack-atlanta-user-survey
+
+This proposal enables direct I/O path from VMs to storage using LVM driver
+with LVM on shared storage volume. By enabling direct I/O path provides
+stable I/O path for VMs and improve throughput to VMs.
+
+Problem description
+===================
+Current LVM driver only supports I/O path from VMs to storage through
+SCSI target such as tgtd or LIO. Using SCSI target has many benefits,
+whereas there are some concerns as follows.
+
+1. VMs can't issue I/O to the storage directly.
+   If the target stops accidentally, all I/O from VMs stops and VMs
+   might be crashed.
+
+2. Total I/O bandwidth is limited to bandwidth of a node which has
+   Cinder volume service because of SCSI target.
+   ex. Total I/O bandwidth is, control node(10Ggps)
+
+This proposal adds support for a LVM(Volume Group) on a shared storage
+volume to LVM driver.
+Supporting shared VG between control node and compute nodes enables
+following.
+
+1. Each node can access simultaneously a single VG on the shared storage
+   directly from their own I/O path.
+
+2. Every LV(Logical Volume) on the VG is also visible with a own I/O
+   path on each control and compute nodes by using LVM metadata.
+
+As a result, VMs which have Cinder volume can issue I/O directly to
+backend storage via FC/iSCSI.
+
+These are benefits of shared LVM driver.
+
+1. Stability of I/O path will be improved, because this driver does
+   not require SCSI target.
+
+2. Total I/O bandwidth will be improved, because each control and
+   compute nodes can use own I/O path directly.
+   ex. Total I/O bandwidth is, control node(10Ggps) + compute(10Gbps)xN.
+
+3. Reduce workloads of control node because tgtd or LIO Target are
+   not necessary.
+
+Use Cases
+---------
+Here is a use case for this proposal.
+
+Commonly volume operations such as creation, deletion, snapshot etc. are
+restricted to system administrators because volume operations might be
+affects all storage user. Thus these operations used to handled system
+administrator after carefully examining status of storage.
+
+However, in OpenStack cloud environment, every user has a permission
+to run storage operations via Cinder. This causes increase of storage
+workloads and it is difficult to manage the workloads because user does
+not care storage status.
+
+In order to use expensive storage more efficiently, I think it is better
+to reduce hardware based storage workload by offloading the workload to
+software based volume operation on a case by case basis.
+
+If Cinder is available two drivers in regards to a storage, user can
+choice appropriate volume type as the situation demands. For example,
+
+1. As for "Standard" type storage, use proposed software based LVM
+   Cinder driver.
+
+   ex. Quicker volume creation and snapshot are available.
+
+2. As for "High performance" type storage, use hardware based Cinder driver.
+   Higher charge than "Standard" volume.
+
+   ex. Efficient snapshot/clone/mirroring features are available.
+
+Project Priority
+----------------
+
+None
+
+Proposed change
+===============
+1. Introduce LibvirtSharedLvmDriver class to virt.libvirt.volume and add
+   the class to list of volume_drivers to handle in case of "lvm".
+
+   In this class, connect_volume() and disconnect_volume() will be
+   implemented to handle LVM on a shared storage volume.
+
+  (a) connect_volume()
+
+    When a storage volume is shared to multiple nodes, volume activation to
+    created logical volume(LV) is necessary before attaching a created volume
+    to an instance.
+    After volume creation at Cinder node, only Cinder node knows the volume and
+    other compute nodes do not know about the created LV. Therefore, these two
+    steps are necessary to attach the created volume.
+
+    - Update LV list and status using "lvs" at a compute node.
+    - Activate LV using "lvchange -ay" is required at a compute node.
+
+    After volume activation, access device path /dev/VG1/LV1 is created at
+    compute node. Libvirt and qemu can handle device path and attach a volume
+    using this.
+
+  (b) disconnect_volume()
+
+    After detaching a Cinder volume from an instance, the compute node does
+    not have to access a Cinder volume. In order to keep a consistency of
+    Cinder volume, volume deactivation using "lvchange -an" is required at a
+    compute node. This operation will remove access device path /dev/VG1/LV1
+    so that the compute node can't access to the volume using the device
+    path until the volume will be attached to an instance again.
+    In this method, underlying connection to LVM on shared storage volume
+    is continuously maintained.
+
+2. Introduce LVM snapshot creation feature using Assisted snapshot
+
+  In order to take an online snapshot for Cinder volume using this driver,
+  assisted snapshot is necessary because LVM snapshot must be taken a node
+  which issues I/O to original volume. Also online snapshot delete
+  is necessary using assisted snapshot.
+  Currently, assisted snapshot is already supported by gluster but supported
+  type is only qcow2. This proposal adds to support LVM type.
+
+Alternatives
+------------
+
+None.
+
+Data model impact
+-----------------
+
+None.
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+There is performance impact of IOPs by using LVM compared to using
+a raw storage volume, but the impact is subtle.
+
+Other deployer impact
+---------------------
+
+To apply this feature to OpenStack environment, storage admin needs
+following preparations.
+
+1. Prepare a storage volume which is shared between Cinder node and
+   compute nodes. And also admin needs to create a volume group on top
+   of the storage volume.
+
+2. Configure cinder.conf to use sharedLVM driver and the created storage
+   volume as a Cinder backend-storage.
+
+Here is a sample of cinder.conf::
+
+ enabled_backends=SharedLVM
+ [SharedLVM]
+ volume_group=cinder-shared-volume
+ volume_driver=cinder.volume.drivers.lvm.SharedLVMDriver
+ volume_backend_name=SharedLVM
+
+Configuration of nova driver is not required in this feature.
+
+Developer impact
+----------------
+
+None.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+   mitsuhiro-tanino
+
+Work Items
+----------
+
+None
+
+Dependencies
+============
+
+This feature depends on following Cinder blueprint.
+
+ https://blueprints.launchpad.net/cinder/+spec/lvm-driver-for-shared-storage
+
+Testing
+=======
+
+Unit tests will be added in nova.
+
+Documentation Impact
+====================
+
+Cinder driver documentation will be added.
+There is no impact to Nova documentation.
+
+References
+==========
+
+Blueprints:
+ https://blueprints.launchpad.net/nova/+spec/lvm-driver-for-shared-storage
+
+ https://blueprints.launchpad.net/cinder/+spec/lvm-driver-for-shared-storage
+
+Review:
+ nova:   https://review.openstack.org/#/c/92443/
+
+ cinder: https://review.openstack.org/#/c/92479/
-- 
1.9.1

