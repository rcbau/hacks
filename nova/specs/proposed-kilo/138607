From 05d5b73451c7adcd1ceab667b2b4dca55647cc66 Mon Sep 17 00:00:00 2001
From: Joshua Harlow <harlowja@yahoo-inc.com>
Date: Wed, 19 Nov 2014 15:31:52 -0800
Subject: [PATCH] Replace service groups with the tooz groups

This specification proposes replacing (or depreciating and
later replacing) the service group and associated drivers
in nova with the usage of tooz (a new oslo library) to provide
the same functionality through tooz's own group membership
abstraction.

Implements: blueprint tooz-for-service-groups

Change-Id: I65198fe507a11a1efd9f90e2f56d91023b689099
---
 specs/kilo/approved/service-group-using-tooz.rst | 518 +++++++++++++++++++++++
 1 file changed, 518 insertions(+)
 create mode 100644 specs/kilo/approved/service-group-using-tooz.rst

diff --git a/specs/kilo/approved/service-group-using-tooz.rst b/specs/kilo/approved/service-group-using-tooz.rst
new file mode 100644
index 0000000..8561eb1
--- /dev/null
+++ b/specs/kilo/approved/service-group-using-tooz.rst
@@ -0,0 +1,518 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================
+Tooz for service groups
+=======================
+
+https://blueprints.launchpad.net/nova/+spec/tooz-for-service-groups
+
+Instead of having nova maintain, test its own group membership code which
+is used to determine what services are up & down which is frequently used to
+determine which members of a given group [compute, scheduler...] are and
+if they are alive or dead (typically the member information includes what
+topic can be used to contact the member as well) we propose to retain a similar
+API in nova but instead using `tooz`_ (a new oslo library that has been under
+development for around a year) to provide the grouping concepts.
+
+Problem description
+===================
+
+The service group code in nova is used to determine what services are up & down
+and used to introspect liveness of those services by other members of the
+groups that all services belong to. To reduce nova from having to maintain,
+test, and support code that does these group membership concepts (currently
+supporting [db, memcache, zookeeper] drivers) the `tooz`_ library can instead
+be used as the entrypoint and driver provider/maintainer that provides
+equivalent concepts.
+
+This will have the following benefits (not an all inclusive listing):
+
+- Increase the robustness of implementation (by utilizing the tooz
+  test suite which is specialized to test the grouping concepts).
+- Provide a cleaner abstraction (since how tooz maintains the grouping
+  concept(s) is now an internal API implementation).
+- Reduce the burden on the nova team around testing, management, maintenance
+  and general upkeep of the servicegroup concepts (seeing as this is something
+  that is not a direct feature of a compute project it seems like this burden
+  should just be placed on a external project instead).
+- Keep the code less stale (ex, the zookeeper driver is using an
+  outdated/dead? `evzookeeper`_ driver from 2012 with a `zookeeper unit test`_
+  that likely isn't/hasn't(?) ever been tested in an automated fashion).
+- World peace...
+
+Use Cases
+----------
+
+- Developer (nova team has to maintain, test, own less code).
+- Deployer (they can *potentially* configure tooz to use a backend of there
+  choosing).
+
+Project Priority
+-----------------
+
+It doesn't appear to be listed in the current `kilo priorities`_ but it does
+feel like an exception should be made for something that *could* bring world
+peace...
+
+Proposed change
+===============
+
+The proposed change would be to refactor the ``servicegroup/api.py`` API
+to have only a single implementation that is backed by tooz. Instead of
+having an API (shown below) that would be backed by custom implementations
+in nova (one of [db, memcache, zookeeper]) there would be a single API that
+would just use tooz.
+
+The API that *currently* exists::
+
+    class ServiceGroupDriver(object):
+
+        def join(self, member_id, group_id, service=None):
+            raise NotImplementedError()
+
+        def is_up(self, member):
+            raise NotImplementedError()
+
+        def leave(self, member_id, group_id):
+            raise NotImplementedError()
+
+        def get_all(self, group_id):
+            raise NotImplementedError()
+
+        def get_one(self, group_id):
+            members = self.get_all(group_id)
+            if members is None:
+                return None
+            length = len(members)
+            if length == 0:
+                return None
+            return random.choice(members)
+
+The idea for this implementation is that it *could* be something like::
+
+    class ToozServiceGroupDriver(ServiceGroupDriver):
+        def __init__(self, backend_url):
+            self._impl = tooz.get_coordinator(backend_url)
+            self._impl.start()
+
+        def join(self, member_id, group_id, service=None):
+            # Call into impl to join (and handle/translate the
+            # appropriate errors)...
+
+        def is_up(self, member):
+            # Call into impl to check if the member is up... (and
+            # handle/translate the appropriate errors)...
+
+        def leave(self, member_id, group_id):
+            # Call into impl to leave (and handle/translate the appropriate
+            # errors)...
+
+        def get_all(self, group_id):
+            # Call into impl to get all the members of the given group (and
+            # handle/translate appropriate errors)...
+
+        def get_one(self, group_id):
+            # Call into impl to get one the members of the given group (and
+            # handle/translate appropriate errors)...
+
+Following this change (and perhaps in the release after this change?) the
+older implementation(s) would be marked as deprecated/no longer supported and
+then in a release after this depreciation they could/would be removed (leaving
+only the tooz implementation).
+
+Alternatives
+------------
+
+- Retain the custom implementations (sadness throughout the land), let them
+  continue to become stale/rot; or update them and continue to maintain
+  them (thus avoiding said staleness and rot).
+
+Data model impact
+-----------------
+
+Lets split this up and consider the three in-tree drivers that exist in nova.
+
+db.py
+~~~~~
+
+This driver uses tables and a data model that is intrinsically tied to the
+internals of nova::
+
+    mysql> describe services;
+    +-----------------+--------------+------+-----+---------+----------------+
+    | Field           | Type         | Null | Key | Default | Extra          |
+    +-----------------+--------------+------+-----+---------+----------------+
+    | created_at      | datetime     | YES  |     | NULL    |                |
+    | updated_at      | datetime     | YES  |     | NULL    |                |
+    | deleted_at      | datetime     | YES  |     | NULL    |                |
+    | id              | int(11)      | NO   | PRI | NULL    | auto_increment |
+    | host            | varchar(255) | YES  | MUL | NULL    |                |
+    | binary          | varchar(255) | YES  |     | NULL    |                |
+    | topic           | varchar(255) | YES  |     | NULL    |                |
+    | report_count    | int(11)      | NO   |     | NULL    |                |
+    | disabled        | tinyint(1)   | YES  |     | NULL    |                |
+    | deleted         | int(11)      | YES  |     | NULL    |                |
+    | disabled_reason | varchar(255) | YES  |     | NULL    |                |
+    +-----------------+--------------+------+-----+---------+----------------+
+    11 rows in set (0.00 sec)
+
+What to do about this driver:
+
+#. Leave the driver in nova (and don't deprecate it).
+#. Something else?
+
+mc.py
+~~~~~
+
+This driver uses a ``memorycache`` from oslo (to be replaced with `dogpile`_
+sometime soon?) that then saves data using the backends that ``memorycache``
+module supports (this being memcache or a local memory implementation that is
+typically/hopefully used just for testing; when a memcache server can not be
+made available to integrate against).
+
+This driver uses keys of the format::
+
+    key = "%(topic)s:%(host)s" % service_ref (the ``service_ref`` object
+                                              is from the database layer)
+
+Things to note:
+
+- This driver only uses the ``memorycache`` (and subsequently
+  memcache) to check liveness via the ``is_up`` implementation (liveness is
+  maintained by a periodic ``set`` call into memcache for each group member).
+  The service group members come from the database (using the same
+  tables that the ``db.py`` driver uses). This is IMHO *confusing* and means
+  that the database ``services`` table (described above) is used but the
+  fields [updated_at, report_count, ...] are not actually used (the
+  fields [disabled, deleted, topic] though are).
+- Memcache is a *cache* and IMHO should *not* be used for this kind of
+  *critical* service liveness information (if the cache gets cleared or a
+  memcache server goes down, you may have just lost a portion of your
+  services/cluster); tooz does provide a memcache driver but it also provides
+  a redis one which can be made/setup in a more consistent and reliable
+  manner (redis supports and/or is building support for `clustering`_,
+  `sharding`_, more consistent/durable strategies while memcache just
+  provides a non-durable cache, unless the deployer starts to use things
+  like `memcachedb`_, but let's not consider that as the deployment
+  solution).
+
+Options for what to do about this driver:
+
+- Remove it/deprecate it, tooz provides the equivalent/better functionality
+  using its own memcache (or redis) implementation. The primary difference
+  would be that tooz does not use nova-conductor (and by side-effect the
+  database) to fetch the members of a group, as tooz maintains this
+  information in memcache (or redis) itself.
+
+  - This disconnect of the database from tooz will be a functionality
+    difference that needs to be documented and tested (and hopefully is a
+    welcome change that reduces the coupling between the ``services`` table,
+    the database, and the various drivers).
+
+- Something else?
+
+zk.py
+~~~~~
+
+This driver uses `evzookeeper`_ (seemingly unmaintained?) to store members
+under the path formed by ``"/".join(CONF.zookeeper.sg_prefix, group)`` in
+zookeeper by placing a ephemeral node representing the member under that
+path. Since zookeeper will remove ephemeral node(s) automatically when the
+client connected to zookeeper has not *reported in* after a given amount of
+time this driver also *internally* uses a green thread that will
+periodically *report in* to zookeeper to ensure that the ephemeral node is
+not automatically removed.
+
+Options for what to do about this driver:
+
+- Remove it/deprecate it, tooz provides the equivalent functionality using its
+  own zookeeper implementation using `kazoo`_ (which is maintained and is more
+  active in the python community). The functionality translation should be
+  relatively simple and straight-forward.
+
+Things to note:
+
+- The `evzookeeper`_ doesn't seem to well tested or used in nova, in fact it
+  has conditional imports around using `evzookeeper`_ so it doesn't seem like
+  this driver is well maintained/tested or integrated with in the nova code
+  base (ie ``evzookeeper`` isn't in the global requirements repo...) so
+  deprecating it would seem like a benefit for all.
+- It still does depend on having its ``is_up`` function being provided
+  a ``service_ref`` object that does appear to come from the database
+  initially (the entity calling that function will have at one point
+  fetched that ``service_ref`` object from the database ORM/conductor layer).
+  This mixed usage is IMHO confusing (and it will hopefully be adjusted
+  and made better when `detach-service-from-computenode`_ is completed) and
+  will likely mean that even if the tooz driver maintains the listing of
+  the groups that there will still be a connection into the ``services``
+  database table for this information for a period of time. We will have to
+  consider how to make this better after the initial migration and deprecation
+  work is completed...
+
+REST API impact
+---------------
+
+N/A
+
+Security impact
+---------------
+
+It doesn't appear that there would be any security impact as none of the
+current drivers (outside of ``db.py``) use any secured connection into the
+backend services (memcache doesn't have any security in the first place;
+although the ``mc.py`` driver is a mixed-mode driver anyway so this statement
+may not be completely fair, and zookeeper has security but it's not being
+used in ``zk.py``). The usage of tooz would have the potential to increase
+security (as tooz itself can configure and interact with those backends
+in a more secure manner when/if it implements said security interactions).
+
+Notifications impact
+--------------------
+
+N/A
+
+Other end user impact
+---------------------
+
+N/A
+
+Performance Impact
+------------------
+
+None expected, there will still need to be periodic heartbeats into memcache
+to ensure the memcache key is not expired, zookeeper clients (even using
+`kazoo`_ also need to periodically check-in with zookeeper) so nothing should
+be drastically different in this arena.
+
+Other deployer impact
+---------------------
+
+Migration plan/strategy
+~~~~~~~~~~~~~~~~~~~~~~~
+
+The other main deployer concern will be around what happens with the existing
+data that exists in zookeeper or memcache (excluding the ``db`` driver for now)
+when they switch from the existing drivers to this new tooz one. To accomodate
+this change in a way that does *not* cause service(s) to report downtime and/or
+disappear (since group membership will change to the tooz model) it would
+be advantegous to have or be able to run the old service group driver
+and the new tooz driver for a period of time (and then after this period has
+elapsed stop running the old service group driver).
+
+For example this would require a variation of the service group driver that
+would/could potentially look something like::
+
+    class MultiServiceGroupDriver(ServiceGroupDriver):
+        def __init__(self, backend_url,
+                           existing_backend, parallel_run_for=86400):
+            self._impls = [
+                existing_backend,
+                tooz.get_coordinator(backend_url),
+            ]
+            for be in self._impls:
+                be.start()
+            # How long we will be running the old and the new, instead of
+            # just running the new...
+            self.end_old = time.time() + parallel_run_for
+
+        def join(self, member_id, group_id, service=None):
+            # Find out which backends we should be running with...
+            if time.time() < self.end_old:
+                call_into = self._impls[:]
+            else:
+                call_into = self._impls[1:]
+            # Call into impl(s) to join (and handle/translate the
+            # appropriate errors); both drivers will be called into there
+            # join() methods; if either fails, a warning will be logged...
+
+        def is_up(self, member):
+            # Find out which backends we should be running with...
+            if time.time() < self.end_old:
+                call_into = self._impls[:]
+            else:
+                call_into = self._impls[1:]
+            # Call into impl(s) to check if the member is up... (and
+            # handle/translate the appropriate errors); if one of the drivers
+            # reports it is up while the other doesn't log a warning...
+
+        def leave(self, member_id, group_id):
+            # Find out which backends we should be running with...
+            if time.time() < self.end_old:
+                call_into = self._impls[:]
+            else:
+                call_into = self._impls[1:]
+            # Call into impl(s) to leave (and handle/translate the appropriate
+            # errors); if one of the drivers reports it is up while the other
+            # doesn't log a warning...
+
+        def get_all(self, group_id):
+            # Find out which backends we should be running with...
+            if time.time() < self.end_old:
+                call_into = self._impls[:]
+            else:
+                call_into = self._impls[1:]
+            # Call into impl(s) to get all the members (join the groups from
+            # the multiple backends if many) of the given group (and
+            # handle/translate appropriate errors).
+
+        def get_one(self, group_id):
+            # Find out which backends we should be running with...
+            if time.time() < self.end_old:
+                call_into = self._impls[:]
+            else:
+                call_into = self._impls[1:]
+            # Call into impl(s) to get one the members (join the groups from
+            # the multiple backends if many) of the given group (and
+            # handle/translate appropriate errors).
+
+The general idea would be that a driver implementation would be provided
+that would exist for this migration period/cycle (not as an offically supported
+driver), it would have the capabilities to internally use *two* drivers for
+providing the service group implementation (one driver would be the legacy
+driver and the secondary driver would be the tooz driver). During the migration
+period the functions that this driver would provide would read/save/write to
+both drivers (and combine results to make it appear as if one driver is being
+used). Then after a period of time (or a release cycle) this *multi* driver
+would be removed and/or stop proxying to both the old and new driver (and then
+the newer tooz driver would take over as the primary driver). This would allow
+for a more seamless transition by allowing the saved data to propagate to the
+tooz drivers backend before the tooz driver becomes the primary driver (thus
+avoiding the sudden transition and lack of data problem that would occur if
+the tooz driver was made the primary driver immediately).
+
+**TLDR:** This special and *temporary* implementation would be used to
+transition away from the old implementation and move the the new
+implementation; avoiding a sudden transition to the tooz driver that would
+likely cause services to stop reporting that they are up and running.
+
+A secondary/fallback option (which only really works for the zookeeper driver
+that exists) is to have tooz be able to read/write the data that the existing
+zookeeper driver reads/writes; this allows for a transition over to the new
+driver in a seamless manner (since no data format changes will occur). This
+would *not* work for the memcache or redis tooz driver since those drivers
+would alter the location & format of the data they are storing (they do not use
+the database like the ``mc.py`` and ``db.py`` drivers do). Overall I am not
+recommending this option since it is fragile, sensitive to data formats
+and it doesn't work for drivers that tooz has no way to support (aka any
+driver that uses the internal ``services`` database table mentioned above).
+
+Configs
+~~~~~~~
+
+There will need to be a new way to configure the service group driver using
+a url that is formatted according to how tooz expects it to be. An few example
+of this url format::
+
+    url = "kazoo://127.0.0.1:2181?timeout=5"
+    url = "memcached://localhost:11211?timeout=5"
+    url = "redis://localhost:6379?timeout=5"
+
+To configure this a new ``cfg.CONF`` option would need to be created (perhaps
+named ``servicegroup_coordination_url``) that would be used to configure what
+driver tooz uses internally and what options it provides to that
+driver. Depending on how the existing drivers are deprecated this should be
+mostly transparent and can be accomplished by making it possible to transition
+from the existing drivers to the tooz one in a manner that will work with
+those who use continuous deployment.
+
+Developer impact
+----------------
+
+- Nova developers may have to interact more with the oslo community (and
+  the tooz subcommunity) when learning, understanding, and integrating with
+  tooz.
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Currently throwing it up there, but likely candidates exist...
+
+Work Items
+----------
+
+#. Implement stated *new* tooz servicegroup driver that derives from the
+   existing ``ServiceGroupDriver`` base class, adding a new configuration
+   option ``servicegroup_coordination_url`` that is used to configure the
+   tooz backed driver.
+#. Create/add tests for tooz servicegroup driver.
+#. Add new ability to run two service group drivers for a given period of
+   time and then remove the *old* one after the period has elapsed (this
+   allows those who are using CD to switch over if they choose).
+#. Test that ability and ensure that it continues to work as expected.
+#. Document the new driver and migration pattern.
+#. Document and form some agreement on a depreciation strategy/plan for the
+   existing *legacy* drivers.
+#. Followup on said depreciation strategy/plan when appropriate.
+
+Dependencies
+============
+
+- Tooz
+
+*Current* dependencies (all but ``pymemcache`` are in the global
+requirements repo, this should be fixed before it becomes a further issue)::
+
+    pbr>=0.6,!=0.7,<1.0
+    Babel>=1.3
+    stevedore>=0.14
+    six>=1.7.0
+    iso8601
+    kazoo>=1.3.1
+    pymemcache>=1.2
+    zake>=0.1.6
+    msgpack-python
+    retrying!=1.3.0
+    futures>=2.1.6
+    oslo.utils>=1.0.0
+    redis>=2.10.0
+
+Testing
+=======
+
+- Tooz already has integration testing with memcache, redis, zookeeper so
+  there shouldn't be a need for nova to reimplement that
+  functionality/integration in its own internal testing suite.
+- Tempest tests will be used/added to test tooz + nova + [redis, memcache,
+  zookeeper] to ensure the whole system works together as expected.
+
+Documentation Impact
+====================
+
+- The *legacy* driver deprecation and associated migration strategy/plan
+  needs to be documented and explained to operators.
+- How to use the new driver and how to configure it needs to be documented
+  appropriatlely.
+
+References
+==========
+
+Tooz adoption by oslo:
+
+- https://review.openstack.org/#/c/122439/
+
+Tooz rtd:
+
+- http://tooz.readthedocs.org
+
+Others:
+
+- https://wiki.openstack.org/wiki/Oslo/blueprints/service-sync
+- http://specs.openstack.org/openstack/ceilometer-specs/specs/juno/central-agent-partitioning.html
+
+.. _detach-service-from-computenode: http://specs.openstack.org/openstack/nova-specs/specs/kilo/approved/detach-service-from-computenode.html
+.. _memcachedb: http://memcachedb.org/
+.. _tooz: https://pypi.python.org/pypi/tooz
+.. _evzookeeper: https://pypi.python.org/pypi/evzookeeper
+.. _kazoo: http://kazoo.readthedocs.org/
+.. _kilo priorities: https://github.com/openstack/nova-specs/blob/master/priorities/kilo-priorities.rst
+.. _zookeeper unit test: https://github.com/openstack/nova/blob/stable/juno/nova/tests/servicegroup/test_zk_driver.py
+.. _dogpile: https://review.openstack.org/#/c/124776/
+.. _clustering: http://redis.io/topics/cluster-spec
+.. _sharding: http://redis.io/topics/sentinel
-- 
1.9.1

