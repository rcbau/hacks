From c9282e9aba74ee72b48f3ca52f2860e34a90d576 Mon Sep 17 00:00:00 2001
From: Robert Li <baoli@cisco.com>
Date: Thu, 20 Nov 2014 12:45:00 -0500
Subject: [PATCH] Support live migration with macvtap SR-IOV

Change-Id: I23390c3e8e453e3f494f910bc7dd69866a255e51
Implements: blueprint sriov-live-migration
---
 specs/kilo/approved/sriov-live-migration.rst | 306 +++++++++++++++++++++++++++
 1 file changed, 306 insertions(+)
 create mode 100644 specs/kilo/approved/sriov-live-migration.rst

diff --git a/specs/kilo/approved/sriov-live-migration.rst b/specs/kilo/approved/sriov-live-migration.rst
new file mode 100644
index 0000000..d31d033
--- /dev/null
+++ b/specs/kilo/approved/sriov-live-migration.rst
@@ -0,0 +1,306 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+Support live migration with macvtap SR-IOV
+==========================================
+
+Include the URL of your launchpad blueprint:
+
+https://blueprints.launchpad.net/nova/+spec/sriov-live-migration
+
+In Juno release, SR-IOV is added into nova (Refer to [NOVA_SRIOV]_). To boot up
+an instance with neutron SR-IOV ports, the ports need to be first created with
+vnic_type being either direct or macvtap. We'll use the terms direct SR-IOV and
+macvtap SR-IOV, respectively, to differentiate the two vnic types in this spec.
+The port IDs can then be provided in the nova boot API. However, instances
+created with SR-IOV ports can not be live migrated in Juno.
+
+Problem description
+===================
+SR-IOV makes it possible for a VM to directly send packets to the VIC (virtual
+interface card) without having the hypervisor involved. In the case of direct
+SR-IOV, the physical VIC driver is running in the guest VM which directly
+controls the hardware. Live migration requires copying of VM's state including
+the hardware state from the source host to the destination host. Due to the
+lack of visibility of the SR-IOV hardware state in the hypervisor, however,
+live migration with direct SR-IOV is not possible without hypervisor change or
+using other techniques, such as that suggested in [EDWIN_LIVE]_ and
+[COMPS_LIVE]_.  Fortunately, libvirt supports live migration with macvtap
+SR-IOV. With macvtap SR-IOV, a macvtap interface residing on the host is
+directly connected to an ethernet interface that corresponds to a virtual
+function (VF) on the VIC. the VM runs a virtual ethernet driver (e.g.
+virtio_net) that delivers the packets to the macvtap interface that passes them
+directly to the VIC. With a bit of sacrifice of the packet latency and network
+thoroughput, it's possible to live migrate VMs that use it. In this spec, we'll
+focus on the support of live migration with macvtap SR-IOV. Live migration with
+direct SR-IOV may be addressed in the future.
+
+Also note that VMs with direct PCI passthrough devices can't be live migrated.
+
+Use Cases
+----------
+
+* As an operator, I want to be able to live migrate my instances with macvtap
+  SR-IOV ports
+
+Project Priority
+-----------------
+
+None
+
+Proposed change
+===============
+
+Check If The VM Is Live Migratable
+----------------------------------
+
+Given that a VM with direct SR-IOV and/or direct PCI passthrough devices can't
+be live migrated, when a user enters the request to live migrate such a VM, the
+request will be rejected with proper reason. Live migration will only be
+attempted on a VM without direct SR-IOV and/or direct PCI passthrough devices.
+
+Modify Domain XML With Target Interfaces
+----------------------------------------
+
+Libvirt allows a modified domain XML during live migration with serious
+restrictions. The configuration must include an identical set of virtual
+devices, to ensure a stable guest ABI (application binary interface) across
+migration. Only parameters related to host side configuration can be changed in
+the XML. Therefore, to live migrate a VM with libvirt, the VM's domain XML
+should not be modified to cause any non-migratable changes.
+
+Fortunately, if the network interfaces used on the target host are known before
+live migration takes place, the domain XML can be modified with the network
+interface names that will be used on the target host. The libvirt
+migrateToURI2() API supports such modification. And as a result, the instance
+will be succesfully brought up on the target host with those network
+interfaces.
+
+An example of the interface XML with macvtap SR-IOV in a domain XML::
+
+  <interface type='direct'>
+    <mac address='fa:16:3e:ae:7f:c5'/>
+    <source dev='eth20' mode='passthrough'/>
+    <model type='virtio'/>
+    <driver name='vhost'/>
+    <alias name='net0'/>
+    <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
+  </interface>
+
+Note that in the source dev clause above, eth20 is local to the compute node
+where the domain is hosted. The interface with the same name eth20 on the
+destination host may have already been used by other VMs. During
+pre_live_migration() (which is a RPC call from the source host to the
+destination host), the PCI device used on the target host will be allocated,
+and the corresponding interface name becomes known. This information can then
+be included in the migrate_data which is passed back to the source host as a
+result of pre_live_migration() RPC call. The source host then modifies the
+above domain xml and replaces the 'srouce dev' with the one used on the
+destination host.
+
+Note that an interface can be identified by its MAC address. The interface
+information carried in the migrate_data will be a dictionary keyed off by the
+interface's MAC address.
+
+Device/VIF Compatibility
+------------------------
+
+In a multi-vendor cloud environment, SR-IOV capable NIC adaptors may come from
+different vendors. Some adaptors support fabric extender (such as Cisco's
+VMFEX), others support hardware virtual ethernet bridge (such as Intel and
+Mellanox). Nova currently supports two VIF types for SR-IOV: VIF_TYPE_802_QBH
+and VIF_TYPE_HW_VEB. Each type has its own interface XML.
+
+With macvtap, the same virtio driver is used in the VM regardless of the VIF
+types. However, to be live migratable, stable guest ABI needs to be maintained
+across the migration.  This means, the PCI devices on the destination host must
+be compatible in terms of VIF types. Otherwise, network connectivity may get
+lost after migration. It's possible to migrate from a host with intel SR-IOV
+adpators to another with Mellanox's, or vice-versa. Currently with the existing
+scheduler, however, it's not possible to select the destination host based on
+compatible VIF types. It is possible, though, to relieve this restriction if
+all the SR-IOV adaptors in a cloud are VIF compatible. A nova boolean config
+option *ensure_sriov-vif-compatible* will be provided for this purpose.
+
+If *ensure_sriov-vif-compatible* is true, live migration is only possible with
+the same type (vendor_id/product_id) of SR-IOV capable NIC adaptors.
+
+In the future, with enhancement in the nova scheduler, the config option may be
+removed.
+
+PCI Requests, Scheduling and SR-IOV Device Allocation
+-----------------------------------------------------
+
+For live migration, the original PCI requests associated with the instance
+cannot be used as is to schedule the destination host and allocate SR-IOV
+devices on it with two reasons:
+
+* they don't contain information to ensure device compatibility
+* they don't contain information to correlate a request with a neutron port
+
+New PCI requests will be created with the flag *is_new* to be true. To ensure
+device compatiblity, and if *sriov-vif-compatible* is false, the original PCI
+requests are added with two new attributes if they are not present in the
+requests: vendor_id and product_id, which can be obtained from the neutron
+ports or PCI devices allocated for the original requests.
+
+The original PCI request ID can be used to correlate the allocated PCI device
+with its requested network. Refer to https://review.openstack.org/#/c/86606/
+for how a PCI request id is initially created. To correlate a PCI request and
+the allocated SR-IOV device to a neutron port, however, the original PCI
+request ID needs to be modified. The neutron port ID can be used as the new PCI
+request ID, and update to the PCI request with the new PCI request ID can be
+made while updating the neutron port with the PCI device information.
+
+To schedule the destination host, the newly-created PCI requests will be added
+into the filter properties right before the call to select_destinations. The
+newly-created PCI requests are also used to allocate SR-IOV devices in the
+pre_live_migration phase on the destination host.
+
+Based on the current live migration workflow, the new PCI requests for live
+migration don't need to be persisted into the database. The new PCI requests
+are saved as part of the instance object and passed along between the source
+and destination nodes. Regardless if the migration succeeds or fails, the PCI
+requests will be garbage collected together with the instance object.
+
+If migration to the destination host fails, SR-IOV devices allocated for the
+instance must be freed.
+
+Note that for migration (live or cold or resize), PCI devices are allocated on
+both the source host and the destination host at the same time. The instance
+object is associated with a list of PCI devices allocated for it. The access
+method to retrieve the PCI devices for an instance doesn't use host ID as
+filter. Therefore, it's possible the method will return PCI devcies on both the
+source and destination nodes. This is not a problem since the PCI request ID
+will make sure a correct PCI device is retrieved.
+
+PCI devices used for the VM being migrated on the source node will be freed as
+usual after the migration is completed.
+
+migrate_instance_finish
+-----------------------
+
+This method in the neutronv2 API module is invoked for instance resize, cold
+and live migration. In this method, neutron ports associated with the instance
+will be updated with the new binding information. This method will be enhanced
+so that PCI information is updated for SR-IOV ports as well.
+
+Note that as a result of updating the binding information which includes the
+new PCI information, operations on the hardware or remote devices may be taking
+place. Success of those operations are not guaranteed. This is an existing
+limitation in the current workflow to support migration, and is out of the
+scope of this specification.
+
+Alternatives
+------------
+
+With the technique of interface renaming, it's possible to live migrate VMs
+with macvtap SR-IOV interfaces without modifying the domain XML. An macvtap
+SR-IOV interface can be renamed using the neutron port id. This involves:
+
+* Renaming the interface and using the new name in the domain XML before the
+  domain is launched.
+* Renaming the target interface during pre_live_migration on the target host.
+
+Although the technique is pretty simple, there is no obvious benefit other than
+direct association of an interface with the VM that is using it (due to the
+naming convention).
+
+There was also discussion about using network XML. It is found, however, the
+running domain XML is not equal to the originally defined domain XML when
+network XMLs are used. The running doamin XML actually directly refers to
+interfaces that are allocated from the network XMLs. Therefore, the domain XML
+has to be modified before live migration.
+
+Data model impact
+-----------------
+
+N/A
+
+REST API impact
+---------------
+
+N/A
+
+Security impact
+---------------
+N/A
+
+Notifications impact
+--------------------
+
+N/A
+
+Other end user impact
+---------------------
+
+N/A
+
+Performance Impact
+------------------
+
+N/A
+
+Other deployer impact
+---------------------
+
+Deployers need to be aware of the limitations imposed with live migration in an
+SR-IOV capable cloud:
+
+* live migration is only possible with macvtap SR-IOV with a bit of sacrifice
+  of thoroughput compared with direct SR-IOV
+* live migration may only be performed with VIF-compatible SR-IOV capable
+  adaptors.
+
+Developer impact
+----------------
+
+N/A
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+  <baoli>
+
+Work Items
+----------
+
+* Check live migratability
+* PCI requests management
+* populate migrate_data with target interface information
+* SR-IOV device allocation
+* migrate_instance_finish
+
+Dependencies
+============
+
+N/A
+
+Testing
+=======
+
+In addition to develop unit tests in accordance to the existing live migration
+unit tests, if any, it should be tested by third party CIs.
+
+Documentation Impact
+====================
+
+Restrictions and the configuration option *ensure_sriov_vif_compatible* should
+be properly documented
+
+References
+==========
+
+.. [NOVA_SRIOV] `enable a nova instance to be booted up with neutron SRIOV ports <https://blueprints.launchpad.net/nova/+spec/pci-passthrough-sriov>`_
+
+.. [EDWIN_LIVE] `Live Migration with Pass-through Device for Linux VM <http://shikee.net/read/VM_OLS08.pdf>`_
+
+.. [COMPS_LIVE] `CompSC: Live Migration with Pass-through Devices <http://www.cl.cam.ac.uk/research/srg/netos/vee_2012/papers/p109.pdf>`_
-- 
1.9.1

