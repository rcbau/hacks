From f425f7baba83b8eabb77e52ad1a89253d4a0733b Mon Sep 17 00:00:00 2001
From: Vladik Romanovsky <vladik.romanovsky@enovance.com>
Date: Wed, 15 Oct 2014 14:36:43 -0400
Subject: [PATCH] libvirt: virtio-net multiqueue

Add support to enable the virtio-net multi queue.

blueprint libvirt-virtio-net-multiqueue

Change-Id: I0fbe9321889bdaa7e91c73e3fa9b19966c0f05d8
---
 specs/kilo/approved/libvirt-virtiomq.rst | 173 +++++++++++++++++++++++++++++++
 1 file changed, 173 insertions(+)
 create mode 100644 specs/kilo/approved/libvirt-virtiomq.rst

diff --git a/specs/kilo/approved/libvirt-virtiomq.rst b/specs/kilo/approved/libvirt-virtiomq.rst
new file mode 100644
index 0000000..45f1879
--- /dev/null
+++ b/specs/kilo/approved/libvirt-virtiomq.rst
@@ -0,0 +1,173 @@
+..
+   This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==========================================
+libvirt: virtio-net multiqueue
+==========================================
+
+https://blueprints.launchpad.net/nova/+spec/virtio-net-multiqueue
+
+This blueprint aims to enhance the libvirt driver to enable the virtio-net
+multiqueue. This will allow the guest instances to increase the total network
+throughput.
+
+Problem description
+===================
+
+Today's high-end servers have more processors, and guests running on them often
+have an increasing number of vCPUs. In single virtio-net queue, the scale of
+the protocol stack in a guest is restricted, as the network performance does
+not scale as the number of vCPUs increases. Guests cannot transmit or
+retrieve packets in parallel, as virtio-net has only one TX and RX queue.
+
+Multiqueue virtio-net provides the greatest performance benefit when:
+
+- Traffic packets are relatively large.
+- The guest is active on many connections at the same time, with traffic
+  running between guests, guest to host, or guest to an external system.
+- The number of queues is equal to the number of vCPUs. This is because
+  multi-queue support optimizes RX interrupt affinity and TX queue
+  selection in order to make a specific queue private to a specific vCPU.
+
+Although the virtio-net multiqueue feature provides a performance benefit,
+it has some limitations, therefor should not be unconditionally enabled:
+
+- Guest OS is limited to 256 MSI vectors. Each NIC queue requires a MSI vector.
+  Defining an instance with multiple virtio NICs and vCPUs might lead to a
+  possibility of hitting the guest MSI limit.
+- virtio-net multiqueue works well for incoming traffic, but can
+  occasionally cause a performance degradation, for outgoing traffic.
+- Enabling virtio-net multiqueue increases the total network throughput,
+  and in parallel increases the CPU consumption.
+- Enabling virtio-net multiqueue in the host QEMU config, does not enables
+  the functionality in the guest OS. The guest OS administrator needs to
+  manually turn it on for each guest NIC that wants it, using ethtool.
+- MSI vectors would still be consumed (wasted), if multiqueue was enabled
+  in the host, but has not been enabled in the guest OS by the administrator.
+- In cases when the number of vNICs in a guest instance is proportional to the
+  number of vCPUs, enabling the multiqueue feature is less important.
+- Each virtio-net queue consumes 64 KB of kernel memory for the vhost driver.
+
+
+Use Cases
+---------
+Guest instance users may benefit from an increased network
+performance and throughput.
+
+Project Priority
+-----------------
+None
+
+
+Proposed change
+===============
+
+In order to address the problem, an administrator should have the control
+over the paralleled packet processing by disabling the multiqueue support
+for certain workloads, where this feature may lead to a
+performance degradation.
+
+Introducing a new parameter to the flavor extra specs, for the administrator
+to control the virtio-net multiqueue feature.
+
+    hw:vif_multiqueue_permitted=true|false  (default true)
+
+Users will be able to set the number of queues as part of the network port
+properties.
+The default number of queues will match the number of vCPUs, defined for the
+instance.
+
+    nova boot -nic -port_id -port_queues=#
+
+NOTE: Virtio-net multiqueue should be enabled in the guest OS manually, using
+ethtool.
+
+Alternatives
+------------
+
+None
+
+Data model impact
+-----------------
+
+None
+
+REST API impact
+---------------
+
+None.
+
+Security impact
+---------------
+
+None.
+
+Notifications impact
+--------------------
+
+None.
+
+Other end user impact
+---------------------
+
+None.
+
+Performance Impact
+------------------
+
+None
+
+Other deployer impact
+---------------------
+
+None.
+
+Developer impact
+----------------
+
+None.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+Primary assignee:
+vladik.romanovsky@enovance.com
+
+Work Items
+----------
+
+* Update the vif config file to set the correct number of VCPUs to
+  virtio-net devices
+* Update the NetworkRequest object to add number of queues per port
+
+Dependencies
+============
+
+None
+
+
+Testing
+=======
+
+Unit tests
+Requires Libvirt >= 1.0.5
+
+Documentation Impact
+====================
+
+The new parameters in flavor extra specs and the new network port property
+"queues" in nova boot command.
+Some recommendations for an effective usage, should be mentioned as well.
+
+
+References
+==========
+
+http://www.linux-kvm.org/page/Multiqueue
+
-- 
1.9.1

