From b75eb4544dad0f91a1591e2aa37008ba5aed8a56 Mon Sep 17 00:00:00 2001
From: Smita Raut <smita.raut@in.ibm.com>
Date: Fri, 24 Oct 2014 22:00:57 +0530
Subject: [PATCH] Volume host affinity based VM placement

Blueprint: volume-host-affinity-based-vm-placement

Cinder scheduler schedules volume creation on certain host based on
scheduler filters in case of multi-node setup. For local volume
backends like LVM, the volume resides locally on the host where
volume create was scheduled.

For clustered filesystems like IBM General Parallel File System
(GPFS) which provides features like FPO (File Placement Optimizer),
the volume create command will be run on that host and volume will
be created locally if FPO is enabled. FPO is designed for efficiency
in case of Hadoop and MapReduce workloads. IBM GPFS Volume Driver
can be used to configure backends that use FPO enabled GPFS storage
pools.

There can be various use cases that leverage the volume locality
information, irrespective of cinder backend being used, and
instantiate the VM closer to where the volume resides.

Change-Id: I53cc43f854e4004b21838b574221c7d154bdd154
---
 .../volume-host-affinity-based-vm-placement.rst    | 251 +++++++++++++++++++++
 1 file changed, 251 insertions(+)
 create mode 100644 specs/kilo/approved/volume-host-affinity-based-vm-placement.rst

diff --git a/specs/kilo/approved/volume-host-affinity-based-vm-placement.rst b/specs/kilo/approved/volume-host-affinity-based-vm-placement.rst
new file mode 100644
index 0000000..b6673f2
--- /dev/null
+++ b/specs/kilo/approved/volume-host-affinity-based-vm-placement.rst
@@ -0,0 +1,251 @@
+..
+   This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+=======================================
+Volume host affinity based VM placement
+=======================================
+
+https://blueprints.launchpad.net/nova/+spec
+/volume-host-affinity-based-vm-placement
+
+Cinder scheduler schedules volume creation on certain host based on scheduler
+filters in case of multi-node setup. For local volume backends like LVM, the
+volume resides locally on the host where volume create was scheduled.
+
+For clustered filesystems like IBM General Parallel File System (GPFS) which
+provides features like FPO (File Placement Optimizer), the volume create
+command will be run on that host and volume will be created locally if FPO
+is enabled. (FPO ensures that the node writing data directs the write to its
+own node for the first copy, and to the disks in other nodes for the second
+and third copy (if specified). This ensures that the entire file resides on
+local disks of one node. FPO is designed for efficiency in case of Hadoop
+and MapReduce workloads. IBM GPFS Volume Driver can be used to configure
+backends that use FPO enabled GPFS storage pools. It also supports file
+level FPO configuration using which one can ensure that the cinder volume
+being created resides completely on one node and is not striped across
+multiple nodes of GPFS cluster.)
+
+There can be various use cases that leverage the volume locality information,
+irrespective of cinder backend being used, and instantiate the VM closer to
+where the volume resides.
+
+
+Problem description
+===================
+
+Use Cases
+---------
+
+If a bootable cinder volume is created that resides completely on a single
+host, then it would be much efficient if the VM is instantiated on the same
+host where the volume resides.
+
+1. A bootable cinder volume is created e.g. LVM volume, GPFS FPO enabled
+   volume.
+2. Create a VM instance from this bootable volume using "nova boot". The
+   instance should    be created on the same host where the volume is
+   residing locally.
+
+
+If the VM that is using cinder volumes with GPFS backend is intended to be
+used for workloads like Hadoop and MapReduce, efficiency of these analytics
+software would be much better if the VM and the cinder volume resides on the
+same host locally. This can be achieved if the VM is instantiated on a node
+where the user data volume resides. This use case is to create a VM instance
+using "nova boot" on the same host where the given user data volume resides.
+This can be used with LVM too.
+
+Project Priority
+-----------------
+
+None
+
+
+Proposed change
+===============
+
+
+Write a new nova scheduler filter:
+
+Nova scheduler filters give a filtered list of hosts which can be candidates
+for VM instance creation. A new filter called VolumeHostAffinityFilter will
+be written that will filter hosts based on where the cinder volume create
+was scheduled to run and return only one host. This host will be used for
+VM instance creation.
+
+
+New scheduler hint:
+
+A new scheduler hint called as "volume_host_affinity" needs to be passed in
+"nova boot" command. The VolumeHostAffinityFilter functions will handle this
+hint. Its possible values could be "boot_volume" or volume_id.
+
+1. "--hint volume_host_affinity=boot_volume" will create the VM instance on
+   the same node where the bootable volume was created.
+2. "--hint volume_host_affinity=<volume_id>" will create the VM instance on
+   the same node where the given volume was created. This is useful when you
+   want to have the VM instance and the user data disk residing on same node
+   especially for hadoop and MapReduce kind of workloads for which FPO is
+   designed.
+
+
+Examples:
+
+1. nova boot --boot-volume 67e34cfd-4768-4d49-ae4c-308fb3535eb4 --flavor 1
+   --hint volume_host_affinity=boot_volume instance1
+2. nova boot --boot-volume 67e34cfd-4768-4d49-ae4c-308fb3535eb4 --flavor 1
+   --hint volume_host_affinity=a4ef8299-4dbd-44e3-8ea4-764e15e667f5 instance2
+
+
+Class VolumeHostAffinityFilter:
+
+A class "VolumeHostAffinityFilter" will be defined in
+nova/scheduler/filters/affinity_filter.py and host_passes() function
+implemented. This will identify the host on which the volume was created.
+Cinder attribute "os-vol-host-attr:host" will be used to get the information
+about host that was used for volume creation.
+
+
+Add "host" to volume summary:
+
+nova/volume/cinder.py will be updated so that
+_untranslate_volume_summary_view() also adds "host" attribute to the volume
+dictionary. This is what is used in VolumeHostAffinityFilter.
+
+Alternatives
+------------
+
+This could be achieved by passing the host to nova boot command into
+--availability-zone as "<zone>:<host>". Host can be found using
+os-vol-host-attr:host attribute from "cinder show" command. Extract the host
+part from its value. Its zone could be determined using "nova host-list".
+
+
+Data model impact
+-----------------
+
+None
+
+
+REST API impact
+---------------
+
+None
+
+
+Security impact
+---------------
+
+None
+
+Notifications impact
+--------------------
+
+None
+
+Other end user impact
+---------------------
+
+None
+
+Performance Impact
+------------------
+
+- Instantiating the VM on host where the bootable volume resides locally will
+  improve the VM boot performance.
+- Having the VM on the same host where user data volume resides will improve
+  performance of analytics workload.
+
+Other deployer impact
+---------------------
+
+Following configuration needs to be added in nova.conf of controller node to
+enable this filter-
+
+"scheduler_available_filters
+      =nova.scheduler.filters.affinity_filter.VolumeHostAffinityFilter"
+
+To add this filter to default filters list, it needs to be added to
+scheduler_default_filters in nova.conf
+
+Backends whose storage does not reside on the host should not use this filter.
+
+Developer impact
+----------------
+
+None
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Primary assignee:
+
+    smita-raut <smita.raut@in.ibm.com>
+
+Other contributors:
+
+    nilesh-bhosale <nilesh.bhosale@in.ibm.com>
+
+
+Work Items
+----------
+
+* Implement a new class for scheduler filter called VolumeHostAffinityFilter
+  (details given in "Proposed Change" section)
+* Update _untranslate_volume_summary_view() function to include host attribute
+  (details given in "Proposed Change" section)
+* Write test cases
+  (Test cases identified in "Testing" section)
+
+
+Dependencies
+============
+
+-- This is a use case extension over the blueprint -
+https://blueprints.launchpad.net/cinder/+spec/gpfs-vm-placement
+
+
+Testing
+=======
+
+Unit Tests: Will be included
+
+Tempest tests:
+
+No additional testcases needs to be written, this feature can be tested with
+existing tempest
+
+
+Documentation Impact
+====================
+
+1. compute-scheduler documentation needs to be updated to include this
+   configuration to enable VolumeHostAffinityFilter-
+
+"scheduler_available_filters
+   =nova.scheduler.filters.affinity_filter.VolumeHostAffinityFilter"
+
+2. compute-scheduler documentation needs to be updated to give details of
+   VolumeHostAffinityFilter and document the new scheduler hint
+   "volume_host_affinity"
+
+
+References
+==========
+
+1. http://docs.openstack.org/trunk/config-reference/content/GPFS-driver.html
+2. http://docs.openstack.org/training-guides/content
+   /associate-computer-node.html
+3. http://devstack.org/guides/multinode-lab.html
+4. http://www-01.ibm.com/support/knowledgecenter/SSFKCN_4.1.0
+   /com.ibm.cluster.gpfs.v4r1.gpfs200.doc/bl1adv_fposettings.htm
+5. http://docs.openstack.org/trunk/config-reference/content
+   /section_compute-scheduler.html
+
-- 
1.9.1

