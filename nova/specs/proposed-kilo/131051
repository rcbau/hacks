From b62a1e23adb3184aef6c412120e9ed940c44f87d Mon Sep 17 00:00:00 2001
From: Smita Raut <smita.raut@in.ibm.com>
Date: Mon, 27 Oct 2014 14:15:08 +0530
Subject: [PATCH] Volume host affinity based VM placement

Blueprint: volume-host-affinity-based-vm-placement

Cinder scheduler schedules volume creation on certain host based on
scheduler filters in case of multi-node setup. For local volume
backends like LVM, the volume resides locally on the host where
volume create was scheduled.

For clustered filesystems like IBM General Parallel File System
(GPFS) which provides features like FPO (File Placement Optimizer),
the volume create command will be run on that host and volume will
be created locally if FPO is enabled. FPO is designed for efficiency
in case of Hadoop and MapReduce workloads. IBM GPFS Volume Driver
can be used to configure backends that use FPO enabled GPFS storage
pools.

There can be various use cases that leverage the volume locality
information, irrespective of cinder backend being used, and
instantiate the VM closer to where the volume resides.

Addressed following review comments-
- Wrapped long lines
- Removed trailing spaces
- Removed the dependency that boot volumes should be FPO enabled
- Added performance impact in "Performance Impact" section

Change-Id: I12f4729d559a7d89c451da0b1196af00b7b27dc6
---
 .../volume-host-affinity-based-vm-placement.rst    | 178 ++++++++++++++-------
 1 file changed, 120 insertions(+), 58 deletions(-)

diff --git a/specs/kilo/volume-host-affinity-based-vm-placement.rst b/specs/kilo/volume-host-affinity-based-vm-placement.rst
index e0f34a0..25e843e 100644
--- a/specs/kilo/volume-host-affinity-based-vm-placement.rst
+++ b/specs/kilo/volume-host-affinity-based-vm-placement.rst
@@ -1,84 +1,119 @@
 =======================================
-
 Volume host affinity based VM placement
-
 =======================================
 
-https://blueprints.launchpad.net/nova/+spec/volume-host-affinity-based-vm-placement
- 
+https://blueprints.launchpad.net/nova/+spec
+/volume-host-affinity-based-vm-placement
 
-Cinder scheduler schedules volume creation on certain host based on scheduler filters in case of multi-node setup. For local volume backends like LVM, the volume resides locally on the host where volume create was scheduled.
+Cinder scheduler schedules volume creation on certain host based on scheduler
+filters in case of multi-node setup. For local volume backends like LVM, the
+volume resides locally on the host where volume create was scheduled.
 
-For clustered filesystems like IBM General Parallel File System (GPFS) which provides features like FPO (File Placement Optimizer), the volume create command will be run on that host and volume will be created locally if FPO is enabled. (FPO ensures that the node writing data directs the write to its own node for the first copy, and to the disks in other nodes for the second and third copy (if specified). This ensures that the entire file resides on local disks of one node. FPO is designed for efficiency in case of Hadoop and MapReduce workloads. IBM GPFS Volume Driver can be used to configure backends that use FPO enabled GPFS storage pools. It also supports file level FPO configuration using which one can ensure that the cinder volume being created resides completely on one node and is not striped across multiple nodes of GPFS cluster.)
+For clustered filesystems like IBM General Parallel File System (GPFS) which
+provides features like FPO (File Placement Optimizer), the volume create
+command will be run on that host and volume will be created locally if FPO
+is enabled. (FPO ensures that the node writing data directs the write to its
+own node for the first copy, and to the disks in other nodes for the second
+and third copy (if specified). This ensures that the entire file resides on
+local disks of one node. FPO is designed for efficiency in case of Hadoop
+and MapReduce workloads. IBM GPFS Volume Driver can be used to configure
+backends that use FPO enabled GPFS storage pools. It also supports file
+level FPO configuration using which one can ensure that the cinder volume
+being created resides completely on one node and is not striped across
+multiple nodes of GPFS cluster.)
 
-There can be various use cases that leverage the volume locality information, irrespective of cinder backend being used, and instantiate the VM closer to where the volume resides.
+There can be various use cases that leverage the volume locality information,
+irrespective of cinder backend being used, and instantiate the VM closer to
+where the volume resides.
 
- 
 
 ========================
 Problem description
 ========================
 
 Use Cases:
------------
-
-If a bootable cinder volume is created that resides completely on a single host, then it would be much efficient if the VM is instantiated on the same host where the volume resides.
+----------------------
 
-1. A bootable cinder volume is created e.g. LVM volume, GPFS FPO enabled volume.
+If a bootable cinder volume is created that resides completely on a single
+host, then it would be much efficient if the VM is instantiated on the same
+host where the volume resides.
 
-2. Create a VM instance from this bootable volume using "nova boot". The instance should be created on the same host where the volume is residing locally.
+1. A bootable cinder volume is created e.g. LVM volume, GPFS FPO enabled
+   volume.
+2. Create a VM instance from this bootable volume using "nova boot". The
+   instance should    be created on the same host where the volume is
+   residing locally.
 
 
-If the VM that is using cinder volumes with GPFS backend is intended to be used for workloads like Hadoop and MapReduce, efficiency of these analytics software would be much better if the VM and the cinder volume resides on the same host locally. This can be achieved if the VM is instantiated on a node where the user data volume resides. This use case is to create a VM instance using "nova boot" on the same host where the given user data volume resides. This can be used with LVM too.
-
- 
-Project Priority
------------------
-
-None
+If the VM that is using cinder volumes with GPFS backend is intended to be
+used for workloads like Hadoop and MapReduce, efficiency of these analytics
+software would be much better if the VM and the cinder volume resides on the
+same host locally. This can be achieved if the VM is instantiated on a node
+where the user data volume resides. This use case is to create a VM instance
+using "nova boot" on the same host where the given user data volume resides.
+This can be used with LVM too.
 
 
 =========================
 Proposed change
 =========================
 
- 
+
 Write a new nova scheduler filter:
 
-Nova scheduler filters give a filtered list of hosts which can be candidates for VM instance creation. A new filter called VolumeHostAffinityFilter will be written that will filter hosts based on where the cinder volume create was scheduled to run and return only one host. This host will be used for VM instance creation.
+Nova scheduler filters give a filtered list of hosts which can be candidates
+for VM instance creation. A new filter called VolumeHostAffinityFilter will
+be written that will filter hosts based on where the cinder volume create
+was scheduled to run and return only one host. This host will be used for
+VM instance creation.
+
 
- 
 New scheduler hint:
 
-A new scheduler hint will be defined called as "volume_host_affinity" in "nova boot" command. Its possible values could be "boot_volume" or volume_id.
+A new scheduler hint will be defined called as "volume_host_affinity" in
+"nova boot" command. Its possible values could be "boot_volume" or volume_id.
 
-1. "--hint volume_host_affinity=boot_volume" will create the VM instance on the same node where the bootable volume was created.
-2. "--hint volume_host_affinity=<volume_id>" will create the VM instance on the same node where the given volume was created. This is useful when you want to have the VM instance and the user data disk residing on same node especially for hadoop and MapReduce kind of workloads for which FPO is designed.
+1. "--hint volume_host_affinity=boot_volume" will create the VM instance on
+   the same node where the bootable volume was created.
+2. "--hint volume_host_affinity=<volume_id>" will create the VM instance on
+   the same node where the given volume was created. This is useful when you
+   want to have the VM instance and the user data disk residing on same node
+   especially for hadoop and MapReduce kind of workloads for which FPO is
+   designed.
 
-The bootable volume or the volume being used in this hint must be FPO enabled.
 
- 
 Examples:
 
-1. nova boot --boot-volume 67e34cfd-4768-4d49-ae4c-308fb3535eb4 --flavor 1 --hint volume_host_affinity=boot_volume instance1
-2. nova boot --boot-volume 67e34cfd-4768-4d49-ae4c-308fb3535eb4 --flavor 1 --hint volume_host_affinity=a4ef8299-4dbd-44e3-8ea4-764e15e667f5 instance2
+1. nova boot --boot-volume 67e34cfd-4768-4d49-ae4c-308fb3535eb4 --flavor 1
+   --hint volume_host_affinity=boot_volume instance1
+2. nova boot --boot-volume 67e34cfd-4768-4d49-ae4c-308fb3535eb4 --flavor 1
+   --hint volume_host_affinity=a4ef8299-4dbd-44e3-8ea4-764e15e667f5 instance2
+
 
- 
 Class VolumeHostAffinityFilter:
 
-A class "VolumeHostAffinityFilter" will be defined in nova/scheduler/filters/affinity_filter.py and host_passes() function implemented. This will identify the host on which the volume was created. Cinder attribute "os-vol-host-attr:host" will be used to get the information about host that was used for volume creation.
+A class "VolumeHostAffinityFilter" will be defined in
+nova/scheduler/filters/affinity_filter.py and host_passes() function
+implemented. This will identify the host on which the volume was created.
+Cinder attribute "os-vol-host-attr:host" will be used to get the information
+about host that was used for volume creation.
+
 
- 
 Add "host" to volume summary:
 
-nova/volume/cinder.py will be updated so that _untranslate_volume_summary_view() also adds "host" attribute to the volume dictionary. This is what is used in VolumeHostAffinityFilter.
+nova/volume/cinder.py will be updated so that
+_untranslate_volume_summary_view() also adds "host" attribute to the volume
+dictionary. This is what is used in VolumeHostAffinityFilter.
 
 
 ==================
 Alternatives
 ==================
 
-This could be achieved by passing the host to nova boot command into --availability-zone as "<zone>:<host>". Host can be found using os-vol-host-attr:host attribute from "cinder show" command. Extract the host part from its value. Its zone could be determined using "nova host-list".
+This could be achieved by passing the host to nova boot command into
+--availability-zone as "<zone>:<host>". Host can be found using
+os-vol-host-attr:host attribute from "cinder show" command. Extract the host
+part from its value. Its zone could be determined using "nova host-list".
 
 
 =====================
@@ -87,7 +122,6 @@ Data model impact
 
 None
 
- 
 
 ====================
 REST API impact
@@ -95,7 +129,6 @@ REST API impact
 
 None
 
- 
 
 ==================
 Security impact
@@ -103,7 +136,6 @@ Security impact
 
 None
 
- 
 
 =======================
 Notifications impact
@@ -111,21 +143,26 @@ Notifications impact
 
 None
 
- 
 
 ======================
 Performance Impact
 ======================
 
-None 
+- Instantiating the VM on host where the bootable volume resides locally will
+  improve the VM boot performance.
+- Having the VM on the same host where user data volume resides will improve
+  performance of analytics workload.
+
 
 =========================
 Other deployer impact
 =========================
 
-Following configuration needs to be added in nova.conf of controller node to enable this filter-
+Following configuration needs to be added in nova.conf of controller node to
+enable this filter-
 
-"scheduler_available_filters=nova.scheduler.filters.affinity_filter.VolumeHostAffinityFilter"
+"scheduler_available_filters
+      =nova.scheduler.filters.affinity_filter.VolumeHostAffinityFilter"
 
 
 =======================
@@ -134,6 +171,7 @@ Developer impact
 
 None
 
+
 ======================
 Implementation
 ======================
@@ -149,50 +187,74 @@ Other contributors:
 
     None
 
+
 Work Items:
 
--- Implement a new class for scheduler filter called VolumeHostAffinityFilter (details given in "Proposed Change" section)
--- Update _untranslate_volume_summary_view() function to include host attribute (details given in "Proposed Change" section)
--- Write test cases (Test cases identified in "Testing" section) 
+-- Implement a new class for scheduler filter called VolumeHostAffinityFilter
+   (details given in "Proposed Change" section)
+-- Update _untranslate_volume_summary_view() function to include host attribute
+   (details given in "Proposed Change" section)
+-- Write test cases
+   (Test cases identified in "Testing" section)
+
 
 ==================
 Dependencies
 ==================
 
--- This is a use case extension over the blueprint - https://blueprints.launchpad.net/cinder/+spec/gpfs-vm-placement
+-- This is a use case extension over the blueprint -
+https://blueprints.launchpad.net/cinder/+spec/gpfs-vm-placement
 
 
 ===============
 Testing
 ===============
 
-* Unit Tests: Existing test_filter_scheduler.py will be enhanced to contain more test cases for Volume Host Affinity Filter. Following test cases will be added-
+Unit Tests:
+
+Existing test_filter_scheduler.py will be enhanced to contain more test cases
+for Volume Host Affinity Filter. Following test cases will be added-
 
-   1. test_affinity_volume_host_filter_passes: Positive test case for when the filter matches
-   2. test_affinity_volume_host_filter_no_value_passes: Test for filter when no hint value is passed
-   3. test_affinity_volume_host_filter_fails: Test that filter fails appropriately in non-matching condition
-   4. test_affinity_volume_host_filter_handles_none: Test that the filter handles the case of no scheduler hints
+   1. test_affinity_volume_host_filter_passes: Positive test case for when
+      the filter matches
+   2. test_affinity_volume_host_filter_no_value_passes: Test for filter when
+      no hint value is passed
+   3. test_affinity_volume_host_filter_fails: Test that filter fails
+      appropriately in non-matching condition
+   4. test_affinity_volume_host_filter_handles_none: Test that the filter
+      handles the case of no scheduler hints
 
-* Tempest tests - No additional testcases needs to be written, this feature can be tested with existing tempest
+Tempest tests:
+
+No additional testcases needs to be written, this feature can be tested with
+existing tempest
 
 
 =========================
 Documentation Impact
 =========================
 
-1. compute-scheduler documentation needs to be updated to include this configuration to enable VolumeHostAffinityFilter-
+1. compute-scheduler documentation needs to be updated to include this
+   configuration to enable VolumeHostAffinityFilter-
+
+"scheduler_available_filters
+   =nova.scheduler.filters.affinity_filter.VolumeHostAffinityFilter"
 
-"scheduler_available_filters=nova.scheduler.filters.affinity_filter.VolumeHostAffinityFilter"
+2. compute-scheduler documentation needs to be updated to give details of
+   VolumeHostAffinityFilter and document the new scheduler hint
+   "volume_host_affinity"
 
-2. compute-scheduler documentation needs to be updated to give details of VolumeHostAffinityFilter and document the new scheduler hint "volume_host_affinity" 
 
 ==================
 References
 ==================
 
 1. http://docs.openstack.org/trunk/config-reference/content/GPFS-driver.html
-2. http://docs.openstack.org/training-guides/content/associate-computer-node.html
+2. http://docs.openstack.org/training-guides/content
+   /associate-computer-node.html
 3. http://devstack.org/guides/multinode-lab.html
-4. http://www-01.ibm.com/support/knowledgecenter/SSFKCN_4.1.0/com.ibm.cluster.gpfs.v4r1.gpfs200.doc/bl1adv_fposettings.htm
-5. http://docs.openstack.org/trunk/config-reference/content/section_compute-scheduler.html
+4. http://www-01.ibm.com/support/knowledgecenter/SSFKCN_4.1.0
+   /com.ibm.cluster.gpfs.v4r1.gpfs200.doc/bl1adv_fposettings.htm
+5. http://docs.openstack.org/trunk/config-reference/content
+   /section_compute-scheduler.html
 
-- 
1.9.1

