From 22c75ecf2ad63144f8b5d29c99fb07057bc02ba8 Mon Sep 17 00:00:00 2001
From: Jay Pipes <jaypipes@gmail.com>
Date: Sun, 20 Sep 2015 20:58:18 -0700
Subject: [PATCH] WIP: Add concept of resource providers

We need a system that can properly account for resource capacity and
usage amounts, including when the deployment uses shared resource pools
like shared storage for ephemeral instance disks.

Change-Id: If6107a41152d8b085ba176d27f64763e4f77fcd6
Blueprint: resource-providers
---
 specs/mitaka/approved/resource-providers.rst | 534 +++++++++++++++++++++++++++
 1 file changed, 534 insertions(+)
 create mode 100644 specs/mitaka/approved/resource-providers.rst

diff --git a/specs/mitaka/approved/resource-providers.rst b/specs/mitaka/approved/resource-providers.rst
new file mode 100644
index 0000000..e9d0241
--- /dev/null
+++ b/specs/mitaka/approved/resource-providers.rst
@@ -0,0 +1,534 @@
+..
+ This work is licensed under a Creative Commons Attribution 3.0 Unported
+ License.
+
+ http://creativecommons.org/licenses/by/3.0/legalcode
+
+==================
+Resource Providers
+==================
+
+https://blueprints.launchpad.net/nova/+spec/resource-providers
+
+This blueprint aims to address the problem of Nova assuming all resources are
+provided by a single compute node by introducing a new concept -- a resource
+provider -- that will allow Nova to accurately track and reserve resources
+regardless of whether the resource is being exposed by a single compute node
+or some shared pool or service.
+
+Problem description
+===================
+
+Within a cloud deployment, there are a number of resources that may be consumed
+by a user. Some resource types are provided by a compute node; these types of
+resources include CPU, memory, PCI devices and local ephemeral disk. Other
+types of resources, however, are not provided by a compute node, but instead
+are provided by some external resource pool. An example of such a resource
+would be a shared storage pool like that provided by Ceph or an NFS share.
+
+Unfortunately, due to legacy reasons, Nova only thinks of resources as being
+provided by a compute node. The tracking of resources assumes that it is the
+compute node that provides the resource, and therefore when reporting usage of
+certain resources, Nova naively calculates resource usage and availability by
+simply summing amounts across all compute nodes in its database. This ends up
+causing a number of problems [1] with usage and capacity amounts being
+incorrect.
+
+Use Cases
+----------
+
+As a deployer that has chosen to use a shared storage solution for storing
+instance ephemeral disks, I want Nova and Horizon to report the correct
+usage and capacity information.
+
+Project Priority
+-----------------
+
+None
+
+Proposed change
+===============
+
+Add tables to the Nova database that represent the following concepts:
+
+`resource_classes` is Lookup table for different types of resources::
+
+    CREATE TABLE resource_classes (
+        id INT UNSIGNED NOT NULL AUTOINCREMENT PRIMARY KEY,
+        name VARCHAR(40) NOT NULL,
+        UNIQUE INDEX (name)
+    );
+
+A set of resource classes should be populated::
+
+    INSERT INTO resource_classes (id, name) VALUES
+    (1, 'vcpus'),
+    (2, 'memory_mb'),
+    (3, 'disk_gb'),
+    (4, 'pci_device_normal'),
+    (5, 'pci_device_sriov_vf'),
+    (6, 'numa_topology');
+
+We are going to need a lookup table for the IDs of various resource
+providers in the system, too. We'll call this lookup table
+`resource_providers`::
+
+    CREATE TABLE resource_providers (
+        id INT UNSIGNED NOT NULL AUTOINCREMENT PRIMARY KEY,
+        uuid VARCAHR (64) NOT NULL,
+        INDEX (uuid)
+    );
+
+We will want to populate this lookup table with a record for each
+compute node in the system, since compute nodes are providers of
+some types of resources. However, the `compute_nodes` table
+currently does not have a UUID column, so we will add one::
+
+    ALTER TABLE compute_nodes ADD COLUMN uuid VARCHAR(64) NULL;
+    UPDATE compute_nodes SET uuid = UUID4();
+    ALTER TABLE compute_nodes MODIFY COLUMN uuid uuid VARCHAR(64) NOT NULL;
+
+And we will now add a record to the `resource_providers` table for
+each compute node in the system::
+
+    INSERT INTO resource_providers (uuid)
+    SELECT uuid FROM compute_nodes;
+
+`resource_pools` is a simple table of resource providers that expose a pool
+of some resource class::
+
+    CREATE TABLE resource_pools (
+        uuid VARCHAR(64) NOT NULL PRIMARY KEY,
+        name VARCHAR(200) NOT NULL,
+        resource_class INT UNSIGNED NOT NULL
+    );
+
+Imagine that the Nova deployment had a collection of compute nodes that
+did *not* provide instances with any local disk, and instead used a shared
+storage pool for housing the instance's ephemeral disks.
+
+We would first create a host aggregate that contained all such compute
+nodes. Then, we would create a resource pool for the shared storage::
+
+    INSERT INTO resource_pools (uuid, name, resource_class) VALUES
+    (UUID4(), 'my shared storage pool', 3);
+    # Note that 3 is the resource class for disk_gb
+
+We would then need to indicate that this resource pool provides disk
+resources to the host aggregate containing our compute nodes. For this,
+we will need a new many-to-many mapping table,
+`host_aggregate_resource_pools`::
+
+    CREATE TABLE host_aggregate_resource_pools (
+        host_aggregate_id INT UNSIGNED NOT NULL,
+        resource_pool_id INT UNSIGNED NOT NULL,
+        PRIMARY KEY (host_aggregate_id, resource_pool_id),
+        INDEX (resource_pool_id)
+    );
+
+and insert a record into this table to attach the host aggregate to the
+shared storage pool::
+
+    INSERT INTO host_aggregate_resource_pools
+    (host_aggregate_id, resource_pool_id) VALUES
+    (<host_agg>, <resource_pool>);
+
+An `inventories` table records the amount of a particular resource that is
+provided by a particular resource provider::
+
+    CREATE TABLE inventories (
+        id INT UNSIGNED NOT NULL AUTOINCREMENT PRIMARY KEY,
+        provider_id INT UNSIGNED NOT NULL,
+        resource_class INT UNSIGNED NOT NULL,
+        total INT UNSIGNED NOT NULL,
+        min_unit INT UNSIGNED NOT NULL,
+        max_unit INT UNSIGNED NOT NULL,
+        INDEX (provider_id),
+        INDEX (resource_class)
+    );
+
+The `min_unit` and `max_unit` fields shall store "limits" information for the
+type of resource. This information is necessary to ensure that a request for
+more or fewer resource that can be provided as a single unit will not be
+accepted. As an example, let us say that a particular compute node has two
+quad-core Xeon processors, providing 8 total physical cores. Even though the
+cloud administrator may have set the `cpu_allocation_ratio` to 16
+(the default), the compute node cannot accept requests for instances needing
+more than 8 vCPUs. So, while there may be 128 total vCPUs available on the
+compute node, the `min_unit` would be set to 1 and the `max_unit` would be
+set to `8` in order to prevent unacceptable matching of resources to requests.
+
+Each compute node will be have a set of records inserted into the `inventories`
+table, one record for each type of resource the compute node provides. The
+`provider_id` field value shall be the ID from the `resource_providers` table
+for the compute node's UUID. For compute nodes that provide no local disk to
+instances, there will be no record for the resource class `3`, which is the
+`disk_gb` resource class.
+
+In order to track resources that have been assigned and used by some consumer
+of that resource, we need an `allocations` table. Records in this table
+indicate the amount of a particular resource that has been allocated, reserved,
+and used by some given consumer of that resource out of a particular resource
+provider::
+
+    CREATE TABLE allocations (
+        id INT UNSIGNED NOT NULL AUTOINCREMENT PRIMARY KEY,
+        provider_id INT UNSIGNED NOT NULL,
+        consumer_id VARCHAR(64) NOT NULL,
+        resource_class INT UNSIGNED NOT NULL,
+        allocated INT UNSIGNED NOT NULL,
+        used INT UNSIGNED NOT NULL,
+        INDEX (provider_id),
+        INDEX (consumer_id),
+        INDEX (resource_class)
+    );
+
+When a consumer of a particular resource claims resources from a provider,
+a record is inserted into to the `allocations` table.
+
+Let's assume that a cloud user has initiated a boot request for a server that
+needs 4 vCPUs, 16GB of RAM, and 200GB of disk.
+
+The scheduler will need to query the database to determine compute nodes that
+have the available resources to meet the request. Though this can be done in
+a single SQL statement, we will assume here that the scheduler will use its
+current naive implementation, which queries all resource usage information
+for all compute nodes for each iteration of the placement algorithm::
+
+    SELECT provider_id, resource_class, amount, min_unit, max_unit
+    FROM inventories;
+    SELECT provider_id, resource_class, SUM(allocated) as total_allocated
+    FROM allocations;
+
+From this information, the scheduler would determine the resource providers
+that had enough vCPUs, memory and disk to satisfy the request. Continuing the
+example from above, none of the compute nodes has any local disk storage that
+it can provide disk for the new instance. However, it belongs to a host
+aggregate that *does* have a shared storage pool attached to it that has
+available disk that it can provide.
+
+The scheduler would be looping through compute nodes and determining whether
+the node was able to provide the requested vCPUs, memory and disk. The
+scheduler code for matching requested resources to available resources might
+look something like this::
+
+    for compute_node in compute_nodes:
+        cn_resources = compute_node.get_inventories()
+        if resources.CPU in cn_resources:
+            if not cn_resources[resources.CPU].has_room_for(4):
+                continue
+        else:
+            continue
+        if resources.RAM in cn_resources:
+            if not cn_resources[resources.RAM].has_room_for(16):
+                continue
+        else:
+            continue
+        if resources.DISK in cn_resources:
+            if not cn_resources[resources.DISK].has_room_for(200):
+                continue
+        else:
+            # Perhaps the compute node belongs to a host aggregate
+            # that is attached to a shared storage pool that can
+            # service this request for disk? Let's check...
+            host_aggs = compute_node.host_aggregates
+            for host_agg in host_aggs:
+                ha_resources = host_agg.get_inventories()
+                if resources.DISK in ha_resources:
+                    if not ha_resources[resources.DISK].has_room_for(200):
+                        continue
+                else:
+                    continue
+        # This compute node has all the resources we need in our
+        # request... let's claim the resources for this instance!
+        scheduler.claim_for_instance(instance, compute_node)
+
+To "claim" the set of resources on this compute node for the requested
+instance, we would insert a record, one for each resource type, into the
+`allocations` table.  Assume that `$CN_ID` below is the ID of the compute node
+in the `resource_providers` table. Assume that `$RP_ID` is the ID of the shared
+storage pool that was attached to the host aggregate that this compute node
+belonged to. Assume that `$INST_ID` is the UUID of the launching instance.::
+
+    INSERT INTO allocations
+    (provider_id, consumer_id, resource_class, allocated, used) VALUES
+    ($CN_ID, '$INST_ID', 1, 4, 4),
+    ($CN_ID, '$INST_ID', 1, 16, 16),
+    ($RP_ID, '$INST_ID', 1, 200, 200);
+
+Alternatives
+------------
+
+We can continue to report incorrect usage information for shared storage pools.
+
+Data model impact
+-----------------
+
+Changes which require modifications to the data model often have a wider impact
+on the system.  The community often has strong opinions on how the data model
+should be evolved, from both a functional and performance perspective. It is
+therefore important to capture and gain agreement as early as possible on any
+proposed changes to the data model.
+
+Questions which need to be addressed by this section include:
+
+* What new data objects and/or database schema changes is this going to
+  require?
+
+* What database migrations will accompany this change.
+
+* How will the initial set of new data objects be generated, for example if you
+  need to take into account existing instances, or modify other existing data
+  describe how that will work.
+
+REST API impact
+---------------
+
+Each API method which is either added or changed should have the following
+
+* Specification for the method
+
+  * A description of what the method does suitable for use in
+    user documentation
+
+  * Method type (POST/PUT/GET/DELETE)
+
+  * Normal http response code(s)
+
+  * Expected error http response code(s)
+
+    * A description for each possible error code should be included
+      describing semantic errors which can cause it such as
+      inconsistent parameters supplied to the method, or when an
+      instance is not in an appropriate state for the request to
+      succeed. Errors caused by syntactic problems covered by the JSON
+      schema definition do not need to be included.
+
+  * URL for the resource
+
+  * Parameters which can be passed via the url
+
+  * JSON schema definition for the body data if allowed
+
+  * JSON schema definition for the response data if any
+
+* Example use case including typical API samples for both data supplied
+  by the caller and the response
+
+* Discuss any policy changes, and discuss what things a deployer needs to
+  think about when defining their policy.
+
+Example JSON schema definitions can be found in the Nova tree
+http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute/schemas/v3
+
+Note that the schema should be defined as restrictively as
+possible. Parameters which are required should be marked as such and
+only under exceptional circumstances should additional parameters
+which are not defined in the schema be permitted (eg
+additionaProperties should be False).
+
+Reuse of existing predefined parameter types such as regexps for
+passwords and user defined names is highly encouraged.
+
+Security impact
+---------------
+
+Describe any potential security impact on the system.  Some of the items to
+consider include:
+
+* Does this change touch sensitive data such as tokens, keys, or user data?
+
+* Does this change alter the API in a way that may impact security, such as
+  a new way to access sensitive information or a new way to login?
+
+* Does this change involve cryptography or hashing?
+
+* Does this change require the use of sudo or any elevated privileges?
+
+* Does this change involve using or parsing user-provided data? This could
+  be directly at the API level or indirectly such as changes to a cache layer.
+
+* Can this change enable a resource exhaustion attack, such as allowing a
+  single API interaction to consume significant server resources? Some examples
+  of this include launching subprocesses for each connection, or entity
+  expansion attacks in XML.
+
+For more detailed guidance, please see the OpenStack Security Guidelines as
+a reference (https://wiki.openstack.org/wiki/Security/Guidelines).  These
+guidelines are a work in progress and are designed to help you identify
+security best practices.  For further information, feel free to reach out
+to the OpenStack Security Group at openstack-security@lists.openstack.org.
+
+Notifications impact
+--------------------
+
+Please specify any changes to notifications. Be that an extra notification,
+changes to an existing notification, or removing a notification.
+
+Other end user impact
+---------------------
+
+Aside from the API, are there other ways a user will interact with this
+feature?
+
+* Does this change have an impact on python-novaclient? What does the user
+  interface there look like?
+
+Performance Impact
+------------------
+
+Describe any potential performance impact on the system, for example
+how often will new code be called, and is there a major change to the calling
+pattern of existing code.
+
+Examples of things to consider here include:
+
+* A periodic task might look like a small addition but if it calls conductor or
+  another service the load is multiplied by the number of nodes in the system.
+
+* Scheduler filters get called once per host for every instance being created,
+  so any latency they introduce is linear with the size of the system.
+
+* A small change in a utility function or a commonly used decorator can have a
+  large impacts on performance.
+
+* Calls which result in a database queries (whether direct or via conductor)
+  can have a profound impact on performance when called in critical sections of
+  the code.
+
+* Will the change include any locking, and if so what considerations are there
+  on holding the lock?
+
+Other deployer impact
+---------------------
+
+Discuss things that will affect how you deploy and configure OpenStack
+that have not already been mentioned, such as:
+
+* What config options are being added? Should they be more generic than
+  proposed (for example a flag that other hypervisor drivers might want to
+  implement as well)? Are the default values ones which will work well in
+  real deployments?
+
+* Is this a change that takes immediate effect after its merged, or is it
+  something that has to be explicitly enabled?
+
+* If this change is a new binary, how would it be deployed?
+
+* Please state anything that those doing continuous deployment, or those
+  upgrading from the previous release, need to be aware of. Also describe
+  any plans to deprecate configuration values or features.  For example, if we
+  change the directory name that instances are stored in, how do we handle
+  instance directories created before the change landed?  Do we move them?  Do
+  we have a special case in the code? Do we assume that the operator will
+  recreate all the instances in their cloud?
+
+Developer impact
+----------------
+
+Discuss things that will affect other developers working on OpenStack,
+such as:
+
+* If the blueprint proposes a change to the driver API, discussion of how
+  other hypervisors would implement the feature is required.
+
+
+Implementation
+==============
+
+Assignee(s)
+-----------
+
+Who is leading the writing of the code? Or is this a blueprint where you're
+throwing it out there to see who picks it up?
+
+If more than one person is working on the implementation, please designate the
+primary author and contact.
+
+Primary assignee:
+  <launchpad-id or None>
+
+Other contributors:
+  <launchpad-id or None>
+
+Work Items
+----------
+
+Work items or tasks -- break the feature up into the things that need to be
+done to implement it. Those parts might end up being done by different people,
+but we're mostly trying to understand the timeline for implementation.
+
+
+Dependencies
+============
+
+* Include specific references to specs and/or blueprints in nova, or in other
+  projects, that this one either depends on or is related to.
+
+* If this requires functionality of another project that is not currently used
+  by Nova (such as the glance v2 API when we previously only required v1),
+  document that fact.
+
+* Does this feature require any new library dependencies or code otherwise not
+  included in OpenStack? Or does it depend on a specific version of library?
+
+
+Testing
+=======
+
+Please discuss the important scenarios needed to test here, as well as
+specific edge cases we should be ensuring work correctly. For each
+scenario please specify if this requires specialized hardware, a full
+openstack environment, or can be simulated inside the Nova tree.
+
+Please discuss how the change will be tested. We especially want to know what
+tempest tests will be added. It is assumed that unit test coverage will be
+added so that doesn't need to be mentioned explicitly, but discussion of why
+you think unit tests are sufficient and we don't need to add more tempest
+tests would need to be included.
+
+Is this untestable in gate given current limitations (specific hardware /
+software configurations available)? If so, are there mitigation plans (3rd
+party testing, gate enhancements, etc).
+
+
+Documentation Impact
+====================
+
+What is the impact on the docs team of this change? Some changes might require
+donating resources to the docs team to have the documentation updated. Don't
+repeat details discussed above, but please reference them here.
+
+
+References
+==========
+
+[1] Bugs related to resource usage reporting and calculation:
+
+* Hypervisor summary shows incorrect total storage (Ceph)
+  https://bugs.launchpad.net/nova/+bug/1387812
+* rbd backend reports wrong 'local_gb_used' for compute node
+  https://bugs.launchpad.net/nova/+bug/1493760
+* nova hypervisor-stats shows wrong disk usage with shared storage
+  https://bugs.launchpad.net/nova/+bug/1414432
+* report disk consumption incorrect in nova-compute
+  https://bugs.launchpad.net/nova/+bug/1315988
+* VMWare: available disk spaces(hypervisor-list) only based on a single
+  datastore instead of all available datastores from cluster
+  https://bugs.launchpad.net/nova/+bug/1347039
+
+History
+=======
+
+Optional section for Mitaka intended to be used each time the spec
+is updated to describe new design, API or any database schema
+updated. Useful to let reader understand what's happened along the
+time.
+
+.. list-table:: Revisions
+   :header-rows: 1
+
+   * - Release Name
+     - Description
+   * - Mitaka
+     - Introduced
-- 
2.1.0

